Id,ParentId,OwnerUserId,CreationDate,Body
49450759,49450592,,2018-03-23T13:28:30,"<p>I would change the message pattern so that you have three messages;</p>&#xA;&#xA;<ol>&#xA;<li><p>First message is published containing order information. The outcome of this message being consumed is a second message being published containing additional information.</p></li>&#xA;<li><p>Second message containing the additional information is processed and the information persisted. This is where you publish the third message.</p></li>&#xA;<li><p>The third and final message acts as a notification to say ""hey, I <em>now</em> have everything needed to actually send the email"".</p></li>&#xA;</ol>&#xA;&#xA;<p>This way you don't have to check a database every x seconds and you can extended the message workflow rather cheaply.</p>&#xA;&#xA;<p>Hope that helps :)</p>&#xA;"
46502806,46471625,,2017-09-30T12:41:10,"<p>You can define the vendor dir to be a shares folder into your composer.json</p>&#xA;&#xA;<pre><code>{&#xA;    ""config"": {&#xA;        ""vendor-dir"": ""/usr/share/php/composer/vendor""&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Can it help you ?</p>&#xA;"
51916127,51916102,,2018-08-19T09:04:48,"<p>Short answer: <strong>No</strong>, these are not the same and not subsets.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Both have different components/services publishing or subscribing to eventBus/messagingQueues and performing the tasks associated with the published event. </p>&#xA;</blockquote>&#xA;&#xA;<p>This is wrong. Microservices are not necessary about events and publishing/subscribing.</p>&#xA;"
50035567,50035311,,2018-04-26T05:39:58,"<p>In my opinion, Sagas are use for the transactions, for example among microservices, to maintain consistency between them. </p>&#xA;&#xA;<blockquote>&#xA;  <p>You have applied the Database per Service pattern. Each service has&#xA;  its own database. Some business transactions, however, span multiple&#xA;  service so you need a mechanism to ensure data consistency across&#xA;  services. For example, lets imagine that you are building an&#xA;  e-commerce store where customers have a credit limit. The application&#xA;  must ensure that a new order will not exceed the customer’s credit&#xA;  limit. Since Orders and Customers are in different databases the&#xA;  application cannot simply use a local ACID transaction.&#xA;  Reference: <a href=""http://microservices.io/patterns/data/saga.html"" rel=""nofollow noreferrer"">http://microservices.io/patterns/data/saga.html</a></p>&#xA;</blockquote>&#xA;"
50056503,35289988,,2018-04-27T06:27:58,"<p>As far as I know, OAuth2 will only give you authorization. But I have seen some solution combining OAuth2 with another protocol / Technology, e.g. Oauth2 + OpenID Connect (OIDC).</p>&#xA;&#xA;<p>To answer to you question, my experience says that it's better to separate authentication from authorisation, as I might be different Services (or Providers). You could use Google or Github for the OAuth2, or AWS Cognito for Auth2+OpenIDConnect. Another known Provider with These two common Technologies (OAuth2+OIDC) is Okta.</p>&#xA;"
50538436,50430808,,2018-05-26T00:59:14,"<p>After investigation I chose AWS lambdas.  My application is a event management application and I want event driven architecture. In general question should have been container vs lambdas and I found a good <a href=""https://blog.bluesoftglobal.com/microservices-on-aws/"" rel=""nofollow noreferrer"">comparison</a>.  Starting with Spring is easy because it is so familiar to one with Java background.  It is good to write micro services with business logic and exposing as HTTP REST APIs.  I was reading domain driven design and it is good to decide micro services size based on boundaries (bounded contexts). </p>&#xA;&#xA;<p>But I also see going with Spring boot I will have to manage authentication (for an event management application we need social media login support), authorization, users, roles, policies, and also finally integrate microservices with API Gateway. AWS Serverless authentication and authorization is very <a href=""https://www.youtube.com/watch?v=VZqG7HjT2AQ&amp;feature=youtu.be"" rel=""nofollow noreferrer"">sophisticated</a>. Using NoSQL Database such as DynamoDB follows some of the microservices guidelines such as CQRS.  With Lambdas, the boundaries will be too small but in my use case AWS Lambdas suits better. </p>&#xA;"
50432324,29434226,,2018-05-20T06:38:30,"<p>If all microservices share the same database then it is not possible to move microservices to another host, it  breaks the principle of minimal data sharing, it increases dependency, and so on.<br>&#xA;Centralised data storage is operationally convenient but microservices should wish to embed all its dependencies in order to achieve independently deployable. Alternate approach is: each microservice embedded all its dependencies including database so moving this microservice to anywhere would be trivial, which is beautiful but not practical in current context.  </p>&#xA;&#xA;<p>Microservices should own its tables because sharing tables between Microservices will kill mobility; but, sharing database cluster installation is absolutely ok (Nadareishvili, Mitra, McLarty, &amp; Amundsen, 2016).  I found this approach to be best: Microservices owning independent schema within same database if database is Oracle or SQL Server.  If database is MySQL then micro services should own independent databases since I believe MySQL only supports one schema per database.  </p>&#xA;&#xA;<p>Microservices can own not only different database but also different type of database such as in my current application we are using MySQL for online transaction processing but using NoSQL for offline processing. </p>&#xA;&#xA;<p>Using Spring Boot and Spring Data JPA I don't face any issues with generating entities in each microservice. I am still duplicating entity code in microservices but it is only for the required entities.  We can always create a common jar and share between microservices but it is not suitable in all situations. &#xA;Similar discussion is in this <a href=""https://stackoverflow.com/questions/47647560/how-to-share-entity-between-rest-service-between-two-microservices"">question</a></p>&#xA;"
49485328,49469599,,2018-03-26T06:37:06,"<p>Let's consider a request that travels from the authenticated user to service A and on to service B. The JWT should be passed on each of these calls.</p>&#xA;&#xA;<p>Both services would first <strong>authenticate</strong> the user which is simply done by validating the JWT. Then each service would extract all information necessary to <strong>authorize</strong> the user from the user, for example the <code>sub</code> claim. It would use this information to decide if the user is authorized with respect to the operation that the service shall perform on behalf of the user. Only after successful authorization would the service actually do anything.</p>&#xA;&#xA;<p>This would not be overhead but necessary to allow service B to both authenticate and authorize the user. Not passing the JWT from sercice A to service B would make it impossible for service B to know anything about the user.</p>&#xA;"
48056373,48019661,,2018-01-02T06:16:55,"<p>I assume you mean SQLServer managed on AWS RDS by saying SQL, and it is not different from any other SQLServer instance. And your problem is not related to microservices or AWS in any way. If you access a same data source from multiple different instances a race condition will occur. There are 2 different ways to deal with this problem in SQL Server: </p>&#xA;&#xA;<ul>&#xA;<li>Pessimistic Locking</li>&#xA;<li>Optimistic Locking</li>&#xA;</ul>&#xA;&#xA;<p>You can choose one of the two options depending on your scenario. More information <a href=""https://stackoverflow.com/questions/129329/optimistic-vs-pessimistic-locking"">here</a></p>&#xA;&#xA;<p>In a <strong>pessimistic lock</strong> you lock the row when you read it via SELECT command. You should use <em>READ COMMITTED</em> isolation level or explicitly specify a <a href=""https://technet.microsoft.com/en-us/library/ms187373(v=sql.105).aspx"" rel=""nofollow noreferrer"">LOCK hint</a> in your query.</p>&#xA;&#xA;<p>For <strong>optimistic locking</strong>, you can define a column in the table, which will be changed every time you update a column: </p>&#xA;&#xA;<p><code>UPDATE RaceTable&#xA;SET UpdatedOn = @currentDate&#xA;WHERE Id = @id AND UpdatedOn = @lastUpdateDate</code></p>&#xA;&#xA;<p>More info about concurrency control in SQLServer:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://technet.microsoft.com/en-us/library/ms189132(v=sql.105).aspx"" rel=""nofollow noreferrer"">https://technet.microsoft.com/en-us/library/ms189132(v=sql.105).aspx</a></li>&#xA;<li><a href=""https://docs.microsoft.com/en-us/sql/t-sql/statements/set-transaction-isolation-level-transact-sql"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/sql/t-sql/statements/set-transaction-isolation-level-transact-sql</a></li>&#xA;<li><a href=""https://technet.microsoft.com/en-us/library/ms175519(v=sql.105).aspx"" rel=""nofollow noreferrer"">https://technet.microsoft.com/en-us/library/ms175519(v=sql.105).aspx</a></li>&#xA;</ul>&#xA;&#xA;<p>Despite all, since you referred microservices architecture, I'd suggest you create separate DB's for each one of your services and communicate your services through API calls (preferably REST). More reading on this: <a href=""https://plainoldobjects.com/2015/09/02/does-each-microservice-really-need-its-own-database-2/"" rel=""nofollow noreferrer"">https://plainoldobjects.com/2015/09/02/does-each-microservice-really-need-its-own-database-2/</a></p>&#xA;"
36762076,36744042,,2016-04-21T07:04:34,"<p>Your service should handle the pagination and not hand it off the SQL. Make these steps:</p>&#xA;&#xA;<ol>&#xA;<li>Get <em>all</em> students from S1 (SQL database) where <code>class = C</code>.</li>&#xA;<li>Using the result, get all students from S2 that are in the result <em>and</em> where <code>state = X</code>.</li>&#xA;<li>Sort the second result in a stable way.</li>&#xA;<li>Get the requested page you want from the sorted result.</li>&#xA;</ol>&#xA;&#xA;<p>All this is done in the code that calls <em>both S1 and S2</em>. Only it has the knowledge to build the pages.</p>&#xA;"
49070369,49069281,4261120,2018-03-02T13:45:22,"<p>Your architectural proposal is moving in the right direction.</p>&#xA;&#xA;<ul>&#xA;<li><strong>Sidekiq OSS</strong>: I truly belive that sidekiq OSS version will properly fit your needs. It has several features to support background jobs supported by Redis. I recommend that you to read <a href=""https://github.com/mperham/sidekiq/wiki"" rel=""nofollow noreferrer"">its documentation</a> to understand best practices and what should be avoided.</li>&#xA;<li><strong>Microservices</strong> is a <a href=""http://microservices.io/articles/scalecube.html"" rel=""nofollow noreferrer"">good architectural choice to achieve scalability</a> as it promotes functional decomposition and you can make specific decisions on how to scale each service individually. However, it will increase the complexity of your system in several ways, requiring you to highly automate your infrastructure. I suggest you to <a href=""https://martinfowler.com/articles/microservice-trade-offs.html"" rel=""nofollow noreferrer"">read this arcitle</a> from Martin Fowler webpage. He discusses the trade-offs related to the adoption of micro-services.</li>&#xA;<li>The advantanges of using <strong>RabbitMQ</strong> are to enable reliable asynchronous communication between multiple services, support scalability, and provide flexible means for communication such as pub/sub and RPC. If you keep the idea of microservices this is the right way to go. Otherwise, a monolithic Rails application will only require the use of the sidekiq.</li>&#xA;<li>If you use RabbitMQ, you should use a proper <a href=""https://www.rabbitmq.com/devtools.html"" rel=""nofollow noreferrer"">RabbitMQ client</a>. For ruby, use <a href=""https://github.com/ruby-amqp/bunny"" rel=""nofollow noreferrer"">Bunny</a> in both the publisher and the consumer services since it will provide a good API to <a href=""http://rubybunny.info/articles/exchanges.html"" rel=""nofollow noreferrer"">publish data on RabbitMQ</a> and will also <a href=""http://rubybunny.info/articles/queues.html"" rel=""nofollow noreferrer"">support background processing for receiving data</a>. Thus, there is no need to use sidekiq to consume data in Fetcher service.</li>&#xA;<li>As you are going to do asynchronous processing, your clients will not know the final result of their request with your HTTP responses. So it would be interesting to use a mechanism that allows your application to notify them when your app finish processing the tasks asynchronously (i.e., through a status update on the user page). For this goal you could use <strong>Websockets</strong>, which is already supported in Rails through Action Cable.</li>&#xA;</ul>&#xA;&#xA;<p>My final tip is: if you're still prototyping your application and experimenting with ideas, I suggest you follow the path of monolithic architecture and be more concerned with validating your proposal. But if you already have a well-structured proposal and know the expected demand for the application, the microservice architecture can be a good choice to develop a scalable and reliable system.</p>&#xA;"
39436977,39414913,526592,2016-09-11T14:14:33,"<p>After reading <a href=""http://microservices.io/patterns/data/database-per-service.html"" rel=""nofollow"">http://microservices.io/patterns/data/database-per-service.html</a> stating:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Implementing queries that join data that is now in multiple databases is challenging. There are various solutions:</p>&#xA;  &#xA;  <p>Application-side joins - the application performs the join rather than the database. For example, a service (or the API gateway) could retrieve a customer and their orders by first retrieving the customer from the customer service and then querying the order service to return the customer’s most recent orders.</p>&#xA;  &#xA;  <p>Command Query Responsibility Segregation (CQRS) - maintain one or more materialized views that contain data from multiple services. The views are kept by services that subscribe to events that each services publishes when it updates its data. For example, the online store could implement a query that finds customers in a particular region and their recent orders by maintaining a view that joins customers and orders. The view is updated by a service that subscribes to customer and order events.</p>&#xA;</blockquote>&#xA;&#xA;<p>I think that the CQRS approach is the most performing solution in terms of serving request/sec. This is because the User-Service could contain a materialised view of exactly that data that needs to be returned in the List queries. The materialised view is updated with the contents of the Events coming from the DiscussionGroup-Service.</p>&#xA;"
51017403,51017164,4786944,2018-06-25T06:24:33,"<ol>&#xA;<li><p>your import path must be valid in <code>$GOPATH/libPath</code> syntax</p></li>&#xA;<li><p>for that you can share your services source code as much as you want and just seprate bin by using multiple main package</p></li>&#xA;<li><p>if you mean directory by second files, there is no best practice for that, as long as it doesnt concern your code complexity, you can do that too</p></li>&#xA;</ol>&#xA;"
47182077,47181503,1707520,2017-11-08T14:19:03,"<p>You are not throwing an exception anywhere, hence catch block is not getting executed. Here is updated code.</p>&#xA;&#xA;<pre>&#xA;<code>&#xA;    @DeleteMapping(""{id}"")&#xA;    fun delete(@PathVariable id: Long): ResponseEntity {&#xA;        try {&#xA;            if (dogRepository.exists(id)) {&#xA;                dogRepository.delete(id)&#xA;                return ResponseEntity.ok().build()&#xA;            }&#xA;            return ResponseEntity.notFound().build()&#xA;        } catch (e: Exception) {&#xA;            return ResponseEntity.notFound().build()&#xA;        }&#xA;    }&#xA;</code>&#xA;</pre>&#xA;&#xA;<p>You can check the response header via curl . E.g.&#xA;<code>curl -v -X  DELETE http://YOUR_API_URL</code></p>&#xA;"
49038200,49036468,2180096,2018-02-28T20:43:11,"<p>One core principle of the microservice architecture is&#xA;defining clear boundaries and responsibilities of each microservice.</p>&#xA;&#xA;<p>I can say that it's the same <em>Single Responsibility Principle</em> from SOLID, but on macro level.&#xA;Сonsidering this principle we get:</p>&#xA;&#xA;<ul>&#xA;<li>Users service is responsible for user management/operations</li>&#xA;<li>Devices service is responsible for operations with devices</li>&#xA;</ul>&#xA;&#xA;<p>You question is </p>&#xA;&#xA;<blockquote>&#xA;  <p>..proper way of asking the server to get all user devices</p>&#xA;</blockquote>&#xA;&#xA;<p>It's 100% responsibility of the Devices service and Users service nothing know about devices.</p>&#xA;&#xA;<p>As I can see you thinking only in routing terms (yes API consistency is also important).</p>&#xA;&#xA;<p>From one side the better and more logical URL is <code>/api/users/{userId}/devices</code>&#xA;- you try to get user's devices, these devices belong to user.</p>&#xA;&#xA;<p>From other side you can use the routes like <code>/api/devices/user/{userId}</code> (<code>/api/devices/{deviceId}</code>) and that can be more easily processed &#xA;by the routing system to send a request to the Devices service.</p>&#xA;&#xA;<p>Taking into account other constraints you can choose the option that is right for your design.</p>&#xA;&#xA;<p>And also small addition to:</p>&#xA;&#xA;<blockquote>&#xA;  <p>needs another HTTP request to communicate with Device service.</p>&#xA;</blockquote>&#xA;&#xA;<p>in the architecture of your solution you can create an additional special and separate component that routes the requests to the desired microservice, not only direct calls are possible from one microservice to another.</p>&#xA;"
49098255,37213471,2180096,2018-03-04T17:33:08,"<p>You should clearly <strong>distinguish sub-domain areas (bounded contexts)</strong> from you domain.</p>&#xA;&#xA;<p>Usually (if everything is fine with your architecture) you already have some separate components in your monolith application which responsible for each sub-domain. These components interact with each other in one process &#xA;(in monolith application) and you should to think about how to put them into separate processes. Of course you need to produce a lot of refactoring when moving one by one parts of the monolith to microservices.</p>&#xA;&#xA;<p>Always remember that every microservice is responsible for some sub-domain.</p>&#xA;&#xA;<p>I strongly recommend you to learn <strong>Domain Driven Design</strong>.</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://books.google.ru/books?isbn=032112521"" rel=""nofollow noreferrer"" title=""Domain-Driven Design: Tackling Complexity in the Heart of Software by Eric Evans"">Domain-Driven Design: Tackling Complexity in the Heart of Software by Eric Evans</a></li>&#xA;<li><a href=""https://books.google.ru/books?isbn=0133039889"" rel=""nofollow noreferrer"" title=""Implementing Domain-Driven Design by Vaughn Vernon"">Implementing Domain-Driven Design by Vaughn Vernon</a></li>&#xA;</ul>&#xA;&#xA;<p>Also learn <a href=""https://martinfowler.com/bliki/CQRS.html"" rel=""nofollow noreferrer"" title=""CQRS pattern"">CQRS pattern</a></p>&#xA;&#xA;<p>At the beginning you also should decide how your micservices will interact with each other.&#xA;There are several options:</p>&#xA;&#xA;<ul>&#xA;<li>Direct calls from one service to another</li>&#xA;<li>Send messages through some <strong>dispatcher service</strong> &#xA;which abstracts the client service from the knowledge where the called (destination) services are located. &#xA;This approach is similar to how proxy server like NGINX works.</li>&#xA;<li>Interact through some <strong>messaging bus (middleware)</strong>, like <a href=""https://www.rabbitmq.com/"" rel=""nofollow noreferrer"" title=""RabbitMQ"">RabbitMQ</a></li>&#xA;</ul>&#xA;&#xA;<p>You can combine these options, for example <em>Query</em> requests can be processed through Dispatcher Service, <em>Commands and Events</em> through message bus.</p>&#xA;&#xA;<p>From my experience the biggest problem will be to go away from a single database, &#xA;which monolith applications is usually used.</p>&#xA;&#xA;<p>In addition some good practices:</p>&#xA;&#xA;<ul>&#xA;<li>Put each microservice in own repository - this isolates from the ability to directly use the code of one micro service in another.&#xA;You also get faster checkouts and builds of each microservice on CI.</li>&#xA;<li>Interactions with any service should occur only through its <em>public contracts</em>.</li>&#xA;<li>It is necessary to aspire that each microservice has its own database</li>&#xA;</ul>&#xA;&#xA;<p><strong>Example of the sub-domains (bounded contexts) for some Tourism Industry application.</strong>&#xA;Each bounded context can be serviced by a microservice.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/HCcIP.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HCcIP.jpg"" alt=""Application sub-domains""></a></p>&#xA;"
40378111,40377377,3955200,2016-11-02T10:51:00,"<p>There is no standard for communication or transport mechanisms for microservices. In general, microservices communicate with each other using widely adopted lightweight protocols, such as HTTP and REST, or messaging protocols, such as JMS or AMQP. In specific cases, one might choose more optimized communication protocols, such as Thrift, ZeroMQ, Protocol Buffers, or Avro.</p>&#xA;&#xA;<p>Communication between microservices can be designed either in synchronous (request-response) or asynchronous (fire and forget) styles. Both approaches have their own merits and constraints. It is not possible to develop a system with just one approach. A combination of both approaches is required based on the use cases.</p>&#xA;&#xA;<p>You should choose ones which suits best to your projects depending on your use cases and requirements.</p>&#xA;"
50679141,50647694,620288,2018-06-04T11:05:59,"<p>On Kubernetes you don't need to, you could use those features from the Kubernetes platform.</p>&#xA;&#xA;<p>If you want to integrate closer with spring cloud you can have a look at: <a href=""https://github.com/spring-cloud-incubator/spring-cloud-kubernetes"" rel=""nofollow noreferrer"">https://github.com/spring-cloud-incubator/spring-cloud-kubernetes</a></p>&#xA;&#xA;<p>Most of the requested features can be provided by Kubernetes Services. These get exposed as environment variables to the pods or you could use Kube DNS to find the needed services. </p>&#xA;"
44410238,44395806,620288,2017-06-07T10:34:45,"<p>Typically you would build a service with rest endpoints (Spring Boot) to ingest the data. This service can then be deployed multiple times behind a api gateway (Zuul, Spring Cloud) that takes care about routing. This is the default spring cloud microservices setup. The ingest service can then convert the data and produce it to a RabbitMQ or Kafka. I recommend using Spring Cloud Stream for the interaction with the queue, it's abstraction on top of RabbitMQ and Kafka, which can be configured using starters/binders. </p>&#xA;&#xA;<p>Spring Cloud Dataflow is a declarative approach for orchestration of your queues and also takes care of deployment on several cloud services/platforms. This can also be used but might add extra complexity to your use case.</p>&#xA;"
42436024,42435307,620288,2017-02-24T10:15:59,"<p>Kubernetes and Docker Swarm are container orchestration tools.&#xA;Spring Cloud is a collection of tools to build microservices/streaming architectures.&#xA;There is a bit of overlap, like service discovery, gateway or configuration services. But you could use Spring Cloud without containers and deploy the jars yourself without needing Kuberentes or Swarm.</p>&#xA;&#xA;<p>So you'll have to choose between Kubernetes and Swarm for the orchestration of your containers, if you'll use containers.</p>&#xA;&#xA;<p>Comparison: <a href=""https://dzone.com/articles/deploying-microservices-spring-cloud-vs-kubernetes"" rel=""nofollow noreferrer"">https://dzone.com/articles/deploying-microservices-spring-cloud-vs-kubernetes</a></p>&#xA;"
47236471,47228231,620288,2017-11-11T09:40:57,"<p>There is an easy way to do this with Spring Cloud Netflix Sidecar: <a href=""http://cloud.spring.io/spring-cloud-static/Camden.SR7/#_polyglot_support_with_sidecar"" rel=""nofollow noreferrer"">http://cloud.spring.io/spring-cloud-static/Camden.SR7/#_polyglot_support_with_sidecar</a></p>&#xA;&#xA;<p>If you want to continue to implementing this yourself you have several options. With client side load balancing you could retrieve all instances from Eureka and then choose one randomly at the consuming side. If you want server side load balancing you will be needing an extra component, like Zuul, and let it do the load balancing and routing. Zuul uses the eureka configuration so it is easy to integrate it.</p>&#xA;"
40436971,39088230,1997056,2016-11-05T10:09:32,"<p>In your <code>version 2.0</code> the contract of the <code>updateSummary()</code> method has changed (ie, now it allows to save and remove summary values): </p>&#xA;&#xA;<p><strong>Option 1:</strong>&#xA;Instead of changing its behaviour, create a new method (ie, <code>updateSummaryV2()</code>) and start using it across the latest version of your clients, while deprecating the old one.</p>&#xA;&#xA;<p>This way, older versions of the client still use the normal <code>updateSummary()</code> without conflicting with new method's contract and assumptions.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p><strong>Option 2:</strong>&#xA;Add an optional field that contains the api version and has a default value set to the latest API version:</p>&#xA;&#xA;<pre><code>struct Summary {&#xA;    1: required string summaryId,&#xA;    2: required i32 summaryCost,&#xA;    3: optional list&lt;i32&gt; summaryValues&#xA;    4: optional i32 apiVersion = 2&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>This way, if <code>apiVersion</code> is not set, you know the request came from an old client, and for future versions, you will known the client version and can react accordingly.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>Alternatively, you could only remove the records if an empty list is provided and do nothing if the list is not set, to respect the previous method contract.</p>&#xA;&#xA;<p><sub>As a side note: having an action depend on something implicit (here, the absence of a list) can be risky in general, even without considering the cross-compatibility issue. It's generally safer (and easier to work with and maintain) when such actions depend on an explicit flag.</sub></p>&#xA;"
34711034,34230517,2587904,2016-01-10T21:32:14,"<h3>Dependencies must be explicitly imported as such:</h3>&#xA;&#xA;<pre><code>lazy val backend = (project in file(""."")).aggregate(api)&#xA;&#xA;lazy val api = project.in(file(""modules/api""))&#xA;  .settings(libraryDependencies ++= backendCommon)&#xA;&#xA;lazy val backendCommon = Seq(&#xA;  ""com.typesafe.akka"" %% ""akka-actor"" % ""2.3.11"",&#xA;  ""com.typesafe.akka"" %% ""akka-testkit"" % ""2.3.11"" % ""test"",&#xA;  ""org.scalatest"" %% ""scalatest"" % ""2.2.4"" % ""test"")&#xA;</code></pre>&#xA;"
45539661,45538292,4950528,2017-08-07T04:57:17,"<p>Kafka has a feature called Consumer Groups that does exactly what you describe.</p>&#xA;&#xA;<p>Every identical instance of B can declare its group.id to be the same string (say ""serviceB"") and Kafka will ensure that each instance gets assigned a mutually exclusive set of topic partitions for all the topics it subscribes to.</p>&#xA;&#xA;<p>Since all instances of C will have a different group.id (say ""serviceC"") then they will also get the same messages as the instances of B but they will be in an independent Consumer Group so messages go only to 1 of N instances of C, up to maximum number of instances which is the total number of topic partitions.</p>&#xA;&#xA;<p>You can dynamically and independently scale up or down the number of instances of B and C. If any instance dies, the remaining instances will automatically rebalance their assigned topic partitions and take over processing of the messages for the instance that died.</p>&#xA;&#xA;<p>Data never has to be stored more than once so there is still one single commit log or ""source of truth"" for all these service instances.</p>&#xA;"
44990549,44980176,4950528,2017-07-08T20:38:25,"<p>If each individual event in these xml files has a timestamp then you can think of the output to Kafka as just a steam of late arriving events. Kafka allows you to set the event time on these messages to be the timestamp of the event rather than the time it was ingested to Kafka. In that way, any downstream processing apps like Kafka Streams can put the event into the right temporal context and aggregate into the proper time windows or session windows or even join with other realtime inputs</p>&#xA;"
45237997,45236389,4950528,2017-07-21T12:41:24,<p>Kafka brokers do not push data to clients. Rather clients poll() and pull data from the brokers. As long as the consumers can connect to the bootstrap servers and you set the Kafka brokers to advertise an IP and port that the clients can connect to and poll() then it will all work fine.</p>&#xA;
50324856,50324219,9762816,2018-05-14T07:14:20,"<p>I am suggesting PHP. Because it is open source and reliable.</p>&#xA;&#xA;<p>Please prefer this source as one example.&#xA;<a href=""https://pilsniak.com/cqrs-es-php-prooph/"" rel=""nofollow noreferrer"">https://pilsniak.com/cqrs-es-php-prooph/</a>.</p>&#xA;"
42641917,42641804,3801345,2017-03-07T06:46:40,"<p>You need at least SSL for A-B and then only you can judge if basic auth works or not.</p>&#xA;&#xA;<p>If C is going to use the same APIs as B then it makes sense to use the same authentication methods, just for simplicity, IMO.</p>&#xA;&#xA;<p>You could also use a token based auth mechanism where each service (remote or local) authenticates and gets a token and uses that for subsequent communication.</p>&#xA;&#xA;<p>See the following for more:</p>&#xA;&#xA;<p><a href=""https://stackoverflow.com/questions/9773664/rest-api-token-based-authentication?rq=1"">REST API Token based authentication</a></p>&#xA;"
41351988,40495213,5704961,2016-12-27T20:43:52,"<p>You shouldn't necessarily think of a Service Fabric service as being a microservice.</p>&#xA;&#xA;<p>The Service Fabric taxonomy of code/services/apps etc. gives you a high flexibility in how you compose to your needs (as already pointed out). Consider the fact that you can have more code packages running in one service and trying to translate that in to a microservice definition, just makes things even harder to cope with.</p>&#xA;&#xA;<p>As the SF Appation is you unit of deployment (whether is contains one or more updated services), you should strive to structure your repo/solution/SF Application setup in a way so you can contain most changes to one SF App (= one solution and one repo).</p>&#xA;&#xA;<p>If you get in a situation where you constantly need to deploy multiple SF Apps to get a change out, you will not be productive.</p>&#xA;"
44442100,44435017,5704961,2017-06-08T17:25:59,"<p>Depending on the communication stack you are using you have a few options.</p>&#xA;&#xA;<p>If HTTP based web API - you can use the reverse proxy and dynamic port assignment for the services: <a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-reverseproxy"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-reverseproxy</a></p>&#xA;&#xA;<p>If using Service Remoting - just use dynamic ports.</p>&#xA;&#xA;<p>For external communication to the cluster - taka look here: <a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-reverseproxy#special-handling-for-port-sharing-services"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-reverseproxy#special-handling-for-port-sharing-services</a></p>&#xA;"
42964441,42519116,5704961,2017-03-22T23:17:15,"<p>Whether your containers are services in the same app or different app, you can still deploy them independently. Only changes are being applied at deployment, you don't even have to have the non-changed services in the deployment package. Look here to see an example for Service Fabric service (not in containers), but deploying containers using service manifest is conceptually the same: <a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-set-up-continuous-integration"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-set-up-continuous-integration</a></p>&#xA;"
35558052,35555602,3284225,2016-02-22T15:54:32,"<p>README.md says:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Building</p>&#xA;  &#xA;  <p>The build requires java8 because of some required libraries that are&#xA;  java8 (servo), but the source and target compatibility are still set&#xA;  to 1.7.</p>&#xA;</blockquote>&#xA;&#xA;<p>Reference:&#xA;<a href=""https://github.com/Netflix/eureka"" rel=""nofollow"">https://github.com/Netflix/eureka</a></p>&#xA;"
41003595,40457443,5582337,2016-12-06T19:47:52,"<p>Reliable Dictionary periodically removes least recently used values from memory. This is to enable </p>&#xA;&#xA;<ul>&#xA;<li>Large Reliable Dictionaries</li>&#xA;<li>Higher Density: Higher density of Reliable Collections per replica and higher density of replicas per node.</li>&#xA;</ul>&#xA;&#xA;<p>The trade-off is that, this can increase read latencies: disk IO is required to retrieve values that are not cached in-memory. </p>&#xA;&#xA;<p>There are couple of options to get lower latency on enumerations.</p>&#xA;&#xA;<p>1) <strong>Key Filtered Enumeration</strong>: You can move the fields that you would like to use in your query in to the TKey of the ReliableDictionary (NameFirst in the above example). This would allow you use the <a href=""https://docs.microsoft.com/en-us/dotnet/api/microsoft.servicefabric.data.collections.ireliabledictionary-2#Microsoft_ServiceFabric_Data_Collections_IReliableDictionary_2_CreateEnumerableAsync_Microsoft_ServiceFabric_Data_ITransaction_System_Func__0_System_Boolean__Microsoft_ServiceFabric_Data_Collections_EnumerationMode_"" rel=""noreferrer"">CreateEnumerbleAsync</a> overload that takes in a key filter. The key filter allows Reliable Dictionary to avoid retrieving values from the disk for keys that do not match your query. One limitation of this approach is that TKey (hence the fields inside it) cannot be updated.</p>&#xA;&#xA;<p>2) <strong>In-memory Secondary Index using Notifications</strong>: <a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-reliable-services-notifications"" rel=""noreferrer"">Reliable Dictionary Notifications</a> can be used to build any number of secondary indices. You could build a secondary index that keeps all of the values in-memory hence trading memory resources to provide lower read latency. Furthermore, since you have full control over the secondary index, you can keep the secondary index ordered (e.g. by reverse of NameFirst in your example).</p>&#xA;&#xA;<p>We are also considering making Reliable Dictionary's in-memory TValue sweep policy configurable. With this, you will be able to configure the Reliable Dictionary to keep all values in-memory if read latencies is a priority for you.</p>&#xA;&#xA;<p>Since in your scenario most of the time in enumeration is spent on disk IO, you can also benefit from using your <a href=""https://docs.microsoft.com/en-us/dotnet/api/microsoft.servicefabric.data.ireliablestatemanager?redirectedfrom=MSDN#Microsoft_ServiceFabric_Data_IReliableStateManager_TryAddStateSerializer__1_Microsoft_ServiceFabric_Data_IStateSerializer___0__"" rel=""noreferrer"">Custom Serializer</a> which can reduce the disk and network footprint.</p>&#xA;&#xA;<p>Thank you for your question.</p>&#xA;"
41049369,41008507,5582337,2016-12-08T21:45:46,"<p>One way to build secondary indicies is to use <a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-reliable-services-notifications"" rel=""nofollow noreferrer"">Notifications</a>. Using notifications with a reference type TKey &amp; TValue, you can maintain a secondary index without creating any copies of your TKey or TValue. </p>&#xA;&#xA;<p>If you need the secondary index to provide snapshot isolation, then the data structure chosen for the secondary index must implement Multi-Version Concurrency Control.</p>&#xA;&#xA;<p>If you do not have such a data structure to host the secondary index, another option is to keep the transaction and the enumeration live across the paged client calls. This way you can use Reliable Dictionary's built-in snapshot support to provide a paged consistent scan over the data without blocking writes. Token in this case would be the <a href=""https://docs.microsoft.com/en-us/dotnet/api/microsoft.servicefabric.data.itransaction#Microsoft_ServiceFabric_Data_ITransaction_TransactionId"" rel=""nofollow noreferrer"">TransactionId</a> allowing your service to find the relevant enumeration to <a href=""https://docs.microsoft.com/en-us/dotnet/api/microsoft.servicefabric.data.iasyncenumerator-1#Microsoft_ServiceFabric_Data_IAsyncEnumerator_1_MoveNextAsync_System_Threading_CancellationToken_"" rel=""nofollow noreferrer"">MoveNextAsync</a> on. The disadvantage of using this option is that Reliable Dictionary will not be able to trim old versions of the values that are kept visible by the potentially long running snapshot transactions.</p>&#xA;&#xA;<p>To mitigate the above disadvantage, you would probably want to throttle the number of in-flight snapshot transactions and how long a client has to complete the paged enumeration before your service disposes the enumeration and the relevant read transaction.</p>&#xA;&#xA;<p>When <a href=""https://docs.microsoft.com/en-us/dotnet/api/microsoft.servicefabric.data.collections.ireliabledictionary-2?redirectedfrom=MSDN#Microsoft_ServiceFabric_Data_Collections_IReliableDictionary_2_CreateEnumerableAsync_Microsoft_ServiceFabric_Data_ITransaction_System_Func__0_System_Boolean__Microsoft_ServiceFabric_Data_Collections_EnumerationMode_"" rel=""nofollow noreferrer"">CreateEnumerableAsync</a> with a key filter is used, Reliable Dictionary will invoke the filter for every key to see if it satisfies the custom filter. Since TKeys are always kept in-memory today, for most key filters we have not seen issues here. The most expensive part of an enumeration tends to be retrieving paged out values from disk.</p>&#xA;"
50295316,50248560,2570241,2018-05-11T15:11:17,<p>Use <code>mvn spring-boot:start</code> instead as this won't block other operations</p>&#xA;
42388406,40936597,2648577,2017-02-22T10:04:41,"<p>Try using this spring boot / cloud versions:</p>&#xA;&#xA;<pre><code>&lt;parent&gt;&#xA;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&#xA;    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;&#xA;    &lt;version&gt;1.5.1.RELEASE&lt;/version&gt;&#xA;    &lt;relativePath /&gt; &lt;!-- lookup parent from repository --&gt;&#xA;&lt;/parent&gt; &#xA;&#xA;&lt;dependencyManagement&gt;&#xA;    &lt;dependencies&gt;&#xA;        &lt;dependency&gt;&#xA;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&#xA;            &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;&#xA;            &lt;version&gt;Dalston.BUILD-SNAPSHOT&lt;/version&gt;&#xA;            &lt;type&gt;pom&lt;/type&gt;&#xA;            &lt;scope&gt;import&lt;/scope&gt;&#xA;        &lt;/dependency&gt;&#xA;    &lt;/dependencies&gt;&#xA;&lt;/dependencyManagement&gt;&#xA;</code></pre>&#xA;&#xA;<p>and maybe setting environment</p>&#xA;&#xA;<pre><code>eureka:&#xA;  environment: dev&#xA;</code></pre>&#xA;&#xA;<p>There is a full example in <a href=""https://github.com/mariocuellar1/eureka-server"" rel=""nofollow noreferrer"">this github project</a></p>&#xA;"
28119575,28112290,4488449,2015-01-23T22:12:05,"<p>On the roadmap are features to enable users to specify config properties along with the {create,bind,update} service instance operation. From the platform's perspective, these properties are arbitrary and opaque, the platform will just pass them through with the request to the broker. The broker will be responsible for validation and returning meaningful user-facing error messages. The broker author will be responsible for communicating to users supported attributes out-of-band.</p>&#xA;&#xA;<p>It sounds like you've got an app which implements the broker API endpoints and which also provides the message-endpoint-aaS. How will your app provide different messages on a single endpoint? Or will your app support an endpoint per service instance?</p>&#xA;&#xA;<p>Eg.</p>&#xA;&#xA;<p>For apps bound to service instance 001, your broker returns credentials ""url"": ""app.route/instances/001""</p>&#xA;&#xA;<p>GET /instances/001/&#xA;{&#xA;""message"": ""message001""&#xA;}</p>&#xA;&#xA;<p>For apps bound to service instance 002, your broker returns credentials ""url"": ""app.route/instances/002""</p>&#xA;&#xA;<p>GET /instances/002/&#xA;{&#xA;""message"": ""message002""&#xA;}</p>&#xA;&#xA;<p>Until we implement the feature I described above, I recommend broker authors enable users to configure the instance directly after provisioning. </p>&#xA;&#xA;<p>Your app could support this with a PUT endpoint a user could curl directly, or with a simple GUI (<a href=""http://docs.cloudfoundry.org/services/"" rel=""nofollow"">see our docs on service dashboard SSO</a>). </p>&#xA;"
42380108,36792713,491265,2017-02-21T23:44:35,<p>It looks like you can try to use router lazy loading and provide custom implementation of <code>NgModuleFactoryLoader</code> which can load components via websocket. </p>&#xA;
51170203,50350198,2211841,2018-07-04T09:23:34,"<p>Try Locked Working Copy from <a href=""https://github.com/scm4j/scm4j-vcs-api"" rel=""nofollow noreferrer"">scm4j API</a>. It can create locked folder in shared network folder which can not be used by another microservices until release. Do any checkout and other work within Working Copy. Then it will be reused if the same repository url is provided so you need to execute just switch+pull, not clone. If another microservice is currently working with the Working Copy then another available will be taken and locked or new one will be created and locked and so on..</p>&#xA;&#xA;<pre><code>public static final String WORKSPACE_DIR = System.getProperty(""java.io.tmpdir"") + ""scm4j-vcs-workspaces""; // or provide network shared folder path&#xA;public static void main(String[] args) {&#xA;    IVCSWorkspace workspace = new VCSWorkspace(WORKSPACE_DIR);&#xA;    String repoUrl = ""https://github.com/scm4j/scm4j-vcs-api"";&#xA;    IVCSRepositoryWorkspace repoWorkspace = workspace.getVCSRepositoryWorkspace(repoUrl);&#xA;    try (IVCSLockedWorkingCopy wc = repoWorkspace.getVCSLockedWorkingCopy()) {&#xA;        // wc.getFolder() is locked folder where you can do any checkouts. Another microservices can not use this folder&#xA;    }&#xA;    // here folder is unlocked and can be reused if the same repository url is provided&#xA;</code></pre>&#xA;&#xA;<p>}</p>&#xA;"
46562303,41024771,2562,2017-10-04T10:12:41,"<p>While Jeroen's answer gets close to the gist of the differences between SOA, microservices, and nanoservices, I think it goes slightly awry at the end.</p>&#xA;&#xA;<p>So SOA breaks system functionality down according to business capabilities  (e.g. Jeroen's OrderService, or perhaps a CustomerService). Services often call other services, so they very much involve the concept of a network of services with dependencies between themselves, and they also embody the concept of service composition, where some services essentially aggregate other services.</p>&#xA;&#xA;<p>Microservices are very similar and potentially align and implement SOA services, which is why people like Mark Little saw them as the same as SOA. However, they also tend to exhibit specific implementation details that were never clearly articulated in the SOA literature. For example, they're frequently scoped or sized to DDD BoundedContexts, they should use their own DB for storage, and they should publish data changes to subscribers.</p>&#xA;&#xA;<p>Nano-services then have a narrower scope than microservices, and are generally at the functional level, along the lines of what's currently being implmented in serverless architectures. You could therefore compose a microservice from nanoservices.</p>&#xA;&#xA;<p>So following the OrderService example:</p>&#xA;&#xA;<ul>&#xA;<li><p>At the top level we have the OrderService, which is the business focused service for handling orders. It is an SOA-level service, but it consists of 3 individual services.</p></li>&#xA;<li><p>OrderManagementService, which is responsible for creating and managing an order's state. It is a microservice and has it's own data store, it could also be considered an SOA service, but it has multiple endpoints (e.g. CreateOrder, CloseOrder etc) and is not a nano-service.</p></li>&#xA;<li><p>CustomerManagementService, which is responsible for handling the customer details. It is a microservice, has it's own data store, and could also be considered an SOA service, but it has multiple endpoints (e.g. RegisterCustomer, DeleteCustomer etc) and is not a nano-service.</p></li>&#xA;<li><p>OrderProcessingService, which is responsible for handling the Order workflow, orchestrating calls to various external services, such as the OrderManagementService and CustomerManagementService, and is also a microservice and an SOA service. It is not a nano-service.</p></li>&#xA;</ul>&#xA;&#xA;<p>Now to nanoservices. The team implementing the CustomerManagementService decide they need a common method to validate customer email addresses, and they implement it as a ValidateEmailAddressService, which they make available via a RESTful endpoint. This is a nanoservice, and can best be thought of a single function made available as a service. </p>&#xA;&#xA;<p>Whether or not it's a good idea to provide such functions as services is up for debate, although in the above example I think calling it an anti-pattern is understandable. However, there's an argument that says nanoservices have a place in serverless architectures, so the debate might rage on.</p>&#xA;"
49252779,49226015,7803650,2018-03-13T09:40:50,"<blockquote>&#xA;  <p>when payment service does not know the ordering service exists how its gonna subscribe to its event (for sure at least ordering system needs to have payment models)?</p>&#xA;</blockquote>&#xA;&#xA;<p>It is OK that a service knows about another service if the business purpose requires it. Just make sure you don't muddle the purposes and ensure independent development processes for either service, e.g. version your APIs correctly and have policies for obsoleting. Google goes as far as imposing internal charges for service usage between teams.</p>&#xA;&#xA;<p>It is also allowed to have code libraries that are used by both services, as long as you treat these library dependencies like you would treat 3rd party libraries.</p>&#xA;&#xA;<blockquote>&#xA;  <p>how its going to return payment and shipping data?</p>&#xA;</blockquote>&#xA;&#xA;<p>Depending on the complexity of the aggregation process you can have either a 3rd service which is responsible for the business scope that relies on payment- and shipping-services, or you have a dedicated aggregator component that will allow to merge/split requests from the front end (sometimes implemented as part of an <a href=""https://www.nginx.com/blog/microservices-api-gateways-part-1-why-an-api-gateway/"" rel=""nofollow noreferrer"">API Gateway</a>). If your aggregation logic is complex, create a dedicated service, otherwise you can use a generic <a href=""http://blog.arungupta.me/microservice-design-patterns/"" rel=""nofollow noreferrer"">aggregator</a>.</p>&#xA;"
43353325,43319806,7803650,2017-04-11T18:11:11,"<p>Checkout the Kong project, which has a nice diagram on their <a href=""https://github.com/Mashape/kong/"" rel=""nofollow noreferrer"">github page</a> showing how you want to think about an API gateway. Also you may want to consider using them for your implementation.</p>&#xA;"
43384951,43384538,7803650,2017-04-13T06:24:28,"<p>Martin's answer is good, but I want to add that because you are using a containerized application you should <em>definitely</em> deploy the database separately from your services containers. The reason being that your services can evolve (independently), and one of the biggest benefits of stateless service containers is that, if you have a cluster of them, you can update them using rolling updates without any impact on your application availability. Updates to the stateful database services are more difficult, but also expected to be less frequent (and new technologies like <a href=""https://www.cockroachlabs.com/"" rel=""nofollow noreferrer"">cockroachdb</a> are on the horizon). <a href=""https://martinfowler.com/microservices/"" rel=""nofollow noreferrer"">Good read</a>.</p>&#xA;"
43353033,43328943,7803650,2017-04-11T17:53:30,"<p>Since it is hard to get security related software right, I would recommend not implementing your own solution but using an existing open source solution. At the time of writing the <a href=""https://github.com/ory/hydra"" rel=""nofollow noreferrer"">Hydra</a> project seems most promising. Specifically look at the <a href=""https://ory.gitbooks.io/hydra/content/access-control.html"" rel=""nofollow noreferrer"">access control features</a> for your use case.</p>&#xA;"
43484270,43465704,7803650,2017-04-19T00:25:46,"<p>The comment from Paolo is correct. You are asking for <a href=""https://martinfowler.com/bliki/CanaryRelease.html"" rel=""nofollow noreferrer"">Canary release process.</a></p>&#xA;&#xA;<p>When deployed a client will have a high chance of reaching the old service and a small chance of reaching the new service. So if a call fails (due to an error in the new service), the client can repeat the call and have a high chance of success with reaching the old service.</p>&#xA;&#xA;<p>How to do this is dependent on the infrastructure you are using. </p>&#xA;&#xA;<p>For example if you were to use kubernetes clusters you would configure your front end load balancer for the service to send only a fraction of the traffic to a second cluster (or second service, if running on the same cluster) that is running the new version of the service.</p>&#xA;&#xA;<p>Another example would be if you are using a DNS based load balancing solution you will have to change the DNS policy to a weighted mode that sends a fraction of the traffic to the server(s) with the new service.</p>&#xA;"
43419184,43418403,7803650,2017-04-14T20:59:01,"<p>Depending on common libraries is not a contradiction to low coupling with microservices. You just want to make sure that the libraries you are sharing are developed for a specific concern that is independent of that of your services and that it doesn't depend on anything within your services.</p>&#xA;&#xA;<p>Think of that library like you would for a 3rd party library. For example you mentioned that you are using spring - Just the fact that you are using spring in both microservices doesn't mean they are coupled. </p>&#xA;&#xA;<p>That also means you have to version your libraries and leave it up to the consuming microservices team to decide when they are ready to change/upgrade versions. Here is a <a href=""https://docs.oracle.com/javase/tutorial/deployment/jar/packageman.html"" rel=""nofollow noreferrer"">link</a> for how to create different library versions in java. Also read this <a href=""https://docs.oracle.com/javase/tutorial/deployment/jar/manifestindex.html"" rel=""nofollow noreferrer"">link</a> with some general information about manifests.</p>&#xA;&#xA;<p>In java you can use one of two approaches for including a versioned library.</p>&#xA;&#xA;<ol>&#xA;<li>Include a version number in the namespace or interface/class name explicitly. This allows for the classloader to have a unique identification and being able to include multiple versions at the same time easily. (Probably the preferred way).</li>&#xA;<li>Use the same name and only include a single version of the jar at a time. This means you have to replace the existing jar dependency with the new one and then adopt any potential changes. Check this great <a href=""https://stackoverflow.com/questions/6105124/java-classpath-classloading-multiple-versions-of-the-same-jar-project"">answer</a>.</li>&#xA;</ol>&#xA;&#xA;<p>If your library is a communication client you want to consider supporting multiple versions concurrently at the server side (as to allow the separate microservices teams to adopt to newer versions in their own pace).</p>&#xA;"
49122045,49113488,7803650,2018-03-06T01:24:06,"<blockquote>&#xA;  <p>Currently I am decided to continue with the last mentioned approach.</p>&#xA;</blockquote>&#xA;&#xA;<p>If you want horizontal scalability (scaling for increasingly large number of client connections) for your database you may be better of with a technology that was designed to work as a scalable, distributed system. Something like <a href=""https://www.cockroachlabs.com/"" rel=""nofollow noreferrer"">CockroachDB</a> or <a href=""https://stackoverflow.com/questions/8729779/why-nosql-is-better-at-scaling-out-than-rdbms"">NoSQL</a>. Cockroachdb for example has built in data sharding and replication and allows you to grow with adding server nodes as required.</p>&#xA;&#xA;<blockquote>&#xA;  <p>when I am designing my databases as distributed, according to functionalities it may contain 5 databases</p>&#xA;</blockquote>&#xA;&#xA;<p>This sounds like you had the right general idea - split by domain functionality. Here's a link to a <a href=""https://stackoverflow.com/q/43612866/7803650"">previous answer</a> regarding general DB design with micro services.</p>&#xA;"
43165699,43142821,7803650,2017-04-02T07:22:16,"<p>It is generally recommended to keep different services in different pods or better deployments that will scale independently. The reasons are what is generally discussed as benefits of a microservices architecture. </p>&#xA;&#xA;<ul>&#xA;<li><p>A more loose coupling allowing the different services to be developed independently in their own languages/technologies, </p></li>&#xA;<li><p>be deployed and updated independently and </p></li>&#xA;<li><p>also to scale independently. </p></li>&#xA;</ul>&#xA;&#xA;<p>The exception are what is considered a ""<em>helper application</em>"" to assist a ""primary application"". Examples given in the k8s docs are data pullers, data pushers and proxies. In those cases a share file system or exchange via loopback network interface can help with critical performance use cases. A data puller can be a side-car container for an nginx container pulling a website to serve from a GIT repository for example. </p>&#xA;"
43224737,43187101,7803650,2017-04-05T07:35:15,"<p>Your example sounds like a prime use case for a streaming platform such as Apache Kafka. It is a scalable cluster itself and acts as a large queue of events (your game inputs) that are stored and made available to stream consumers (all your game servers). This has a very high performance and should be able to handle millions of inputs per second with a low latency. </p>&#xA;&#xA;<p>You should also make sure to split up your game world into broader ""zones"" as to make sure that not every server requires the data from all others always. I'm sure that no player has all other players on his screen at any point in time.</p>&#xA;&#xA;<p>Look into the <a href=""https://kafka.apache.org/uses"" rel=""nofollow noreferrer"">Kafka examples</a> </p>&#xA;&#xA;<p>And the <a href=""https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines"" rel=""nofollow noreferrer"">performance measurements</a> with comparison to traditional DBs.</p>&#xA;"
43284591,43282528,7803650,2017-04-07T18:10:36,"<p>I would not recommend building an app that has to persist data without any permanent store. Even though it is possible to store an event queue forever it is not very good for random data access. Imagine your app needs to access some user information that is stored in the middle of the queue. Since you dont have the event ID you'll have to reprocess the queue in order to find that information which will be very slow. </p>&#xA;&#xA;<p>The event queue is useful to decouple service dependencies but it is not a good permanent data storage. Typically you'll want to process the queue with service dependent consumers that transform and move the data into a format and storage as useful for the service.</p>&#xA;&#xA;<p>Also see <a href=""https://stackoverflow.com/questions/17708489/using-kafka-as-a-cqrs-eventstore-good-idea/22597637#22597637"">this answer</a></p>&#xA;"
43224459,43212533,7803650,2017-04-05T07:21:18,"<p>It is all about tradeoffs. With eventual consistency in your example it may mean that the user cannot edit for maybe a few seconds since most of the eventual consistent technologies would not take too long to replicate the data across nodes. So in this use case it is absolutely acceptable since users are pretty slow in their actions.</p>&#xA;&#xA;<p>For example :</p>&#xA;&#xA;<blockquote>&#xA;  <p>MongoDB is consistent by default: reads and writes are issued to the&#xA;  primary member of a replica set. Applications can optionally read from&#xA;  secondary replicas, where data is eventually consistent by default.</p>&#xA;</blockquote>&#xA;&#xA;<p><a href=""https://www.mongodb.com/faq#consistency"" rel=""nofollow noreferrer"">from official MongoDB FAQ</a></p>&#xA;&#xA;<p>Another alternative that is getting more popular is to use a streaming platform such as Apache Kafka where it is up to your architecture design how fast the stream consumer will process the data (for eventual consistency). Since the stream platform is very fast it is mostly only up to the speed of your stream processor to make the data available at the right place. So we are talking about milliseconds and not even seconds in most cases.</p>&#xA;"
43285176,43243883,7803650,2017-04-07T18:48:19,"<p>It is not easy to set up fault tolerance correctly. I remember for the netflix stack they implemented a dedicated module: <a href=""https://github.com/Netflix/hystrix"" rel=""nofollow noreferrer"">hystrix</a>. Maybe that will help you.</p>&#xA;"
43881693,43870576,7803650,2017-05-10T00:14:41,"<p>The pattern you are looking for is <a href=""http://microservices.io/patterns/apigateway.html"" rel=""nofollow noreferrer"">API Gateway</a>. Sometimes also called ""Edge"" or ""EdgeService"". It can be used to as a single entrypoint to your cluster and to aggregate service call results. Other use cases include central authentication and/or authorization as well as routing, monitoring and resiliency.</p>&#xA;&#xA;<p>With aggregation an API Gateway will potentially allow you to decouple your services (but of course it also depends on what your use case actually is, since you didn't give any details).</p>&#xA;&#xA;<p>Some people only route external calls through a gateway, others route also internal calls through the gateway.</p>&#xA;&#xA;<p>Here some technologies to look into:</p>&#xA;&#xA;<p><a href=""https://github.com/Netflix/zuul/wiki"" rel=""nofollow noreferrer"">Zuul</a> from the Netflix stack. You have write a filter for aggregation. See <a href=""https://github.com/Netflix/zuul/wiki/Writing-Filters"" rel=""nofollow noreferrer"">this</a> document.</p>&#xA;&#xA;<p><a href=""https://aws.amazon.com/api-gateway/"" rel=""nofollow noreferrer"">Amazon API gateway</a> - If you are running on AWS. You would typically use your own lambda service for aggregation.</p>&#xA;&#xA;<p><a href=""https://getkong.org/about/"" rel=""nofollow noreferrer"">Kong</a>. Doesn't have native aggregation support, but you can forward to a separate aggregation service that you provide.</p>&#xA;"
43927835,43896718,7803650,2017-05-12T01:12:24,"<p>It sounds like your system should support an event driven architecture. For example any action that is potentially relevant to other parts of the system should be logged in an event queue. So now whenever your graph service is ready to update, it can read the event queue to the end and update its own state. This would be a solution that is decoupled as much as possible and resilient to data loss or other potential issues.</p>&#xA;&#xA;<p>Typically people use a technology like Apache Kafka to realize a simple, lightweight, fast event queue for such purposes. It allows you to define queues, called topics, and consumers that process the topics. See <a href=""https://kafka.apache.org/"" rel=""nofollow noreferrer"">the docs</a>.</p>&#xA;"
43944652,43943033,7803650,2017-05-12T18:48:52,"<p>In my opinion using a Kafka topic for multiple services or apps to consume is the right approach as long as your services don't rely on it repeatedly. Meaning a service should read the queue once, translate the data into whatever it requires and store it by itself if required. This way the topic doesn't become a permanent data store but a rather a decoupled way to input data (as if you were to call the service directly with that <code>raw</code> data, but in a more decoupled fashion by allowing the service to read the topic whenever ready for it in whatever frequency that is required). This increases the resilience of your overall system. </p>&#xA;&#xA;<p>And there <em>is</em> a coupling, that is the <code>raw</code> data. But from my perspective it is totally OK for multiple services to understand the same data format (of the topic) - As long as its format is mostly stable. The assumption here is that this is <code>raw</code> data that each service has to transform into a form that is useful for itself. You just have to make sure the <code>raw</code> data format is versioned correctly whenever changes are necessary. And to allow services to continue to work you will have to potentially deliver multiple versions concurrently until all services support the latest version. This type of architectural style is used by many large systems and works, as long as you don't have a scenario where you need to require the <code>raw</code> data format to change very frequently in a way that makes it incompatible with your service designs. (If that were the case you'd probably need another layer of stable meta-model below that can describe the dynamic raw-data.)</p>&#xA;"
43959688,43950808,7803650,2017-05-14T01:58:08,"<p>The <code>Microservices</code> architectural style tries to allow organizations to have small teams own services independent in development and at runtime. See this <a href=""https://martinfowler.com/articles/microservices.html"" rel=""noreferrer"">read</a>. And the hardest part is to define the service boundaries in a useful way. When you discover that the way you split up your application results in requirements impacting multiple services frequently that would tell you to rethink the service boundaries. The same is true for when you feel a strong need to share entities between the services. </p>&#xA;&#xA;<p>So the general advice would be to try very hard to avoid such scenarios. However there may be cases where you cannot avoid this. Since a good architecture is often about making the right trade-offs, here some ideas.</p>&#xA;&#xA;<ol>&#xA;<li><p>Consider expressing the dependency using service interfaces (API) instead of a direct DB dependency. That would allow each service team to change their internal data schema as much as required and only worry about the interface design when it comes to dependencies. This is helpful because it is easier to add additional APIs and slowly deprecate older APIs instead of changing a DB design along with all dependent Microservices (potentially at the same time). In other words you are still able to deploy new Microservice versions independently, as long as the old APIs are still supported. This is the approach recommended by the Amazon CTO, who was pioneering a lot of the Microservices approach. Here is a recommended read of an <a href=""http://queue.acm.org/detail.cfm?id=1142065"" rel=""noreferrer"">interview in 2006</a> with him.</p></li>&#xA;<li><p>Whenever you really really cannot avoid using the same DBs and you are splitting your service boundaries in a way that multiple teams/services require the same entities, you introduce two dependencies between the Microservice team and the team that is responsible for the data scheme: a) Data Format, b) Actual Data. This is not impossible to solve, but only with some overhead in organization. And if you introduce too many of such dependencies your organization will likely be crippled and slowed down in development.</p></li>&#xA;</ol>&#xA;&#xA;<p><strong>a) Dependency on the data scheme</strong>. The entities data format cannot be modified without requiring changes in the Microservices. To decouple this you will have to version the entities data scheme <em>strictly</em> and in the database support all versions of the data that the Microservices are currently using. This would allow the Microservices teams to decide for themselves when to update their service to support the new version of the data scheme. This is not feasible with all use cases, but it works with many.</p>&#xA;&#xA;<p><strong>b) Dependency on the actual collected data.</strong> The data that has been collected and is of a known version for a Microservice is OK to use, but the issue occurs when you have some services producing a newer version of the data and another service depends on it - But was not yet upgraded to being able to read the latest version. This problem is hard to solve and in many cases suggests you did not chose the service boundaries correctly. Typically you have no choice but to roll out all services that depend on the data at the same time as upgrading the data in the database. A more wacky approach is to write different versions of the data concurrently (which works mostly when the data is not mutable).</p>&#xA;&#xA;<p>To solve both a) and b) in some other cases the dependency can be reduced by <code>hidden data duplication</code> and <code>eventual consistency</code>. Meaning each service stores its own version of the data and only modifies it whenever the requirements for that service change. The services can do so by listening to a public data flow. In such scenarios you would be using an event based architecture where you define a set of public events that can be queued up and consumed by listeners from the different services that will process the event and store whatever data out of it that is relevant for it (potentially creating data duplication). Now some other events may indicate that internally stored data has to be updated and it is each services responsibility to do so with its own copy of the data. A technology to maintain such a public event queue is <a href=""https://kafka.apache.org/"" rel=""noreferrer"">Kafka</a>.</p>&#xA;"
43789866,43785728,7803650,2017-05-04T18:03:35,"<p>It sounds like the pattern you are looking for is <a href=""http://microservices.io/patterns/apigateway.html"" rel=""nofollow noreferrer"">API Gateway</a>. Sometimes also called ""Edge"" or ""EdgeService"". It can be used to as a single entrypoint to your cluster and to aggregate service call results. Other use cases include central authentication and/or authorization as well as routing, monitoring and resiliency.</p>&#xA;&#xA;<p>Some people only route external calls through a gateway, others route also internal calls through the gateway.</p>&#xA;&#xA;<p>Here some technologies to look into:</p>&#xA;&#xA;<p><a href=""https://github.com/Netflix/zuul/wiki"" rel=""nofollow noreferrer"">Zuul</a> from the Netflix stack. You have write a filter for aggregation. See <a href=""https://github.com/Netflix/zuul/wiki/Writing-Filters"" rel=""nofollow noreferrer"">this</a> document.</p>&#xA;&#xA;<p><a href=""https://aws.amazon.com/api-gateway/"" rel=""nofollow noreferrer"">Amazon API gateway</a> - If you are running on AWS. You would typically use your own lambda service for aggregation.</p>&#xA;&#xA;<p><a href=""https://getkong.org/about/"" rel=""nofollow noreferrer"">Kong</a>. Doesn't have native aggregation support, but you can forward to a separate aggregation service that you provide.</p>&#xA;"
43927774,43926591,7803650,2017-05-12T01:02:49,"<p>Your question is a bit unclear, but if you just want to scale a service horizontally you should look into a container orchestration technology that will allow you that - For example <code>Kubernetes</code>. I recommend reading the <a href=""https://kubernetes.io/"" rel=""nofollow noreferrer"">introduction</a>.</p>&#xA;&#xA;<p>All you would need to do for adding additional service containers is to update the number of desired <code>replicas</code> in the <code>Deployment</code> configuration. For more information read <a href=""https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment"" rel=""nofollow noreferrer"">this</a>. </p>&#xA;&#xA;<p>Using kubernetes (or short k8s) you will benefit from deployment automation, self healing and service discovery as well as load balancing capabilities in addition to the horizontal scalability.</p>&#xA;&#xA;<p>There are other orchestration alternatives, too ( e.g. <a href=""https://docs.docker.com/engine/swarm/swarm-tutorial/"" rel=""nofollow noreferrer"">Docker Swarm</a>), but I would recommend to look into kubernetes first.</p>&#xA;&#xA;<p>Let me know if that solves your issue or if you have additional requirements that weren't so clear in your original question.</p>&#xA;&#xA;<p>Links for your follow up questions: </p>&#xA;&#xA;<p><a href=""https://stackoverflow.com/questions/42642170/kubernetes-how-to-run-kubectl-commands-inside-a-container"">1 - Run kubectl commands inside container</a></p>&#xA;&#xA;<p><a href=""https://medium.com/@marko.luksa/kubernetes-autoscaling-based-on-custom-metrics-without-using-a-host-port-b783ed6241ac"" rel=""nofollow noreferrer"">2 - Kubernetes autoscaling based on custom metrics</a></p>&#xA;&#xA;<p><a href=""https://kubernetes.io/docs/concepts/containers/container-environment-variables/"" rel=""nofollow noreferrer"">3 - Env variables in Pods</a></p>&#xA;"
51150066,51143213,7803650,2018-07-03T08:36:25,"<p>In your domain example I would not let the message service know anything about bank or user details. Instead the message service should just receive instructions to send messages to recipients along with the given content. I would use a dedicated scheduled job (maybe implemented as an account notification service) that performs the work of acquiring the user and account data from the corresponding services, compiles the information for the message service and instructs it to actually send the messages. This introduces another ""higher level, business purpose entity/service"" but allows you to keep a clear separation of concerns. </p>&#xA;&#xA;<p>In general it will happen frequently that your ""basic"" domain services are used by another service that represents a specific business purpose and requires their data. Dependency in itself is not a bad thing as long as concerns are seperated clearly and interfaces versioned, changes communicated etc. </p>&#xA;&#xA;<p>Don't forget the whole idea of microservices is for allowing teams to have dedicated responsibilities with clear interfacing. It is about organization as much as it is about architecture.</p>&#xA;"
48615385,48557437,7803650,2018-02-05T03:11:55,"<p>The answer depends on the technology stack you are using. Which language? Which framework? Where to deployed? Do you use client sessions or JWT? Oauth or Saml or custom auth service? </p>&#xA;&#xA;<p>If you can give more details we can help better. Here are three random examples: </p>&#xA;&#xA;<ol>&#xA;<li><p>If you have an AWS based stack (e.g. serverless) you can use AWS API Gateway with a custom auth handler. See <a href=""https://docs.aws.amazon.com/apigateway/latest/developerguide/use-custom-authorizer.html"" rel=""nofollow noreferrer"">this</a>.</p></li>&#xA;<li><p>If you develop with a framework that supports middleware you can write a small middleware to handle auth. Example for golang <a href=""https://jacobmartins.com/2016/04/06/practical-golang-writing-a-simple-login-middleware/"" rel=""nofollow noreferrer"">here</a>. Example for laravel <a href=""https://stackoverflow.com/questions/32164695/using-laravel-auth-middleware"">here</a>. </p></li>&#xA;<li><p>Hosting your own Zuul gateway with oauth <a href=""https://piotrminkowski.wordpress.com/2017/02/22/microservices-security-with-oauth2/"" rel=""nofollow noreferrer"">example</a>.</p></li>&#xA;</ol>&#xA;&#xA;<p>For a lot of technologies you will find standard oauth or saml components that you can use as middleware.</p>&#xA;"
48653152,48616949,7803650,2018-02-06T22:40:42,"<blockquote>&#xA;  <p>In which scenario, we should not use micro services architecture? </p>&#xA;</blockquote>&#xA;&#xA;<p>You don't have to apply the microservices architectural style for scenarios that do not come with the problems that the style tries to solve. So which problems are those? </p>&#xA;&#xA;<p>Before giving the answer - Microservices is a buzz word and describes an architectural style that tries to answer the age-old question of ""how to componentize an application"". While there is no exact definition, it usually comes down to independently develop-able and deploy-able components with a specific domain purpose. So with that in mind - Here are the problems that the microservices architectural style tries to solve:</p>&#xA;&#xA;<ol>&#xA;<li><p>The most important problem is scaling development in large organizations. Microservices allow independent teams to move fast with independent release cycles. Compare that with a large ""monolith"" that has to aggregate all dependencies and can spit out a release every couple months at best. So whenever a company is trying to organize hundreds of developers to develop a large server based application they should probably look into setting up teams being responsible for microservices. This is because ""Architecture follows Organization"" (<a href=""https://en.wikipedia.org/wiki/Conway%27s_law"" rel=""nofollow noreferrer"">Conway's law</a>). </p></li>&#xA;<li><p>Scalability of the application itself. In conjunction with a stateless design the services can be deployed at scale in a clustered fashion. So whenever the user base is supposed to be growing over time you will most likely benefit from a microservices architectural style.</p></li>&#xA;<li><p>Enforce separation of concerns. In a ""monolithic"" development it is common to minimize persistent data schemas and have many components use the same database persistence. This increases complexity by adding indirect dependencies and thus adds to the communication and organizational overhead of releasing new software versions. Microservices are not allowed to share data persistency so this issue can be avoided.</p></li>&#xA;<li><p>Communication issues. If the communication between components is not well defined, the effort for reusing components increases drastically. Especially over the lifecycle of multiple versions of a component. Thus the microservices architectural style has a strong focus on well defined interfaces to the outside world. This includes the definition of interface versions (Major/Minor) for downstream consumers.</p></li>&#xA;</ol>&#xA;&#xA;<p>Then about your conclusion:</p>&#xA;&#xA;<blockquote>&#xA;  <p>One of the basic use case I will not recommend for POC (Proof of concepts) projects.</p>&#xA;</blockquote>&#xA;&#xA;<p>Based on the above points your statement does not necessarily hold true. If it is a POC development with a large team and you anticipate any of the mentioned problems you will still benefit from the microservices architectural style.</p>&#xA;"
48678494,48677693,7803650,2018-02-08T05:51:42,"<p>Microservices can be best described as an architectural style. Beside architectural decisions the style also includes organizational and process relevant considerations. </p>&#xA;&#xA;<p>The architectural elements include:</p>&#xA;&#xA;<ol>&#xA;<li>Componentizing by business concern.</li>&#xA;<li>Strict decoupling in terms of persistence.</li>&#xA;<li>Well defined interfacing and communication.</li>&#xA;<li>Aim for smaller service sizes.</li>&#xA;</ol>&#xA;&#xA;<p>The organizational elements include:</p>&#xA;&#xA;<ol>&#xA;<li>Team organization around components (Conway's Law).</li>&#xA;<li>Team size limitations (two-pizza team).</li>&#xA;</ol>&#xA;&#xA;<p>The process relevant elements include:</p>&#xA;&#xA;<ol>&#xA;<li>Less centralized governance.</li>&#xA;<li>Smaller, more frequent releases.</li>&#xA;<li>Higher degree of freedom for technology decisions.</li>&#xA;<li>Product oriented development (agile, MVP, lean, etc).</li>&#xA;</ol>&#xA;&#xA;<p>For more details I recommend reading the <a href=""https://martinfowler.com/articles/microservices.html"" rel=""nofollow noreferrer"">articles</a> from Martin Fowler.</p>&#xA;"
48743777,48743105,7803650,2018-02-12T10:04:19,"<p>Here is a basic function that can do the parsing from a POST request. The result is a two-dimensional array of strings, the first dimension being the rows and the second dimension being the values in a row. It uses the golang's own ""encoding/csv"" package. Go playground example available <a href=""https://play.golang.org/p/xJnXyKdwuxa"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;&#xA;<pre><code>func ReadCSVFromHttpRequest(req *http.Request) ([][]string, error) {&#xA;    // parse POST body as csv&#xA;    reader := csv.NewReader(req.Body)&#xA;    var results [][]string&#xA;    for {&#xA;        // read one row from csv&#xA;        record, err := reader.Read()&#xA;        if err == io.EOF {&#xA;            break&#xA;        }&#xA;        if err != nil {&#xA;            return nil, err&#xA;        }&#xA;&#xA;        // add record to result set&#xA;        results = append(results, record)&#xA;    }&#xA;    return results, nil&#xA;}&#xA;</code></pre>&#xA;"
48655708,48653762,7803650,2018-02-07T03:53:20,"<p>Microservice is just a buzzword and people have been practicing that architectural style before calling it microservice. Here are the commonly agreed attributes of  a microservice. If your service fulfills those points, chances are you can call it a microservice:</p>&#xA;&#xA;<ol>&#xA;<li>Independent life cycles (development, testing, deployment, etc) with independent development teams (to utilize <a href=""https://en.wikipedia.org/wiki/Conway%27s_law"" rel=""nofollow noreferrer"">Conway's Law</a> to your advantage).</li>&#xA;<li>Focused scope - Implement exactly one specific domain aspect to keep it as ""micro"" as possible. (Separation of concerns)</li>&#xA;<li>Clearly defined interface to the outside world. Typically with changes only versioned and phaseout time frames.</li>&#xA;<li>Scalable design. Often stateless on purpose to allow for deployment of application clusters.</li>&#xA;<li>No sharing of databases/persistence. This is important to avoid indirect dependencies.</li>&#xA;</ol>&#xA;&#xA;<p>Additionally some people like to include things like freedom of implementation technologies, infrastructure automation and asynchronous communication.</p>&#xA;&#xA;<p>The cons of not following these practices may not be relevant if you are only a single developer or small team starting out. But if you have a large development organization you would start to suffer under the crippling <a href=""https://www.nearform.com/blog/microservices-series-4-beware-monolith/"" rel=""nofollow noreferrer"">""monolithis""-disease</a>, which potentially slows down development and release cycles significantly.</p>&#xA;&#xA;<p>For more detailed understanding I recommend reading the <a href=""https://martinfowler.com/articles/microservices.html"" rel=""nofollow noreferrer"">Martin Fowler material</a>. Especially the <a href=""https://martinfowler.com/articles/microservice-trade-offs.html"" rel=""nofollow noreferrer"">section about trade-offs</a>.</p>&#xA;"
48655944,48624757,7803650,2018-02-07T04:19:44,"<p>A general foreword - Don't do it if you don't need it. Introducing a queue system can be a big improvement if you are dealing with high number of events and events backing up issues etc. But if you don't face any issues you are probably better off with the lower complexity of a direct service communication.</p>&#xA;&#xA;<p>Back to your question - It sounds like you want to abstract your communication with the queue because you are worried about the effort for replacing the queue with a different system - Is that correct?</p>&#xA;&#xA;<p>In this case you can either do what you proposed - Develop a new service in the middle. This comes with all the baggage of a physical service (including deployment, scaling, etc).</p>&#xA;&#xA;<p>Or the second alternative is to write a client library that abstracts the queue the way you want and allows you to reuse it in all services requiring to participate in the queue. This way you don't have to physically deploy another service for this purpose but you are still in full control of what your interface to the queue should look like and you have a single piece of code to incorporate changes (at least toward the direction of the queue). This would work given you are sure the app-facing side of the library can be stable enough.</p>&#xA;&#xA;<p>But, again, don't do any of those in the first iteration when you are not sure you need all the complexity. (Over-engineering is a dangerous thing)</p>&#xA;"
48700265,48699742,7803650,2018-02-09T06:54:16,"<p>If you want to write your own auth, it can be a service like the others. Part of it would be a small client <strong>middleware</strong> that you run at all other service endpoints which require protection (<a href=""http://www.alexedwards.net/blog/making-and-using-middleware"" rel=""nofollow noreferrer"">golang example for middleware</a>). </p>&#xA;&#xA;<p>An alternative to a middleware is to run a dedicated <a href=""http://microservices.io/patterns/apigateway.html"" rel=""nofollow noreferrer"">API Gateway</a> that queries the auth service before relaying the requests to the actual services. AWS also has a <a href=""https://aws.amazon.com/api-gateway/"" rel=""nofollow noreferrer"">solution</a> for those and you can write custom authentication handlers that will call your own auth service.</p>&#xA;"
48652680,48635782,7803650,2018-02-06T21:59:36,"<blockquote>&#xA;  <p>I am going to implement the orchestration of a set of microservices in my application.</p>&#xA;</blockquote>&#xA;&#xA;<p>This is a hard problem to solve by yourself. You are probably better off using an existing orchestration system (see below).</p>&#xA;&#xA;<blockquote>&#xA;  <p>Is there any other powerful tool?</p>&#xA;</blockquote>&#xA;&#xA;<p>You should look into kubernetes, which seems to be the standard in orchestration these days. It has many additional benefits (enable scalability, self-healing, etc). See the following links:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/"" rel=""noreferrer"">Kubernetes webpage</a></li>&#xA;<li><a href=""https://www.stratoscale.com/blog/kubernetes/container-orchestration-kubernetes-12-key-features/"" rel=""noreferrer"">Article</a></li>&#xA;</ul>&#xA;&#xA;<p>Regarding comparing zookeeper, eureka and kubernetes:</p>&#xA;&#xA;<ul>&#xA;<li>Zookeeper is a distributed key value store. It can be used as the basis to implement service discovery (similar to etcd).</li>&#xA;<li>Eureka is primarily a load balancer (distribute client calls to members of an application cluster).</li>&#xA;<li>Kubernetes is a container orchestration solution that includes the deployment, discovery and self-healing of services. For a complete list of features, check the link above. The service discovery in kubernetes is based on  dns in the virtual network it spans and builds upon etcd.</li>&#xA;<li>Consul (mentioned in the other answer) is a service discovery framework with a REST interface and some additional features. It has its own internal distributed key value store.</li>&#xA;</ul>&#xA;"
48778355,48763985,7803650,2018-02-14T01:31:00,"<p>What you are describing is a ""traditional"" single server environment and does not have much in common with a microservices deployment. However keep in mind that this may be OK if it is only you, or a small team working on the whole application. The microservices architectural style was introduced to be able to handle huge, complex applications with large development teams that require to scale out immensely due to fast business growth. Here an <a href=""https://www.infoq.com/presentations/uber-darwin"" rel=""nofollow noreferrer"">example story from Uber</a>.</p>&#xA;&#xA;<p>Please read <a href=""https://martinfowler.com/microservices/"" rel=""nofollow noreferrer"">this</a> for more information about how and why the microservices architectural style was introduced as well as the benefits/drawbacks. Now about your question:</p>&#xA;&#xA;<blockquote>&#xA;  <p>""Can I choose this EC2 + Docker for deployment of my microservice for actual production environment? ""</p>&#xA;</blockquote>&#xA;&#xA;<p>Your question can be simply answered: You can, but it is probably not a good idea assuming you have a large enough project to require a microservices architecture. </p>&#xA;&#xA;<p>You would have to implement all of the following deployment aspects yourself, which is typically covered by an orchestration system, like <a href=""https://kubernetes.io/docs/getting-started-guides/aws/"" rel=""nofollow noreferrer"">kubernetes</a>: </p>&#xA;&#xA;<ol>&#xA;<li>Service Discovery and Load Balancing</li>&#xA;<li>Horizontal Scaling</li>&#xA;<li>Multi-Container Application Deployment</li>&#xA;<li>Container Health-Management / Self-Healing</li>&#xA;<li>Virtual Networking</li>&#xA;<li>Rolling Updates</li>&#xA;<li>Storage Orchestration</li>&#xA;</ol>&#xA;&#xA;<blockquote>&#xA;  <p>""Since It will gives me about a serverless production environment to&#xA;  me.""</p>&#xA;</blockquote>&#xA;&#xA;<p>EC2 is by definition not serverless, of course. You will have to maintain your EC2 instances, including OS updates, security patches etc. And if you only have a single server you will have service outages because of it.</p>&#xA;"
48700420,48699765,7803650,2018-02-09T07:07:02,"<p>The decision depends on organizational aspects (See <a href=""https://stackoverflow.com/a/48678494/7803650"">another answer</a> about the essence of microservices). So you have to ask: Will you need multiple separate teams for the frontend parts? Will they be separate from the backend service development teams? In some organizations the service development teams would also provide front end components and then a separate dedicated UI-team would use these components and glue them together to create the seamless user experience. In other organizations it makes more sense to have separate teams for backend and anything UI related. So there could be a UI team for each UI component and then one for the integration/final UI. In general the larger the project size the more component-ization you will need to do to keep the teams a 2-pizza-size. And be ready to split teams/components as needed when your project grows.</p>&#xA;"
50095326,50065026,7803650,2018-04-30T06:51:53,"<p>Here a list of differences from the top of my head:</p>&#xA;&#xA;<p>AWS ECS / Kubernetes:</p>&#xA;&#xA;<ul>&#xA;<li><p>Proprietary AWS implementation / Open source solution</p></li>&#xA;<li><p>Runs on AWS / Supported by most cloud providers and on premise</p></li>&#xA;<li><p>Task Definitions / PODs have different features</p></li>&#xA;<li><p>Runs on your EC2 machines or allows for serverless with Fargate (in beta) / Runs on any cluster of (physical/virtual/cloud) machines running the kubernetes controller.</p></li>&#xA;<li><p>Support for AWS VPCs /  Support for multiple networking models</p></li>&#xA;</ul>&#xA;&#xA;<p>I would also argue that kubernetes has a slightly steeper learning curve but ultimately provides more freedom and is probably a safer bet for the future given the wide adoption. </p>&#xA;&#xA;<p>Features supported in both systems:</p>&#xA;&#xA;<ul>&#xA;<li>Horizontal application scalability</li>&#xA;<li>Cluster Scalability</li>&#xA;<li>Load Balancing</li>&#xA;<li>Rolling upgrades</li>&#xA;<li>Logging (with additional logging systems)</li>&#xA;<li>Container Health Checks</li>&#xA;<li>APIs</li>&#xA;</ul>&#xA;&#xA;<p>Amazon has bowed to customer pressure and currently has a managed kubernetes support in beta (EKS). </p>&#xA;&#xA;<p>Here is one <a href=""https://platform9.com/blog/compare-kubernetes-vs-ecs/"" rel=""nofollow noreferrer"">article</a> about the topic. </p>&#xA;"
50223532,50217813,7803650,2018-05-07T22:53:34,"<p>Firewalls / security groups (in cloud), private (virtual) networks and multiple network adapters are usually used to differentiate public vs private network access. Cloud vendors (AWS, Azure, etc) and hosting infrastructures usually have such mechanisms built in, e.g. Kubernetes, Cloud Foundry etc. </p>&#xA;&#xA;<p>In a productive environment Kong's external endpoint would run with public network access and all the service endpoints in a private network. </p>&#xA;&#xA;<p>You are currently running everything locally on a single machine/network, so your best option is probably to use a <a href=""https://en.wikipedia.org/wiki/Firewall_(computing)"" rel=""nofollow noreferrer"">firewall</a> to restrict access by ports. </p>&#xA;"
43583668,43568325,7803650,2017-04-24T08:52:43,"<p>There is no rule about Microservices that code should not reused. In fact stating that DRY is wrong in general with Microservices is dangerous. Rather you should ask the following questions: <code>Will the shared code be a separate module with a dedicated purpose that rectifies a separately managed lifecycle and releases?</code> If yes you should definitely go for reusing it as a separately released module - Similar to how you would reuse any third party library. This ensures that you do not have strong coupling between the shared code and your Microservices, because each Microservice team can decide for themselves if they want to stay with a specific release version of the module or upgrade to a newer version. What you want to avoid is a dependency that forces you to change your microservices when the reused library changes (avoid the ripple effect). </p>&#xA;&#xA;<p>One more thing - Because you are mentioning it being DB related schemas you have to ask another question: <code>Will the schemas being reused in separate DBs or will they refer to the same physical DB in the end?</code>. If they will end up using a shared DB you are effectively tightly coupling the two Microservices and thus they should probably be considered to be part of the same service rather than separate.</p>&#xA;&#xA;<p>I feel this is as much as I can say without knowing more about your services and goals.</p>&#xA;"
43618284,43612866,7803650,2017-04-25T18:24:47,"<p>You are not likely to benefit from a Microservices architecture if all the services share the same database tables. This is because you are effectively tightly coupling the services. If a database table changes all the services will have to change. </p>&#xA;&#xA;<p>You have to understand that the whole reason for a Microservices architecture is to reduce dependencies between development teams and allow them to move ahead independently with fast releases. </p>&#xA;&#xA;<p>Here is a quote from Werner Vogels, the Amazon CTO (Amazon pioneered a lot of the Microservices style architecture):</p>&#xA;&#xA;<blockquote>&#xA;  <p>For us service orientation means encapsulating the data with the&#xA;  business logic that operates on the data, with the only access through&#xA;  a published service interface. No direct database access is allowed&#xA;  from outside the service, and there’s no data sharing among the&#xA;  services.</p>&#xA;</blockquote>&#xA;&#xA;<p>For more information read <a href=""https://martinfowler.com/articles/microservices.html#DecentralizedDataManagement"" rel=""noreferrer"">this</a> and <a href=""https://www.nginx.com/blog/microservices-at-netflix-architectural-best-practices/"" rel=""noreferrer"">this</a>.</p>&#xA;"
43671406,43660545,7803650,2017-04-28T03:18:59,"<p>I would solve this by deploying the correct users/credentials to a DB as part of the automated deployment of the service from a continuous integration / continuous deployment server (CI/CD server). You can store the user credentials as <code>secrets</code> in the CI/CD server. Additionally cluster infrastructure systems, such as kubernetes, allow you to <a href=""https://kubernetes.io/docs/concepts/configuration/secret/"" rel=""nofollow noreferrer"">store secrets</a> as well.</p>&#xA;&#xA;<p>For example if you use Jenkins for deploying you would include an action in the deployment task to create/update the required user/credentials when the DB and schema are deployed/updated. </p>&#xA;&#xA;<p>Also one important general note - If you are worried about the high number of services, that indicates to me that you may be planning too fine grained services. The general motivation for a service scope is to allow a dedicated <code>two pizza max team</code> to be able to own a single service and it's complete life cycle. You would wanna have a developer to service ration of at least 1 to 1. I don't think you will benefit very much from the Microservices architectural style if you have each developer being responsible for many tiny services.</p>&#xA;&#xA;<p>Here a quote from the <a href=""https://martinfowler.com/articles/microservices.html"" rel=""nofollow noreferrer"">Martin Fowler site</a> (which is a very recommended read btw):</p>&#xA;&#xA;<blockquote>&#xA;  <p>How big is a microservice?</p>&#xA;  &#xA;  <p>Although “microservice” has become a popular name for this&#xA;  architectural style, its name does lead to an unfortunate focus on the&#xA;  size of service, and arguments about what constitutes “micro”. In our&#xA;  conversations with microservice practitioners, we see a range of sizes&#xA;  of services. The largest sizes reported follow Amazon's notion of the&#xA;  Two Pizza Team (i.e. the whole team can be fed by two pizzas), meaning&#xA;  no more than a dozen people. On the smaller size scale we've seen&#xA;  setups where a team of half-a-dozen would support half-a-dozen&#xA;  services.</p>&#xA;  &#xA;  <p>This leads to the question of whether there are sufficiently large&#xA;  differences within this size range that the service-per-dozen-people&#xA;  and service-per-person sizes shouldn't be lumped under one&#xA;  microservices label. At the moment we think it's better to group them&#xA;  together, but it's certainly possible that we'll change our mind as we&#xA;  explore this style further.</p>&#xA;</blockquote>&#xA;"
43671659,43639036,7803650,2017-04-28T03:49:23,"<p>Your question is too unspecific to give a good answer. What is a <code>good</code> architecture totally depends on the details of your use cases. Are you serving web pages, streaming media, amass data for analysis, or something completely different? We would also need to know what are you requirements in terms of concurrency, consistency and scalability? What are the constraints for budget/size of development teams, ease of development, dev skills, etc?</p>&#xA;&#xA;<p>For example the decisions you have taken may be considered <code>good</code> if you have strong requirements for a highly scalable input of large data sets and very frequent data collection as well as the team to support it. But it may be considered <code>bad</code> if you have a small team only and are trying to get a quick and cheap MVP for a new service that has limited scalability requirements (because the complexity of the solution slows down your development unnecessarily).</p>&#xA;&#xA;<p>It may be <code>good</code> because the development team is familiar with those technologies and can effectively develop with those. Or it may be <code>bad</code> because your team does not know anything about those and the investment in learning those will not be justifiable by long term gains.</p>&#xA;&#xA;<p>Don't forget that one of the ideas of the microservices architectural style is that each service can be owned by a distinct team that makes its own decisions about what technology to use for implementation (for whatever reason: ease of development, business reasons etc). So in other words the microservices style embraces the old <a href=""https://en.wikipedia.org/wiki/Conway%27s_law"" rel=""nofollow noreferrer"">wisdom</a> <code>architecture follows organization</code>.</p>&#xA;&#xA;<p>Here a <a href=""https://martinfowler.com/articles/microservices.html"" rel=""nofollow noreferrer"">link to a recommended further read</a>.</p>&#xA;"
43690226,43674924,7803650,2017-04-28T23:36:09,"<p>In short: You have to open specific issues/bugs on each repository regarding the specific shortcomings of the service in the repository. The microservices architectural style is all about loose coupling, so an issue should in general be local and treated as such. It is expected that a service is owned by a dedicated team that will be able to fix the specific issue in their service, but nothing in other teams services anyway.</p>&#xA;&#xA;<p>If in your specific architecture it happens often that you find yourself fixing a single issue in many services you probably have to rethink the service boundaries because they may not be as loosely coupled as desirable. Note that finding the right service boundaries is probably the hardest problem when applying the microservices architectural style.</p>&#xA;&#xA;<p>Here is a further <a href=""https://auth0.com/blog/introduction-to-microservices-part-4-dependencies/"" rel=""nofollow noreferrer"">read</a>.</p>&#xA;"
43594895,43584079,7803650,2017-04-24T18:04:02,"<blockquote>&#xA;  <p>I'm wondering if it's a smart thing to do to take microServices approach and split it into few hosted projects.</p>&#xA;</blockquote>&#xA;&#xA;<p>This strongly depends on the amount of organizational pain you are experiencing today. Is your development team losing a lot of time by coordinating the monolith development and issues arising from dependencies between the different parts of your monolith? Is your monolith taking hours to build? Would your business benefit greatly from faster releases? If the answers are yes, then you should consider switching to microservices. If the answers are no or maybe, then you either have a well designed/working monolith and/or the pressure to change is low. Refactoring a monolith to microservices is most likely very expensive and you have to make a cost/benefit calculation. This is a <a href=""https://martinfowler.com/articles/microservice-trade-offs.html"" rel=""nofollow noreferrer"">good read</a> and <a href=""http://philcalcado.com/2015/09/08/how_we_ended_up_with_microservices.html"" rel=""nofollow noreferrer"">this too</a>.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Ideas ? Is there any existing pattern for that?</p>&#xA;</blockquote>&#xA;&#xA;<p>I found this article to be a good general guide about <a href=""https://www.nginx.com/blog/refactoring-a-monolith-into-microservices/"" rel=""nofollow noreferrer"">how to change a monolith</a>. Additionally here are some resources about the experiences of some large companies (<a href=""https://queue.acm.org/detail.cfm?id=1142065"" rel=""nofollow noreferrer"">Amazon1</a>, <a href=""https://vimeo.com/29719577"" rel=""nofollow noreferrer"">Amazon2</a>, <a href=""https://developers.soundcloud.com/blog/building-products-at-soundcloud-part-2-breaking-the-monolith"" rel=""nofollow noreferrer"">Soundcloud</a>, <a href=""https://www.infoq.com/presentations/migration-cloud-native"" rel=""nofollow noreferrer"">Netflix</a>).</p>&#xA;&#xA;<p>In short:</p>&#xA;&#xA;<ul>&#xA;<li>You want to <code>avoid a big bang</code> refactoring that tries to change the whole monolith or large portions in one shot.</li>&#xA;<li><p>Identify the modules from your monolith that will give the biggest benefit when refactored into microservices.</p></li>&#xA;<li><p>Start by changing the interfacing to your application without changing the underlying implementation (for the highest value module/service) to reflect a independent service.</p></li>&#xA;<li><p>The changed interfacing gives you the freedom to refactor the actual implementation at your own pace.</p></li>&#xA;</ul>&#xA;"
43701589,43700905,7803650,2017-04-29T23:56:15,"<p>Manual setup of DNS is possible, as stated by the other answers, but I would recommend to use an infrastructure that supports the service discovery in all respects. For example kubernetes has built in DNS support and makes it very easy to expose a service that can consist of any number of Pods. </p>&#xA;&#xA;<p>An infrastructure technology like kubernetes will also make many other respects of the microservices architectural style easier to implement, including high availability and scalability.</p>&#xA;&#xA;<p>Please see the official <a href=""https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/"" rel=""nofollow noreferrer"">docs</a> for some more information.</p>&#xA;"
43549005,43538070,7803650,2017-04-21T17:44:32,"<p>For uninterrupted upgrade you should use clusters of nodes providing your service and perform a rolling update, which takes out a single node at a time, upgrading it, leaving the rest of the nodes for continued servicing. I recommend looking at the concept of virtual services (e.g. in <a href=""https://coreos.com/kubernetes/docs/latest/services.html"" rel=""nofollow noreferrer"">kubernetes</a>) and <a href=""https://kubernetes.io/docs/tutorials/kubernetes-basics/update-intro/"" rel=""nofollow noreferrer"">rolling updates</a>.</p>&#xA;&#xA;<p>For inducing state I would recommend looking into container initialization mechanisms. For example in docker you can use <a href=""https://docs.docker.com/engine/reference/builder/#entrypoint"" rel=""nofollow noreferrer"">entrypoint scripts</a> or in kubernetes there is the concept of <a href=""https://kubernetes.io/docs/concepts/workloads/pods/init-containers/"" rel=""nofollow noreferrer"">init containers</a>. You should note though that today there is a trend to decouple services and state, meaning the state is kept in a DB that is separate from the service deployment, allowing to view the service as a stateless component that can be replaced without losing state (given the interfacing between the service and required state did not change). This is good in scenarios where the service changes more frequently and the DB design less frequently.</p>&#xA;&#xA;<p>Another note - I am not sure that representing state in a namespace is a good idea. Typically a namespace is a static construct for organization (of code, services, etc.) that aims for stability. </p>&#xA;"
50308263,50289914,7803650,2018-05-12T16:12:21,"<p>What you are describing sounds like Service A is dependent on Service B and there has to be a necessary service interface. If the states are information that the service provides they should be seen as part of the API and thus made available for consumers as part of the API description. </p>&#xA;&#xA;<p>Also a general note - I assume the reason you are asking the question is insecurity about the exact service boundaries. Note that drawing the right service boundaries is the hardest part of designing any system in the microservices architectural style. The boundaries should typically be domain specific, but potentially also need to consider organizational and technical constraints. See this <a href=""https://martinfowler.com/articles/microservice-trade-offs.html#boundaries"" rel=""nofollow noreferrer"">article</a> and I recommend reading all of Fowler's materials about the subject.</p>&#xA;"
48837504,48833778,7803650,2018-02-17T03:09:18,"<p>You'll definitely want to avoid sharing database schema/tables. See this <a href=""http://blog.christianposta.com/microservices/the-hardest-part-about-microservices-data/"" rel=""nofollow noreferrer"">blog</a> for an explanation. Use a purpose built interface for dependency between the services.</p>&#xA;&#xA;<p>Any decision to ""copy"" data into your other service <a href=""https://martinfowler.com/articles/microservices.html#DecentralizedGovernance"" rel=""nofollow noreferrer"">should be made by the service's team</a>, but they better have a real good reason in order for it to make sense. Most designs won't require duplicated data because the service boundary should be domain specific and non-overlapping. In case of user ids they can be often be treated as contextual references without any attached logic about users.</p>&#xA;&#xA;<p>One pattern observed is: If you have auth protected endpoints, you will need to make a call to your auth service anyway - for security - and that same call should allow you to acquire whatever user id information is necessary. </p>&#xA;&#xA;<p>All the regular best practices for API dependencies apply, e.g. regarding stability, versioning, deprecating etc.</p>&#xA;"
48845752,48841121,7803650,2018-02-17T20:41:21,"<p>DCOS is just a tool for your deployment, so in order to give a better answer you'd have to share more about your technology stack in your question. But here some general thoughts: There are different types/levels of testing and there are different considerations for each.</p>&#xA;&#xA;<ol>&#xA;<li><p>On unit level - This depends on what technology you use for implementing your services and is the reason why languages like go become more and more popular for server development. If you use <a href=""https://blog.alexellis.io/golang-writing-unit-tests/"" rel=""nofollow noreferrer"">go</a> for example, you can easily run any service you are currently developing locally (not containerized) on the dev machine. And you can easily run attached unit tests. You would either run dependent services locally or mock them up (probably depending on effort). You may also prefer asking each service team to provide mock services as part of their regular deliveries. &#xA;And you will require special environment settings and service configuration for the local environment.So summarized this approach will require you to have means in place to run services locally and depending on the implementation technologies you use it will be easier or harder.</p></li>&#xA;<li><p>On deployment/integration level - Setting up a minimal cluster on the <a href=""https://yurisubach.com/2016/07/06/dcos-local-deployment/"" rel=""nofollow noreferrer"">dev's local machine</a> and/or using dedicated testing- and staging clusters. This allows you to test the services including containerization and in a more final deployment environment with dependencies. You would probably write special test clients for this type of test. And this approach will also require you to have separate environment settings, configuration files, etc. for the different environments. Tools like <a href=""https://github.com/jaegertracing/jaeger"" rel=""nofollow noreferrer"">Jaeger</a> become more popular for helping with debugging errors through multiple services here.</p></li>&#xA;<li><p><a href=""https://martinfowler.com/bliki/CanaryRelease.html"" rel=""nofollow noreferrer"">Canary</a> testing. Like the name suggests - You deploy your latest service version to a small portion of your production cluster in order to test it on a limited number of users first before rolling it out to the masses. In this stage you can run user level tests and in fact your users become the testers, so it is to be used carefully. Some organizations prefer to have special beta-type-users that will only get access to those environments.</p></li>&#xA;</ol>&#xA;"
48961173,48961000,7803650,2018-02-24T09:10:07,"<blockquote>&#xA;  <p>The evils of too much coupling between services are far worse than the problems caused by code duplication</p>&#xA;</blockquote>&#xA;&#xA;<p>The author is very unspecific when he uses the generic word ""coupling"". I would agree with certain types of coupling being a strict no-no (like sharing databases or using internal interfaces). However the use of common libraries is not one of those. For example if you develop two micro services using golang you already have a shared dependency (towards golang's basic libraries). The same applies to libraries that you develop yourself for sharing purpose. Just pay attention to the following points:</p>&#xA;&#xA;<ul>&#xA;<li>Treat libraries that are shared like you would dependencies to 3rd party entities.</li>&#xA;<li>Make sure each component / library / service has a distinct business purpose.</li>&#xA;<li>Version them correctly and leave the decision which version of the library to use to the corresponding micro service teams.</li>&#xA;<li>Set up responsibilities for development and testing of shared libraries separately from the micro services teams.</li>&#xA;</ul>&#xA;&#xA;<p>Don't forget - The microservices architectural style is not so much focusing on code organization or internal design patterns, but on the larger organizational and process relevant aspects to allow scaling application architectures, organizations and deployments. See <a href=""https://stackoverflow.com/a/48678494/7803650"">this answer</a> for an overview.</p>&#xA;"
48970814,48966893,7803650,2018-02-25T06:19:37,"<blockquote>&#xA;  <p>""The 3rd party registrar might only have superficial knowledge of the state of the service instance, e.g. RUNNING or NOT RUNNING and so might not know whether it can handle requests.""</p>&#xA;  &#xA;  <p>What information does a micro service send to the registrar when it starts? Why the registrar is not able to know information about the service and its location ?</p>&#xA;</blockquote>&#xA;&#xA;<p>The service will typically not contact the registry by itself. The pattern that has emerged is rather that an <a href=""https://kubernetes.io/"" rel=""nofollow noreferrer"">orchestration system</a> starts up the service and makes sure the service is registered and its status is checked. This is helpful so you don't have to worry about these things when you design your service - The service should have a pure business focus and not have any knowledge about <a href=""https://kubernetes.io/docs/concepts/services-networking/service/"" rel=""nofollow noreferrer"">service discovery mechanisms</a>. And the registry will of course need to know about the service and its location(s). Because it's part of the orchestration system it provides this information to the rest of the service cluster.</p>&#xA;&#xA;<p>Then about the quote: It refers to the fact that the registrar is a separate entity and there is a need for communication between the registry and the service. The scope of communication is usually confined to the purpose of service readiness and availability (e.g. through a health probe). However it is not uncommon that systems with a service registry allow custom <a href=""https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/"" rel=""nofollow noreferrer"">health probes</a> for your own service types. Since those are in your control, you can define the exact communication and what APIs and return values make your service healthy or not. </p>&#xA;&#xA;<p>Why is this very basic information about the service status sufficient? </p>&#xA;&#xA;<p>The status information is what is required to divert traffic to healthy services when a service fails and / or automatically replace unhealthy service containers. These are the typical use cases and thus supported out of the box by a typical registration or orchestration system.</p>&#xA;"
48979837,48979234,7803650,2018-02-26T00:10:34,"<blockquote>&#xA;  <p>We are thinking on make a new microservice that will be used as an orchestrator, but we don't know if this will be a good solution for microservices concepts.</p>&#xA;</blockquote>&#xA;&#xA;<p>From all you described that sounds like the most reasonable thing to do. You describe this service as having its own business purpose which indicates to me the potential need for a dedicated service. And the fact that it requires input from other (more basic) services would not be unusual for a complex domain service. Also you already listed the alternative of aggregating on the front end as something that doesn't work in your domain.</p>&#xA;&#xA;<p>Something to consider is just making sure that the development teams for the basic services treat their APIs as customer facing (with the customer being your other services). That means they have to do clean work in terms of versioning/deprecating/etc.&#xA;And the downstream services need to treat the consumed APIs like they would a 3rd party API. For example Google went so far to allow internal service consumption be charged real money to incentivize optimizing the implementation of dependent services.</p>&#xA;"
49021834,49019203,7803650,2018-02-28T04:11:23,"<blockquote>&#xA;  <p>Any tips or advice for patterns / frameworks to do this. I'd like something fairly simple and not over complex.</p>&#xA;</blockquote>&#xA;&#xA;<p>What you describe is the good ol' domain of A,B,C,D and E. Because the dependencies and engagement rules between the letters are complex enough, it's good to create a dedicated service for this domain. It could be as simple as this overarching service just being triggered by queue events.</p>&#xA;&#xA;<p>The only other alternative is to do more on the client side and organize the service calls from there. But that isn't feasible in every domain for security reasons or other issues.</p>&#xA;&#xA;<p>And since it sounds like you already got an event queue going, I'll not recommend one (Kafka). </p>&#xA;"
49021966,49004413,7803650,2018-02-28T04:24:30,"<blockquote>&#xA;  <p>I want to make a microservice that can take a keyword and create a instance of that code for that keyword and gather tweets, for each keyword an instance should be created.</p>&#xA;</blockquote>&#xA;&#xA;<p>You could use <a href=""https://kubernetes.io/"" rel=""nofollow noreferrer"">kubernetes</a> as an underlying cluster/deployment infrastructure. It has an <a href=""https://kubernetes.io/docs/concepts/overview/kubernetes-api/"" rel=""nofollow noreferrer"">API</a> that allows you to deploy new services programmatically. So what you would have to do is:</p>&#xA;&#xA;<ul>&#xA;<li>Set up a basic service container for your twitter-service that is available in a <a href=""https://www.aquasec.com/wiki/display/containers/What+is+a+Container+Image+Repository"" rel=""nofollow noreferrer"">container repository</a>.</li>&#xA;<li>Then you deploy a first <a href=""https://kubernetes.io/docs/concepts/services-networking/service/"" rel=""nofollow noreferrer"">service</a> based on your container. The service configuration will <a href=""https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/"" rel=""nofollow noreferrer"">contain the keyword</a> that the service uses as well as information about the kubernetes cluster (how to access the cluster API and where to find the container in the repository).</li>&#xA;<li>Now your first service has all the information it needs to automatically create additional service descriptions for kubernetes (with other key words) and deploy those additional services by calling the kubernetes cluster API.</li>&#xA;<li>Since the additional services will be passed all the necessary information as well, they themselves can then start even more services and so on.</li>&#xA;</ul>&#xA;&#xA;<p>You probably need to put some effort into figuring out the cluster provisioning, but that can also be done automatically with auto-scaling (available for Google or AWS clouds for example).</p>&#xA;&#xA;<p>A different approach would be to run a horizontally scaled cluster of your basic twitter services that use a self organization algorithm to involve all the keywords put into a database or event queue.</p>&#xA;"
48970918,48963386,7803650,2018-02-25T06:36:31,"<blockquote>&#xA;  <p>This model does some coupling of the domains? The Gateway is validating subresources ID's to ensure data consistency in one Elasticsearch database (the case of A pointing to B).</p>&#xA;</blockquote>&#xA;&#xA;<p>You answered your own question with the following sentence:</p>&#xA;&#xA;<blockquote>&#xA;  <p>All read requests are handled by the ""Microservice Gateway""</p>&#xA;</blockquote>&#xA;&#xA;<p>If that is true your system does not have a domain separation at all. In fact you have a god-Gateway that needs to know about all domains.</p>&#xA;&#xA;<p>A gateway should be constrained to very dedicated purposes that the domain specific services cannot fulfill (e.g. security, routing/proxy, load balancing, consolidation, resilience, etc.).</p>&#xA;&#xA;<blockquote>&#xA;  <p>I don't know if this model fits reports requests, there are some complex reports that process a lot of data with input parameters from the user and from his point of view, the operation must be ""synchronous"" (request/response REST)</p>&#xA;</blockquote>&#xA;&#xA;<p>I would see reports as a separate service that ideally uses the existing services to get whatever information is required. Depending on your domain and requirements you may want to have some dedicated non-public endpoints at your services for this or you may want to access the database directly (e.g. with specialized queries on graph databases).</p>&#xA;&#xA;<blockquote>&#xA;  <p>Is the validation of requests/new state a part of the business logic (related to DDD)? If so, my model is incorret to separate them into two microservices?</p>&#xA;</blockquote>&#xA;&#xA;<p>It depends on the type of validation. For validating correctness from a domain point of view you need the corresponding service. If you want to validate other things other parts of your system could be responsible (logging, authorization, thwarting malicious attempts, ...) </p>&#xA;&#xA;<p>*For your edit:</p>&#xA;&#xA;<blockquote>&#xA;  <ol>&#xA;  <li>Instead of having a gateway acting as a part of the microservice, let gateway only for: routing (microservice discovery), balancing and auth stuffs (calling a dedicated microservice for authentication/authorization)</li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p>Few things about this: </p>&#xA;&#xA;<ol>&#xA;<li>Typically you don't do the actual service discovery on the API gateway. Instead it should be a function of the cluster orchestration (see <a href=""https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/"" rel=""nofollow noreferrer"">how kubernetes does it for you</a>). The gateway would just use the service registry to proxy correctly (for example use the right DNS entries).</li>&#xA;<li>The microservices architectural style was invented to allow very large organizations to scale fast growing server applications. It does have drawbacks, especially when setting up for smaller projects that need to have results quickly. Most people from those larger organizations would tell you to start with a monolithic initially and separate out when teams grow too big (by drawing component boundaries you can prepare for that separation in initial development). Even with a ""monolithic"" service you can still have a modern build/test/deployment infrastructure with CICD and the service can still have horizontal scalability. Since I don't know your domain / project size I just wanted to give that as a general thought.</li>&#xA;</ol>&#xA;"
44032235,44029177,7803650,2017-05-17T18:22:29,"<p>That's a bit of a silly question and will most likely be closed as too broad. My general answer is: <code>As much as possible, of course.</code> You always want to know as much as possible about any topics that you would consider important for your profession. By the fact that you ask this question I assume that these topics may be important to your career.</p>&#xA;&#xA;<p>The more specific answer is: It depends on where you want to be with your career and personal life. You should of course always learn all topics that would be required in your current job at some point. For deciding if you want to learn additional topics you should consider the following:</p>&#xA;&#xA;<ol>&#xA;<li>Will it help your career in the future?</li>&#xA;<li>Do you have a personal interest in learning it?</li>&#xA;<li>Are there other topics that would benefit me more?</li>&#xA;<li>Is it ok to spend time on it for my work / life balance?</li>&#xA;<li>Is it better to learn a technology or an application domain?</li>&#xA;<li>Is it fun for me?</li>&#xA;</ol>&#xA;&#xA;<p>...</p>&#xA;&#xA;<p>Also note that in general as a software engineer you won't be able to learn every technology available and you will always have to choose what to learn broadly and what to specialize in. These choices are very important and will define what your earnings potential and future career options are.</p>&#xA;"
44077303,44060464,7803650,2017-05-19T19:03:49,"<p>In short: You may want to look at a solution that is more specialized for time series data. For example <a href=""https://github.com/influxdata/influxdb"" rel=""nofollow noreferrer"">influxdb</a>. And for making your system more robust you may want to include a fast stream processor, such as Apache <a href=""https://kafka.apache.org/"" rel=""nofollow noreferrer"">Kafka</a>, as well. </p>&#xA;&#xA;<p>Here the answers to your questions:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Exclude the cost of PC we use to deploy our database and micro service&#xA;  of whether measurement. Is this deployment an efficient practise ?</p>&#xA;</blockquote>&#xA;&#xA;<p>This question is not too clear as to what you are asking, but I assume you are asking whether it is efficient to use a serverless cloud deployment for your DB/Service setup. If so then the answer is probably: Yes, because as a supposedly small team you will not have to deal with the setup and maintenance of hardware (avoid that cost).</p>&#xA;&#xA;<blockquote>&#xA;  <p>Are there any way to manage this kind of Measurement table ? (Data is&#xA;  increasing each 10 second and can be queried many times) ?</p>&#xA;</blockquote>&#xA;&#xA;<p>Again, look at <a href=""https://github.com/influxdata/influxdb"" rel=""nofollow noreferrer"">influxdb</a> as a more specialized solution that will help you with a lot of the typical issues around management of time series data.</p>&#xA;&#xA;<blockquote>&#xA;  <p>If there is a way to optimize my table, please let me know ?</p>&#xA;</blockquote>&#xA;&#xA;<p>See the other, wonderful answers of all the DB specialists.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Should I deploy sensor measurement collecting function as micro&#xA;  services to increase performance and scalability ?</p>&#xA;</blockquote>&#xA;&#xA;<p>Your collection function is effectively a data stream endpoint, so you may want to use a stream processor, such as Kafka for this purpose. Now when your stream is saved in a big queue (called topic in kafka), then you have all the time in the world to process it with any big data technology (e.g. with <a href=""http://spark.apache.org/"" rel=""nofollow noreferrer"">spark</a>/<a href=""http://hadoop.apache.org/"" rel=""nofollow noreferrer"">hadoop</a>) and store it in whatever formats / analysis's required (this is where most likely a traditional rdb or nosql db comes into play).</p>&#xA;&#xA;<p>Microservices is an architectural style that aims to help with the organizational challenges of large organizations with complex solutions. Depending on how big your application setup is, but if you are more than 10 people in the development/devops team, you probably want to think about splitting your implementation into multiple Microservices. For more information, please read <a href=""https://martinfowler.com/articles/microservices.html"" rel=""nofollow noreferrer"">this awesome article</a>.</p>&#xA;"
44170803,44169046,7803650,2017-05-25T01:14:46,"<p>There are multiple dimensions to your question. First you want to consider using an infrastructure that provides resilience and <code>self healing</code>. Meaning you want to deploy a cluster of containers, all containing your Service A. Now you use a load balancer or API gateway in front of your service to distribute calls/load. It will also periodically check for the health of your service. When it detects a container does not respond correctly it can kill the container and start another one. This can be provided by a container infrastructure such as kubernetes / docker swarm etc. </p>&#xA;&#xA;<p>Now this does not protect you from losing any requests. In the event that a container malfunctions there will still be a short time between the failure and the next health check where requests may not be served. In many applications this is acceptable and the client side will just re-request and hit another (healthy container). If your application requires absolutely not losing requests you will have to cache the request in for example an API gateway and make sure it is kept until a Service has completed it (also called <code>Circuit Breaker</code>). An example technology would be Netflix Zuul with Hystrix. Using such a Gatekeeper with built in fault tolerance can increase the resiliency even further. As a side note - Using an API gateway can also solve issues with central authentication/authorization, routing and monitoring.</p>&#xA;&#xA;<p>Another approach to add resilience / decouple is to use a fast streaming / message queue, such as Apache Kafka, for recording all incoming messages and have a message processor process them whenever ready. The trick then is to only mark the messages as processed when your request was served fully. This can also help in scenarios where faults can occur due to large number of requests that cannot be handled in real time by the Service (<code>Asynchronous Decoupling with Cache</code>).</p>&#xA;"
44187496,44181863,7803650,2017-05-25T18:30:16,"<p>Here's the (hopefully) simplest answer to your question:</p>&#xA;&#xA;<ul>&#xA;<li><p>Microservices are a <code>different</code> (micro-) application each. Each with its own application logic and database.</p></li>&#xA;<li><p>Load Balancers are usually used to distribute client requests to a cluster of instances of the <code>same</code> application. </p></li>&#xA;</ul>&#xA;&#xA;<p>That means: You can also use a load balancer to distribute requests for a microservice that is deployed in a cluster with many instances. But a load balancer can also be used to distribute requests to many instances of a large monolithic application (as opposed to micro).</p>&#xA;&#xA;<p>The probably <a href=""https://martinfowler.com/articles/microservices.html"" rel=""nofollow noreferrer"">best overview</a> for what Microservices are supposed to be.</p>&#xA;"
44187811,44177107,7803650,2017-05-25T18:50:07,"<p>Fault tolerance or resilience has to do more with your internal application architecture than with using Microservices or another architectural style. For example - If you compare a well structured monolith with internal error handling and fallback strategies to a bunch of Microservices that are designed with interdependencies but no built in resilience, the Microservices will be way more likely to fail all together.</p>&#xA;&#xA;<p>Here some ideas of how to build a resilient system:</p>&#xA;&#xA;<ol>&#xA;<li>Avoid interdependencies. Most important, but not always possible. </li>&#xA;<li>Use an infrastructure with built in self-healing capabilities, such as Kubernetes.</li>&#xA;<li>Use an API gateway with built in resilience, such as Zuul.</li>&#xA;<li>Use specialized libraries for resilient calling with promises and circuit breakers. Such as Hystrix.</li>&#xA;<li>Cache requests in a stream processor, such as Kafka, to protect against load spikes, intermittent service failures.</li>&#xA;<li>Design your APIs idempotent.</li>&#xA;</ol>&#xA;&#xA;<p>When you ask for measuring fault tolerance you should look into automated testing of your application. For example you can write tests for your application that use randomized input/wrong input or ultra-high loads in an attempt to disturb the services. So measuring/proving fault tolerance really is a task for the testing team.</p>&#xA;"
44188493,44183595,7803650,2017-05-25T19:34:10,"<p>You can use:</p>&#xA;&#xA;<p>CLI: <code>kubectl get services --selector=YOUR-LABEL-NAME</code>.</p>&#xA;&#xA;<p>API: <code>GET /api/v1/namespaces/{namespace}/services</code> with <code>labelSelector</code> parameter see <a href=""https://kubernetes.io/docs/api-reference/v1.6/#list-161"" rel=""nofollow noreferrer"">API docs</a>.</p>&#xA;"
44077597,44043159,7803650,2017-05-19T19:22:40,"<p>In short - Such an integration testing would not be part of the microservices development/deployement team and process, but a separate team having its own process. You can automate as much as possible in that team, but in the end you need a decision whether to release or not.</p>&#xA;&#xA;<p>The longer explanation:</p>&#xA;&#xA;<p>The Microservices architectural style was invented to help large organizations to manage large applications and avoiding overhead of communication and dependencies between teams. So if you want to follow this style, you should really have 3 independent teams - One for each service. Each of those teams would have complete responsibility over the whole life-cycle of their respective service. Now when you want to do end-to-end testing (often called integration testing) you would set up a 4th team that is responsible for those tests. And you would have one person being the responsible release manager who owns a staging/testing cluster and decides on when testing proves sufficiently to release a new version of a service into the wild. Your goal should be to decouple the teams as much as possible in terms of dependencies and release cycles of their services. If you want complete independence of the services teams you can also make the integration testing part of each team. Meaning you have testing/staging cluster for each team and a responsible testing/release manager role in each team.</p>&#xA;"
48163027,48134800,7803650,2018-01-09T07:12:37,"<p>VonCs answer is great. I'd just like to add one thing: Golang is perfect for containers because it has built in support for self-sustained binaries. And that means you can build containers that are just the size of a few MB instead of the typical containers with alpine etc. that are often hundreds of MB.</p>&#xA;&#xA;<p>Check out this <a href=""https://blog.codeship.com/building-minimal-docker-containers-for-go-applications/"" rel=""nofollow noreferrer"">read</a>. </p>&#xA;"
48161082,48160266,7803650,2018-01-09T03:35:45,"<p>In theory a ""traditional"" canonical data model should not be required with a well designed microservices architecture due to the fact that each service has its unique domain of responsibility and only models the data from their specific domain. So when a service consumes other services the data overlap should be minimal. And when consuming another service the data modeling responsibility lies with the source rather than the consumer.</p>&#xA;&#xA;<p>However in practice that may not always be the case, e.g. you'll have to pull data from similar but not identical sources. So to your question - if you find the need to transform the services' data into a canonical model you'll probably want to perform translation to a single representation (your first idea) as soon as possible, rather than keeping both representations (your second idea). This will help with simplicity of consumption further downstream (imagine the messy consumption code where you need to check multiple places in a data structure). If the services are under your control you may want to evolve them towards providing data in the canonical model in the first place.</p>&#xA;"
45151384,45146319,7803650,2017-07-17T18:39:29,"<p>This is really an infrastructure question. Typically you want to have a private network with all your resources internally - the so called <a href=""https://en.wikipedia.org/wiki/DMZ_(computing)"" rel=""nofollow noreferrer"">De-Militarized-Zone</a> or DMZ - and then have a second network or endpoint bridge that provides external access. The internal network should not be reachable from the internet. The endpoint could be a single server or an array of servers that are implemented as a <a href=""https://en.wikipedia.org/wiki/Bastion_host"" rel=""nofollow noreferrer"">bastion</a> host and will authenticate and authorize callers and forward calls to the private network that are legitimate. </p>&#xA;&#xA;<p>The API gateway (or edge-server) pattern is often used to realize this. A good configuration of the gateway is important. </p>&#xA;&#xA;<p>Here is an <a href=""http://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html"" rel=""nofollow noreferrer"">article</a> of how to do it with the Amazon cloud.</p>&#xA;&#xA;<p>And here a <a href=""https://getkong.org/"" rel=""nofollow noreferrer"">link</a> to Kong, a general API gateway that you can deploy yourself.</p>&#xA;"
43836979,43814764,7426,2017-05-07T21:36:19,"<p>You can do this in a standard Go unit test using the <a href=""https://golang.org/pkg/net/http/httptest/"" rel=""nofollow noreferrer"">httptest</a> package. This allows you to create mock <code>Request</code> and <code>ResponseWriter</code> objects that can be passed to any <code>Handler</code> or <code>HandleFunc</code>. You create the appropriate <code>Request</code>, pass it to your handler, then read the response out of the <code>ResponseRecorder</code> and check it against the expected response.</p>&#xA;&#xA;<p>If you're using the default mux (calling <code>http.Handle()</code> to register handlers) you can test against <code>http.DefaultServeMux</code>. I've used it for microservices in the past with good results. Works for benchmarking handlers, routing, and middleware as well.</p>&#xA;"
41021955,40938756,5121282,2016-12-07T16:00:00,"<p>After read some examples I noticed that zuul is always the start for applications so I can add hystrix monitor to zuul and call the microservices, but I still have doubt about the circuit breaker in use with zuul so I will keep searching.</p>&#xA;"
45176301,44433488,7357186,2017-07-18T20:17:23,"<p>As an idea: check how your services depend on each other. If your dependencies are acyclic, you might be able to backup all your data outside-in or inside-out, without running into consistency issues.</p>&#xA;&#xA;<p>Doing so would guarantee you to have no elements in services depending on an inner one after your restore.</p>&#xA;&#xA;<p>If your services show cyclic dependencies, you might be better serviced to have each service redundantly (e.g. master slave replication). Then you can take down the slave instances, taking a backup from the whole lot of slaves while they are offline. That would allow you to create an atomic backup accross all services. However your quality of the backup is then based on the quality of your master slave replication at each service.</p>&#xA;&#xA;<p>Lastly you could keep record of change per service, plus a full backup. Thus you can write your rollback and the start applying the record of change until you reach a consistent state accross the service instances. I think that requires you to have logical dependencies (request identifier) that allows you to correlate the record of change elements (i.e. apply them across the services without the risk to apply them in a way that defies the logical dependencies that occured when clients actually interacted with your services).</p>&#xA;&#xA;<p>I hope these ideas can help you solve your problem :)</p>&#xA;"
41882979,41880524,544258,2017-01-26T21:06:27,"<blockquote>&#xA;  <p>""There are only two hard problems in Computer Science:cache invalidation and naming things.""-- Phil Karlton</p>&#xA;</blockquote>&#xA;&#xA;<p>The main problem with local cache in app-server is that it makes cache invalidation much more hard that it was before.</p>&#xA;&#xA;<p>Each time a resource change, it has to be invalidated (and updated) on all the local caches. This would require a system that knows about all the cache servers running at any point of time. This system would have to be informed about all updates so that it can co-ordinate the data invalidation on all servers. It will also have to take care of retries, handling failed servers, etc.</p>&#xA;&#xA;<p>If your application server has it's own local cache, you will have to  solve these problems yourselves using a separate system or in the application code. A distributed caching system, would have solved those problems for you. You can make an update call and on success have a guarantee of data consistency (or eventual consistency). </p>&#xA;&#xA;<p>It is separation of concerns. With a separate cache cache cluster, the caching logic and the associated problems are handled at one place. The same cluster can be reused for multiple applications easily, rather than redoing the same for each application you develop.</p>&#xA;&#xA;<p>Another minor disadvantage is that you would have to warm up the cache each time you spawn a new server, if you don't want a performance degradation. This would lead to longer time to spawn servers.</p>&#xA;"
38420959,37749087,1804802,2016-07-17T12:00:23,"<p>First of all, the systems we are building have a purpose, typically to increase revenue and profit by making customers happy and coming back. So messages/events which originated from customer actions <em>must</em> be processed (assuming, the company in question is prioritizing customer experience.....as in willing to invest money into it). </p>&#xA;&#xA;<p>By the way, the customer-enterprise relationship is the one in the whole system that we want to be <em>tightly</em> coupled, unlike all the other ones internally. So in these case, it is an example of ""authority"" rather than autonomy. We guarantee an SLA, represented by the brand.</p>&#xA;&#xA;<p>But the spectrum of message importance should be more fine-grained than ""must deliver"" or not, rather reflecting priorities. Similar to capabilities becoming more fine grained (microservices). more about this later</p>&#xA;&#xA;<p>So the goal of ensuring messages/events are getting processed by subscribers can either be achieved by   ensuring that services are never down (like the ""virtual actor"" concept in MS Orleans), or by putting more error handling logic into the delivery mechanism. </p>&#xA;&#xA;<p>The latter option seems to be more centralistic/coupled rather than autonomous/decoupled. But if you assume that services are not always available (as we should) then you need to consider to remove your other assumption about ""transient"" messages. </p>&#xA;&#xA;<p>The first option leaves the decisions of how to guarantee availability to the service and therefore to the agile team owning the service, while performance is measured via output metrics.</p>&#xA;&#xA;<p>Besides, if services as encapsulated capabilities guarantee a high service level (""never down""), then control of the outcome of the overall system (=enterprise) can be continously adapted by adjusting message priorities as well as injecting new services and events into the system.</p>&#xA;&#xA;<p>The other important aspect is the fact that synchronous architectures (=call stack based) provide three features that async architectures (event driven) don't exhibit for the sake of dependency reduction: coordination, continuation and context (see Hohpe, ""Programming without a call stack"", 2006). </p>&#xA;&#xA;<p>We still need these features for our customers at the business level, hence they need to be covered elsewhere. Hohpe suggests that the configuration and monitoring of the behaviour of a loosely coupled system requires an additional code layer that is as important as the core business capabilities (Complex Event Processing to understand the relationship between events)</p>&#xA;&#xA;<p>These modern CEP systems which have to deal with massive amounts of data, different velocities, structures and correctness levels could be implemented on top of modern data processing and big data systems (e.g. Spark) to be used for understanding, decision making and optimisation both by agile teams (to improve their service) and management teams at their level.</p>&#xA;"
38433758,38265103,630786,2016-07-18T09:57:46,"<p>MSF4J does not directly support serving static content. From your question what I understood is that you want to point the MSF4J server to a directory and serve resources in that directory by their relative path or something similar. In this case what you can do is to write an MSF4J service method with a wildcard path and serve the static content that matches the path of the request.</p>&#xA;&#xA;<pre><code>@Path(""/"")&#xA;public class FileServer {&#xA;&#xA;    private static final String BASE_PATH = ""/your/www/dir"";&#xA;&#xA;    @GET&#xA;    @Path(""/**"")&#xA;    public Response serveFiles(@Context Request request) {&#xA;        String uri = request.getUri();&#xA;        System.out.println(""Requested: "" + uri);&#xA;        File file = Paths.get(BASE_PATH, uri).toFile();&#xA;        if (file.exists()) {&#xA;            return Response.ok().entity(file).build();&#xA;        } else {&#xA;            return Response.status(404).entity(""&lt;h1&gt;Not Found&lt;/h1&gt;"").build();&#xA;        }&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
51623630,51602270,256770,2018-08-01T00:03:03,<p>It turns out that when the code was split across two separate services (separate modules in same multi-project gradle) I also need the service with the owner entity (foo-service) to have a BarRepository for the owned Entity (Bar). When I copy the BarRepository from bar-service module to foo-service module then I no longer get a 404 and the relationship between Foo to Bar can be observed in the API. Of course this only works if foo-service and bar-service share the same database (which at present in my case they do).</p>&#xA;&#xA;<p>I would be interested in an answer that describes how to solve this problem in a general case where the two services do not share a database.</p>&#xA;
34321340,34295221,4207875,2015-12-16T20:20:54,"<p>According to the answer to this question (in Programmers Stack Exchange) <a href=""https://softwareengineering.stackexchange.com/a/277043"">How do you handle shared concepts in a microservice architecture?</a> seems that in fact the initial proposed approach is a good one. If separation of concerns is done properly when strangling the different parts into different services, then there shouldn't be coupling issues.</p>&#xA;&#xA;<p>As for fault tolerance, is hard to generalize, so it might be better to study the specifics in a case by case basis, and determine in each situation, how to gracefully downgrade the service when other services aren't available.</p>&#xA;&#xA;<p>Regarding questions 2 and 3, I tried to take a generalized abstract approach at first, but after considering an specific case, I ended up with the conclusion that is also hard to generalize. So for this specific graph model, I came up with this possible solution:</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/Kj3aM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Kj3aM.png"" alt=""Microservices""></a></p>&#xA;&#xA;<p>So basically, to question 3 the answer is yes, because this way, the relationship of the users following other users can be used by some other service, and it won't be forced to deppend on the recommendation system.</p>&#xA;&#xA;<p>And to the question 2, it deppends on the domain model, since it made sense to split the user service apart from the friendship service in the first place, that relationship don't need to be replicated in the recommendation service, while all the other relationships are indeed related, it makes sense to keep them together, at least while there is no need to split them again in order to be able to be reused for other services.</p>&#xA;"
31833886,26866479,4739075,2015-08-05T13:25:46,"<p>Api gateway pattern of Microservice <a href=""http://microservices.io/patterns/apigateway.html"" rel=""nofollow"">apigateway</a> is the first point from where you can start distributing or forwarding the calls to different services </p>&#xA;"
39916187,39916051,1729795,2016-10-07T11:30:50,"<p>Have you considered using spring boot actuator to retrieve health (and more) application information?</p>&#xA;&#xA;<p><a href=""http://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#production-ready"" rel=""nofollow"">http://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#production-ready</a></p>&#xA;&#xA;<p>You could then have another microservice that retrieves that information from each of your services, or just simply check it on then hitting the different endpoints (/health, /env, etc.).</p>&#xA;&#xA;<p><strong>UPDATE:</strong></p>&#xA;&#xA;<p>If you have you custom health logic you can even implement your own actuator endpoint for it. Furthermore, you can create your own library to reuse it in all your microservices:</p>&#xA;&#xA;<p><a href=""http://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#production-ready-customizing-endpoints-programmatically"" rel=""nofollow"">http://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#production-ready-customizing-endpoints-programmatically</a></p>&#xA;&#xA;<blockquote>&#xA;  <p><strong>46.4 Adding custom endpoints</strong> </p>&#xA;  &#xA;  <p>If you add a @Bean of type Endpoint then it will automatically be exposed over JMX and HTTP (if there is an&#xA;  server available). An HTTP endpoints can be customized further by&#xA;  creating a bean of type MvcEndpoint. Your MvcEndpoint is not a&#xA;  @Controller but it can use @RequestMapping (and @Managed*) to expose&#xA;  resources.</p>&#xA;  &#xA;  <p><strong>[Tip]</strong>&#xA;  If you are doing this as a library feature consider adding a&#xA;  configuration class annotated with @ManagementContextConfiguration to&#xA;  /META-INF/spring.factories under the key&#xA;  org.springframework.boot.actuate.autoconfigure.ManagementContextConfiguration.&#xA;  If you do that then the endpoint will move to a child context with all&#xA;  the other MVC endpoints if your users ask for a separate management&#xA;  port or address. A configuration declared this way can be a&#xA;  WebConfigurerAdapter if it wants to add static resources (for&#xA;  instance) to the management endpoints.</p>&#xA;</blockquote>&#xA;"
50121050,49944806,3972355,2018-05-01T17:32:16,"<p>I'd suggest to read below content before implementing if you have any doubt regarding usage of Eureka and Zuul, you will get all answers yourself.</p>&#xA;&#xA;<blockquote>&#xA;  <p><a href=""https://github.com/Netflix/eureka/wiki/Eureka-at-a-glance"" rel=""nofollow noreferrer"">https://github.com/Netflix/eureka/wiki/Eureka-at-a-glance</a></p>&#xA;  &#xA;  <p><a href=""https://github.com/Netflix/zuul/wiki"" rel=""nofollow noreferrer"">https://github.com/Netflix/zuul/wiki</a></p>&#xA;</blockquote>&#xA;&#xA;<p><em>As who are building microservices follows CI/CD approach, how developer verify working of their micro services before pushing code as we don't use eureka, zuul,ribbon,feign in production pcf.</em> </p>&#xA;&#xA;<p><strong>Answer to this question is:</strong> You must be aware of JUnit test cases, so you can run you test cases using deployment pipelines to make sure all your functionalities are working as expected or you can use Test Automation for the same.</p>&#xA;&#xA;<p><em>(how to simulate pcf environment in developer machine?).</em>&#xA;Answer to this one: &#xA;You can use eclipse plugin you are using eclipse/STS IDE. Or you can connect all PCF services from you local machine using <code>CloudFactory</code></p>&#xA;&#xA;<pre><code>@Bean&#xA;public Cloud cloud() {&#xA;    return new CloudFactory().getCloud();&#xA;}&#xA;</code></pre>&#xA;&#xA;<p><a href=""https://docs.pivotal.io/pivotalcf/2-1/buildpacks/java/sts.html"" rel=""nofollow noreferrer"">https://docs.pivotal.io/pivotalcf/2-1/buildpacks/java/sts.html</a></p>&#xA;"
50119793,50089242,3972355,2018-05-01T16:02:42,"<p>Finchley.M9 supports <code>Spring boot 2.0.0.RELEASE</code></p>&#xA;&#xA;<p>But I tried it on my local for <code>Spring boot 2.0.1.RELEASE</code> and it's working fine.</p>&#xA;&#xA;<p>Try with this combination.</p>&#xA;&#xA;<pre><code>dependencyManagement {&#xA;    imports {&#xA;        mavenBom ""io.pivotal.spring.cloud:spring-cloud-services-dependencies:1.4.1.RELEASE""&#xA;        mavenBom ""org.springframework.cloud:spring-cloud-dependencies:Finchley.M9""&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Here are few other useful links you might need to dig further.</p>&#xA;&#xA;<p><a href=""http://docs.pivotal.io/spring-cloud-services/1-4/common/client-dependencies.html"" rel=""nofollow noreferrer"">http://docs.pivotal.io/spring-cloud-services/1-4/common/client-dependencies.html</a>&#xA;<a href=""https://github.com/spring-projects/spring-cloud/wiki/Spring-Cloud-Finchley-Release-Notes"" rel=""nofollow noreferrer"">https://github.com/spring-projects/spring-cloud/wiki/Spring-Cloud-Finchley-Release-Notes</a>&#xA;<a href=""https://spring.io/blog/2018/03/23/spring-cloud-finchley-m9-has-been-released"" rel=""nofollow noreferrer"">https://spring.io/blog/2018/03/23/spring-cloud-finchley-m9-has-been-released</a></p>&#xA;"
37247122,37246989,1220355,2016-05-16T05:15:50,"<p>This is a great question, although it's way too broad to be answered meaningfully here without more specifics on the problem you're solving. </p>&#xA;&#xA;<p>However, if I were you, I'd start by reading the source code for existing API libraries. It's a fantastic way to learn about API design principles from very practical examples in products you likely use every day. </p>&#xA;&#xA;<p>Personally, I've found that the Stripe Ruby SDK (<a href=""https://github.com/stripe/stripe-ruby"" rel=""nofollow"">https://github.com/stripe/stripe-ruby</a> specifically, <code>lib/stripe/api_operations</code>), the Slack Ruby SDK (<a href=""https://github.com/dblock/slack-ruby-client"" rel=""nofollow"">https://github.com/dblock/slack-ruby-client</a>), AWS Ruby SDK (<a href=""https://github.com/aws/aws-sdk-ruby"" rel=""nofollow"">https://github.com/aws/aws-sdk-ruby</a> though this one's pretty big), and Github Ruby SDK (<a href=""https://github.com/aws/aws-sdk-ruby"" rel=""nofollow"">https://github.com/aws/aws-sdk-ruby</a>) are all well designed and worth studying.</p>&#xA;"
43951308,43950808,1876739,2017-05-13T09:12:16,"<p><strong>Theoretical Limitations</strong></p>&#xA;&#xA;<p>One important caveat to remember is the <a href=""https://en.wikipedia.org/wiki/CAP_theorem"" rel=""nofollow noreferrer"">CAP theorem</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p>In the presence of a partition, one is then left with two options:&#xA;  consistency or availability. When choosing consistency over&#xA;  availability, the system will return an error or a time-out if&#xA;  particular information cannot be guaranteed to be up to date due to&#xA;  network partitioning.</p>&#xA;</blockquote>&#xA;&#xA;<p>So by ""requiring"" that certain entities are consistent across multiple services you increase the probability that you will have to deal with timeout issues.</p>&#xA;&#xA;<p><strong>Akka Distributed Data</strong></p>&#xA;&#xA;<p>Akka has a <a href=""http://doc.akka.io/docs/akka/current/scala/distributed-data.html"" rel=""nofollow noreferrer"">distributed data module</a> to share information within a cluster:</p>&#xA;&#xA;<blockquote>&#xA;  <p>All data entries are spread to all nodes, or nodes with a certain&#xA;  role, in the cluster via direct replication and gossip based&#xA;  dissemination. You have fine grained control of the consistency level&#xA;  for reads and writes.</p>&#xA;</blockquote>&#xA;"
40378258,40377377,5033475,2016-11-02T10:58:34,"<p>I have personally used an Eureka discovery service. This is basically the ""master of puppets"" of the microservices, if you will. Each microservice registers itself to a separate microservice (the discovery service) on startup. The discovery service hence knows the address and port of each microservice and each microservice can ask the discovery service which (other) microservices are registered. In addition, each microservice can simply ask the discovery service for information about another microservice. All communication (in my case) was done with REST, but this is a choice, as Spring Boot with the Eureka discovery service dependency promotes it. </p>&#xA;&#xA;<p>With VERY little configuration you can get this whole framework to function.</p>&#xA;&#xA;<p>This is based on the framework used by Netflix. I believe Eureka is even a netflix library for that matter.</p>&#xA;"
36009851,35890054,1300227,2016-03-15T11:21:59,"<p>For this kind of testing I like to use <a href=""https://github.com/realestate-com-au/pact"" rel=""nofollow"">Pact</a>. (I know you said Python, but I couldn't find anything similar in that space, so I hope you (or other people searching) will find this excellent Ruby gem useful.)</p>&#xA;&#xA;<p>For testing from outside in, you can just use <a href=""https://github.com/bethesque/pact-provider-proxy"" rel=""nofollow"">the proxy component</a> - hope this at least gives you some ideas.</p>&#xA;"
48328438,48327939,6151939,2018-01-18T18:54:26,"<p>Check this out: <a href=""http://www.dotnettricks.com/learn/webapi/difference-between-wcf-and-web-api-and-wcf-rest-and-web-service"" rel=""nofollow noreferrer"">Difference between WCF and Web API and WCF REST and Web Service</a></p>&#xA;"
48328106,48327939,6151939,2018-01-18T18:34:05,<p>Why you want to use WCF for REST service when you can use Web API? Is there something reasonable?</p>&#xA;
31412954,31404820,257027,2015-07-14T17:02:35,"<p>Sure you can use DNS. Many times you should. But, if your needs are greater than what DNS offers and you need to build something custom on top of it, you can check already existing tools like ZooKeeper, Consul, etcd, Eureka and others.</p>&#xA;&#xA;<p>You might decide to use them if you have a problem they can solve.</p>&#xA;&#xA;<p>For example, you might want to detect when your service goes down or loses connectivity - and remove the affected endpoints from the list of your services. All of the mentioned tools offer this out of the box.</p>&#xA;&#xA;<p>There is a nice writeup by Spotify relevant to this topic:&#xA;<a href=""https://labs.spotify.com/2013/02/25/in-praise-of-boring-technology"" rel=""nofollow"">https://labs.spotify.com/2013/02/25/in-praise-of-boring-technology</a></p>&#xA;"
43570283,27865238,129795,2017-04-23T11:00:28,<p>Here there is one issue with dependency and dependency management. Say one of your micro service wants to upgrade to newer version of common for some reason...you cant do that as you have parent. I do understand temptation of reducing duplication of redundant things like plugin configuration. In micro service we need to think more about independence of each service. </p>&#xA;&#xA;<p>Some config like say your repository or release configuration etc can be common. </p>&#xA;
46099037,46098754,626692,2017-09-07T14:33:18,"<p>This is a super broad topic, and rather hard to answer in general terms.</p>&#xA;&#xA;<p>However...</p>&#xA;&#xA;<p>A key requirement for a micro service architecture is that each service should be independent from the others. You should be able to deploy, modify, improve, scale your micro service independently from the others.</p>&#xA;&#xA;<p>This means you do not want to share anything other than API definitions. You certainly don't want to share a schema; each service should be able to define its own schema, release new versions, change data types etc. without having to check with the other services. That's almost impossible with a shared schema.</p>&#xA;&#xA;<p>You may not want to share a physical server. Sharing a server means you cannot make independent promises on scalability and up-time; a big part of the micro service approach means that the team that builds it is also responsible for running it. You really want to avoid the ""well, it worked in dev, so if it doesn't scale on production, it's the operations team's problem"" attitude. Databases - especially clustered, redundant databases - can be expensive, so you <em>might</em> compromise on this if you really need this.</p>&#xA;&#xA;<p>As most microservice solutions use containerization and cloud hosting, it's quite unlikely that you'd have the ""one database server to rule them all"" sitting around. You may find it much better to have each micro service run its own persistence service, rather than sharing.</p>&#xA;&#xA;<p>The common approach to dealing with inconsistencies is to accept them - but to use <a href=""https://martinfowler.com/bliki/CQRS.html"" rel=""nofollow noreferrer"">CQRS</a> to distribute data between microservices, and make sure the micro services deal with their internal consistency requirements.</p>&#xA;&#xA;<p>This also deals with the ""should I upgrade my database when I release a new version?"" question. If your observers understand the version for each message, they can make decisions on how to store them. For instance, if version 1.0 uses a different set of attributes to version 1.1, the listener can do the mapping.</p>&#xA;&#xA;<p>In the comments, you ask about consistency. This is a super complex topic - especially in micro service architectures.</p>&#xA;&#xA;<p>If you have, for instance, a ""customers"" service and an ""orders"" service, you must make sure that all orders have a valid customer. In a monolithic application, with a single database, and exclusively synchronous interactions, that's easy to enforce at the database level.</p>&#xA;&#xA;<p>In a micro service architecture, where you might have lots of data stores, with no dependencies on each other, and a combination of synchronous and asynchronous calls, it's really hard. This is an inevitable side effect of reducing dependencies between micro services.</p>&#xA;&#xA;<p>The most common approach is ""<a href=""https://en.wikipedia.org/wiki/Eventual_consistency"" rel=""nofollow noreferrer"">eventual consistency</a>"". This typically requires a slightly different application design. For instance, on the ""orders"" screen, you would invoke first the client microservice (to get client data), and then the ""orders"" service (to get order details), rather than have a single (large) service call to retrieve everything.</p>&#xA;"
50628698,50627087,626692,2018-05-31T16:18:54,"<p>There are 3 obvious solutions.</p>&#xA;&#xA;<p>The simplest, cleanest and fastest is to use foreign keys and joins between your ""posts"" database and your ""users"" database. In this case, when you show a list of posts, you can get both the post and user data in a single query, and there's no need to keep things up to date.</p>&#xA;&#xA;<p>The next option is to store a copy of the user data alongside your posts. This leads to entertaining failure modes - data in the user database may get out of sync. However, this is a fairly common strategy when using 3rd party authentication systems (e.g. logging on with your Google/Facebook/Github/Stack Exchange credentials). The way to make this work is to minimize the amount of data you duplicate, and have it be safe if it's out of date. For instance, a user's display name is probably okay; current bank account balance is probably not. </p>&#xA;&#xA;<p>The final option is to store the primary key for users in your posts database, and to retrieve the user data at run time. This is less likely to lead to bugs with data getting out of sync, but it can cause performance problems - retrieving user details for 1000 posts one by one is obviously much slower than retrieving everything through a joined query.</p>&#xA;&#xA;<p>The choice then is ""do I have a service which combines post and user data and my UI retrieves everything from that service, or do I let the UI retrieve posts, and then users for each post"". That's mostly down to the application usage, and whether you can use asynchronous calls to retrieve user information. If at all possible (assuming you're building a web application), the simplest option might be to return the posts and user IDs and use Ajax requests to retrieve the user data as needed.</p>&#xA;&#xA;<p>The <a href=""https://martinfowler.com/bliki/CQRS.html"" rel=""nofollow noreferrer"">CQRS approach</a> (common to microservice architectures) provides some structure for this.</p>&#xA;"
40381402,40377076,4391429,2016-11-02T13:39:47,<p>I build zulu server.&#xA;When i put in application.properties</p>&#xA;&#xA;<pre><code>zuul.routes.name-service.url=http://localhost:8888&#xA;ribbon.eureka.enabled=true&#xA;server.port=8080&#xA;</code></pre>&#xA;&#xA;<p>works.</p>&#xA;&#xA;<p>How to connect to read data from eureka registry?&#xA;I tried like this but not working </p>&#xA;&#xA;<pre><code>zuul.routes.name-service.path=/name-service/**&#xA;zuul.routes.name-service.serviceId=name-service&#xA;eureka.client.serviceUrl.defaultZone=http://localhost:8761 &#xA;</code></pre>&#xA;
46825394,46824730,927493,2017-10-19T08:13:20,"<p>The main reasons to have a multi-module project is to manage dependencies (in the maven sense) between the different modules (and build them together in the right order). Your microservices may call each other, but are not dependent on each other in the Maven sense. So I would not use multi-module projects for them.</p>&#xA;&#xA;<p>Nevertheless, you can define a parent pom for your microservices that defines common configuration and dependencies.</p>&#xA;"
47197102,47184194,927493,2017-11-09T08:24:42,"<p>The Maven way to do this is to have a parent POM. You can either construct a multi-module project or have an external parent POM.</p>&#xA;&#xA;<p>Maven has no way of ""importing xml"" into a given POM, except for the <code>&lt;scope&gt;import&lt;/scope&gt;</code> for dependencyManagement. So you cannot define a set of plugins, properties etc. and just import them.</p>&#xA;"
39416543,39416301,4734725,2016-09-09T17:05:44,"<p>Probably no orchestation but there is a pattern called API Gateway</p>&#xA;&#xA;<blockquote>&#xA;  <p>Using an API Gateway</p>&#xA;  &#xA;  <p>Usually a much better approach is to use what is known as an API&#xA;  Gateway. An API Gateway is a server that is the single entry point&#xA;  into the system. It is similar to the Facade pattern from&#xA;  object-oriented design. The API Gateway encapsulates the internal&#xA;  system architecture and provides an API that is tailored to each&#xA;  client. It might have other responsibilities such as authentication,&#xA;  monitoring, load balancing, caching, request shaping and management,&#xA;  and static response handling. pattern call API Gateway</p>&#xA;</blockquote>&#xA;&#xA;<p><a href=""https://www.nginx.com/blog/building-microservices-using-an-api-gateway/"" rel=""nofollow"">https://www.nginx.com/blog/building-microservices-using-an-api-gateway/</a></p>&#xA;&#xA;<p><a href=""http://microservices.io/patterns/apigateway.html?utm_source=building-microservices-using-an-api-gateway&amp;utm_medium=blog"" rel=""nofollow"">http://microservices.io/patterns/apigateway.html?utm_source=building-microservices-using-an-api-gateway&amp;utm_medium=blog</a></p>&#xA;&#xA;<p><a href=""https://www.nginx.com/blog/microservices-reference-architecture-nginx-proxy-model/"" rel=""nofollow"">https://www.nginx.com/blog/microservices-reference-architecture-nginx-proxy-model/</a></p>&#xA;"
34344908,34341322,4734725,2015-12-17T21:50:22,"<p>You need a queue, the queue can be independent service (RabbitMq, Hornet, etc) o you can use a queue inside your notification service.</p>&#xA;&#xA;<ol>&#xA;<li><p>Independent Queue, in this case you can use a message broker in the middle of the services, the frontendservices send the message to the queue and the notification service read the message from the queue and then send it.</p></li>&#xA;<li><p>Internal Queue, you can have in your notification service an internal queue, like the queues in SpringIntegration. Whe the service receive a message inmediately put it on the queue and return an OK code. The frontendservice doesn't need to wait until the notification is send. Then the notification service process the messages in its queue on its own pace.</p></li>&#xA;</ol>&#xA;"
32168162,32167881,3555845,2015-08-23T15:10:34,<p><code>locations</code> are processed in order. The <code>/</code>-location matches before <code>/products</code> so the later is never reached. Put <code>/</code> at the end of the config file.</p>&#xA;
50970304,50873541,346629,2018-06-21T13:53:10,"<p>My conclusion is: JSend <em>is</em> the most widely adopted wrapper format for json responses, so that is the way to go.</p>&#xA;"
43544890,43532494,24069,2017-04-21T14:10:51,"<p>I explain here in this <a href=""https://youtu.be/-j6cNZc5wYM?t=41m11s"" rel=""nofollow noreferrer"">devoxx video</a> how clustered caching can help each of you docker instance share the same cache</p>&#xA;"
48781128,48726644,2257925,2018-02-14T06:41:03,"<p>I solved the problem by create extended EurekaDiscoveryClient just for toLowerCase(Locale.ROOT) option. </p>&#xA;&#xA;<pre><code>import com.netflix.appinfo.EurekaInstanceConfig;&#xA;import com.netflix.discovery.EurekaClient;&#xA;import com.netflix.discovery.shared.Application;&#xA;import com.netflix.discovery.shared.Applications;&#xA;import org.springframework.beans.factory.annotation.Qualifier;&#xA;import org.springframework.cloud.client.discovery.DiscoveryClient;&#xA;import org.springframework.cloud.netflix.eureka.EurekaDiscoveryClient;&#xA;import org.springframework.stereotype.Component;&#xA;&#xA;import java.util.*;&#xA;&#xA;@Component&#xA;public class CustomEurekaDiscoveryClient extends EurekaDiscoveryClient implements DiscoveryClient {&#xA;&#xA;&#xA;    private final EurekaInstanceConfig config;&#xA;&#xA;    private final EurekaClient eurekaClient;&#xA;&#xA;    public CustomEurekaDiscoveryClient(EurekaInstanceConfig config, @Qualifier(""eurekaClient"") EurekaClient eurekaClient) {&#xA;        super(config, eurekaClient);&#xA;        this.config = config;&#xA;        this.eurekaClient = eurekaClient;&#xA;    }&#xA;&#xA;    @Override&#xA;    public List&lt;String&gt; getServices() {&#xA;        Applications applications = this.eurekaClient.getApplications();&#xA;        if (applications == null) {&#xA;            return Collections.emptyList();&#xA;        }&#xA;        List&lt;Application&gt; registered = applications.getRegisteredApplications();&#xA;        List&lt;String&gt; names = new ArrayList&lt;&gt;();&#xA;        for (Application app : registered) {&#xA;            if (app.getInstances().isEmpty()) {&#xA;                continue;&#xA;            }&#xA;            names.add(app.getName().toLowerCase(Locale.ROOT));&#xA;&#xA;        }&#xA;        return names;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Upper case 'I' transform to lower case 'ı' not 'i' in Turkish OS and jvm that's why i need to override the <strong>public List getServices()</strong> method. </p>&#xA;&#xA;<p>If the toLowerCase(Locale.ROOT) method calls without <strong>Locale.ROOT</strong> java transform character to 'ı' but use the <strong>Locale.ROOT</strong> option, method transform to 'i' and project works fine.</p>&#xA;"
48607158,48604664,1014789,2018-02-04T10:35:52,"<p>When dealing with a mircoservice architecture it is usually the case that you have a distributed system. </p>&#xA;&#xA;<p>Most microservices that communicate with each other are not on the same machine, instance or container. Communication between them is most commonly done via http, though there are many other ways. </p>&#xA;&#xA;<p>I would suggest designing mircoservices around a single concern of your application. For example, in your case, you could have a ""persistence microservice"" that would be responsible for dealing with data persistence operations on a single or multiple types data-stores. It could possibly deal with relational DBs, noSQL, file storage etc. Then, via REST endpoints, you can expose any persistence functionality to the mircoservices that deal with business logic.</p>&#xA;&#xA;<p>A very easy way to build a REST service like this would be with the help of <a href=""https://projects.spring.io/spring-data-rest/"" rel=""nofollow noreferrer"">Spring Data REST</a> project.</p>&#xA;&#xA;<p>To answer your actual question, I'm not aware of any way to share actual connections between processes. Beyond that, having many microservices running on the same instance is not a good practice most of the time. </p>&#xA;&#xA;<p>Mircoservices are very popular these days and everybody is trying to transition to them. My advice would be to make sure you don't ""over-engineer"" your project.</p>&#xA;&#xA;<p>Hope I didn't misunderstand your question, but to be fair it is a little vague. If you could provide a longer more detailed description of your architecture and use case I can suggest more tools/frameworks you can use to achieve your <em>cloudy</em> goals.</p>&#xA;"
49158518,49158331,5210117,2018-03-07T18:02:55,"<p>There are couple of options;  </p>&#xA;&#xA;<ul>&#xA;<li>On the dependent projects pom.xml execute <code>mvn&#xA;versions:use-latest-releases</code> command to automatically upgrade&#xA;dependency versions.</li>&#xA;<li>If all the projects are inheriting from same parent(and dependency configuration is done through dependency management), you can use <code>mvn versions:set -DnewVersion=xx</code> on the aggregator pom.xml to upgrade versions.</li>&#xA;<li>If versioning will happen quite often, Snapshot versions can be used.</li>&#xA;</ul>&#xA;"
41624699,41624628,1147141,2017-01-12T23:09:42,<p>What you probably need is an stub rest service. Create a webapp that takes the expected output using a path that is not part of the public api. When you invoke the public api it sends what it just received</p>&#xA;
39687196,39672919,4107781,2016-09-25T13:10:16,"<p>The most important aspect of designing microservices is the separation of concerns which means each microservice will have a defined boundary under which they need to work.</p>&#xA;&#xA;<p>Each microservice is designed to do a defined work so, first you need to find the independent functionalities in you project and try to create a microservice for it.</p>&#xA;&#xA;<p>The most important thing to note is you should first start with a monolithic architecture and if you identify that some functionalities needs to be separated then you can create a microservice out of it.</p>&#xA;&#xA;<p>As far as sails is considered then it is a good candidate for MVC and if the project is monolithic but if the number of microservices is large then it is not a good choice because running large number of microservices with sails.js will consume more of your system RAM.Sails.js internally uses so many libraries which you will not need. You can make a simple microservice with just node.js core modules and they will consume less memory too.</p>&#xA;&#xA;<p>Also when each microservices handles small functionalities so the amount of &#xA;code will be less and there is no need for mvc arcitecture. you can use less number libraries to create it.</p>&#xA;&#xA;<p>Conclusion</p>&#xA;&#xA;<ol>&#xA;<li>If number of services is less and you don't worry about system RAM then go for multiple sails application.</li>&#xA;<li>If number of services going to be more then try to make your services without using sails</li>&#xA;</ol>&#xA;"
51081573,50986816,1366533,2018-06-28T11:18:01,"<p>Below is a very bare-bones example how you could implement the <strong>UI Service</strong> so it works with a normal HTTP Request/Response flow. It uses the node.js <code>events.EventEmitter</code> class to ""route"" the responses to the right HTTP handler.</p>&#xA;&#xA;<p>Outline of the implementation:</p>&#xA;&#xA;<ol>&#xA;<li><p>Connect producer/consumer client to Kafka</p>&#xA;&#xA;<ol>&#xA;<li>The producer is used to send the request data to the internal micro-services</li>&#xA;<li>The consumer is used to listen for data from the micro-services that means the request has been processed and I assume those Kafka items also contain the data that should be returned to the HTTP client. </li>&#xA;</ol></li>&#xA;<li><p>Create a global <strong>event dispatcher</strong> from the <code>EventEmitter</code> class</p></li>&#xA;<li>Register a HTTP request handler that&#xA;&#xA;<ol>&#xA;<li>Creates an UUID for the request and includes it in the payload pushed to Kafka</li>&#xA;<li>Registers a event listener with our event dispatcher where the UUID is used as the event name that it listens for</li>&#xA;</ol></li>&#xA;<li>Start consuming the Kafka topic and retrieve the UUID that the HTTP request handler is waiting for and emit an event for it. In the example code I am not including any payload in the emitted event, but you would typically want to include some data from the Kafka data as an argument so the HTTP handler can return it to the HTTP client.</li>&#xA;</ol>&#xA;&#xA;<p>Note that I tried to keep the code as small as possible, leaving out error and timeout handling etc! </p>&#xA;&#xA;<p>Also note that <code>kafkaProduceTopic</code> and <code>kafkaConsumTopic</code> are the same topics to simplify testing, no need for another service/function to produce to the <strong>UI Service</strong> consume topic.</p>&#xA;&#xA;<p>The code assumes the <code>kafka-node</code> and <code>uuid</code> packages have been <code>npm</code> installed and that Kafka is accessible on <code>localhost:9092</code></p>&#xA;&#xA;<pre><code>const http = require('http');&#xA;const EventEmitter = require('events');&#xA;const kafka = require('kafka-node');&#xA;const uuidv4 = require('uuid/v4');&#xA;&#xA;const kafkaProduceTopic = ""req-res-topic"";&#xA;const kafkaConsumeTopic = ""req-res-topic"";&#xA;&#xA;class ResponseEventEmitter extends EventEmitter {}&#xA;&#xA;const responseEventEmitter = new ResponseEventEmitter();&#xA;&#xA;var HighLevelProducer = kafka.HighLevelProducer,&#xA;    client = new kafka.Client(),&#xA;    producer = new HighLevelProducer(client);&#xA;&#xA;var HighLevelConsumer = kafka.HighLevelConsumer,&#xA;    client = new kafka.Client(),&#xA;    consumer = new HighLevelConsumer(&#xA;        client,&#xA;        [&#xA;            { topic: kafkaConsumeTopic }&#xA;        ],&#xA;        {&#xA;            groupId: 'my-group'&#xA;        }&#xA;    );&#xA;&#xA;var s = http.createServer(function (req, res) {&#xA;    // Generate a random UUID to be used as the request id that&#xA;    // that is used to correlated request/response requests.&#xA;    // The internal micro-services need to include this id in&#xA;    // the ""final"" message that is pushed to Kafka and consumed&#xA;    // by the ui service&#xA;    var id = uuidv4();&#xA;&#xA;    // Send the request data to the internal back-end through Kafka&#xA;    // In real code the Kafka message would be a JSON/protobuf/... &#xA;    // message, but it needs to include the UUID generated by this &#xA;    // function&#xA;    payloads = [&#xA;        { topic: kafkaProduceTopic, messages: id},&#xA;    ];&#xA;    producer.send(payloads, function (err, data) {&#xA;        if(err != null) {&#xA;            console.log(""Error: "", err);&#xA;            return;&#xA;        }&#xA;    });&#xA;&#xA;    responseEventEmitter.once(id, () =&gt; {&#xA;        console.log(""Got the response event for "", id);&#xA;        res.write(""Order "" + id + "" has been processed\n"");&#xA;        res.end();&#xA;    })&#xA;});&#xA;&#xA;s.timeout = 10000;&#xA;s.listen(8080); &#xA;&#xA;// Listen to the Kafka topic that streams messages&#xA;// indicating that the request has been processed and&#xA;// emit an event to the request handler so it can finish.&#xA;// In this example the consumed Kafka message is simply&#xA;// the UUID of the request that has been processed (which&#xA;// is also the event name that the response handler is&#xA;// listening to).&#xA;//&#xA;// In real code the Kafka message would be a JSON/protobuf/... message&#xA;// which needs to contain the UUID the request handler generated.&#xA;// This Kafka consumer would then have to deserialize the incoming&#xA;// message and get the UUID from it. &#xA;consumer.on('message', function (message) {&#xA;    responseEventEmitter.emit(message.value);&#xA;});&#xA;</code></pre>&#xA;"
37282319,37143689,5310982,2016-05-17T17:02:39,"<p>I found the same behavior when using the latest JHipster Registry pulled from github running locally with IntelliJ IDEA -- the server would start but the frontend would not serve up.</p>&#xA;&#xA;<p>It turned out to be an issue with a blank working directory in the Run/Debug Configuration.  Once that is populated, things work find from the IDE.</p>&#xA;"
52101885,52098392,3871750,2018-08-30T16:39:59,"<p>Internally, use kubernetes service of ClusterIp type (can also be headless). Externally use service of NodePort or LoadBalancer type to expose your application directly, or Ingress (assuming you have ingress controller deployed/available) to define HTTP(S) based routing from external HTTP(S) level loadbalancer by vhost/path.</p>&#xA;"
51317735,51317424,1584390,2018-07-13T04:37:49,"<p>ESHOP_OCELOT_VOLUME_SPEC which is an environment variable. The variable value may be exported/set in some place of the code or in the instance. ESHOP_OCELOT_VOLUME_SPEC will be replaced with value, that's why you where not able to see ESHOP_OCELOT_VOLUME_SPEC in docker instead the value in ESHOP_OCELOT_VOLUME_SPEC.  </p>&#xA;"
40539879,39791667,148998,2016-11-11T01:36:29,"<p><strong>Nope this is <em>NOT</em> a bad idea, in fact this is a great idea, let me explain.</strong> </p>&#xA;&#xA;<p>First consider your bounded contexts, in reality they should know nothing about each other, even when multiple <code>Contexts</code> are working together to create a complete solution, all a context <code>knows</code> about is itself, its <em>OWN</em> concerns.</p>&#xA;&#xA;<p>Take an <code>OnboardingContext</code> responsible for when a new employee starts at a company, here an <code>Employee</code> entity is added for the first time into the system. Here the employee has a Name, Phonenumber, Start date, Address, Marital Status and more.</p>&#xA;&#xA;<p>Consider the <code>PayrollContext</code>, it too has an <code>Employee</code> entity but here all this entity has is an Id, a Salary, Start Date and End Date - here it doesn't care about the address, or marital status, it doesn't even necessarily care about the Name, becuase in <em>THIS</em> context name is not important, only the Salary, Start Date and End Date.</p>&#xA;&#xA;<p>So if the two contexts, shouldn't know about each other, but maybe care about some information relating to both, how do we get the fact that a new <code>Employee</code> has started and needs to get paid?</p>&#xA;&#xA;<p><strong>Domain Events</strong></p>&#xA;&#xA;<p>Domain events are used with distributed systems. The model of course, will become more complex, but also more scalable. Domain Events are used in an <a href=""https://en.wikipedia.org/wiki/Event-driven_architecture"" rel=""nofollow noreferrer"">event driven architecture</a></p>&#xA;&#xA;<p><strong>Heres how it might work</strong> </p>&#xA;&#xA;<ol>&#xA;<li>A new employee starts and is added to the system in the <code>OnboardingContext</code>, everything is correct and the model is persisted successfully.</li>&#xA;<li>The <code>OnboardingContext</code> raises an event, the event is called <code>NewEmployeeEvent</code></li>&#xA;</ol>&#xA;&#xA;<p>Thats it for the <code>OnboardingContext</code> as far as it is concerned it is done. It's handled the new employee, saved it, and raised a nifty system wide event to say that something (an event) has occured.</p>&#xA;&#xA;<p>Now to the <code>PayrollContext</code></p>&#xA;&#xA;<ol>&#xA;<li>The <code>PayrollContext</code> is interested in a couple of things, in particular it want's to know when a new emplyee starts.</li>&#xA;<li>It subscribes to the <code>NewEmployeeEvent</code> this event is of a common type, it doesn't know where the event comes from, in fact it could come from anywhere, but it is interested in a little bag of data, or inormation on that event.</li>&#xA;<li>When the event is raised this context <code>handles</code> that event, in this case grabbing pertinent info about the Salary and Employee Number, it persists this to it's own data store for use later during payroll.</li>&#xA;</ol>&#xA;&#xA;<p>Boom done. </p>&#xA;&#xA;<p>Your system now listens to and responds to events, these flow throughout your system, in any direction, in all directions. </p>&#xA;&#xA;<p>When something of interest happens and event is raised, anyone (context) interested in that event subscribes, handles and does what it want's with the data relating to that event.</p>&#xA;&#xA;<p><strong>So how do you do this?</strong></p>&#xA;&#xA;<p>There is lots of reading to do, just google DDD and Domain Events.</p>&#xA;&#xA;<p>You will come across a lot of articles from Jimmy Bogard (<a href=""https://lostechies.com/jimmybogard/2014/05/13/a-better-domain-events-pattern/"" rel=""nofollow noreferrer"">a better domain events pattern</a>) and Udi Dahan (<a href=""http://udidahan.com/2009/06/14/domain-events-salvation/"" rel=""nofollow noreferrer"">Domain Events Salvation</a>) on the subject</p>&#xA;&#xA;<p>Take a look at <a href=""https://particular.net/nservicebus"" rel=""nofollow noreferrer"">nservicebus</a> (paid for) and <a href=""http://masstransit-project.com/"" rel=""nofollow noreferrer"">masstransit</a> (open source) they are fantastic out of the box eventing, and messaging systems.</p>&#xA;&#xA;<p>Nservice bus has some great videos on the subject at <a href=""https://docs.particular.net/nservicebus/architecture/principles"" rel=""nofollow noreferrer"">https://docs.particular.net/nservicebus/architecture/principles</a></p>&#xA;"
44581341,44579396,4883718,2017-06-16T05:18:24,"<p>Actually there's no one answers to this question. It depends :-)</p>&#xA;&#xA;<p>With RPC from one hand you have more tied relation between services but from another hand you have more simple request/response communication model. So, one service sending request  expects some kind of response from another or timeout if it isn't accessible. RPC with TCP also has session and bidirectional communication way.</p>&#xA;&#xA;<p>With pub/sub model one service sends some message about some happened event and doesn't care how this message will be handled by another services.  Or service sends message to one particular service and doesn't expect response from it - (one-way request). Of course second service may send a message to the first service too passing some work results. </p>&#xA;&#xA;<p>Loosely coupled is not a goal itself - it's a way how to make system more reliable, stable, scalable but it comes with some price, work overheads, in some cases it's impossible or simply is unnecessary.</p>&#xA;&#xA;<p>So, depending of your need you can choose the first or the second approach. The RPC is used when you can't simply send request and forget or it has no sense, you need response anyway. RPC is easier to implement. Pub/sub is good for some kind of ""heavy"" background jobs like image, video, file processing, sending emails and so on where you send request and it will be processed in queue. With pub/sub you can utilize resources more efficiently. </p>&#xA;"
44612071,44610425,4883718,2017-06-18T05:38:40,"<p>It's better to avoid exposing 500 status explicitly but in some cases it's necessary. A user works with your system not with particular service and for him it doesn't matter what is inside. Internal system implementation can vary but user interaction can stay the same.</p>&#xA;&#xA;<p>Let's A will be for instance a e-commerce service, B - billing service and C - billing gateway. User buys a product via A which send billing request to B and B communicates with C to perform transaction. 401 between B and C can be for different reasons. If it is simply internal configuration problem (not updated password, expired certificate and so on) it is an internal system bug and you need to tell user that service is unavailable now or something like that, not to pass all internal error details of course. You can use 5xx code in this case. Perhaps service B can put request to some kind of queue and tell service A that's everything is OK, your request will be processed later. But if it is because of user tries to use bad credit card or don't have enough money (not authorized request) A needs to show the proper message and 4xx response code.</p>&#xA;&#xA;<p>In general a service exposes resources and it doesn't matter how many internal or external services, databases, data sources and so on are behind it. Perhaps 401 between B and C means for B to go to D service (C alternate) and A  service shouldn't know about 401 at all. So, it depends on what you need to expose to user and how you need to handle different cases. </p>&#xA;"
44617319,44582199,4883718,2017-06-18T16:49:32,"<p>Microservices shouldn't be a goal itself, it's an approach to achieve something. So, you need to answer the question what goals do you want to achieve? </p>&#xA;&#xA;<p>One reason why people start using microservices in front-end is scalability. If your application is big enough and many teams work with one frontend it's better to use microservices in order to reduce dependencies between teams, making development more efficient - every team can develop its part independently. For instance Microsoft Azure Portal is a good example for this. It has a lot of frontend code and many teams work simultaneously on it.</p>&#xA;&#xA;<p>But important thing here is deployment. Mostly frontend is deployed as one component, frontend microservices are not fully microservices according to classic definition. Consider looking at Angular 2 modules. Often people use  microservice term in frontend as a module actually, not dedicated component which can be developed and deployed separately.</p>&#xA;&#xA;<p>If your application is small enough (3-4 devs) and apparently won't grow to a few dedicated frontend teams then there's no point of using microservices in frontend at all adding additional complexity.</p>&#xA;&#xA;<p>Your first option can be used when isolation between frontend parts are huge, there's no interaction between them. For instance you have a few applications which should look for user as one having the same style, components and single sign-on but these applications are totally different. Then it's better to locate them in different urls, different services (perhaps even with separate deployment) sharing common styles and components between them. That's easier to implement.</p>&#xA;&#xA;<p>A master application can be used when parts are tied to each other and it's really one frontend application. But this approach is harder to implement and requires more interaction between teams.</p>&#xA;&#xA;<p>The third approach takes also place as described in mentioned <a href=""https://stackoverflow.com/questions/39904153/front-end-micro-services-with-angular-2?noredirect=1&amp;lq=1"">post</a>.</p>&#xA;"
41837882,41837637,4883718,2017-01-24T19:59:24,"<p>Microservice and layered architecture are a little bit different things. Microservice architecture it’s about how your application is constructed, what components (services) it has and how these services communicate with each other, how they are developed, deployed and so on. </p>&#xA;&#xA;<p>Multi-layered architecture it’s about logical dividing application into layers where every layer has its own logical function (presentation, domain and so on).  Very often multi-layered architecture is related to monolithic architecture and service design.</p>&#xA;&#xA;<p>According to your description you aren’t going to break down your layers, your architect wants to split logic into different services. This two architecture styles can be used together. For instance, you can have 3 services and every service can have presentation, domain and service layers. If your current layers in one service are heavy enough it makes sense to split them out making development and testing easier. Backend for frontend style has also its benefits especially if you want to add mobile app. </p>&#xA;&#xA;<p>About this</p>&#xA;&#xA;<blockquote>&#xA;  <p>Is multi-layered architecture out of fashion already?</p>&#xA;</blockquote>&#xA;&#xA;<p>No, they are used both but with microservices as a rule layers should be much thinner than in monolithic application.</p>&#xA;&#xA;<blockquote>&#xA;  <p>What benefits and problems can bring such architecture design for my&#xA;  application?</p>&#xA;</blockquote>&#xA;&#xA;<p>I recommend you look at <a href=""https://www.mulesoft.com/resources/api/microservices-vs-monolithic"" rel=""nofollow noreferrer"">comparison between microservice and monolithic architecture</a> styles and this <a href=""https://stackoverflow.com/questions/33041733/microservices-vs-monolithic-architecture"">post</a>. To divide or not you should consider size and complexity of your project, size of your team. Dividing has to bring benefits to whole application making development easier. Monolithic application has it’s own benefits and up to some size of project it can be a good decision. Of course, it’s a nightmare working with huge monolithic application as well as with one hundred very small (nano) services.</p>&#xA;"
41868152,41853686,4883718,2017-01-26T06:49:41,"<p>If you started using containers, than it's the right way to go. But if you your app is deployed in cloud (AWS, Azure and so on) it's better to use cloud queue service which is already configured, is updated automatically, has monitoring and so on.</p>&#xA;&#xA;<p>I'd like also to point out that docker containers it's only a way to deploy your application components. Application shouldn't take care about how your components (services, dbs, queues and so on) are deployed. For app service a message queue is simply a service located somewhere, accessible by connection parameters.</p>&#xA;"
41765670,41745964,4883718,2017-01-20T14:14:50,"<p>Frontend and backend API, two services (components) – it’s already some kind of microservice architecture. The questions are how big your components, what kind of logic they have, will you have benefits if you split some logic to different services?</p>&#xA;&#xA;<p>Per microservice architecture every service (component) of your system should have some dedicated logic (domain), solves some related problems, persists data to its own data-store, can be developed and deployed separately. In some cases, data-store can be shared between services.</p>&#xA;&#xA;<p>So, the goal of splitting logic into different services is making your application easier to develop, maintenance, support and understand. Too many small services can bring a lot of overheads. To create a service, you need to spend additional development time, a service is also a deployment item, communication between services has network overheads. So, you should careful consider all pros and cons of splitting logic into services. Some balance should be found.</p>&#xA;&#xA;<p>Going back to the question if ""Users"" and ""Purchase History"" are totally different, they don’t have common logic, can be stored in different databases and both are complicated enough, so you can split them into two services. The same about UI parts. The main thing is that splitting should bring you benefits - not overheads.</p>&#xA;&#xA;<p>About using rest - it’s up to you, rest architecture is not required for microservice architecture but very often they are used together. Rest is about design of your services, how they expose API and so on.</p>&#xA;"
41652720,41433856,4883718,2017-01-14T17:12:14,"<p>Sending emails it's like making request to another service (via SMTP). So, that's a good approach when every service will be able to send emails. </p>&#xA;&#xA;<p>But, of course there's some common logic for sending emails like rendering templates, sending code, configurations and so on. This logic should be shared between services via common code (dll, package and so on).</p>&#xA;&#xA;<p>So, in this way:</p>&#xA;&#xA;<ol>&#xA;<li>Every service doesn't depend on another service when it needs to send an email</li>&#xA;<li>Common code for sending email is shared between services</li>&#xA;<li>You don't have development, deployment and network overheads in the case of having dedicated email sending serviced </li>&#xA;</ol>&#xA;&#xA;<p>One drawback of this approach is that every service should have the same email configuration (SMTP address, login, password and so on). But if you share configurations between all services it's not a problem.</p>&#xA;"
43033574,43032883,560902,2017-03-26T19:36:13,"<p>Use RSA for your JWT generation. You can make the public key available to all other microservices. If the client uses CORS to talk to different services directly (vs. an API gateway), pack the JWT in the Authorization header as usual.</p>&#xA;"
52031373,52031350,5483526,2018-08-27T00:52:56,"<p>The answer is that your API just doesn't expose any methods that let you create an invalid state.</p>&#xA;&#xA;<p>Each method has to do a complete and valid transaction.  Instead of having methods to add and remove money from accounts, for example, you have methods that transfer money between accounts.  Any record keeping or notifications that must occur when you transfer money has to be done (or at least queued up) by those methods, too.</p>&#xA;&#xA;<p>Sometimes this requires you to create special ""valid"" states for transactions that you might otherwise consider to be incomplete.  When you're reserving seats in theater, for example, there is a state where your seats are reserved until some time (5 minutes or so) when you haven't paid for them.</p>&#xA;"
47171453,47162798,5483526,2017-11-08T04:11:55,"<p>Well, solution 2 is out, because you're going to use categories for lots of things in different services and it would ruin the separation of concerns to make the category service implement searches in all those different domains.</p>&#xA;&#xA;<p>Also, a realistic product or user search could include <em>other criteria</em> in addition to category. It's not usually a good implementation to have services for all those various criteria implement their own product searches, because merging the results in a multi-criteria search can be costly, so this pattern of implementing the search in the service that defines the criteria object does not scale.</p>&#xA;&#xA;<p>Solution 1 is close, but there's no reason to make the product service know about or call the user service.  The product service should have an API that searches for products using various criteria, including category.  It would probably be better to implement the category search by having the client pass a list of category IDs instead of a user ID.  Then it doesn't have to call the user service itself, and you keep the product and user services independent.  Also, searching for products by category is useful for other things (like browsing products!), and since you didn't tie the category search to users, you can use the same API to handle those other cases.</p>&#xA;"
28350490,28329738,47110,2015-02-05T17:36:47,"<p>Looks like you may want to look into <a href=""http://blog.docker.com/2014/12/announcing-docker-machine-swarm-and-compose-for-orchestrating-distributed-apps/"" rel=""nofollow"">docker swarm</a>, they're actively working on these use cases. I wouldn't recommend building your own communication channel, stick with http or maybe use spdy if you're really concerned about performance. Anything you introduce will make using these upcoming solutions more difficult. Also keep in mind you don't need a heavy-duty web server in most cases, you can always introduce a layer above one or more of your services using nginx or haproxy for example.</p>&#xA;"
52076758,52076009,6541574,2018-08-29T11:49:55,"<p>You need to go much deeper in Jhipster, it is call service discovery of other microservice, a time you init jhisper app it will ask you to add microservices, you can go through below links.</p>&#xA;&#xA;<p><a href=""https://www.jhipster.tech/api-gateway/"" rel=""nofollow noreferrer"">API DOC </a> , <a href=""https://github.com/jhipster/jhipster-sample-app-gateway/blob/master/README.md"" rel=""nofollow noreferrer"">Example</a></p>&#xA;"
49028906,49021899,3402502,2018-02-28T11:50:32,<p>You could orgianize package as:</p>&#xA;&#xA;<pre><code>orderfeature/&#xA;├── OrderController &lt;== here calls Facade public&#xA;└── service/&#xA;   ├── CheckoutFacade &lt;== here calls orderService(go to facade and back to self package) and personService - public for test purpouse      &#xA;   ├── OrderService (might be package scope)&#xA;   └── PersonService (might be package scope)&#xA;</code></pre>&#xA;&#xA;<p>or simply </p>&#xA;&#xA;<pre><code>orderfeature/&#xA;├── OrderController &lt;== here calls Facade public&#xA;├── CheckoutFacade &lt;== here calls orderService(go to facade and back to self package) and personService - (might be package scope)      &#xA;├── OrderService (might be package scope)&#xA;└── PersonService (might be package scope)&#xA;</code></pre>&#xA;
44894420,44893574,934407,2017-07-03T22:07:41,"<p>So I think this should be possible although whether you really want to do it or not is another question! I haven't tried this but you'd need to do something along the lines of:</p>&#xA;&#xA;<ol>&#xA;<li><p>Use the same signing certificate from both IdentityServers</p></li>&#xA;<li><p>To validate your JWTs, use the Microsoft extension <a href=""https://www.nuget.org/packages/Microsoft.AspNetCore.Authentication.JwtBearer/"" rel=""nofollow noreferrer"">""UseJwtBearerAuthentication""</a> instead of using the IdentityServer validator (which is set with the UseIdentityServerAuthentication extension not UseIdentityServerBearerTokenAuthentication - the latter is IdentityServer3 I think). IdentityServer uses the Microsoft validator under the hood anyway, <a href=""https://github.com/IdentityServer/IdentityServer4.AccessTokenValidation/blob/dev/src/IdentityServer4.AccessTokenValidation/IdentityServerAuthenticationMiddleware.cs"" rel=""nofollow noreferrer"">code</a> and <a href=""https://github.com/IdentityServer/IdentityServer4/blob/e5a41475481071e6ab7eb6ab9f6c5bccfd3c4680/docs/topics/apis.rst#the-identityserver-authentication-middleware"" rel=""nofollow noreferrer"">docs</a>:</p></li>&#xA;</ol>&#xA;&#xA;<blockquote>&#xA;  <p>Our authentication middleware serves the same purpose as the above middleware (in fact it uses the Microsoft JWT middleware internally)</p>&#xA;</blockquote>&#xA;&#xA;<ol start=""3"">&#xA;<li><p>The reason for using the Microsoft validator directly is that now you can avoid setting the authority when you set it up, this would normally be used to validate the ""iss"" (issuer) as part of validating the JWT. Instead you can configure multiple valid issuers (i.e. your two IdentityServers) and point the validator directly at the signing cert that you are using in step 1:</p>&#xA;&#xA;<pre><code>var tokenValidationParameters = new TokenValidationParameters&#xA;{&#xA;    ValidateIssuerSigningKey = true,&#xA;    ValidateIssuer = true,&#xA;    ValidIssuers = new[]&#xA;    {&#xA;        ""http://my_first_IdentityServer/"", &#xA;        ""http://my_second_IdentityServer/""&#xA;    },&#xA;    IssuerSigningKey = new X509SecurityKey(new X509Certificate2(certLocation))&#xA;};&#xA;&#xA;app.UseJwtBearerAuthentication(new JwtBearerOptions()&#xA;{&#xA;    Audience = ""http://localhost:5001/"", &#xA;    AutomaticAuthenticate = true,&#xA;    TokenValidationParameters = tokenValidationParameters&#xA;});&#xA;</code></pre></li>&#xA;</ol>&#xA;&#xA;<p>The above is adapted from the article <a href=""https://blogs.msdn.microsoft.com/webdev/2017/04/06/jwt-validation-and-authorization-in-asp-net-core/"" rel=""nofollow noreferrer"">here</a>. As it says in the article, you could equally (and preferably) use a certificate store rather than a cert file.</p>&#xA;&#xA;<p>Btw, we're obviously talking authentication here, how you authorise users across the APIs will no doubt add extra complexity.</p>&#xA;"
44169116,44169046,3167751,2017-05-24T21:56:24,"<p>Polling is a waste of bandwidth.  You want to use a <em>transactional</em> queue.  </p>&#xA;&#xA;<p>Throw all your outbound messages in the queue, and have some other process to handle the messages.</p>&#xA;&#xA;<p>How this will work is - after your process reads from the queue, and tries to send to the REST service:</p>&#xA;&#xA;<ul>&#xA;<li>If it works, commit the transaction (for the queue)</li>&#xA;<li>If it doesn't work, don't commit.  Start a delay (minutes, seconds - you know best) until you read from the queue again.</li>&#xA;</ul>&#xA;"
48361510,48361365,2223111,2018-01-20T21:59:05,<p>premature optimization is seldom useful.</p>&#xA;&#xA;<p>300-400 object growth per month with a 30k base object is nothing at all for any DB to handle.</p>&#xA;&#xA;<p>loading all 30k object at once on the browser is awful and may affect performance while querying result in the DB will not have this problem until you have LOT of and LOT of users accessing the DB.</p>&#xA;&#xA;<p>You should be building the service using the Database and then if/when you reach a bottleneck you can think about optimization trick such as caching frequent queries on the database.</p>&#xA;
49624832,29761872,3152904,2018-04-03T07:45:15,"<p>I would separate the solutions for the area of use, on let’s say operational and reporting.</p>&#xA;&#xA;<p>For the microservices that operate to provide data for single forms that need data from other microservices (this is the operational case) I think using API joins is the way to go. You will not go for big amounts of data, you can do data integration in the service.</p>&#xA;&#xA;<p>The other case is when you need to do big queries on large amount of data to do aggregations etc. (the reporting case). For this need I would think about maintaining a shared database – similar to your original scheme and updating it with events from your microservice databases. On this shared database you could continue to use your stored procedures which would save your effort and support the database optimizations.</p>&#xA;"
50400650,50400384,3811080,2018-05-17T21:18:35,"<p>I would say ""they"" are wrong and you are right. There are several issues with copy and pasting client code:</p>&#xA;&#xA;<ul>&#xA;<li>If there is a bug in your client code, you will have to fix the bug in 15 places instead of just 1.</li>&#xA;<li>It slows things down. You now have to test and maintain multiple copies of the same code.</li>&#xA;<li>It is common practice to create client libraries and distribute them via a standard dependency manager like maven. Amazon does this <a href=""https://github.com/aws/aws-sdk-java"" rel=""nofollow noreferrer"">https://github.com/aws/aws-sdk-java</a> along with virtually everyone else.</li>&#xA;</ul>&#xA;&#xA;<p>In summary, you are right and Amazon is the strongest example supporting your opinion. They do exactly what you are suggesting for their web services, and they are arguably the largest most powerful player in the microservices space.</p>&#xA;&#xA;<p>Also to address the concern of tight coupling in the other answer. Good apis are backward compatible, so a change to the api would not require upgrading all the clients even if they use the same client library.</p>&#xA;"
36713635,36694873,5520136,2016-04-19T09:03:56,"<p>I'm not sure it will answer your question but you could take a look at WSSE authentication mechanism, it is stateless and works with headers so it may be what your are looking for.</p>&#xA;&#xA;<p>There is a symfony2 implementation example in the official doc <a href=""http://symfony.com/doc/current/cookbook/security/custom_authentication_provider.html"" rel=""nofollow"">here</a>, which is also nicely explained <a href=""http://obtao.com/blog/2013/06/configure-wsse-on-symfony-with-fosrestbundle/"" rel=""nofollow"">here</a>.</p>&#xA;&#xA;<p>You can find github libs which implements WSSE for Silex as well, maybe you should give them a try :</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://github.com/alcalyn/silex-wsse"" rel=""nofollow"">by alcalyn</a></li>&#xA;<li><a href=""https://github.com/guigzz/silex-wsse"" rel=""nofollow"">by me</a></li>&#xA;</ul>&#xA;&#xA;<p>Hope this will help you or other people looking for a way to achieve nice RESTful-ish/stateless mechanism in Silex, and sorry if it's not what your are looking for.</p>&#xA;"
51566032,51552739,6775560,2018-07-27T21:57:55,"<p>Basically it depends on your budget and size of your system. You can make some robust and complex implementations using tools like <a href=""http://projects.spring.io/spring-cloud/"" rel=""nofollow noreferrer"">Spring Cloud</a> or <a href=""http://www.krakend.io/"" rel=""nofollow noreferrer"">KrakenD</a>. As a matter of fact, your question is too broad. I've some microservices architecture knowledge and I can recommend just splitting your functionality into containerized solutions, probably orchestrated by Kubernetes. In that way, you can expose for example, the User microservice with loopback, and another Authentication microservice with loopback and/or any other language/framework.</p>&#xA;&#xA;<p>You could (but shouldn't) add communication between those microservices (as you should expose some REST functionality) with something like <a href=""https://grpc.io/"" rel=""nofollow noreferrer"">gRPC</a>.</p>&#xA;&#xA;<p>The biggest cloud providers have some already made solutions, eg AWS has <a href=""https://aws.amazon.com/ecs/"" rel=""nofollow noreferrer"">ECS</a> or <a href=""https://aws.amazon.com/fargate/"" rel=""nofollow noreferrer"">Fargate</a>. For GCP you have <a href=""https://cloud.google.com/kubernetes-engine/"" rel=""nofollow noreferrer"">Kubernetes</a>.</p>&#xA;"
40825216,40807355,93448,2016-11-27T02:30:29,"<blockquote>&#xA;  <p>Or is it better to use a job scheduler just to push messages to the message queue, e.g. using rundeck in combination with RabbitMQ?</p>&#xA;</blockquote>&#xA;&#xA;<p>yes.</p>&#xA;&#xA;<p>RabbitMQ is not designed to handle scheduling, and attempting to use it for that will just be painful (at best).</p>&#xA;&#xA;<p>It is best to use another scheduling system, like cron jobs or rundeck or any of the other numerous scheduling tools available. From that tool, you can execute code that will push messages across RabbitMQ, triggering work in other parts of your system.</p>&#xA;"
32413143,32398245,93448,2015-09-05T12:37:42,"<p>take advantage of your web server in between RabbitMQ and Socket.io. </p>&#xA;&#xA;<p>When a message is done processing and ready to be delivered to the user, the web server can use socket.io to check if the user is online. if they are, send the message to the user. if they are not currently connected to socket.io, store the message in a database. </p>&#xA;&#xA;<p>when the user reconnects, check the database to see if there are any messages waiting for that user and send them at that point.</p>&#xA;"
40910943,40910259,93960,2016-12-01T12:40:29,"<p>If you search about microservices architecture, you'll find that much people recommend to share nothing between microservices as you want to be able to evolve them independently as much as possible. This means that your approach 1 is preferred, you could add a 3rd approach which is to generate your REST client code from swagger JSON spec . JHipster has a module for doing so: <a href=""https://github.com/cbornet/generator-jhipster-swagger-cli"" rel=""nofollow noreferrer"">https://github.com/cbornet/generator-jhipster-swagger-cli</a></p>&#xA;&#xA;<p>Also communication between services is usually something to consider carefully, it could be an indication that your service boundaries are wrong.</p>&#xA;"
38510554,38507565,93960,2016-07-21T17:20:06,<p><code>Caused by: java.lang.IllegalStateException: No instances available for elseruaa</code> reported by RibbonLoadBalancerClient. </p>&#xA;&#xA;<p>I'm not up to date with our uaa support but it seems that your gateway was unable to find a service named <code>elseruaa</code> in the list of Eureka clients that have registered to your JHipster registry (Eureka server). So either you forgot to start elseruaa or it registered with another name.</p>&#xA;
47402241,47398736,93960,2017-11-20T22:32:17,"<p>You found it, the secret key is used by the gateway to sign the token when it generates it, same key is used by microservices to verify signature. The gateway is a Zuul proxy that passes the authentication header to proxified microservices.</p>&#xA;&#xA;<p>This property in Consul is available to all these apps through a local Consul agent at port 8500, see <a href=""https://cloud.spring.io/spring-cloud-consul/"" rel=""nofollow noreferrer"">Spring Cloud Consul</a>.</p>&#xA;"
44964707,44946517,93960,2017-07-07T07:13:45,"<p>In development: </p>&#xA;&#xA;<ul>&#xA;<li>run JHipster registry</li>&#xA;<li>on gateway, run both <code>mvnw</code> and <code>yarn start</code> at same time and then open app in browser at port 9000, all calls to services will be proxied to the java applications transparently: gateway + microservices apps</li>&#xA;<li>run microservices apps with <code>mvnw</code></li>&#xA;</ul>&#xA;&#xA;<p>EDIT:</p>&#xA;&#xA;<p>I looked at your link though I hesitated because it was an archive of 135 MB !!!&#xA;Few remarks:</p>&#xA;&#xA;<ul>&#xA;<li>Next time, please publish source on github rather than packaging node_modules folders</li>&#xA;<li>agreeGatewayV1 uses mysql as dev database which made it difficult for me to run. h2 is much easier to work with especially for a gateway where you are not going to modify the data schema. </li>&#xA;<li>Application-2 does not include a .yo-rc.json file so I couldn't inspect its configuration. Same as your gateway, it uses mysql in dev so I could not test it</li>&#xA;<li>I found 5 java crash reports in your archive, your environment seems very unstable. Maybe you should upgrade your OS and jvm or maybe you don't have enough RAM</li>&#xA;</ul>&#xA;&#xA;<p>So sorry but I can't help you, it would take too much time.</p>&#xA;"
40437082,40436389,93960,2016-11-05T10:21:31,"<p>You can't do that and you should not do it. You must not try to share JPA entites and Spring repositories, it completely defeats the purpose of microservices, it's like trying to build a distributed monolith, it's an anti pattern.</p>&#xA;&#xA;<p>Each microservice has its own database, if microservice2 needs to access some of microservice1 data it must do it through microservice1 REST API. See <a href=""https://github.com/jhipster/generator-jhipster/issues/3649"" rel=""nofollow noreferrer"">https://github.com/jhipster/generator-jhipster/issues/3649</a> for details how to do it.</p>&#xA;&#xA;<p>Alternatively, if you have such needs, it may be a strong signal that you must refactor your services and re-consider their boundaries. This is the hardest part of microservice architecture.</p>&#xA;"
48272159,48269479,93960,2018-01-15T23:34:09,"<p>JHipster does not support DB2 but you can modify generated code and datasource definition, Liquibase supports DB2, so it's probably a reasonable work as long as you are fluent in Spring Boot, JPA and Hibernate.</p>&#xA;&#xA;<p>Doing this manually after code generation will probably makes it harder for you to run <code>jhipster upgrade</code> so an alternative would be <a href=""http://www.jhipster.tech/modules/creating-a-module/"" rel=""nofollow noreferrer"">to create a JHipster module</a>.</p>&#xA;&#xA;<p>I don't think that microservices would help here as it's the same code generation that would be impacted.</p>&#xA;"
50654940,50653819,93960,2018-06-02T08:24:04,"<p>It depends on the use case.</p>&#xA;&#xA;<p>For user requests, a common approach is: the calling service forwards the token it received to the other service without going through the gateway suing <code>@AuthorizedFeignClient</code>.</p>&#xA;&#xA;<p>For background tasks like scheduled jobs, your approach can be applied or you could also issue long life tokens as long as they have limited permissions through roles. This way you don't have to go through gateway.</p>&#xA;&#xA;<p>Keycloak's <a href=""https://www.keycloak.org/docs/3.2/server_admin/topics/sessions/offline.html"" rel=""nofollow noreferrer"">offline tokens</a> approach could also inspire you.</p>&#xA;"
39743983,39742117,93960,2016-09-28T09:53:36,"<p>First solution: you can easily unprotect authenticate endpoint in SecurityConfioguration on your micro service so that you don't require a token for it then you will have to create a Zuul route on gateway for /api/authenticate.</p>&#xA;&#xA;<p>Second solution is a well known question about using multiple datasources in Spring Boot which has many well docuemnted answers.</p>&#xA;&#xA;<p>There could be another solution consisting in using an existing third party identity server like uaa or KeyCloak if you can configure them to your existing user database.</p>&#xA;&#xA;<p>So for a prototype, I'd go with the second solution.</p>&#xA;"
39243217,39224930,93960,2016-08-31T07:23:12,"<p>You could either convert your monolith into a service, or re-generate it from your entity definitions.</p>&#xA;&#xA;<p>First approach requires a good understanding about Spring Cloud, you'd start by annotating your app with <code>@EnableEurekaClient</code>, add missing depdendencies on Spring cloud to your <code>pom.xml</code>, add missing properties to your <code>application*.yml</code>, create <code>bootstrap*-yml</code> files. Then you would move your client part to your gateway. This is not easy especially if you're new to spring cloud.</p>&#xA;&#xA;<p>Second approach requires you to generate a microservices app with same options as your monolith, then copy to it your <code>.jhipster</code> folder which contains your entity definitions and re-generate them running <code>yo jhipster:entity &lt;entityName&gt;</code> for each entity in same order as you created them initially and then generate htem also on gateway for generating the client part.</p>&#xA;&#xA;<p>You should also take time to think about why you're migrating, if you turn your monolith app into a single service then it might be a bad idea as you'll only add complexity, it makes sense only if you are planning to add more services and/or split your monolith into several services. There is a good free ebook and video at O'Reilly: ""Microservices AntiPatterns and Pitfalls""</p>&#xA;"
41186364,41164987,93960,2016-12-16T14:20:01,"<p>First option using JWT claim makes sense and it could be proposed as an enhancement request to JHipster project. I can see one minor drawback: any user changing her locale won't get correct localised content until token expires or she logs out and re-authenticates.</p>&#xA;&#xA;<p>An alternative would be to pass the locale as a parameter of your resources requests, this makes sense as client is supposed to know it and also in a broader scope client could a be a partner application using another kind of authentication like an API key.</p>&#xA;"
48998311,48993781,93960,2018-02-26T22:43:20,"<ol>&#xA;<li>JHipster gateways have several features which aim to protect your services and ensure scalability:&#xA;&#xA;<ul>&#xA;<li>Authenticating users and generate tokens that are passed to underlying services to provide them with user identity and granted roles</li>&#xA;<li>Routing and load balancing requests to services using Zuul proxy and Ribbon. Hystrix helps also by enabling retries and timeouts. This is crucial when you want to scale by having several instances of same service. </li>&#xA;<li>Rate limiting</li>&#xA;<li>Serving Angular application bundles.</li>&#xA;</ul></li>&#xA;<li>JHipster registry is an Eureka server (service discovery) and a Spring Cloud Config server to centralize service configuration, its dashboard offers also more features than Spring Cloud Eureka dashboard. JHipster registry can be replaced by Hashicorp Consul as shown on the diagram.</li>&#xA;</ol>&#xA;"
44046857,44038536,93960,2017-05-18T11:45:06,"<p>Developing using a shared gateway makes little sense as it means that you cannot use webpack dev server to hot reload your UI changes. Gateways and microservices can run without the registry just use local application properties and define static zuul routes. If your microservices are well defined, most developers will only need to run a small subset of them to develop new features or fix bugs.</p>&#xA;&#xA;<p>The UAA server can be shared but alternatively you can create a security configuration simulating authentication that you would activate through a specific profile. This way when a developer works on one single web service, she can test it with a simple REST client like curl or swagger without having to worry about tokens.</p>&#xA;&#xA;<p>Another possibility if you want to share the registry is to assign a spring profile per developer but it might be overkill compared to above approach.</p>&#xA;"
45923778,45917894,93960,2017-08-28T16:44:46,"<p>When you are in dev, you access the UI on port 9000 (webpack) not on port 8080 (spring boot), the port 8080 is used only for API calls. This is the only way to get hot reload for the UI.</p>&#xA;&#xA;<p>If you really want to access the UI in dev on port 8080, each time you make a change you have to run <code>yarn webpack:build</code> and then restart the spring boot app.</p>&#xA;"
47309774,47309649,569864,2017-11-15T14:18:06,<p>Technically it doesn't matter</p>&#xA;&#xA;<p>But within an overall REST approach the URL should be easily readable and comprehensible by a humain.</p>&#xA;&#xA;<p>using your first approach is the correct form as it's easily readable as </p>&#xA;&#xA;<pre><code>The Api of Version 1 that exposes ...&#xA;</code></pre>&#xA;
37426013,35441660,705288,2016-05-25T00:29:48,"<p>It depends on what you want to get out of microservices.</p>&#xA;&#xA;<p>Some of the developers at my organisation looked at Spring Boot but concluded that it's best off being run as a standalone container rather than in JBoss, otherwise you've effectively got two container frameworks competing (SB and JBoss) and a range of associated issues.</p>&#xA;&#xA;<p>Deploying microservices in JBoss won't give you the same flexibility as a true container system like Docker. With Docker you create standalone packages for your microservices that contain all the code, system tools, runtime environment, etc. It can be as small or large as it needs to be. JBoss on the other hand is a large container running a single JVM designed to hold multiple applications. The level of isolation is not the same, and it's not efficient to have JBoss as a container for a single microservice so you have to appropriately size and then deploy to the instance to make use of the resources it has available.</p>&#xA;&#xA;<p>If you're looking at microservices as a way to gain greater control over service lifecycle management (deployment, versioning, deprecating, etc.) as opposed to an automated, web-scale component deployment model a la Netflix or LinkedIn, you could do this adequately with JBoss.</p>&#xA;&#xA;<p>I'm actually looking to do something along these lines here. It won't be true microservices but by packaging and deploying individual, properly versioned APIs rather than monolithic applications and following most of the other principles of microservice development (componentisation, business function focus, stateless etc.) we will be hopefully better able to manage and benefit from our APIs.</p>&#xA;&#xA;<p>Our APIs will all be behind an API gateway and load balancer so we can choose how we distribute the microserves distributed across the JBoss instances and balance resource usage as required. Note that our organisation is relatively small and has relatively low and predictable traffic so this approach should work fine. Your needs however may be different.</p>&#xA;"
43960290,43950808,5557256,2017-05-14T04:11:26,"<p>I think you can approach this issue from 2 angles, service collaboration and data modelling:</p>&#xA;&#xA;<p>Service collaboration</p>&#xA;&#xA;<p>Here you can choose between service orchestration and service choreography. You already mentioned the exchange of messages or events between services. This would be the choreography approach which as you said might work but involves writing code in each service that deals with the messaging part. I'm sure there are libraries for that though. Or you can choose service orchestration where you introduce a new composite service - the orchestrator, which can be responsible for managing the data updates between the services. Because the data consistency management is now extracted into a separate component, this would allow you to switch between eventual consistency and strong consistency without touching the downstream services.</p>&#xA;&#xA;<p>Data modelling</p>&#xA;&#xA;<p>You can also choose to redesign the data models behind the participating microservices and to extract the entities that are required to be consistent across multiple services into relationships managed by a dedicated relationship microservice. Such a microservice would be somewhat similar to the orchestrator but the coupling would be reduced because the relationships can be modelled in a generic way.</p>&#xA;"
43942983,43939970,5557256,2017-05-12T17:00:57,"<ol>&#xA;<li>What is the best approach to creating an account for John in my User service ?</li>&#xA;</ol>&#xA;&#xA;<p>There is not much to do here, just get the user details from FB and call your user create endpoint. For a RESTful API you will probably want to do a POST to <a href=""https://your_api_gateway/users"" rel=""nofollow noreferrer"">https://your_api_gateway/users</a> </p>&#xA;&#xA;<ol start=""2"">&#xA;<li>Once logged in as John, how does the WebApp propagate the identity of John to the Order service ?</li>&#xA;</ol>&#xA;&#xA;<p>One option is to use a token microservice. At login time you would create a long-lived auth token and a short-lived access token. The auth token is the source of trust that you never share. You return the access token to the client webapp. All calls from the client to any of your microservices will send that access token as part of the request. Another option is to simply use the access token generated by FB/Google.</p>&#xA;&#xA;<ol start=""3"">&#xA;<li>How does the Order service validate that John is logged in ?</li>&#xA;</ol>&#xA;&#xA;<p>Your Order service would receive an access-token in the request. As long as the access-token is valid, you can assume that John is logged in.</p>&#xA;&#xA;<ol start=""4"">&#xA;<li>How can interdependent services Order &amp; User trust each other ?</li>&#xA;</ol>&#xA;&#xA;<p>The access token is signed by the token microservice - which should be a trusted service - and it can contain additional information that can be further verified by any of your microservices</p>&#xA;&#xA;<ol start=""5"">&#xA;<li>Downstream services will become very ""chatty"" with the Authorisation Server (Facebook, Google)</li>&#xA;</ol>&#xA;&#xA;<p>Once you generate the access token, you don't need to call FB or Google again until your webapp decides that the user needs to be authenticated again.</p>&#xA;"
44417852,44416904,5557256,2017-06-07T16:11:30,<p>Kong can front any RESTful API and through a transformation plugin you should be able to deal with GraphQL API as well.</p>&#xA;
44033963,44021110,5557256,2017-05-17T20:09:39,"<p>You may find the most concise explanation in Fielding's dissertation on REST:</p>&#xA;&#xA;<blockquote>&#xA;  <p>A resource is a conceptual mapping to a set of entities</p>&#xA;</blockquote>&#xA;&#xA;<p>If that's the context you were looking for, you can read more <a href=""http://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm#sec_5_2_1_1"" rel=""nofollow noreferrer"">here</a></p>&#xA;"
44034314,44000273,5557256,2017-05-17T20:34:24,"<p>Since you are looking for a design pattern, I think you might want to compare the pros and cons of using microservices orchestration vs choreography in the context of your project.</p>&#xA;"
44051917,35890054,5557256,2017-05-18T15:26:23,"<ul>&#xA;<li>Keep unit and contract tests with the microservice implementation</li>&#xA;<li>Component tests make sense in the context of composite microservices,&#xA;so keep them together</li>&#xA;<li>Have the integration and E2E tests in a&#xA;separate repo, grouped by use cases</li>&#xA;</ul>&#xA;"
44189176,44169046,5557256,2017-05-25T20:19:58,"<p>Service ""A"" should fire a ""ready"" event when it becomes available. Just listen to that and resend your request.</p>&#xA;"
44145066,44124914,5557256,2017-05-23T21:08:30,"<p>With AWS API Gateway you could deploy multiple versions of your code and switch between them from the mapping templates, as explained <a href=""https://aws.amazon.com/blogs/compute/using-api-gateway-mapping-templates-to-handle-changes-in-your-back-end-apis/"" rel=""nofollow noreferrer"">here</a>. You might also want to look into <a href=""http://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html"" rel=""nofollow noreferrer"">stage variables</a>.</p>&#xA;"
48473695,48457264,9172232,2018-01-27T07:39:13,"<p>From what I understand, despite it's name, the 'hub' isn't a stand-alone service at all, so you dont have to install it anywhere. SignalR is a library used by your service(s) to directly communicate with your clients. SignalR can be used in two ways, firstly the 'Persistent Connection' API gives the service a way to send arbitrary data to your clients. Its main aim is to abstract away the underlying transport mechanism (e.g. Websockets, Ajax Long polling etc...) similar to WCF. Secondly, the 'Hub' API is an higher level layer of abstraction (built upon the 'Persistent Connection' API) that allows the server to call 'methods' on the client (i.e. similar to RPC in behaviour). Therefore each client has an in-process 'hub' that dispatches incoming messages (from the persistent connection) by calling functions that you write in your client code (i.e. event handlers).</p>&#xA;&#xA;<p>It may also be confusing in the terminology, as in some descriptions I've seen the service hosting the hub instance referred to as a 'hub'. So in your case any of your services could have a built-in hub, or alternatively you could have a dedicated hub service that is shared by your other services. Either way, the hub lives within one or more services.</p>&#xA;"
48389596,48385783,9172232,2018-01-22T20:17:35,"<p>There is always some level of shared infrastructure, if nothing else, at the network level. I think the key is to share infrastructure in such a way so that it doesn't compromise your ability to deploy your microservice independently of other teams. For example, if you share some infrastructure such as a database, then the database team should take responsibility for the management of the server (hardware, replication, monitoring, allocation of resources etc), and the microservice team should take ownership of their database, including the ability to change it's schema. Changes to the infrastructure can still break your microservice e.g. database server version update, but it is less likely you will break the infrastructure by your changes. You also really don't want to be in a position where you need to submit a change request to an external team in order to run a script to update your schema, as that cancels out the key benefit of adopting a microservice architecture. An alternative is to have a structure where each microservice team is allocated a team member who has authority to make timely changes to the infrastructure, but this doesn't necessarily mean that there wont also be delays and conflicts.</p>&#xA;&#xA;<p>What you should definitely try to avoid though, is 'integrating' your microservices via the infrastructure e.g. two distinctly separate microservices using the same database schema/instance. As well as coupling your microservice to the infrastructure, you also tightly couple your microservices, hugely compromising your ability to deploy.</p>&#xA;"
48175056,48172337,9172232,2018-01-09T19:04:55,"<p>One way is to use a datastore that has automatic expiry for keys. Then clients must periodically re-add the key to the store to continue using it, and if they fail then the key is automatically removed. Redis offers this kind of feature (<a href=""https://redis.io/commands/expire"" rel=""nofollow noreferrer"">https://redis.io/commands/expire</a>). </p>&#xA;&#xA;<p>Alternatively if you dont have this feature, you can simply store a timestamp when you set the key. If another client reads the key but the time has expired it can safely use the key.</p>&#xA;"
43143968,43142821,4265993,2017-03-31T15:05:58,"<p>Right image is better option. Easier management, upgrades, scaling.</p>&#xA;"
49698420,46236744,2387977,2018-04-06T17:46:33,"<p>If you are using Java, there is an alternative. I never tested this, but this idea could guide you for a solution for another platforms.</p>&#xA;&#xA;<p>You can use the ""good"" and old <strong>Javadoc</strong> for that and the Swagger module from <a href=""http://enunciate.webcohesion.com"" rel=""nofollow noreferrer"">Enunciate</a>, as explained <a href=""https://stackoverflow.com/a/33667644/2387977"">here</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p>You can generate swagger-ui from Javadoc by using Enunciate, which has a Swagger module</p>&#xA;</blockquote>&#xA;&#xA;<p>It's just a maven plugin. In the final, <a href=""https://github.com/stoicflame/enunciate/wiki"" rel=""nofollow noreferrer"">you have</a> a <em>Full HTML documentation of your services, scraped from your JavaDocs.</em>.</p>&#xA;"
49654179,47571761,167945,2018-04-04T14:53:25,"<p>This can be accomplished by hosting your API in process, interacting with it - in process -  whilst using test doubles at  the edges of you API,  or (put another way) using test doubles for external collaborators.  </p>&#xA;&#xA;<p>This allows an API (and its core behaviour) to be tested in a test (unit/integration depending on opinion) without ever hitting a network or database or filesystem. </p>&#xA;&#xA;<p>This testing will provide coverage but obviously not all the coverage you need </p>&#xA;&#xA;<p>In .NET core you can use <code>TestServer</code> included in the Microsoft.AspNetCore.TestHost package.</p>&#xA;&#xA;<p>If your API is simple enough you can perhaps reduce your types of testing (N.B. I am not suggesting reducing coverage)<br>&#xA;This approach is alluded to in the slide deck you mention on page 8/25</p>&#xA;&#xA;<p>Paraphrasing. If the logic in the API is small, component tests may be favourable to isolated and sociable unit tests</p>&#xA;&#xA;<p>This approach is also recommended in the Spotify ""Honeycomb"" style testing.  They call component testing of an API Integration testing (which coincidentally Microsoft also call it when they are using TestServer in their tests (second link for concrete examples)</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://labs.spotify.com/2018/01/11/testing-of-microservices/"" rel=""nofollow noreferrer"">https://labs.spotify.com/2018/01/11/testing-of-microservices/</a></li>&#xA;<li><a href=""https://docs.microsoft.com/en-us/aspnet/core/testing/integration-testing"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/aspnet/core/testing/integration-testing</a></li>&#xA;</ul>&#xA;&#xA;<p>I think that the thrust of the approach (regardless of its specific name)  is that in process testing is fast and external collaborators are replaed with test doubles. </p>&#xA;&#xA;<p>One thing worth noting is that ""out of process"" component tests are basically end to end tests but with all external collaborators swapped out with test doubles.  </p>&#xA;&#xA;<p>These may be a bit slower than  in process component tests but exercise a bit more of the ""stack"".  I'd suggest that if you are going to end-end test anyway,  I'd try and do in process component testing as much as possible. </p>&#xA;"
37704801,37697171,1808649,2016-06-08T14:04:33,"<p>Finally, I achieve to call external services using an <code>external_link</code>:</p>&#xA;&#xA;<pre><code>rails-app-A:&#xA;  build: .&#xA;  dockerfile: ""DockerfileA""&#xA;  environment:&#xA;    RAILS_ENV: development&#xA;  links:&#xA;    - db&#xA;  external_links:&#xA;    - app_B&#xA;&#xA;  ...ommited lines...&#xA;</code></pre>&#xA;&#xA;<p><code>app_B</code> should be a running container, previously executed like this:</p>&#xA;&#xA;<pre><code>docker-compose run --service-ports --name app_B rails-app-B&#xA;</code></pre>&#xA;&#xA;<p>Adding this <code>--name app_B</code> makes it accessible using this name.</p>&#xA;&#xA;<p>What I still don't understand is how to <code>build</code> rails-app-A without previously open rails-app-B.  If rails-app-A has rails-app-B as external link and the same happens in the other direction, <code>builds</code> become unmanageable. </p>&#xA;"
45680515,45673863,6794249,2017-08-14T18:12:24,"<p>I can propose to look at this example (that uses ""server"" scope): <a href=""https://github.com/sqshq/PiggyMetrics"" rel=""nofollow noreferrer"">https://github.com/sqshq/PiggyMetrics</a> and for more advanced configuration: <a href=""http://cloud.spring.io/spring-cloud-config/spring-cloud-config.html#_security"" rel=""nofollow noreferrer"">http://cloud.spring.io/spring-cloud-config/spring-cloud-config.html#_security</a></p>&#xA;"
32382900,32334161,5297929,2015-09-03T18:25:40,"<p>Load balancers typically need the endpoints of the resources it balances the traffic load. With the growth of microservices and container based applications, runtime created dynamic containers (docker containers) are ephemeral and doesnt have static end points. These container endpoints are ephemeral and they change as they are evicted and created for scaling or other reasons. Service discovery tools like Consul are used to store the endpoints info of dynamically created containers (docker containers). Tools like consul-registrator running on container hosts registers container end points in service discovery tools like consul. Tools like Consul-template will listen for changes to containers end points in consul and update the load balancer (nginx) for sending the traffic to. Thus both Service Discovery Tools like Consul and Load Balancing tools like Nginx co-exist to provide runtime service discovery and load balancing capability respectively.</p>&#xA;&#xA;<p>Follow up: what are the benefits of ephemeral nodes (ones that come and go, live and die) vs. ""permanent"" nodes like traditional VMs?</p>&#xA;&#xA;<p>[DDG]: Things that come quickly to my mind: Ephemeral nodes like docker containers are suited for stateless services like APIs etc. (There is traction for persistent containers using external volumes - volume drivers etc)</p>&#xA;&#xA;<ol>&#xA;<li><p>Speed: Spinning up or destroying ephemeral containers (docker containers from image) takes less than 500 milliseconds as opposed to minutes in standing up traditional VMs</p></li>&#xA;<li><p>Elastic Infrastructure: In the age of cloud we want to scale out and in according to users demand which implies there will be be containers of ephemeral in nature (cant hold on to IPs etc). Think of a markerting campaign for a week for which we expect 200% increase in traffic TPS, quickly scale with containers and then post campaign, destroy it.</p></li>&#xA;<li><p>Resource Utilization: Data Center or Cloud is now one big computer (compute cluster) and containers pack the compute cluster for max resource utilization and during weak demand destroy the infrastructure for lower bill/resource usage. </p></li>&#xA;</ol>&#xA;&#xA;<p>Much of this is possible because of lose coupling with ephemeral containers and runtime discovery using service discovery tool like consul. Traditional VMs and tight binding of IPs can stifle this capability.</p>&#xA;"
42103720,42096392,7532041,2017-02-08T02:40:19,"<p>Short answer to your question..Yes &#xA;infact, a polyglot approach is recommended. this will help you not get obsolete with new tech &amp; adopt new programming languages which may server different functional needs.</p>&#xA;&#xA;<p>Having said this, you need to be be careful &amp; cautious about the pitfalls of microservice design.(granularity of services, Eventual data consistency, deployment pipeline, containerization, service catalog, service registries, dynamic load-balancing  and service granularity etc.,)</p>&#xA;&#xA;<p>node.js is getting very popular on MSA</p>&#xA;&#xA;<p>-Good Luck</p>&#xA;"
42114560,42105805,7532041,2017-02-08T13:33:14,"<p>Api gateway is not needed for Internal service to service communication&#xA;But, you need a service registry or some kind of dynamic  load balancing mechanism to reach the services </p>&#xA;"
28878080,28828573,717065,2015-03-05T12:39:37,"<p>I have now evolved from n00b to n00b++ regarding Angular.</p>&#xA;&#xA;<p>The solution lies within the HTML-snippets used for templates within Angular.</p>&#xA;&#xA;<p>I can define a html template with a JS file and an HTML file. The JS file refers to the HTML page by a URL. Usually (in the samples I have seen) the path is relative like:</p>&#xA;&#xA;<pre><code>angular.module('myApp.view1', ['ngRoute'])&#xA;.config(['$routeProvider', function($routeProvider) {&#xA;  $routeProvider.when('/view1', {&#xA;    templateUrl: 'view1/view1.html',&#xA;    controller: 'View1Ctrl'&#xA;  });&#xA;}])&#xA;.controller('View1Ctrl', [function() {}]);&#xA;</code></pre>&#xA;&#xA;<p>But by using an absolute path I can refer to an external resource:</p>&#xA;&#xA;<p>angular.module('myApp.view2', ['ngRoute'])</p>&#xA;&#xA;<pre><code>.config(['$routeProvider', function($routeProvider) {&#xA;  $routeProvider.when('/view2', {&#xA;    templateUrl: host + '/app/view2/view2.html',&#xA;    controller: 'View2Ctrl'&#xA;  });&#xA;}])&#xA;.controller('View2Ctrl', [function() {}]);&#xA;</code></pre>&#xA;&#xA;<p>Notice I added a parameter 'host' calculated by looking at the script tag including this JS file.</p>&#xA;&#xA;<pre><code>var scripts = document.getElementsByTagName('script');&#xA;var src = scripts[scripts.length - 1].src;&#xA;var host = src.match(new RegExp('https?://[^/]*'))[0];&#xA;</code></pre>&#xA;&#xA;<p>But the third problem to address is to handle the CORS problem. In my test environment (node.js+express) I just added the 'cors' library and I could access my ""external"" site.</p>&#xA;&#xA;<pre><code>var express = require('express'), &#xA;    cors = require('cors'), &#xA;    app = express();&#xA;&#xA;app.use(cors());&#xA;app.use(""/"", express.static(__dirname));&#xA;&#xA;app.listen(8001, function(){&#xA;    console.log('CORS-enabled web server listening on port', 8001);&#xA;});&#xA;</code></pre>&#xA;"
52011410,52011129,62985,2018-08-24T20:39:08,"<p>You can specify <code>extra_hosts</code> in the compose format and pass in your host's IP address as an environment variable.</p>&#xA;&#xA;<pre><code>extra_hosts:&#xA;       - ""my_host:${HOST_IP}""&#xA;</code></pre>&#xA;&#xA;<p><a href=""https://docs.docker.com/compose/compose-file/#extra_hosts"" rel=""nofollow noreferrer"">https://docs.docker.com/compose/compose-file/#extra_hosts</a></p>&#xA;"
31470592,26975640,1445898,2015-07-17T07:32:13,"<p>Because services are isolated and pipes are reused.</p>&#xA;&#xA;<p>Core idea of microservices is isolation - any part of the system can be replaced without affecting other services. Smart pipes means they have configuration, they have state, they have complex (which often means hard-to-predict) behavior. Thus, smart pipes are less likely to retain their exact behavior over time.</p>&#xA;&#xA;<p>But - pipe change will affect every service attached while service change affects only other services that use it.</p>&#xA;"
31470741,28339882,1445898,2015-07-17T07:41:35,"<p>First question.</p>&#xA;&#xA;<p>Yes there is. First, you can do better failure recovery - for example, retry failed requests to another node without showing any errors to client. Next, you can do better balancing than ELB offers. Next, you can automatically add/remove nodes to/from cluster w/o altering ELB configuration. This is very useful if your nodes have healthchecks. More importantly, software balancer can do this fast.</p>&#xA;&#xA;<p>Second question.</p>&#xA;&#xA;<p>Have connection pool per node. i.e.&#xA;[api method in client code] -> [software balancer] -> [node connection pool] -> [node connection] -> [use this connection to make request]</p>&#xA;"
31471273,30288968,1445898,2015-07-17T08:18:58,"<p>First of all, thanks for your question. It is similar to Main Problem Of Document DBs: how to sort collection by field from another collection? I have my own answer for that so i'll try to comment all your solutions:</p>&#xA;&#xA;<p>Solution 1: It is good if client wants to work with Countries/Building/Floors independently. But, it does not solve problem you mentioned in Solution 2 - sorting 10k workers by building gonna be slow</p>&#xA;&#xA;<p>Solution 2: Similar to Solution 1 if all client wants is a list enriched workers without knowing how to combine it from multiple pieces</p>&#xA;&#xA;<p>Solution 3: As you said, unacceptable because of inconsistent data.</p>&#xA;&#xA;<p>Solution 4: Gonna be working, most of the time. But:</p>&#xA;&#xA;<ul>&#xA;<li>Huge data duplication. If you have 20 entities, you are going to have x20 data.</li>&#xA;<li>Large complexity. 20 entities -> 20 different procedures to update related data</li>&#xA;<li>High cohesion. All your services must know each other. Data model change will propagate to every service because of update procedures</li>&#xA;<li>Questionable eventual consistency. It can be done so data will be consistent after failures but it is not going to be easy</li>&#xA;</ul>&#xA;&#xA;<p>Solution 5: Kind of answer :-)</p>&#xA;&#xA;<p>But - you do not want everything. Keep separated services that serve separated entities and build other services on top of them.</p>&#xA;&#xA;<p>If client wants enriched data - build service that returns enriched data, as in Solution 2.</p>&#xA;&#xA;<p>If client wants to display list of enriched data with filtering and sorting - build a service that provides enriched data with filtering and sorting capability! Likely, implementation of such service will contain ES instance that contains cached and indexed data from lower-level services. Point here is that ES does not have to contain everything or be shared between every service - it is up to you to decide better balance between performance and infrastructure resources.</p>&#xA;"
31471421,30649582,1445898,2015-07-17T08:27:57,"<p>Keep master data normalized. But - if you have things like ""ContactID"" you normalized your data wrongly!</p>&#xA;&#xA;<p>Normalized entity must be not a table but business entity - a document that have business meaning. Why do you need ContactID? If you have list of companies with contacts, keep contacts within companies. If you want contacts to be shared information between, say, companies and contracts - have Contact service that returns contact information by email or company name or any other field that makes sense for business.</p>&#xA;&#xA;<p>Fully denormalized data is for derived data that is built from master data. It is used mainly to build indexes upon it for searching and sorting.</p>&#xA;"
31474001,31412249,1445898,2015-07-17T10:43:04,"<p>Here are some considerations:</p>&#xA;&#xA;<ol>&#xA;<li><p>Security. If your micro services are data-oriented (as they should be) do they know about end-user accounts and implement necessary checks? If you are going to call them via AJAX directly, they must be secured.</p></li>&#xA;<li><p>Performance. To get good performance, microservices must  be called in parallel. Does your client architecture support that?</p></li>&#xA;<li><p>Service locator. What do you use to locate service instances, do healthcheck and failover? Usually all that is too complex to be used directly from javascript.</p></li>&#xA;<li><p>API. Think about cost of maintaining backward compatibility for multiple small all-around APIs vs single application-specific API.</p></li>&#xA;<li><p>I'm not saying that you should go and create Gateway. Because not creating gateway is less code and less APIs. You should weight everything and decide for yourself.</p></li>&#xA;</ol>&#xA;"
31474393,31342583,1445898,2015-07-17T11:05:39,"<p>Your workers sound like (api-less) services itself. So, your requirements can be reformulated as:</p>&#xA;&#xA;<ul>&#xA;<li>Knows about deployed services</li>&#xA;<li>Knows about nodes that can host there services</li>&#xA;<li>Can deploy services to nodes</li>&#xA;<li>Can [send job updates to services] = redeploy services/invoke some API on deployed services</li>&#xA;<li>Can redeploy service if service or node dies</li>&#xA;</ul>&#xA;&#xA;<p>Look at Docker to deploy, run and manage isolated processes on host.</p>&#xA;"
31474803,31206417,1445898,2015-07-17T11:29:12,"<p>If you are having long flow of messages processed by multiple, independently written components, you should use component-specific data formats.</p>&#xA;&#xA;<p>Due to <a href=""https://en.wikipedia.org/wiki/Conway%27s_law"" rel=""nofollow"">Conway's law</a>, your components should belong to different business entities that have different views on domain model. For example, ""Order"" can mean completely different things and need to have different data for different departments.</p>&#xA;&#xA;<p>As for your question - every component should send messages that are specific to his business entity. Receiving side knows the edge between different business worlds and should do transformation and enrichment of incoming messages for further processing.</p>&#xA;&#xA;<p>Of course, additional data is needed to do that enrichment. It must be provided by other services owning dictionaries and configuration.</p>&#xA;&#xA;<p>P.S. ""add new flows without redeployment"" is main reason to fail for many and many projects. First, there is no reason to fear redeployment in modern architecture - all your services must be clustered and handle failure gracefully. Second, you should strictly define what can be done by changing configuration/rules/etc and what can not. And more importantly WHO can do that changes. Do not expect business people to write business rules by default :-)</p>&#xA;"
31473745,31468806,1445898,2015-07-17T10:28:13,<p>What you need is OAuth2.</p>&#xA;&#xA;<p>Use clustered Authorization server that gives tokens to clients and services and checks those tokens.</p>&#xA;
43930215,43927492,1445898,2017-05-12T05:52:54,"<p>It can be solved if your microservices allow passing metadata along with requests. </p>&#xA;&#xA;<p>Good microservice architecture should use central service discovery, also every service should be able to take metadata map along with request payload. Known fields of this map can be somehow interpreted and modified by the service then passed to next service.</p>&#xA;&#xA;<p>Most popular usage of per-request metadata is request tracing (i.e. collecting tree of nodes used to process this request and timings for every node) but it also can be used to tell entire system <em>which nodes to use</em></p>&#xA;&#xA;<p>Thus plan is</p>&#xA;&#xA;<ol>&#xA;<li>register your local node in dev environment service discovery</li>&#xA;<li>send request to entry node of your system along with metadata telling everyone to use your local service instance instead of default one</li>&#xA;<li>metadata will propagate and your local node will be called by dev environment, then local node will pass processed results back to dev env</li>&#xA;</ol>&#xA;&#xA;<p>Alternatively:</p>&#xA;&#xA;<ul>&#xA;<li>use code generation for inter-service communication to reduce risk of failing because of mistakes in RPC code</li>&#xA;<li>resort to integration tests, mocking all client apis for microservice under development</li>&#xA;<li>fully automate deployment of your system to your local machine. You will possibly need to run nodes with reduced memory (which is generally OK as memory is commonly consumed only under load) or buy more RAM.</li>&#xA;</ul>&#xA;"
43951363,43950808,1445898,2017-05-13T09:17:57,"<p>""accordingly update the linked entities in their respective databases"" -> data duplication -> FAIL.</p>&#xA;&#xA;<p>Using events to update other databases is identical to caching which brings cache consistency problem which is the problem you arise in your question.</p>&#xA;&#xA;<p>Keep your local databases as separated as possible and use pull semantics instead of push, i.e. make RPC calls when you need some data and be prepared to gracefully handle possible errors like timeouts, missing data or service unavailability. Akka or Finagle gives enough tools to do that right.</p>&#xA;&#xA;<p>This approach <em>might</em> hurt performance but at least you can choose what to trade and where. Possible ways to decrease latency and increase throughput are:</p>&#xA;&#xA;<ul>&#xA;<li>scale data provider services so they can handle more req/sec with lower latency</li>&#xA;<li>use local caches with short expiration time. That will introduce eventual consistency but really helps with performance.</li>&#xA;<li>use distributed cache and face cache consistency problem directly</li>&#xA;</ul>&#xA;"
31510505,31493493,1445898,2015-07-20T07:04:10,"<p>If you do want to use SSO, let all your services accept the same token, returned by Authorization server when user log in. Encrypt it and put to cookies for example - in this case your web frontend will authenticate user by validating that token against Authorization server and return user-specific content.</p>&#xA;&#xA;<p>The same applies to cart service - it can get username by token and return user-specific cart items.</p>&#xA;&#xA;<p>Pros: User can't access other user's data by design, user logout disables every service.</p>&#xA;&#xA;<p>Cons: You will need ""superuser"" or additional API for background or analytical tasks.</p>&#xA;"
31510639,31404820,1445898,2015-07-20T07:12:40,"<p>Thanks to @igorbel for interesting article leading to more interesting articles on topic.</p>&#xA;&#xA;<p>As for answer, DNS can't be good solution because of:</p>&#xA;&#xA;<ul>&#xA;<li>Complex centralized configuration. That really complicates adding new hosts.</li>&#xA;<li>Large and unpredictable propagation time. Removing failed node can take some time due to layered DNS caching which leads to longer periods of system instability.</li>&#xA;</ul>&#xA;"
47287935,27007353,1911306,2017-11-14T14:12:04,"<p>It is indeed one of key problems in microservices which is quite conviniently omitted in most of articles. Fortunatelly there are solutions for this. As a basis for discussion let's have tables which you have provided in the question.&#xA;<a href=""https://i.stack.imgur.com/pASjv.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/pASjv.jpg"" alt=""enter image description here""></a>&#xA;Image above shows how tables will look like in monolith. Just few tables with joins.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>To refactor this to microservices we can use few strategies:</p>&#xA;&#xA;<h2>Api Join</h2>&#xA;&#xA;<p>In this strategy foreign keys between microservices are broken and microservice exposes an endpoint which mimics this key. For example: Product microservice will expose <code>findProductById</code> endpoint. Order microservice can use this endpoint instead of join.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/V2TjN.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/V2TjN.jpg"" alt=""enter image description here""></a>&#xA;It has an obvious downside. It is slower.</p>&#xA;&#xA;<h2>Read only views</h2>&#xA;&#xA;<p>In the second solution you can create copy of the table in the second database. Copy is read only. Each microservice can use mutable operations on its read/write tables. When it comes to read only tables which are copied from other databases they can (obviously) use only reads&#xA;<a href=""https://i.stack.imgur.com/Ng7JZ.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Ng7JZ.jpg"" alt=""enter image description here""></a></p>&#xA;&#xA;<h2>High performance read</h2>&#xA;&#xA;<p>It is possible to achieve high performance read by introducing solutions such as redis/memcached on top of <code>read only view</code> solution. Both sides of join should be copied to flat structure optimized for reading. You can introduce completely new stateless microservice which can be used for reading from this storage. While it seems like a lot of hassle it is worth to note that it will have higher performance than monolithic solution on top of relational database.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>There are few possible solutions. Ones which are simplest in implementation have lowest performance. High performance solutions will take few weeks to implement.</p>&#xA;"
46823234,46822414,8798730,2017-10-19T05:18:02,"<p>You set <code>&lt;property name=""totalCount"" value=""1""/&gt;</code>, it make service runs as cluster singleton. try remove it.</p>&#xA;"
48878242,37854185,1462282,2018-02-20T05:10:21,"<p>Ideally, in the Microservices architecture we should not call the other services within a service. We should instead use the <strong>Publisher/Subscriber</strong> approach for server to server communication.</p>&#xA;&#xA;<p>Even though if you wish to do it, just <strong>pass the Auth-Token as a Header used for authenticating the service to the downstream service</strong>, where the token will again be authenticated and thus will give response once authenticated.</p>&#xA;"
36970975,36957369,5136650,2016-05-01T19:29:02,"<p>Somebody helped me on GitHub with the following:</p>&#xA;&#xA;<p>In order to set maximum size for the remoting transport, you can use the following attribute and place it on your actor interface assembly or configure the maximum message size in the ServiceProxyFactory and ServiceRemotingListener creation.</p>&#xA;&#xA;<pre><code>[assembly: FabricTransportActorRemotingProvider(MaxMessageSize = 1073741824)]&#xA;</code></pre>&#xA;&#xA;<p><a href=""https://msdn.microsoft.com/en-us/library/azure/microsoft.servicefabric.actors.remoting.fabrictransport.fabrictransportactorremotingproviderattribute.aspx"" rel=""noreferrer"">https://msdn.microsoft.com/en-us/library/azure/microsoft.servicefabric.actors.remoting.fabrictransport.fabrictransportactorremotingproviderattribute.aspx</a></p>&#xA;&#xA;<p>The discussion in <a href=""https://github.com/Azure/service-fabric-issues/issues/1"" rel=""noreferrer"">GitHub</a>.</p>&#xA;"
36742722,36705199,5136650,2016-04-20T11:42:12,"<p>The solution was to use Azure's redis to store the locks in-memory and have micro-services that manage those locks using the redis store. </p>&#xA;&#xA;<p>The <code>lock()</code> and <code>unlock()</code> operations are atomic and the lock has a TTL, so that a queue won't be locked indefinitely.</p>&#xA;"
49725988,45830528,421642,2018-04-09T05:14:21,"<p>if you really want to decouple the user-management from your app, you can use specifications like <a href=""https://oauth.net/2/"" rel=""nofollow noreferrer"">oAuth2.0</a> or <a href=""http://openid.net/"" rel=""nofollow noreferrer"">OpenID</a> - they are two different specs, and you should have a look and see what fits you the best.</p>&#xA;&#xA;<p>If you write your code in Java, you can use (for free) <a href=""https://projects.spring.io/spring-security/"" rel=""nofollow noreferrer"">Spring Security</a> together with <a href=""http://auth-flows-demo.appspot.com/"" rel=""nofollow noreferrer"">authentication-flows</a> - that will cover all security issues as well as all user management flows like registration, forgot password, change password etc.</p>&#xA;"
28729714,28727632,499466,2015-02-25T21:13:18,"<p>As with every strongly typed system, one of the major advantanges is without a doubt that if you make mistakes, it <strong>fails early</strong> in the process, typically at the compilation stage, which is a good thing. </p>&#xA;&#xA;<p>Second biggest advantage IMHO is what you already said: because the fields and types are well known, the compiler, libraries and related code know what data to expect and thus can be written/organized in a more efficient manner - or in short: <strong>performance</strong>.</p>&#xA;&#xA;<p>In contrast, a losely typed system (like Avro), while allowing for much greater <strong>flexibility</strong> without the need of recompiling, comes with the other side of the same coin: the downside of being prone to errors regarding the contents of the message at runtime. </p>&#xA;&#xA;<p>This is because a losely defined system defines only the syntax of a valid document (like for example XML) and leaves the message-level semantics of what's in the document up to the upper layers. A strongly typed system has the knowledge about those message-level semantics already built in <strong>at compile time</strong>. Therefore, it is easy to detect/decide whether a particular document or message is not only well-formed but valid with regard to the message contents. If you need to do the same with the losely defined system, you need to provide additional information <strong>at runtime</strong> (like XML schema) and validate your document against it.</p>&#xA;&#xA;<h2>Bottom line</h2>&#xA;&#xA;<p>What system you prefer is more or less a matter of taste in most cases. I'd make the decision based on the question, how variable the data are that I have to deal with. If it makes sense to use a strongly typed system, I'd go that way, because I like it very much to get informed about errors and mistakes early. </p>&#xA;&#xA;<p>However, if there is a need for very flexible data structures, it may make more sense to go the other road. Although designing a losely typed schema on top of a strongly typed system is surely possible, it is somewhat contradicting and you'll end up with some overly complicated, while overly generic, thing.</p>&#xA;"
35677367,35673254,499466,2016-02-28T00:46:19,"<blockquote>&#xA;  <p>Also, I have been doing some reading on how Apache Thrift and RPC's can help with this. Can anyone elaborate on that as well? </p>&#xA;</blockquote>&#xA;&#xA;<p>The goal of an RPC framework like Apache Thrift is </p>&#xA;&#xA;<ul>&#xA;<li>to significantly reduce the manual programming overhead </li>&#xA;<li>to provide efficient serialization and transport mechanisms</li>&#xA;<li>across all kinds of programming languages and platforms</li>&#xA;</ul>&#xA;&#xA;<p>In other words, this allows you to send your data as a very compactly written and compressed packet over the wire, while most of the efforts required to achieve this are provided by the framework. </p>&#xA;&#xA;<p>Apache Thrift provides you with a pluggable transport/protocol stack that can quickly be adapted by plugging in different </p>&#xA;&#xA;<ul>&#xA;<li>transports (Sockets, HTTP, pipes, streams, ...) </li>&#xA;<li>protocols (binary, compact, JSON, ...) </li>&#xA;<li>layers (framed, multiplex, gzip, ...)</li>&#xA;</ul>&#xA;&#xA;<p>Additionally, depending on the target language, you get some infrastructure for the server-side end, such as TNonBlocking or ThreadPool servers, etc.</p>&#xA;&#xA;<p>So coming back to your initial question, such a framework can help to make communication easier and more efficient. But it cannot magically remove latency from other parts of the OSI stack.</p>&#xA;"
45339150,45321939,54538,2017-07-26T23:47:06,"<p>The bit that was missing is how would you connect your WebAPI to the NServiceBus endpoint? At the end of the day, your WebAPI is your application interface and needs to communicate and send the work to the backend service to process the requests. If you plan to also do this via messaging you can do the following:</p>&#xA;&#xA;<ul>&#xA;<li><p>Host the WebAPI separately, you can go with IIS or OWIN. In your WebAPI, you can create and configure a 'SendOnly' endpoint which would send messages to the Windows Service.</p></li>&#xA;<li><p>Host the backend services using NServiceBus as a Windows Service. This will receive and process the messages from the WebAPI.</p></li>&#xA;</ul>&#xA;&#xA;<p>This has the simplicity as you said and would decouple WebAPI and the backend. As for IIS process/app pool getting recycled, since the 'SendOnly' endpoint doesn't really do any background work, app pool / IIS process recycle is not that big of a deal since you start the endpoint on AppStart anyway. When a request comes in if the endpoint has been shutdown a new instance is created as a part of the incoming request.</p>&#xA;&#xA;<p>There is a <a href=""https://docs.particular.net/samples/web/send-from-aspnetcore-webapi/?version=core_6"" rel=""nofollow noreferrer"">sample</a> on our Documentation website that shows you how to do this.</p>&#xA;"
50890871,50889657,3076874,2018-06-16T19:02:10,"<p>There are multiple factors that can drive your decision:</p>&#xA;&#xA;<ul>&#xA;<li><p>Required any acknowledgement from your Auth Service?<br>&#xA;if yes:  </p>&#xA;&#xA;<ul>&#xA;<li>For Immediate acknowledgement, use http</li>&#xA;<li>For not so immediate acknowledgement, Callback pattern can be implemented.&#xA;In your case, user profile sends request via Kafka to auth service and it calls&#xA;endpoint of user-profile to report status of the job.   </li>&#xA;</ul>&#xA;&#xA;<p>if no:<br>&#xA;Use queue one for better resiliency. </p></li>&#xA;<li><p>Error Handling<br>&#xA;Think of auth service failure? What should be the reaction of user service ?</p>&#xA;&#xA;<ul>&#xA;<li>if on auth-service failure, user-service should also fail<br>&#xA;Use http</li>&#xA;<li>if on auth-service failure, user service  should not fails.<br>&#xA;Use queue</li>&#xA;</ul></li>&#xA;</ul>&#xA;"
36129279,36129008,1128459,2016-03-21T11:01:26,"<p>According to the <a href=""https://github.com/senecajs/seneca-web#route-action-mapping"" rel=""nofollow"">senecajs documentation</a>, you <em>should</em> be able to just invoke <code>done()</code> within your <code>getData</code> method to return/send a value/response. Consider the following:</p>&#xA;&#xA;<p>Here, I was able to hit <code>/api/getData</code> and receive <code>{foo: 'bar'}</code> the response.</p>&#xA;&#xA;<pre><code>""use strict""; &#xA;const express = require('express');&#xA;&#xA;const seneca = require('seneca')();&#xA;const app = express();&#xA;&#xA;seneca.add('role:api,cmd:getData', getData);&#xA;&#xA;seneca.act('role:web',{use:{&#xA;    prefix: '/api',&#xA;    pin: {role:'api',cmd:'*'},&#xA;    map:{&#xA;        getData: {GET:true}          // explicitly accepting GETs&#xA;    }&#xA; }});&#xA;&#xA; app.use(seneca.export('web'));&#xA;&#xA; app.listen(3002, function () {&#xA;     console.log('listening on port 3002');&#xA; });&#xA;&#xA;function getData(arg, done){&#xA;    done(null, {foo: 'bar'});         &#xA;}&#xA;</code></pre>&#xA;"
36313075,36307211,218635,2016-03-30T15:19:07,"<p>Had to register a custom bean to override the hard coded value like this in the application class:</p>&#xA;&#xA;<pre><code>@Bean&#xA;public CustomHystrixStreamEndpoint customHystrixStreamEndpoint() {&#xA;    return new CustomHystrixStreamEndpoint();&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>and the create the custom wrapper class like this:</p>&#xA;&#xA;<pre><code>    public class CustomHystrixStreamEndpoint extends ServletWrappingEndpoint {&#xA;&#xA;        public CustomHystrixStreamEndpoint() {&#xA;            super(HystrixMetricsStreamServlet.class, ""customHystrixStream"",&#xA;                  ""/tenacity/hystrix.stream"",&#xA;                  false, true);&#xA;        }&#xA;    }&#xA;</code></pre>&#xA;&#xA;<p>and then turn off the default one like this in the config file:</p>&#xA;&#xA;<pre><code>hystrix.stream.endpoint.enabled: false&#xA;</code></pre>&#xA;&#xA;<p>FYI the default wrapper class is called HystrixStreamEndpoint  </p>&#xA;"
49868988,49836268,91403,2018-04-17T02:33:26,"<p>Apart from @notionquest answer, there is another approach which does not involve having an API gateway;</p>&#xA;&#xA;<p>You can share a <code>SESSION_SECRET</code> among all your services, so the only task of your Authentication Service is to validate username and password against the database and then encrypt this information using <code>SESSION_SECRET</code> and return a <a href=""https://jwt.io/"" rel=""nofollow noreferrer"">jwt token</a>. All other services won't need to interact with Authentication Service but simply check if the jwt token is valid (can be decrypted) with the <code>SESSION_SECRET</code>.</p>&#xA;&#xA;<p>You then have two other options; </p>&#xA;&#xA;<ol>&#xA;<li><p>Store all user data you need in the token - this will increase the amount of data in transit from your client to the micro-services. This can be prohibitive depending on the size of this information  </p></li>&#xA;<li><p>You can store only the userId, and request additional data as needed per each micro service, which depending on how often/how big your data is will generate a problem as you described.</p></li>&#xA;</ol>&#xA;&#xA;<p>Note that you will not always be able to use this approach but depending on your specific scenario and requirements having this architecture in mind can be useful.</p>&#xA;&#xA;<p>Also keep in mind that rotating the <code>SESSION_SECRET</code> can be tricky (although necessary for security reasons). AWS has just released a service called <a href=""https://aws.amazon.com/pt/secrets-manager/"" rel=""nofollow noreferrer"">AWS Secrets Manager</a>, so one idea to make things simple would be to have your micro-services periodically query a service like this for the current valid <code>SESSION_SECRET</code> instead of having this values hardcoded or as environment variables.</p>&#xA;"
49613070,49612911,91403,2018-04-02T14:23:37,"<p>This does not really qualify as a microservice architecture.</p>&#xA;&#xA;<p>The whole code you provided is small enough to be considered one single microservice (containing two routes), but this is not an example of a microservice architecture. </p>&#xA;&#xA;<p>According to this definition;</p>&#xA;&#xA;<blockquote>&#xA;  <p>""Microservices are small, <strong>autonomous</strong> services that <strong>work together</strong>""<br>&#xA;  <a href=""https://rads.stackoverflow.com/amzn/click/1491950358"" rel=""noreferrer"">Building Microservices</a> &lt;-- tip: you should read this book</p>&#xA;</blockquote>&#xA;&#xA;<p>Both service1 and service2 to be considered a microservice should be autonomous, what is not happening when you place them together in the same express app. For example; you cant restart one without not-affecting the other. You cant upgrade version of service1 without also having to deploy service2. They are not distributed in the sense that they can leave in separate machines.</p>&#xA;"
42531423,42527724,1617419,2017-03-01T12:13:17,"<p>Please check header <code>X-Marathon-App-Id</code> with marathon load balancer.</p>&#xA;&#xA;<p>Example:</p>&#xA;&#xA;<pre><code>curl -v -H ""X-Marathon-App-Id: /your-app-id-in-marahton"" -X GET \&#xA;  http://marathon-lb.marathon.mesos:9091/yourAppEndPoint&#xA;</code></pre>&#xA;&#xA;<p>More info <a href=""https://hub.docker.com/r/mesosphere/marathon-lb/"" rel=""nofollow noreferrer"" title=""marathon-lb docs"">marathon-lb docs</a>.</p>&#xA;"
41208344,41188108,6538763,2016-12-18T12:31:43,"<p>There is no such thing as automatic routing till now for atleast we need to declare <a href=""https://github.com/Netflix/zuul"" rel=""nofollow noreferrer"">Zuul</a> in your yml file which will handle routing.</p>&#xA;&#xA;<p>In your application.yml add these line</p>&#xA;&#xA;<pre><code> zuul:&#xA;  routes:&#xA;   users:&#xA;    path: /myusers/**&#xA;    serviceId: users_service&#xA;</code></pre>&#xA;&#xA;<p>For ignoring any routing use ignoredServices property like</p>&#xA;&#xA;<pre><code> zuul:&#xA;  ignoredServices: '*'&#xA;</code></pre>&#xA;&#xA;<p>For more information please see this <a href=""http://cloud.spring.io/spring-cloud-netflix/spring-cloud-netflix.html"" rel=""nofollow noreferrer"">this</a> link.</p>&#xA;&#xA;<p>This will ignore all automatic routing of your services.</p>&#xA;&#xA;<p>Hope this helps you out.</p>&#xA;&#xA;<p>Thanks.</p>&#xA;"
44285756,42002512,7730443,2017-05-31T13:10:04,<p>Have you tried using spring-cloud-bus with rabbit MQ it is basically to broadcast your configuration changes to all the instances of your application through POST bus/refresh.</p>&#xA;
46764397,46764158,1443084,2017-10-16T06:46:41,"<p>If you want to build a lightweight simple layer (single app)  to cater all these requirements, I would recommend using <a href=""https://stackoverflow.com/questions/8845186/what-exactly-is-apache-camel"">Apache Camel</a>. This single app can listen to rest/soap requests, read from file system, JMS store, database etc. You can even embed it into another application and have all sorts of integration with different data source and excellent and easy to configure routing and transformation engine. Plus the documentation and community is awesome.</p>&#xA;"
46120540,46115104,662796,2017-09-08T16:05:26,"<p>There's a couple of ways how you could solve your issue:</p>&#xA;&#xA;<ul>&#xA;<li>In order to load new data classes to the cluster you could use the <code>User code deployment</code> feature. In 3.8 it works only from members. In 3.9 it will include support for loading from clients. Here's more info: <a href=""http://docs.hazelcast.org/docs/latest/manual/html-single/index.html#user-code-deployment-beta"" rel=""nofollow noreferrer"">http://docs.hazelcast.org/docs/latest/manual/html-single/index.html#user-code-deployment-beta</a></li>&#xA;<li>You could also have a set of well-known top-level objects and simply modify them accordingly when needed. For easier versioning of these objects you could use the Portable serialization format. See the following links for more info: <a href=""http://docs.hazelcast.org/docs/latest/manual/html-single/index.html#implementing-portable-serialization"" rel=""nofollow noreferrer"">Portable Serialization</a> &amp; <a href=""http://docs.hazelcast.org/docs/latest/manual/html-single/index.html#versioning-for-portable-serialization"" rel=""nofollow noreferrer"">Versioning for Portable Serialization</a></li>&#xA;</ul>&#xA;"
43952696,43927492,4859404,2017-05-13T11:38:28,"<p>For java microservices, you should try <a href=""https://github.com/azagniotov/stubby4j"" rel=""nofollow noreferrer"">Stybby4j</a>. This will mock the json responses of other microservices using Stubby server. If you feel that mocking is not enough to map all the features of your microservices, you should setup a local docker environment to deploy the dependent microservices.</p>&#xA;"
46599726,45412334,6051596,2017-10-06T06:41:14,"<p>To Answer your question , it is close to option #2 that you have mentioned . The API gateway will generally check the validity of the authentication token and then pass over the request to your micro-service . However you need to decide at design time if your micro-service will also do another level of verification of the token.</p>&#xA;&#xA;<p>Please do note that the API gateway will not be enforcing Authorization , the authorization is something that your micro-service will have to enforce.</p>&#xA;"
48948969,48942153,1017868,2018-02-23T13:29:10,"<p>I think you're confused about the microservices stuff :) They're a solution to design a scalable application, nothing to do with db transactions. Even more, a db transaction (ideally) shouldn't span across processes, nevermind microservices.</p>&#xA;&#xA;<p>The pub-sub approach is how different micro services (works for in process models as well) communicate. Nothing to do with db transaction. Event sourcing is all about looking at the domain state as a collection of relevant changes. Very different objective compared to microservices.</p>&#xA;&#xA;<p>We're using ES because we like to store domain events as being the 'single source of truth', microservices or not. It's all about domain model design. </p>&#xA;&#xA;<p>Learn more about ES, DDD, CQRS and leave microservices for the future. They're too buzzwordy for now, few apps needs them, few developers can actually use them properly.</p>&#xA;"
47300537,47296131,4033292,2017-11-15T06:12:43,"<p>A transaction is a unit of work which either is %100 successful or %0 successful. The transaction is managed and implemented by the database, not your application. That is why you can scale out an application (create more instances) and your data remains consistent.</p>&#xA;&#xA;<p>Likewise many different applications can talk to the same database, this doesn't matter. Say you are transferring money, which corresponds to 2 rows in a database being updated in the same transaction. Lets say you do it incorrect and create 2 transactions instead to update each row individually. Whether you are running 1 or many applications or instances doesn't matter, if you did this incorrect in 2 transactions then you would risk having inconsistent data, say if another transaction debited an account and there wasn't enough money anymore in-between your transactions, then you might get an account with less than 0 dollars, not good not good.</p>&#xA;&#xA;<p>On another note this has a cost. Transactions are expensive, they take more time, and updates to a row will occur sequentially not at the same time, which is why systems which need to scale to extreme performance sometimes use append only databases / NoSQL and rely on eventual consistency instead.</p>&#xA;"
40794815,40786831,973324,2016-11-24T21:37:56,"<p>Seems that <a href=""https://redspread.com/"" rel=""nofollow noreferrer"">https://redspread.com/</a> (open source) addresses this particular issue, but needs more development before it'll be production ready - at least from my team quick glance at it.</p>&#xA;&#xA;<p>We'll stick with keeping yaml files in git together with the deployed application for now I guess. </p>&#xA;"
40796079,40733857,973324,2016-11-25T00:18:14,"<p>nginx ingress controller will do the job: <a href=""https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx"" rel=""nofollow noreferrer"">https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx</a></p>&#xA;"
46971441,46947956,7332364,2017-10-27T09:10:19,"<p>Here maybe a simple approach.</p>&#xA;&#xA;<p>Let’s assume you have the transaction:</p>&#xA;&#xA;<ol>&#xA;<li>write data to DB</li>&#xA;<li>send message over ZMQ</li>&#xA;<li>write to DB that sending was OK </li>&#xA;</ol>&#xA;&#xA;<p>So assume your app crash while you are in step 2 or 3. If so, you don’t know if the last message did receive the customers queue and you have to resend after restart all messages without the last confirmation (step 3).</p>&#xA;&#xA;<p>The problem is on the consumer side, because it’s possible, that they receives a message twice. To solve this problem, you can send with each message an transaction-ID which is always increasing. The consumer have to notice the transaction-ID of the last message. When the incoming message have a transaction-ID that is not higher than the transaction-ID of the last message the message can be ignored.</p>&#xA;&#xA;<p>The question is now if you can modify the message structure and which transaction-ID you can use.</p>&#xA;"
51840406,51829581,9705485,2018-08-14T11:31:25,"<p>It might help to think of a division between infrastructure, applications and platform. Think of infrastructure as hardware - servers, disk, compute, network routing etc. that you can use. Let's call 'application' the executables that you build from your code in order to implement business logic and satisfy your end users. Then platform is a connecting layer - tools and standards to help your applications make use of infrastructure.</p>&#xA;&#xA;<p>AWS is most famous for providing cloud infrastructure but it also provides a lot of services that could fall under platform. For example, it provides an API gateway service and container orchestration services with ECS or EKS (kubernetes) - these are more platform-level services as they are services that help your applications to scale and to talk to each other in the cloud.</p>&#xA;&#xA;<p>Spring Cloud is a set of tools that help you address common problems faced by cloud applications. Concerns like how to get applications to talk reliably to each other in the cloud (eureka, hystrix and ribbon for http, streams for messaging) how to provide a single entry-point for consumers to access a set of microservices (zuul and spring cloud gateway) and how to manage configuration across microservices (spring cloud config). Mostly I would put these concerns under 'platform' but there are grey areas. You normally add the spring cloud libraries to components you build, which is a bit more 'application'-like. Some of the same concerns are addressed by certain services available in AWS (especially ECS and EKS(kubernetes)).</p>&#xA;&#xA;<p>So the core areas of concern are very different for AWS (primarily infrastructure) and spring cloud (platform or platform-application bridge). But there can be some overlap at the platform level because AWS and spring cloud both offer so many options. It is tricky to find any direct comparisons because there are so many options but if you focus on EKS(kubernetes) in particular a good article comparing it with spring cloud is <a href=""https://dzone.com/articles/deploying-microservices-spring-cloud-vs-kubernetes"" rel=""nofollow noreferrer"">https://dzone.com/articles/deploying-microservices-spring-cloud-vs-kubernetes</a> </p>&#xA;"
51894741,51863914,9705485,2018-08-17T11:44:47,"<p>Finding an optimum solution can often be a moving target as a project evolves but I can share some points to be aware of. As you suggest, if your services are all inside Kubernetes and have a <a href=""https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service"" rel=""nofollow noreferrer"">ClusterIP then they will be automatically load-balanced</a>. <a href=""https://medium.com/@pczarkowski/kubernetes-services-exposed-86d45c994521"" rel=""nofollow noreferrer"">'Headless' services or external services, from outside the cluster</a>, would not. So if you have those then you could consider ribbon for doing the load-balancing inside the services. There is an implementation that uses <a href=""https://github.com/spring-cloud/spring-cloud-kubernetes"" rel=""nofollow noreferrer"">kubernetes for discovery</a> if you decide against eureka. Ribbon has the advantage of working with hystrix so you could then do rate-limiting in addition to load-balancing.</p>&#xA;&#xA;<p>There are also more sophisticated options available within the kubernetes ecosystem. You can have services of type <a href=""https://rancher.com/load-balancing-in-kubernetes/"" rel=""nofollow noreferrer"">LoadBalancer or use Ingress</a> but these tend to be used for external exposure whereas I think your question is more about service-to-service communication. Another way of addressing these concerns would be to use a service mesh and Istio is gaining a lot of attention in the Kubernetes space. You could use it to address service-to-service <a href=""https://istio.io/docs/concepts/traffic-management/#discovery-and-load-balancing"" rel=""nofollow noreferrer"">load-balancing</a> and <a href=""https://istio.io/docs/tasks/policy-enforcement/rate-limiting/"" rel=""nofollow noreferrer"">rate-limiting</a> and also <a href=""https://istio.io/docs/tasks/traffic-management/ingress/#configuring-ingress-using-an-istio-gateway"" rel=""nofollow noreferrer"">provide a gateway/s</a>. </p>&#xA;&#xA;<p>Using a kubernetes-native approach or mesh would have the advantage of doing this in a language-neutral way. If you needed ribbon/hystrix in your services and then wanted to add a python-based service you'd have to think about how to get consistent behaviour.</p>&#xA;&#xA;<p>I'd suggest that you be sure to consider what you and your team finds easiest to work with. There are a lot of options and nuances and it is good to be aware of them but you may find that you don't need anything fancy.</p>&#xA;"
51875794,51863731,9705485,2018-08-16T11:19:15,"<p>That <a href=""https://piotrminkowski.wordpress.com/2018/04/26/quick-guide-to-microservices-with-spring-boot-2-0-eureka-and-spring-cloud/"" rel=""nofollow noreferrer"">particular guide</a> says to set <code>--spring.profiles.active=native</code> in the startup parameters. It doesn't need a git repo with that option set as then it uses the native profile. You can also set it in the bootstrap.yaml file. Since that guide sets the port too you'd want your <a href=""https://github.com/piomin/sample-spring-microservices-new/blob/master/config-service/src/main/resources/bootstrap.yml"" rel=""nofollow noreferrer"">bootstrap.yaml file</a> to have:</p>&#xA;&#xA;<pre><code>spring:&#xA;  profiles:&#xA;    active: native&#xA;server:&#xA;  port: 8088&#xA;</code></pre>&#xA;&#xA;<p>I cloned <a href=""https://github.com/piomin/sample-spring-microservices-new"" rel=""nofollow noreferrer"">the project</a> and saw the same error and found that it then started without error after modifying the bootstrap.yaml as above.</p>&#xA;"
51799435,51798009,9705485,2018-08-11T11:58:28,"<p>The behaviour you describe would suggest a port mapping problem. Is your Spring boot service on the default port of 8080? Does the internal port of your Service match the port the Spring boot app is running on (it'll be in your app startup logs). The port in your screenshot seems to be 8899. It's also possible your pod is in a different namespace from your service. It would be useful to include your app startup logs and the output of 'kubectl get pods --all-namespaces', and 'kubectl describe service producer-service'.</p>&#xA;"
50084521,49880941,9705485,2018-04-29T07:57:10,"<p>Just to add to saptarshi basu's answer, you might want to look at <a href=""https://dzone.com/articles/deploying-microservices-spring-cloud-vs-kubernetes"" rel=""nofollow noreferrer"">https://dzone.com/articles/deploying-microservices-spring-cloud-vs-kubernetes</a> as it walks through the comparison and asks which responsibilities you might want to be handled by which components when using Spring cloud on kubernetes</p>&#xA;"
29499219,29479454,1318669,2015-04-07T19:02:04,"<p>I'm assuming Nginx has access to all internal docker ips (microservices). Unless I'm missing something, proxy_pass (<a href=""http://nginx.org/en/docs/http/ngx_http_proxy_module.html"" rel=""nofollow"">http://nginx.org/en/docs/http/ngx_http_proxy_module.html</a>) should work for you. You could assume a certain (externally available) url pattern to proxy to your microservice container without exposing the microservice port to the world.</p>&#xA;"
50129546,50128046,9121293,2018-05-02T07:49:22,"<p>Microservices is a collection loosely coupled services. For example, if you are creating an ecommerce application, <em>user management</em> can be a service, <em>order management</em> can be a service and <em>refund &amp; chargeback management</em> can be another service. Now each of these services can be further divided into smaller units, lets call them API Endpoints. For example - <em>user management</em> can have <em>login</em> as an endpoint and <em>signup</em> as another endpoint.</p>&#xA;&#xA;<p>If you want to leverage the power of Microservice architecture in its true sense, here is what I would suggest. For the above example, create 3 Springboot Applications for each service. First thing that you should do after this, is establish trust between those applications. I would prefer JWTs for trust establishment. After that everything is a piece of cake. Here are the answers you are looking for : </p>&#xA;&#xA;<ol>&#xA;<li><p>You should ideally use a database, as opposed to keeping the values in config server, for fetching a list of countries so that you need not recompile your code every time a new country is added.</p></li>&#xA;<li><p>You can easily restrict access using <code>@PreAuthorize</code> if Role based access is what you are referring to.</p></li>&#xA;<li><p>You can use OkHttp or any other HttpClient in this usecase. And you certainly need not maintain any local db. However, you can cache the output of the webservice call if that is a requirement.</p></li>&#xA;</ol>&#xA;&#xA;<p><em>P.S.: Establishing trust between microservices can be a complex task if you dont understand all the delicacies. In which case, I would recommend going ahead with a single Springboot application; which is a monolithic architecture. I would still recommend JWTs though.</em></p>&#xA;"
51126182,51120978,2511885,2018-07-01T19:04:23,"<p>I'm assuming you are looking at the linking of multiple HTTP Restful services.</p>&#xA;&#xA;<p>If you are designing the messaging architecture for a Restful set of microservices. You need to take into account the risk and benefits of each communication technology. Remember you don't have to use one, you can use the best tool for each job.</p>&#xA;&#xA;<p>The most common seemingly are HTTP and AMQP (Advanced Message Queuing Protocol). They both serve different purposes.</p>&#xA;&#xA;<p>If you are providing the back end to a website as micro services. Then chaining calls together in a synchronous fashion might be required, if the user requires a response from the website. Also it's likely your inbound requests will be HTTP at this point.</p>&#xA;&#xA;<p>Alternatively if no response is required, you might want to consider fire and forget messaging queues. These would allow messages to be passed to other micro-services asynchronously.</p>&#xA;&#xA;<p>There is a great e-book that covers this topic in great detail.&#xA;<a href=""https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/</a> [.NET Microservices. Architecture for Containerized .NET Applications]</p>&#xA;&#xA;<p>This page covers the exact topic you are discussing.&#xA;<a href=""https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/architect-microservice-container-applications/communication-in-microservice-architecture"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/architect-microservice-container-applications/communication-in-microservice-architecture</a> [Communication in a microservice architecture]</p>&#xA;"
51126311,51122354,2511885,2018-07-01T19:21:28,"<p>In the eShopOnContainers example( <a href=""https://github.com/dotnet-architecture/eShopOnContainers"" rel=""nofollow noreferrer"">https://github.com/dotnet-architecture/eShopOnContainers</a> ), they add the following fields at the time an order item is created:</p>&#xA;&#xA;<pre><code>        public void AddOrderItem(int productId, string productName, decimal unitPrice, decimal discount, string pictureUrl, int units = 1)&#xA;</code></pre>&#xA;&#xA;<p>This duplicates the catalog information, however as this is a point in time snapshot of the record. This is far safer than linking to the original data. </p>&#xA;&#xA;<p>At this point in the user journey you are in the domain of an order, if a user is viewing an order you can provide a link to the catalogue. However, the order service should be able to operate independently. Linking back to the catalogue for the information on products prevents the Order service owning it's own data.</p>&#xA;&#xA;<p>A nightmare scenario being that a price changes in the catalogue... What would happen to the historic orders?</p>&#xA;"
36204646,36197572,5788941,2016-03-24T16:00:21,"<p>In my company, we use JMS to add a ""intern"" communication stack to our microservice stack. It is reliable, simple to use, efficient and very performant.</p>&#xA;&#xA;<p>We use <a href=""http://activemq.apache.org/"" rel=""nofollow"" title=""Apache Activemq"">Apache ActiveMQ</a> as implementation, but <a href=""https://www.rabbitmq.com/"" rel=""nofollow"" title=""RabbitMQ"">RabbitMQ</a> is also widely used.</p>&#xA;"
49971583,49931495,3836941,2018-04-22T23:09:25,"<p>@Eric is correct. &#xA;Here I just show the problem I met. Hope which can also help others meeting the same problem.</p>&#xA;&#xA;<p>On the server side, I found there are many many Exceptions, such as</p>&#xA;&#xA;<pre><code>Apr 19, 2018 5:06:07 PM io.grpc.internal.SerializingExecutor run&#xA;SEVERE: Exception while executing runnable io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable@752265fc&#xA;redis.clients.jedis.exceptions.JedisException: Could not return the resource to the pool&#xA;    at redis.clients.jedis.JedisPool.returnResource(JedisPool.java:256)&#xA;    at redis.clients.jedis.JedisPool.returnResource(JedisPool.java:16)&#xA;    at redis.clients.jedis.Jedis.close(Jedis.java:3409)&#xA;    ...&#xA;    at io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:251)&#xA;    at io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:251)&#xA;    at io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:592)&#xA;    at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)&#xA;    at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:107)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;    at java.lang.Thread.run(Thread.java:748)&#xA;Caused by: redis.clients.jedis.exceptions.JedisConnectionException: java.net.SocketTimeoutException: Read timed out&#xA;    at redis.clients.util.RedisInputStream.ensureFill(RedisInputStream.java:202)&#xA;    at redis.clients.util.RedisInputStream.readByte(RedisInputStream.java:40)&#xA;    at redis.clients.jedis.Protocol.process(Protocol.java:151)&#xA;    at redis.clients.jedis.Protocol.read(Protocol.java:215)&#xA;    at redis.clients.jedis.Connection.readProtocolWithCheckingBroken(Connection.java:340)&#xA;    at redis.clients.jedis.Connection.getAll(Connection.java:310)&#xA;    at redis.clients.jedis.Connection.getAll(Connection.java:302)&#xA;    at redis.clients.jedis.Pipeline.sync(Pipeline.java:99)&#xA;    at redis.clients.jedis.Pipeline.clear(Pipeline.java:85)&#xA;    at redis.clients.jedis.BinaryJedis.resetState(BinaryJedis.java:1781)&#xA;    at redis.clients.jedis.JedisPool.returnResource(JedisPool.java:252)&#xA;    ... 13 more&#xA;Caused by: java.net.SocketTimeoutException: Read timed out&#xA;</code></pre>&#xA;&#xA;<p>Since it mentions the redis, I reviewed the related code, and found that the JedisPoolConfig might not have enough threads, and that the default timeout might be too short.</p>&#xA;&#xA;<p>So I enlarged both, and the problem solved.</p>&#xA;&#xA;<p>In other words, the server does not have enough resources to process the client side requests in a desired time. Which caused the gRPC server to fail, and therefore called the client side onError() methods.</p>&#xA;&#xA;<p>Thank you @Eric.</p>&#xA;"
29734520,29610354,1217549,2015-04-19T19:09:36,"<p>I am working on an open format for API discovery. Its not associated with any existing gateway or proxy architecture. I'm just looking to create way to define the meta data for API operations, which includes machine readable definitions like API Blueprint, Swagger, etc. - <a href=""http://apisjson.org"" rel=""nofollow"">http://apisjson.org</a>. Let me know if you have any questions, or I can help further.</p>&#xA;"
47048952,47023636,4458510,2017-11-01T05:47:44,"<p>It's hard to give a good answer without knowing more about what service (1) has to do when it is 'active'. It sounds you want cron to launch a task every minute.</p>&#xA;&#xA;<p>You can use cron in conjunction with push queues: <a href=""https://cloud.google.com/appengine/docs/standard/go/taskqueue/push/"" rel=""nofollow noreferrer"">https://cloud.google.com/appengine/docs/standard/go/taskqueue/push/</a></p>&#xA;&#xA;<p>When creating a push queue task, you can set the property <code>delay</code> before adding it to the queue: <a href=""https://cloud.google.com/appengine/docs/standard/go/taskqueue/reference#Task"" rel=""nofollow noreferrer"">https://cloud.google.com/appengine/docs/standard/go/taskqueue/reference#Task</a></p>&#xA;&#xA;<p>(For me in Python they called it <code>countdown</code> <a href=""https://cloud.google.com/appengine/docs/standard/python/refdocs/google.appengine.api.taskqueue.taskqueue#google.appengine.api.taskqueue.taskqueue.add"" rel=""nofollow noreferrer"">https://cloud.google.com/appengine/docs/standard/python/refdocs/google.appengine.api.taskqueue.taskqueue#google.appengine.api.taskqueue.taskqueue.add</a>)</p>&#xA;&#xA;<p>You could have a cron job that fires every 24 hrs. That cron job would load up your push queue with tasks who's delays are staggered. The delay of the first one is 1 min, the delay of the second one is 2 min, etc. </p>&#xA;"
39698276,39690572,266254,2016-09-26T08:44:29,"<p>use <a href=""http://docs.spring.io/spring-security/oauth/apidocs/org/springframework/security/oauth2/config/annotation/web/configuration/EnableResourceServer.html"" rel=""nofollow"">@EnableResourceServer</a> instead of <code>@EnableOAuth2Resource</code></p>&#xA;"
41363417,41358754,13070,2016-12-28T13:46:31,"<p>You would use AWS Lambda for this. Have API Gateway call a Lambda function. Have the Lambda function perform the actions you have described, calling your ECS services, and then return the final result.</p>&#xA;"
41983871,41983447,13070,2017-02-01T15:44:16,"<p>API Gateway with Usage Plans enabled, to enable rate limiting via API key, is going to be the recommendation for a solution on AWS. You can also look into <a href=""http://docs.aws.amazon.com/waf/latest/developerguide/tutorials-rate-based-blocking.html"" rel=""nofollow noreferrer"">doing something like this</a> in order to support rate-limiting by IP (although if I had to do all that for IP rate-limiting I'd probably look hard at third-party products like traefik).</p>&#xA;&#xA;<p>As mentioned in the comments, you can configure catch-all routes in API Gateway so that you don't have to modify the configuration every time you add a new route.</p>&#xA;"
34767836,34767582,4276238,2016-01-13T13:21:40,"<p>Well, there's no point to force microservices, if your application works fine as a signle app. They are great if you have for example separate databases for different things. Also, if one microservice fails, the main application is still running fine, so you don't have to worry about that.&#xA;Good example of microservices are basically advertisements on websites. They are not part of the main app, but they are there - working on their own. You can also read about many use cases, for example in Netflix.</p>&#xA;"
47859164,47837207,2706190,2017-12-17T20:31:49,"<p>Looks like security is a part of business logic here. In both examples. &#xA;Then security could be a part of data scheme. &#xA;For example,<br>&#xA;Patient can see his tests:<br>&#xA;<code>select * from test_result where patient_id=*patient_id*</code><br>&#xA;Doctor can see all test from his medical department:<br>&#xA;<code>select * from test_result where branch_id=*doctor_branch*</code>  </p>&#xA;&#xA;<p>I believe that to have separate MS for access control is a really bad idea and could lead serious performance problems. Just imagine situation that somebody with zero entity access tries to fetch all entities each time :) You will always need to handle larger result sets than actually needed. </p>&#xA;"
48951244,42651456,2182670,2018-02-23T15:30:34,"<p>A very good article is <a href=""http://ryanjbaxter.com/cloud/spring%20cloud/spring/2017/03/15/retrying-http-requests-in-spring-cloud-netflix.html"" rel=""nofollow noreferrer"">here</a> by <a href=""https://github.com/ryanjbaxter"" rel=""nofollow noreferrer"">Ryan Baxter</a> on the fixes for this issue in Brixton and Camden release. </p>&#xA;"
50167292,50161783,7756558,2018-05-04T04:41:54,"<p>So you have correctly specified the order in which container needs to be started but this doesn't guarantee that previous container (in your case config server is healthy or not) since ur docker-compose version is 3.1 &#xA;you can define healthchecks in your compose file</p>&#xA;&#xA;<p>eg:</p>&#xA;&#xA;<pre><code> registry:&#xA;    build:&#xA;      context: ../service-registry&#xA;      dockerfile: Dockerfile&#xA;    container_name: registry&#xA;    links:&#xA;      - configuration-server&#xA;    depends_on:&#xA;      configuration-server:&#xA;         condition: service_healthy&#xA;</code></pre>&#xA;&#xA;<p>and</p>&#xA;&#xA;<pre><code> configuration-server:&#xA;build:&#xA;  context: ../configuration-server&#xA;  dockerfile: Dockerfile&#xA;image: xyz/configuration-server&#xA;container_name: configuration-server&#xA;environment:&#xA;  - SPRING_PROFILES_ACTIVE=dev&#xA;  - SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKER=kafka&#xA;  - SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKER_PORT=9092&#xA;  - SPRING_CLOUD_STREAM_KAFKA_BINDER_ZKNODE=zookeeper&#xA;  - SPRING_CLOUD_STREAM_KAFKA_BINDER_ZKPORT=2181&#xA;depends_on:&#xA;  kafka:&#xA;    condition: service_healthy&#xA;  zookeeper:&#xA;    condition: service_healthy&#xA;healthcheck:&#xA;    test: ""exit 0""&#xA;</code></pre>&#xA;&#xA;<p>notice the healthcheck in config server which would be invoked from registry server (condition:service healthy)&#xA;you can implement your own custom healthcheck to something more sophesticated like </p>&#xA;&#xA;<pre><code>healthcheck:&#xA;  test: [""CMD"", ""curl"", ""-f"", ""http://localhost""]&#xA;  interval: 1m30s&#xA;  timeout: 10s&#xA;  retries: 3&#xA;  start_period: 40s&#xA;</code></pre>&#xA;&#xA;<p>refer this:<a href=""https://docs.docker.com/compose/compose-file/compose-file-v2/#healthcheck"" rel=""nofollow noreferrer"">https://docs.docker.com/compose/compose-file/compose-file-v2/#healthcheck</a></p>&#xA;&#xA;<p>i guess that should be enough to kickstart your containers, let me know if it works</p>&#xA;"
50169027,50134195,7756558,2018-05-04T07:00:36,"<p>yes, Oauth2 is the right way to go about it</p>&#xA;&#xA;<p>you have to implement a authorization server which will grant you an access token, and you will register client ,user and scopes in this authorization server, your microservices will work as resource servers, endpoints which you want to expose to certain users with certain privileges would be annotated with @preauthorize(hasScope('readThis') &amp;&amp; hasAuthority('admin'), similarly you can restrict other endpoints, resource server will authenticate token with auth server so you would have to maintain either inMemoryTokern store or jdbc token store, </p>&#xA;&#xA;<p>in case you want to some information in your access token, lets say organization etc, implement jwt also, in that way you will also save extra call from resource server to auth server for validating tokens and you wont have to store token in inmemory or jdbc store</p>&#xA;"
50563211,50562495,7756558,2018-05-28T09:32:24,"<p>As mentioned in the comment, there are many ways you can go about it,</p>&#xA;&#xA;<p>case 1: all are independent services, trivial case, no need to do anything, call all the services in blocking or non-blocking way, calling service 2 will in both case result in timeout</p>&#xA;&#xA;<p>case 2: services are dependent M2 depends on M1 and M3 depends on M2</p>&#xA;&#xA;<p>option a) M1 can wait for service M2 to come back up, doing periodic pings or fetching details from registry or naming server if M2 is up or not</p>&#xA;&#xA;<p>option b) use hystrix as a circuit breaker implementation and handle fallback gracefully in M3 or your orchestrator(guy who is calling these services i.e M1,M2,M3 in order)</p>&#xA;"
50636472,50635469,7756558,2018-06-01T05:12:51,"<p>You can definitely use API gateway lets say for that matter (netflix -zuul) for inter-service calls, only thing of concern for you would be,</p>&#xA;&#xA;<p>what happens when you start versioning your services, assuming you'll be using eureka as a naming server from which zuul gateway will fetch all registered services, but now in your case zuul will get two instances of your service (version previous and verison next) and ribbon will load balance the requests between the two, this point is already thoughtfully covered in </p>&#xA;&#xA;<p><a href=""https://stackoverflow.com/q/31313170/7756558"">How to route in between microservices using Spring Cloud &amp; Netflix OSS</a> </p>&#xA;&#xA;<p>Basically if you are familiar with BlueGreen Deployment model, implementing that would be a problem, surely there are proper workarounds for that as in defining/registering  some metadata along with your previous and latest versions which would later be picked by ribbon client to route accordingly</p>&#xA;"
40044489,40044128,4552718,2016-10-14T13:33:17,"<p>Looks like it has issue in finding the cert file. I would try something like this.</p>&#xA;&#xA;<pre><code>   ssl:&#xA;      trust-store: file:/Certs/caCert/server.p12&#xA;</code></pre>&#xA;&#xA;<p>Even better, I would add my certificate into a keystore file using keytool so I can access my keystore file like </p>&#xA;&#xA;<pre><code>   ssl:&#xA;      enabled: true&#xA;      key-store: file:/Certs/keystore&#xA;      key-password: keypwd&#xA;</code></pre>&#xA;"
32866501,32863771,1807118,2015-09-30T12:46:13,"<p>Even though the question was already answered, I'll show another way I managed to solve the problem.</p>&#xA;&#xA;<p>I used type <code>Object</code> instead of <code>String</code>:</p>&#xA;&#xA;<pre><code>@RequestMapping(method = RequestMethod.POST, value=""/data"")&#xA;ResponseEntity&lt;Object&gt; createUnit(@RequestBody Object data) {&#xA;    URI uri = util.getServiceUrl(""dataservice"");&#xA;    String url = uri.toString() + ""/data"";&#xA;&#xA;    ResponseEntity&lt;Object&gt; result = restTemplate.postForEntity(url, data, Object.class);&#xA;    return new ResponseEntity&lt;Object&gt;(result.getBody(), HttpStatus.OK);&#xA;}&#xA;</code></pre>&#xA;"
34904774,34903605,1807118,2016-01-20T16:01:50,"<p>The pro's of micro-services are that your applications scale very well. You divide your application in tiny services. It's a hard thing to decide which bounded services you will need. But once you have done this, you can easy scale specific services up to multiple times (the services that actually get the most load) rather than needing to scale your whole application up.</p>&#xA;&#xA;<p>Another advantage is that it is easier for new developers to start creating new features for your application, since everything is divided into these separated service --> Each service has its own (small) codebase.</p>&#xA;&#xA;<p>The biggest disadvantage of using microservices are of course that there is a higher chance of failure during the communication between the different services.</p>&#xA;&#xA;<p>The usage of microservices usually only starts paying off when your application is getting too big to maintain as a monolithic application.&#xA;Try starting as a monolithic but keep your bounded contexts in mind while developing (try to keep it as separated as possible) to you can migrate to microservices if needed later on.</p>&#xA;"
33042544,33041733,1807118,2015-10-09T15:55:09,"<p>While I'm relatively new to the microservices world, I'll try to answer your question as complete as possible.</p>&#xA;&#xA;<p>When you use the microservices architecture, you will have increased decoupling and separation of concerns. Since you are litteraly splitting up your application.</p>&#xA;&#xA;<p>This results into that your <strong>codebase will be easier to manage</strong> (each application is independent of the other applications to stay up and running). Therefore, <strong>if you do this right</strong>, it will be <strong>easier in the future to add new features</strong> to your application. Whereas with a monolithic architecture, it might become a very hard thing to do if your application is big (and you can assume at some point in time it will be).</p>&#xA;&#xA;<p>Also <strong>deploying the application is easier</strong>, since you are building the independent microservices separately and deploying them on separate servers. This means that you can build and deploy services whenever you like without having to rebuild the rest of your application.</p>&#xA;&#xA;<p>Since the different services are small and deployed separately, it's obvious <strong>easier to scale</strong> them, with the advantage that you can scale specific services of your application (with a monolithic you scale the complete ""thing"", even if it's just a specific part within the application that is getting an excessive load).</p>&#xA;&#xA;<p>However, for applications that are not intended to become too big to manage in the future. It is better to keep it at the monolithic architecture. Since the microservices architecture has some serious difficulties involved. I stated that it is easier to deploy microservices, but this is only true in comparison with big monoliths. Using microservices you have the added complexity of distributing the services to different servers at different locations and you need to find a way to manage all of that. Building microservices will help you in the long-run if your application gets big, but for smaller applications, it is just easier to stay monolithic.</p>&#xA;"
36715821,36715395,1807118,2016-04-19T10:34:37,"<p>Generally, there is a <code>discovery service</code> where the services can register theirselves. If a service needs to communicate with another service, it requests instances of these service at the discovery service and most of the time there is client-side load balancing as well.</p>&#xA;&#xA;<p>I've also used <code>Hystrix</code> to wrap HTTP calls to handle failure.</p>&#xA;&#xA;<p>I'm not familiar with building microservices with the <code>Play framework</code>, but when using <code>Spring</code> I've used:</p>&#xA;&#xA;<ul>&#xA;<li>Netflix Eureka - as the discovery service</li>&#xA;<li>Hystrix - To handle HTTP failures / timeouts when services go down</li>&#xA;<li>Ribbon - Client side load-balancing</li>&#xA;</ul>&#xA;&#xA;<p>A blog series that provides an example using the full stack is <a href=""http://callistaenterprise.se/blogg/teknik/2015/05/20/blog-series-building-microservices/"" rel=""nofollow"">this one</a>. However, it is using <code>Spring</code> but I guess it can be helpful either way.</p>&#xA;"
36701558,36701111,1807118,2016-04-18T18:22:30,"<p>I'm not sure if what I am going to answer is thé right way. I'm still learning myself.. But I can tell you how I've implemented my microservices attempts.. </p>&#xA;&#xA;<p>First, I started with <code>HTTP</code> communication based microservices <a href=""http://callistaenterprise.se/blogg/teknik/2015/05/20/blog-series-building-microservices/"">using this blog</a>. This works fine, but the problem is, that you create <em>dependendies</em> between your services. <strong>Service A</strong> needs to be aware of a <strong>service B</strong> and needs to call it <em>directly</em> (via service discovery etc of course). This is what you generally are trying to avoid when developing microservices.</p>&#xA;&#xA;<p>Another approach that I've started with lately, is using a <code>message bus</code>. It's actually the 3rd option that you touched in your question.</p>&#xA;&#xA;<p>I have a <strong>service A</strong>, which stores persons (just an example). What the service does when it creates a new person is: It sends an <code>event</code> on a <code>RabbitMQ</code> bus: <code>personCreatedEvent</code>. &#xA;If there are any other services interested in events like this, they can <em>subcribe</em> to them. These interested services keep the <strong>relevant information</strong> that they are interested in, in their own datastores.</p>&#xA;&#xA;<p>With this last approach, there is not really a dependency between your  services, because they don't communicate with each other directly. <strong>Service A</strong> is not aware of <strong>service B</strong>, because B just sends events to <code>RabbitMQ</code> to whichever service is interested to these events and vice versa.</p>&#xA;&#xA;<p>Of course, you have duplications between datastores over the service. But this can be profitable as well, e.g. service B doesn't need to use the same schema or data store mechanism as service A. It only stores the relevant information in the way that is best for this service.</p>&#xA;"
35379769,35379246,3117838,2016-02-13T12:23:03,"<p>If you want to use same models in different python apps (not modules, but apps, with different uwsgi-instances etc.), then I think that the much common and handy solution - provide interface for these models through some API:</p>&#xA;&#xA;<ol>&#xA;<li>You can build REST interface in each of your projects, and then call appropriate API methods to get result. </li>&#xA;<li>You can use Celery package to build internal API based on the AMQP transport. Then you need to configure your apps to listen some RabbitMQ queue which will be used as message storage for your projects communnication.</li>&#xA;</ol>&#xA;"
50897009,45453061,2076174,2018-06-17T13:13:12,"<p>I got simple answer <a href=""https://www.quora.com/What-is-micro-services-and-how-is-it-different-from-web-services"" rel=""nofollow noreferrer"">here</a> that says :&#xA;Microservices is a new software architecture. It is based on web services. But it can be any service implemented as an independent feature that has its own database and can be deployed independently.</p>&#xA;&#xA;<p>Microservices is an architecture wherein all the components of the system are put into individual components, which can be built, deployed, and scaled individually.</p>&#xA;"
43386163,43369008,6997262,2017-04-13T07:33:12,"<p>This command copy the cluster credentials to your machine. Background processes are ssh to your cluster VM and copy the credentials. </p>&#xA;&#xA;<p>So, <strong>you should ensure you could ssh to the master VM manual</strong>. If you could not ssh to master VM manual, <code>az</code> command also could not do it. You could get your <code>master-dns-name</code> on Azure Portal.</p>&#xA;&#xA;<pre><code>ssh -i id_rsa &lt;user&gt;@&lt;master-dns-name&gt;&#xA;</code></pre>&#xA;&#xA;<p>Notes: If <code>az</code> command does not work and you could ssh to master VM, you could download credentials to your machine. They are same. You could check your <a href=""https://docs.microsoft.com/en-us/azure/container-service/container-service-connect#connect-to-a-kubernetes-cluster"" rel=""nofollow noreferrer"">link</a> about this.</p>&#xA;&#xA;<p>You also need check your azure cli version. You could use the following commands</p>&#xA;&#xA;<pre><code>az --version&#xA;</code></pre>&#xA;&#xA;<p>My version is 2.02. It works for me.</p>&#xA;"
45605406,45592666,6997262,2017-08-10T05:48:04,"<blockquote>&#xA;  <p>-Without application gateway i can access my microservices without any issue.</p>&#xA;</blockquote>&#xA;&#xA;<p>When your VM is added to a application gateway,  it will not have an impact on the original access. Which your port is your service listening. You could check on linux VM.</p>&#xA;&#xA;<pre><code>netstat -ant|grep &lt;port&gt;&#xA;</code></pre>&#xA;&#xA;<p>Please ensure your service is listening and you could access the service with <code>curl &lt;privateip&gt;:&lt;port&gt;</code>.</p>&#xA;&#xA;<p>According to your description, I suggest you could add VM's nic to application in backend pool. Like below:</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/RLxgQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RLxgQ.jpg"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>If your service is not listening on 80, you also need check application gateway HTTP setting.</p>&#xA;&#xA;<p>More information please refer to this <a href=""https://docs.microsoft.com/en-us/azure/application-gateway/application-gateway-create-gateway-portal"" rel=""nofollow noreferrer"">link</a>.</p>&#xA;"
51927515,51926575,4516110,2018-08-20T09:13:17,"<p>Ideally, there should be a <code>REST</code> call to get the data from other <code>MS</code>. </p>&#xA;&#xA;<p>But if these calls are very expensive you should consider changing the DB design and put this required data at a commonplace. </p>&#xA;"
50129007,50128046,4516110,2018-05-02T07:13:56,"<p><strong>First of all, there is no single way design micro-service</strong> <strong>, one has to choose according to the use case and project requirement.</strong> </p>&#xA;&#xA;<ol>&#xA;<li><blockquote>&#xA;  <p>Can I keep these in a Config Server? or should it come from Database?</p>&#xA;</blockquote></li>&#xA;</ol>&#xA;&#xA;<p>Again, it depends upon the use case and requirement. However, because every MS should have their own DB then you can use DB if the countries have only names. But if they have some relationship with City/State then you should use DB only.</p>&#xA;&#xA;<ol start=""2"">&#xA;<li><blockquote>&#xA;  <p>If DB should each of the Microservice have their own DB for lookup&#xA;  value or can it be a common one?</p>&#xA;</blockquote></li>&#xA;</ol>&#xA;&#xA;<p>No, IMO multiple MS should not depend on a single DB.Because if the DB fails then all the MS will fail, which should not be done. Each MS should work alone with depending on other DB or MS.</p>&#xA;&#xA;<ol start=""3"">&#xA;<li><blockquote>&#xA;  <p>should the validation service be a common microservice for all&#xA;  UseCases/Screens</p>&#xA;</blockquote></li>&#xA;</ol>&#xA;&#xA;<p>Same as point 2</p>&#xA;&#xA;<ol start=""4"">&#xA;<li><blockquote>&#xA;  <p>How do I deal with a use-case where the backend is not a Database call&#xA;  but another Web-service call? Will I need some local DB still to&#xA;  maintain some state in between these calls and finally pass on the&#xA;  status to GUI?</p>&#xA;</blockquote></li>&#xA;</ol>&#xA;&#xA;<p>If you are using HTTP then you should not save the state of any request. If you want to redirect the request to another MS then you can use  <a href=""https://cloud.spring.io/spring-cloud-netflix/multi/multi_spring-cloud-feign.html"" rel=""nofollow noreferrer"">Feign client</a> which provides a very good way to call <code>rest-api</code> and other important features like: <code>Load balancing</code>.</p>&#xA;"
47473115,36407520,5962766,2017-11-24T12:14:24,"<p>I would use Spring Boot because it's more popular, has good documentation/examples and more support on Stack Overflow. Also provides convenient tools for testing. Some comparisons:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://blog.akquinet.de/2017/01/12/the-lightweightness-of-microservices-comparing-spring-boot-wildly-swarm-and-haskell-snap/"" rel=""nofollow noreferrer"">The lightweightness of microservices – Comparing Spring Boot, WildFly Swarm, and Haskell Snap</a></li>&#xA;<li><a href=""https://medium.com/@dersoz/spring-boot-vs-wildfly-swarm-in-the-land-of-enterprise-java-4ebd210fbc4a"" rel=""nofollow noreferrer"">Spring Boot vs WildFly Swarm In the Land of Enterprise Java</a></li>&#xA;<li><a href=""http://ksoong.org/wildfly-swarm-spring-boot"" rel=""nofollow noreferrer"">WildFly Swarm vs Spring Boot</a></li>&#xA;</ul>&#xA;"
44503569,44503237,1380111,2017-06-12T15:45:29,"<p>I'd say it's the common way tu use some kind of reverse proxy as security gateway, that generates some kind of security tokens, like SAML.</p>&#xA;&#xA;<p>Otherwise for example, if you have some chained calls, like micro service user calls micro service transaction log, each and every microservice does the auth for itself again and again. This can get quite expensive.</p>&#xA;"
41780941,41672971,5446671,2017-01-21T15:11:36,"<p>Add your yml or property file configuration to </p>&#xA;&#xA;<pre><code>zuul:&#xA;  sensitiveHeaders: Cookie,Set-Cookie&#xA;  add-proxy-headers: true&#xA;</code></pre>&#xA;&#xA;<p>then zuul allow header to go other end points.</p>&#xA;"
41843358,41756234,5446671,2017-01-25T04:23:59,"<p>spring boot configuration are ok. then go to consul ui using localhost:8500 and select key/value and add key value like below</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/qi5t3.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/qi5t3.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>then create it.then create configuration are like below </p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/HNtFv.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/HNtFv.png"" alt=""enter image description here""></a> </p>&#xA;&#xA;<p>then restart your springboot application. your port change to 8084. </p>&#xA;&#xA;<blockquote>&#xA;  <p>note: you can use YMAL or GIT2consul for more configuration</p>&#xA;</blockquote>&#xA;"
43171772,43100199,5446671,2017-04-02T18:10:33,"<p>you need to allow sensitiveHeaders. </p>&#xA;&#xA;<pre><code>     zuul:&#xA;        routes:&#xA;            resource:&#xA;                path: /resource/**&#xA;                url: http://localhost:8002/&#xA;                sensitiveHeaders: Cookie,Set-Cookie&#xA;</code></pre>&#xA;"
50484827,50484157,5446671,2018-05-23T09:35:28,"<p>Zuul provides only the routing part of the Gateway pattern. But If you are using replicated micro-services the Ribbon come to the action. Ribbon default use round robin method to distribute the message to each replica.</p>&#xA;&#xA;<p>EX: Suppose there is 3 clients come make requests. According to the figure, client's requests come to zuul and ribbon distribute 1st client to replica1 and 2nd to replica 2nd and 3rd to replica 3rd likewise. That mean Load balancing the request. </p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/Y58yE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y58yE.png"" alt=""enter image description here""></a></p>&#xA;"
34445035,34424706,211727,2015-12-23T23:03:35,"<p>Or you could share data between the two servers using DDP:</p>&#xA;&#xA;<pre class=""lang-js prettyprint-override""><code>var conn = DDP.connect('http://admin-server');&#xA;&#xA;Tracker.autorun(function() {&#xA;  var status = conn.status();&#xA;&#xA;  if(status.connection) {&#xA;    var messages = new Mongo.Collection('messages', {connection: conn});&#xA;    conn.subscribe('messages', function() { console.log('I haz messages'); });&#xA;  }&#xA;});&#xA;</code></pre>&#xA;&#xA;<p>This creates a local collection named <code>messages</code> that pulls data from the ""admin server"" over DDP. This collection only exists in memory - nothing is created in mongo. You can do this on the server or client. Definitely not the best choice for large datasets. Limit the data transfer with publications.</p>&#xA;"
48482668,48482639,80911,2018-01-28T02:41:49,"<p>Generally you don't need to maintain a map of in-flight requests, because this is basically done for you by node.js's http library.</p>&#xA;&#xA;<p>Just use express as it's intended, and this is probably something you never really have to worry about, as long as you avoid any global state.</p>&#xA;&#xA;<p>If you have a weirder pattern in mind to build, and not sure how to solve it. It might help to share a simple example. Chances are that it's not hard to rebuild and avoid global state.</p>&#xA;"
44195514,44195150,2512399,2017-05-26T06:59:20,"<p>Have one class with a main method that looks like the following:</p>&#xA;&#xA;<pre><code>final class Forever&#xA;{&#xA;    private Forever ()&#xA;    {}&#xA;&#xA;    public static void main (String [] args)&#xA;    {&#xA;        while (true)&#xA;        {&#xA;           try&#xA;           {&#xA;               // don't do anything here&#xA;               // especially do NOT connect to any databases&#xA;           }&#xA;           catch (Throwable t)&#xA;           {&#xA;&#xA;           }    &#xA;        }&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Put this java code on a space probe, running Linux, with an infinite energy source. Make sure it stays clear of planets, exploding stars, black holes, etc. </p>&#xA;&#xA;<p><strong>Note</strong>: Don't allow humans to connect to the probe.</p>&#xA;&#xA;<p>This may not get the forever you desire; however, it may make it up until the end of the universe.</p>&#xA;&#xA;<p><strong>Update</strong>:&#xA;So far, I've been running this for about <strike>5</strike> 10 minutes on my development environment and it is still running! I may push this to QA soon.</p>&#xA;"
35465541,35465175,621583,2016-02-17T19:08:36,"<p>DISCLAIMER: These are answers based on my point of view based on my experience working with various microservice based architecture.</p>&#xA;&#xA;<ol>&#xA;<li><p>Ideally microservices should be independent and they shouldn't share binary dependencies (specially for domain concepts). In reality, specially for big architectures, that is more challenging than it seems at first sight. One of the idea behind microservices is that you could swap one for another without any other microservice noticing, provided they reply to the same messages. This becomes easily a nightmare when you start introducing binary depdencies.</p></li>&#xA;<li><p>One of the possible approaches would be to treat other microservice as a particular client, giving them a specific role that allows for certain operations to be executed. However, you can consider deploy a specific microservice that only answers on a subnet non visible by the outside network. I would still perform some kind of access control between parties (although I never used it, <a href=""http://jwt.io/"" rel=""nofollow"">http://jwt.io/</a> seems to be picking up). The mechanism for authentication would be similar to the one used to authenticate your clients.</p></li>&#xA;<li><p>I wouldn't - specially because of what I described on point 1. You'll be introducing a high risk of services stepping on each other toes, losing the ""independent"" part which is key for a microservices oriented architecture. Every service should own its own storage, because you should be able to be change storage engine for a single microservice at any given time without changing behaviour of the other services.</p></li>&#xA;<li><p>I'm not entirely sure what you mean here. It's very important to maintain backwards compatibility when dealing i.e. with frontend client. You can version your microservices (e.g. <code>/v1/messages</code> if you're using a <code>REST</code> like approach over HTTP), then have your client use a specific version of the service. By using different version, you can decouple frontend and backend service releases so - again - they're independent.</p></li>&#xA;</ol>&#xA;&#xA;<p>It looks like a lot to think about (and it actually is), but using good practices from the start will lead to avoid a lot of problems later.</p>&#xA;"
51679289,51665264,5209103,2018-08-03T19:52:24,"<p>You can, but its not designed for it. Will create congestion because the entirety of the file will be kept in memory until transfer is complete.  Just set up a http server, communicate the url through a consumer and upload it over http. Then you get all http support as well. </p>&#xA;&#xA;<p>If you don't want a perm http server for it, just start one whenever a request for an upload comes in. </p>&#xA;"
40833400,40832097,571407,2016-11-27T20:13:54,"<p>You're using a mock profileService in your test, and you never tell that mock what to return. So it returns null.</p>&#xA;&#xA;<p>You need something like</p>&#xA;&#xA;<pre><code>when(profileService.create(any(User.class)).thenReturn(new Profile(...));&#xA;</code></pre>&#xA;&#xA;<p>Note that using</p>&#xA;&#xA;<pre><code>when(profileService.create(user).thenReturn(new Profile(...));&#xA;</code></pre>&#xA;&#xA;<p>will only work if you properly override equals() (and hashCode()) in the User class, because the actual User instance that the controller receives is a serialized/deserialized copy of the user you have in your test, and not the same instance.</p>&#xA;"
45232259,45208766,1884431,2017-07-21T08:02:00,"<p>A message queue provide an <strong>asynchronous communications protocol</strong> - You have the option to send a message from one service to another without having to know if another service is able to handle it immediately or not. Messages can wait until the responsible service is ready. A service publishing a message does not need know anything about the inner workings of the services that will process that message. This way of handling messages <strong>decouple</strong> the producer from the consumer. </p>&#xA;&#xA;<p>A message queue will keep the processes in your application separated and independent of each other; this way of handling messages could create a system that is <strong>easy to maintain and easy to scale</strong>. </p>&#xA;&#xA;<p><a href=""https://www.cloudamqp.com/blog/2017-09-25-breaking-down-a-monolithic-system-into-microservices.html"" rel=""noreferrer"">Here</a> is a story explaining how Parkster (a digital parking service) are breaking down their system into multiple microservices by using RabbitMQ.</p>&#xA;&#xA;<p><a href=""https://www.cloudamqp.com/blog/2015-05-18-part1-rabbitmq-for-beginners-what-is-rabbitmq.html"" rel=""noreferrer"">This guide</a> follow a scenario where a web application allows users to upload information to a web site. The site will handle this information and generate a PDF and email it back to the user. Handling the information, generating the PDF and sending the email will in this example case take several seconds and that is one of the reasons of why a message queue will be used. </p>&#xA;&#xA;<p><a href=""https://www.cloudamqp.com/blog/2017-01-03-a-microservice-architecture-built-upon-rabbitmq.html"" rel=""noreferrer"">Here</a> is a story about <em>how</em> and <em>why</em> CloudAMQP used message queues and RabbitMQ between microservices. </p>&#xA;&#xA;<p>Please note that I work for the company behind CloudAMQP.</p>&#xA;"
40378158,40377377,6212879,2016-11-02T10:53:36,"<p>You are onto the right way. Exposing a service with a REST architecture is powerful and simple. Each microservice exposes some functionalities that can be invoked by others microservices. You can do this with SpingMVC and the annotation @RestController. To invoke REST API you may use the Spring class RestTemplate.</p>&#xA;&#xA;<p>You probably also need a gateway that redirects requests to the right service. I suggest you to try the Netflix Cloud Stack:</p>&#xA;&#xA;<ul>&#xA;<li>Zuul. This is the entry point of your application. Every request is issued to it. It should orchestrate the whole ecosystem. </li>&#xA;<li>Eureka Client - Eureka Server. All of your microservices should somehow tell somebody that they are up and running and can accept requests. So you can use Eureka Server to accept registrations from your services and mark your microservices as Clients. </li>&#xA;<li>Ribbon. Another one important thing is the loadbalancing of the requests. With Ribbon you can do this easily.</li>&#xA;</ul>&#xA;&#xA;<p>If you are using Spring Boot you can setup this architecture quickly with some annotations.</p>&#xA;&#xA;<p>You can find here a simple example: <a href=""https://cloud.spring.io/spring-cloud-netflix/"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-netflix/</a></p>&#xA;"
50400667,50400384,1174543,2018-05-17T21:19:42,"<p>I agree with the statements about coupling. I'm forced to use a specific set of Maven dependencies in a reuse scenario, and no one is allowed to update them. The result is that creating new services gets harder because the frameworks are out of date, and so is the documentation.</p>&#xA;&#xA;<p>On the other hand, code reuse can save a lot of time and money, especially boilerplate code used in services, if it is well constructed and has proper tests. </p>&#xA;&#xA;<p>I think there is a middle ground here that involves versioning and a certain amount of routine maintenance.</p>&#xA;"
49275687,49242181,6158863,2018-03-14T10:46:38,"<p>The below error is trying to tell you that you are registering <code>IDatabaseInitializer</code> as a <strong>scoped service</strong> but trying to access it outside the <strong>scope</strong>:</p>&#xA;&#xA;<pre><code>""System.InvalidOperationException: 'Cannot resolve scoped service 'Actio.Common.Mongo.IDatabaseInitializer' from root provider.'""&#xA;</code></pre>&#xA;&#xA;<p>Try to create a scope and then use the service like:</p>&#xA;&#xA;<pre><code>using (var serviceScope = app.ApplicationServices.CreateScope())&#xA;{&#xA;    serviceScope.ServiceProvider.GetService&lt;IDatabaseInitializer&gt;().InitializeAsync();&#xA;}&#xA;</code></pre>&#xA;"
50431196,50427036,151312,2018-05-20T02:40:43,"<p>FYI, this is an ""opinion question"", which some people frown upon on StackOverflow, but I'm not one of those people so I'll give you my 2¢.</p>&#xA;&#xA;<h1>Integrated Solution vs Composite Solution</h1>&#xA;&#xA;<p>The scenario that you're describing is one of the reasons that I created <a href=""https://git.coolaj86.com/coolaj86/greenlock-express.js"" rel=""nofollow noreferrer"">Greenlock.js</a> (suite of ACME / Let's Encrypt client library, cli, and web server).</p>&#xA;&#xA;<p>I wanted a fully integrated solution that could automatically provision certificates without manual intervention (also, at the time certbot was very difficult to install and used so much RAM that I couldn't use it on the IoT devices I was working with).</p>&#xA;&#xA;<p>In my case I created a plugin system to allow for different storage mechanisms (fs, redis, sql, aws s3, azure storage, etc) and then other authors supplied most of those mechanisms.</p>&#xA;&#xA;<p>It sounds like certbot will probably work for you as a composite solution (wrapping it), but if you're going to go through the trouble of creating certificate stores and such, you might also want to integrate through with a Java ACME library (just make sure it supports ACME draft 11 / Let's Encrypt v02).</p>&#xA;&#xA;<p>Another thought would be to use something like Greenlock as the https frontend that reverse proxies to your application (though Greenlock may not be the thing that meets your needs - a java or go solution, if one exists, might work better for you from the sound of it).</p>&#xA;&#xA;<p>(It also sounds interesting to me to create some REST APIs around Greenlock to allow it to function as a microserice for the distribution of certificates and it wouldn't take much work to do that - but I'd have to learn more about your project to better understand)</p>&#xA;&#xA;<p>Recap:</p>&#xA;&#xA;<ul>&#xA;<li>compose with (wrap) certbot on each service and sync files to a remote store as a microservice</li>&#xA;<li>integrate a native ACME / Let's Encrypt solution and sync with a plugin for storage to allow various types of existing storage services</li>&#xA;<li>create a separate service to handle certificate issuance, use a rest api on each service</li>&#xA;</ul>&#xA;&#xA;<p>They're all valid and depending on what code is already available they'll all pretty easy to do.</p>&#xA;&#xA;<p>The only problem with running certbot on each instance is that it may be challenging to hook into the system it uses to check for certificates to have it use a remote service instead.</p>&#xA;&#xA;<h3>Best choice?</h3>&#xA;&#xA;<p>I personally believe the second option (integrating ACME code into the services and having a plugin architecture for storage) is the best because in the case that the microservice that handles the ACME certificates fails, your other services are still capable of getting their own (the lookup fails, they get a certificate rather than using an existing one). It's a progressive enhancement. This is also what the plugin architecture of Greenlock lends itself to quite nicely.</p>&#xA;&#xA;<h1>Formats &amp; Bundles</h1>&#xA;&#xA;<p>Some might say that you want to have a keystore with passphrases and such using P12, and I think that's valid.</p>&#xA;&#xA;<p>However, this is going to be encrypted in transit already and it's almost certainly exposed in such a way that if your webserver were compromised, the passphrase would also be compromised, so I'd lean to using simple PEM and JWK.</p>&#xA;&#xA;<p>In your use case it sounds like you probably don't need JWK, so that would mean just PEM.</p>&#xA;&#xA;<p>PEM only requires stripping whitespace and comments and then decoding from standard Base64 if, for whatever reason, you needed to decode it to DER manually. Likewise, it can be converted to Base64URLSafe by removing comments and whitespace and then replacing <code>-</code> with <code>_</code> and <code>/</code> with <code>+</code>.</p>&#xA;&#xA;<p>Also, I really like the pattern of storing and distributing these pieces:</p>&#xA;&#xA;<ul>&#xA;<li><code>cert.pem</code></li>&#xA;<li><code>chain.pem</code></li>&#xA;<li><code>privkey.pem</code></li>&#xA;</ul>&#xA;&#xA;<p>Because it's easy to combine those in whatever way you'd need in order to deliver them to any type of webserver.</p>&#xA;&#xA;<ul>&#xA;<li><code>fullchain.pem</code> (<code>cert.pem</code> + <code>chain.pem</code>) for Apache, Nginx, Node, etc</li>&#xA;<li><code>bundle.pem</code> (<code>fullchain.pem</code> + <code>privkey.pem</code>) for HAProxy</li>&#xA;</ul>&#xA;&#xA;<p>So I'd say send a JSON object with the PEM:</p>&#xA;&#xA;<pre><code>{ ""cert"": ""...""&#xA;, ""chain"": ""...""&#xA;, ""privkey"": ""...""&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>And then let the client do <code>response.cert + '\\r\\n' + response.chain</code>, etc to construct <code>fullchain.pem</code> or <code>bundle.pem</code> as needed.</p>&#xA;&#xA;<h3>Best choice?</h3>&#xA;&#xA;<p>Whatever is simplest and most portable - probably PEM, then JWK after that, perhaps Base64URLSafe next, but not a custom format for any specific Java library. You may expand to supporting non-Java services in the future.</p>&#xA;"
51797196,38380827,6448656,2018-08-11T06:45:59,<p>This is what we do in our enterprise applications:</p>&#xA;&#xA;<p>Create a new maven project which will serve as an uber jar for all projects. &#xA;Add all the applications you need as an dependency in this project.&#xA;Add maven shade plugin in this.&#xA;Create a <code>runner class</code> with <code>public static void main(String[] args)</code> in this uber jar which initialises all beans and starts all applications.</p>&#xA;&#xA;<p>Hope it helps.</p>&#xA;
40160262,40132631,7049232,2016-10-20T16:56:58,<p>You have to change the network setting in the STS.</p>&#xA;&#xA;<p>Select Window Tab -> Preferences -> General -> Network Connection and change the setting.</p>&#xA;&#xA;<p>Set Active Provider to manual then edit the host and port details in proxy entries. Apply the setting and click ok.</p>&#xA;&#xA;<p>This worked for me. Hope it works for you</p>&#xA;
45072218,44097193,8037904,2017-07-13T05:39:26,"<p>Use Docker from dtag-dev-sev</p>&#xA;&#xA;<p><a href=""https://github.com/dtag-dev-sec/suricata"" rel=""nofollow noreferrer"">https://github.com/dtag-dev-sec/suricata</a></p>&#xA;&#xA;<p>dockerized suricata</p>&#xA;&#xA;<p>suricata is a Network IDS, IPS and Network Security Monitoring engine.</p>&#xA;&#xA;<p>This repository contains the necessary files to create a dockerized version of suricata.</p>&#xA;"
48553461,48549944,6938128,2018-02-01T00:06:19,"<p>The API you mention contains method: projects.locations.functions.get , that requires one of the following OAuth scopes:</p>&#xA;&#xA;<pre><code>https://www.googleapis.com/auth/cloudfunctions&#xA;&#xA;https://www.googleapis.com/auth/cloud-platform&#xA;</code></pre>&#xA;&#xA;<p>You may have a look at the <a href=""https://cloud.google.com/solutions/authentication-in-http-cloud-functions"" rel=""nofollow noreferrer"">Authentication in HTTP Cloud Functions</a> online document. </p>&#xA;"
49288792,45622414,9494288,2018-03-14T22:42:31,"<p>Although the answer that your individual functional requirements should dictate the decision of whether to perform auth between microservices is correct, there's a security concern to take into account (I guess it'd be a part of the requirements): if a malicious user compromised even a single service, they've got them all.</p>&#xA;"
32979576,32889746,1958672,2015-10-06T20:55:36,"<p>What you may be looking for is to set nodes as <code>hidden</code> (<a href=""http://www.erlang.org/doc/reference_manual/distributed.html#id87289"" rel=""nofollow"">http://www.erlang.org/doc/reference_manual/distributed.html#id87289</a>)  if you are trying to have a large number of connected nodes that can communicate to each other through explicit connections, but don't want them to be seen otherwise due to propagation.</p>&#xA;&#xA;<p>This allows nodes to connect, but not be spread about through the gossip protocol, so the nodeA knows about the connected hidden node nodeB, but nodeA doesn't tell nodeC about about nodeB when they exchange information about the network topology.</p>&#xA;"
46359792,46333040,4711184,2017-09-22T08:04:43,"<p>You can use the <code>codeship-services.yml</code> in the same manner as the <code>docker-compose.yml</code>. So you can define your services and spin up several containers via the <code>link</code> key.</p>&#xA;&#xA;<p>I do exactly the same in my <code>codeship-services.yml</code>. I do some testing on my frontend service and that service spins up all depended services (backend, DB, etc.) when I run it via the <code>codeship-steps.yml</code>, just like in <code>docker-compose.yml</code>.</p>&#xA;&#xA;<p>At the beginning it was a bit confusing for me to have 2 files which are nearly the same. I actually contacted the Codeship support with that question and the answer was that it could be the same file (because all unavailable features in the compose file are just ignored, see <a href=""https://documentation.codeship.com/pro/builds-and-configuration/services/#unavailable-features"" rel=""nofollow noreferrer"">here</a>) but in almost all cases they have seen it was easier to have two separate files at the end, one for CI/CD and one for running docker-compose.</p>&#xA;&#xA;<p>And the same turned out true for me as well, because I need a lot of services which are only for CI/CD like deploying or special test containers which are just doing cURL tests e.g..</p>&#xA;&#xA;<p>I hope that helps and doesn't confuse you more ;)</p>&#xA;"
47352310,47335455,4795665,2017-11-17T13:58:46,"<p><strong>Option #1</strong></p>&#xA;&#xA;<p>If you need to access the parameters defined in your service's setting.xml, the next should work - </p>&#xA;&#xA;<ol>&#xA;<li>In your non-SF project, install&#xA;<strong>Microsoft.Extensions.Configuration</strong> and <strong>ServiceFabric.Extensions.Configuration</strong> NuGet packages  </li>&#xA;<li><p>Wherever you decide to access the parameters, use the next code snippet -</p>&#xA;&#xA;<pre><code>var builder = new ConfigurationBuilder().AddFabricConfiguration(""Config"");&#xA;var configuration = builder.Build();&#xA;var section = configuration.GetSection(""MyConfigSection"");&#xA;var parameterValue = section[""MyParameter""];&#xA;</code></pre></li>&#xA;</ol>&#xA;&#xA;<p>One note though - you will get access to only one SF service at a time. That's because AddFabricConfiguration() works by calling FabricRuntime.GetActivationContext(), which ties settings being loaded with the SF service you're calling a non-SF code from.</p>&#xA;&#xA;<p><strong>Option #2</strong></p>&#xA;&#xA;<p>Next option will work from any place where you could establish connection with the SF. Using code below, you could read any parameter passed into the app manifest - </p>&#xA;&#xA;<pre><code>var fClient = new FabricClient();&#xA;var namespaceManager = new XmlNamespaceManager(new NameTable());&#xA;namespaceManager.AddNamespace(""ns"", ""http://schemas.microsoft.com/2011/01/fabric"");&#xA;var manifest = XDocument.Parse(fClient.ApplicationManager.GetApplicationManifestAsync(""YOUR_APP_TYPE_NAME"", ""YOUR_APP_TYPE_VERSION"").Result);         &#xA;var parameterValue = manifest.XPathSelectElement(""/ns:ApplicationManifest/ns:Parameters/ns:Parameter[@Name='PARAMETER_NAME']"", namespaceManager).Attribute(""DefaultValue"").Value;&#xA;</code></pre>&#xA;"
46568052,46562115,4795665,2017-10-04T14:57:42,"<p>I'd say that Service Fabric is a very flexible and powerful beast to cater many different needs, and you should definitely be able to build the desired architecture.  </p>&#xA;&#xA;<p>To create and control the flow, you could consider using SF Actors. You could easily build a pipeline or some sort of a chain where each Actor knows only how to complete its task, what next Actor to call upon the success and whom to complain if something has failed. To avoid having Actors doing long-running and intensive I/O work or something, you could leverage the reminders. For instance, as new portion of input arrives, call another SF service(an actual worker) where the console apps logic is implemented in to start the job, save the task id or any other attribute in actor's state to identify the 'worker', and set the reminder to check the status of the task in-progress. SF provides many different ways for its services to communicate with each other, so you could set up the interaction between Actors and 'workers' with a blink of an eye.</p>&#xA;&#xA;<p>You could also play with GuestExecutables in SF if the console applications of yours have a huge non-trivial logic and you don't want to spend much time migrating the code in the beginning of your crusade. </p>&#xA;&#xA;<p>It's just first thoughts and some very generic design. My point is, the SF is a very good choice when it comes to 'doing' services because of the variety of nice and robust options to handle many different cases. </p>&#xA;&#xA;<p><strong>P.S.</strong></p>&#xA;&#xA;<p>Check out <a href=""https://stackoverflow.com/questions/33726653/service-fabric-reliable-services-pipeline-design/33758450"">Service Fabric Reliable Services Pipeline design</a>. The discussion pertains to a similar topic and sheds a light on the details that you might find handy.</p>&#xA;"
46094421,46061589,4795665,2017-09-07T10:47:45,"<ul>&#xA;<li><p>I bet your clusterManifest.xml is missing Certificates property within NodeTypes definition. Since this file gets generated automatically, did you alter manifest manually? If so, you need to get Certificates section back with all the required certificates(Cluster, Server, Client).</p></li>&#xA;<li><p>Next, you should verify the certificates you have in the config are properly installed, and you have correct thumbprints(do not forget about a special hidden character at the beginning of the thumbprint). </p></li>&#xA;<li><p>Finally, your SF installation might be corrupted or something, so you could reinstall all the SF artifacts.</p></li>&#xA;</ul>&#xA;"
37265515,35756663,2313489,2016-05-17T01:04:26,"<p>It is easier to think about them if you realize they aren't mutually exclusive. Think of an API gateway as a specific type reverse proxy implementation.</p>&#xA;&#xA;<p>In regards to your questions, it is not uncommon to see the both used in conjunction where the API gateway is treated as an application tier that sits behind a reverse proxy for load balancing and health checking. An example would be something like a WAF sandwich architecture in that your Web Application Firewall/API Gateway is sandwiched by reverse proxy tiers, one for the WAF itself and the other for the individual microservices it talks to.</p>&#xA;&#xA;<p>Regarding the differences, they are very similar. It's just nomenclature. As you take a basic reverse proxy setup and start bolting on more pieces like authentication, rate limiting, dynamic config updates, and service discovery, people are more likely to call that an API gateway.</p>&#xA;"
34056038,34049118,354577,2015-12-03T01:00:14,"<p>It would be a bit unusual to use both JIRA and GitHub Issues together. JIRA offers virtually everything that GitHub Issues does and more.</p>&#xA;&#xA;<p><a href=""https://help.github.com/articles/integrating-jira-with-your-projects/"" rel=""nofollow"">This guide</a> from GitHub shows how you can integrate JIRA directly with GitHub, skipping Issues altogether. When properly configured you will see links to GitHub in mentioned JIRA issues. You can also trigger JIRA workflow changes based on keywords in your commit messages, much like GitHub Issues does out of the box.</p>&#xA;"
34504087,34502682,3973137,2015-12-29T03:42:56,"<p>Using stock http library of NodeJS doesn't allow you to use that syntax.</p>&#xA;&#xA;<p>Take a look at <a href=""https://github.com/request/request"" rel=""nofollow"">RequestJS</a> as a much simpler solution. It will make your life a lot easier and allow you to use the syntax you want.</p>&#xA;&#xA;<p>This is the solution to do it with stock Node.</p>&#xA;&#xA;<p><a href=""https://nodejs.org/api/http.html#http_http_request_options_callback"" rel=""nofollow"">https://nodejs.org/api/http.html#http_http_request_options_callback</a></p>&#xA;&#xA;<p>Relevant Parts:</p>&#xA;&#xA;<pre><code>var postData = querystring.stringify({&#xA;  'msg' : 'Hello World!'&#xA;});&#xA;</code></pre>&#xA;&#xA;<p>And then, at the end:</p>&#xA;&#xA;<pre><code>// write data to request body&#xA;req.write(postData);&#xA;req.end();&#xA;</code></pre>&#xA;&#xA;<p>But use a library unless you absolutely can't.</p>&#xA;"
51026605,51017946,5816337,2018-06-25T15:05:52,"<p>In JHipster v5.0.0+, the goal <code>bootRepackage</code> doesn't exist. It has changed to <code>bootWar</code>, so you need to use:</p>&#xA;&#xA;<pre><code>./gradlew bootWar -Pprod buildDocker&#xA;</code></pre>&#xA;"
44357857,44350408,5816337,2017-06-04T19:13:24,"<p>The error is indicated here:</p>&#xA;&#xA;<p><code>Could not locate PropertySource: I/O error on GET request for ""http://localhost:8761/config/DataGateway/dev/master"": Connection refused; nested exception is java.net.ConnectExc&#xA;</code></p>&#xA;&#xA;<p>It means you didn't start the JHipster Registry.&#xA;For that, you have 2 solutions:</p>&#xA;&#xA;<p>1) with Docker, simply launch: <code>docker-compose -f src/main/docker/jhipster-registry.yml up -d</code></p>&#xA;&#xA;<p>2) clone the project <a href=""https://github.com/jhipster/jhipster-registry"" rel=""nofollow noreferrer"">https://github.com/jhipster/jhipster-registry</a>, then <code>yarn install</code> and start the registry: <code>./mvwn</code></p>&#xA;&#xA;<p>Then, try to log into the JHipster Registry at <a href=""http://localhost:8761"" rel=""nofollow noreferrer"">http://localhost:8761</a></p>&#xA;&#xA;<p>More information at <a href=""https://jhipster.github.io/api-gateway/"" rel=""nofollow noreferrer"">https://jhipster.github.io/api-gateway/</a></p>&#xA;&#xA;<p>PS: I don't understand very well how you use Tomcat here, as your Gateway already uses embedded Undertow, so you don't need another server.</p>&#xA;"
48899381,48438747,5816337,2018-02-21T06:24:59,<p>You can simply use the following configuration : <code>server.port=0</code></p>&#xA;&#xA;<p>Your Spring Boot application will scan for a free port on your OS and use it. </p>&#xA;
45090526,44977364,3457809,2017-07-13T20:41:14,"<p>I have tried to explain it using this diagram of a fictitious CMS. With micro services architecture, we can independently scale each micro service. Each micro service may be developed by a different team, they may be even developed using different technology. But we great flexibility comes great maintenance overhead, I believe it is worth it as most of it can be automated.<br>&#xA;Put simply, each module in a molithic application is a potential candidate for microservice. Howerver, microservices can be more granular than a traditional module. </p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/NzwIv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NzwIv.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>This provides a good job at explaining how to decompose your monolithic application.   <a href=""http://microservices.io/patterns/decomposition/decompose-by-business-capability.html"" rel=""nofollow noreferrer"">http://microservices.io/patterns/decomposition/decompose-by-business-capability.html</a> </p>&#xA;"
46738307,46677030,8772625,2017-10-13T21:32:15,"<p>I think the decision to carve off policy decision/authorization into a separate service depends on how much cross-service, common, global, and/or contextual policies you have in your policy requirements.  For instance, if an ownership comparison of 'permit if user.id==vehicle.owner' that could be applied for both cars and boats, that would be a rule you only manage in one place with a consolidated policy.  As you start adding context like 'deny if risk>moderate' or preventing confused deputy 'permit if sub==vehicle.resourceOwner' or 'deny if user.goodStanding!=true'... If car and boat policy is completely independent, it's probably fine to have it in each service, as long as you don't plan to grow the functionality over time.</p>&#xA;&#xA;<p>Regards,</p>&#xA;&#xA;<p>Matt</p>&#xA;"
48398038,48397327,3342866,2018-01-23T09:18:15,"<p>For the internal microservices communication, consider using Edge Microgateway instead of the standard Edge API proxy framework.The external clients will continue to route through Edge API proxy framework</p>&#xA;"
48739435,48732814,3342866,2018-02-12T04:11:47,"<p>This is not a limitation of Spring rather it is more to do with the Application Architecture.</p>&#xA;&#xA;<p>For instance, the scenario that you have is commonly solved using Aggregate Design Pattern&#xA;<a href=""https://i.stack.imgur.com/xz6qm.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xz6qm.jpg"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>While this solution is quite prevalent,it has the limitation of being synchronous, and thus blocking. Asynchronous behaviour in such scenarios should be implemented in an application specific way.</p>&#xA;&#xA;<p>Having said that if you have to call other services in order to be able to serve a response to a request from a client(outside), this is typically an architectural problem. It really doesn’t matter if you are using HTTP or asynchronous message passing (with a request-reply pattern), the overall response time for the outside client will be bad</p>&#xA;&#xA;<p>Also, I have seen quite a few applications which uses synchronous REST calls for external clients, but when communication is needed between internal MicroServices, it should always be asynchronous. You can read an interesting paper on this topic here <a href=""http://cidrdb.org/cidr2005/papers/P12.pdf"" rel=""nofollow noreferrer"">MicroServices Messaging Patterns</a></p>&#xA;"
48799653,48792602,3342866,2018-02-15T03:44:04,"<p>Having all Microservices accessing the same database will result in Loose Cohesion and Strong Coupling</p>&#xA;&#xA;<p>Try to see if you can define separate Schema for each of the Microservices, so that you can ensure Microservices doesn't refer to the tables of other MicroServices.</p>&#xA;&#xA;<p>This way in future, you can seamlessly move to separate Database for each service when your infrastructure cost concern goes off. </p>&#xA;"
48778965,48760583,3342866,2018-02-14T02:57:51,"<p>Following is an option that you can consider</p>&#xA;&#xA;<ol>&#xA;<li><p>Create a new MicroService for Token Management. All MicroServices will access the third Party Auth Component through this Service</p></li>&#xA;<li><p>In the Token Management Service, you create a Spring Singleton bean which would be automatically initialized when this services starts (@Autowired)</p></li>&#xA;<li><p>Inside this bean, you can have the logic for invoking the third party API</p></li>&#xA;<li><p>Based on your requirement, you can decide if a caching Service like Redis/memcache is needed or a RDBMS table would do</p></li>&#xA;<li><p>In this token Management service, when you get the token expired response from the third party API, you can have logic for renewing the token</p></li>&#xA;</ol>&#xA;"
48819881,48808722,3342866,2018-02-16T04:02:31,"<p>Sharing Libraries is not a recommended option. A huge benefit of Microservices is independence and that would go for a toss if you do this.</p>&#xA;&#xA;<p>A better option would be to see if you can provide access to API based on scope. That way, when your Authorization Server issues JWT token, it sends all the applicable scope for the user.</p>&#xA;&#xA;<p>Then, in your Resource Server(s) , you can enable access to Microservice using the following annotation in Spring Boot&#xA;    </p>&#xA;&#xA;<pre><code>   @PreAuthorize(""#oauth2.hasScope('read')"")&#xA;</code></pre>&#xA;&#xA;<p>Another approach is, you can create roles and PreAuthorize using Roles.</p>&#xA;&#xA;<p>If the above options doesn't work out, the current approach that you are following based on Proxy Service is perfectly fine. Only aspect that you should consider is to see if the JWT token validation can be moved to the respective Microservices so that all your services are protected.</p>&#xA;&#xA;<p>Again, Code duplicacy is perfectly fine when you are implementing Microservices and if that is your main concern, don't hesitate to add the same logic in every service. With Microservices, Duplication is better than wrong abstraction </p>&#xA;"
48941628,48934158,3342866,2018-02-23T05:23:44,"<p>As long as your application is self-contained and you have externalised your configurations, you should not have any issue.</p>&#xA;&#xA;<p>Go through this link which discusses what it takes to deploy an App to Cloud <a href=""https://content.pivotal.io/ebooks/beyond-the-12-factor-app"" rel=""nofollow noreferrer"">Beyond 15 factor</a></p>&#xA;&#xA;<p>Use AWS BeanStalk to deploy and Manage your application. Dockerizing your app is not a predicament inorder to deploy your app to AWS. </p>&#xA;"
49026693,49026008,3342866,2018-02-28T10:00:18,"<p>The answer is No!</p>&#xA;&#xA;<p>Assuming you are using Spring Boot, the Service name is going to be your spring-application-name. Hence, you cannot have more than one name registered from a single Jar</p>&#xA;&#xA;<p>But you have mentioned that you have the constraint that the address should also be same. May be that if that can be relaxed, through Spring Cloud Config you can start multiple instance of your Service with different Profile names and under each profile's config you can override your spring-application-name.</p>&#xA;"
48920237,48912603,3342866,2018-02-22T05:27:20,"<p>Key questions here are</p>&#xA;&#xA;<p><strong>Modern Applications are resilient</strong></p>&#xA;&#xA;<p>While on theory it may sound correct, there are still quite of lot of modern apps which fails miserably owing to wide range of factors like poor design, excessive load etc</p>&#xA;&#xA;<p>If your app has zero-downtime deployment and MTBF(Mean Time Between Failures) is close to 0, then your app is resilient and you need not have to look for Kafka</p>&#xA;&#xA;<p><strong>Kafka Complexity</strong></p>&#xA;&#xA;<p>If for some-reason you are not able to achieve zero-down time or MTBF close to zero, Kafka is a powerful option to you. Simple reason being, you can configure Kafka listener with retry, so if your app is down for sometime, the message can still be processed after the service restarts. Additionally, you would be able to use the powerful Kafka streaming capabilities like transaction processing</p>&#xA;&#xA;<p>But beware that Kafka only provides a total order over messages within a partition, not between different partitions in a topic.</p>&#xA;&#xA;<p>If you have a topic with single partition, the ordering is guaranteed. If your consumer if performing well, you don't have to worry. </p>&#xA;&#xA;<p>Also since message processing is going to be asynchronous, you will not be guaranteed when the message would be processed , hence if you application is a client facing one or client is waiting for the response, that will bring in more complexity</p>&#xA;&#xA;<p>You could evaluate if Akka framework adds more value to as you will get the following features out-of-the-box</p>&#xA;&#xA;<ol>&#xA;<li><strong>Event-driven:</strong> Using Actors, one can write code that handles requests asynchronously and employs non-blocking operations exclusively.</li>&#xA;<li><strong>Scalablity:</strong> In Akka, adding nodes without having to modify the code is possible, thanks both to message passing and location transparency.</li>&#xA;<li><strong>Resilience:</strong> Any application will encounter errors and fail at some point in time. Akka provides “supervision” (fault tolerance) strategies to facilitate a self-healing system.</li>&#xA;<li><strong>Responsive:</strong> Many of today’s high performance and rapid response applications need to give quick feedback to the user and therefore need to react to events in an extremely timely manner. Akka’s non-blocking, message-based strategy helps achieve this.</li>&#xA;</ol>&#xA;"
39462284,39395503,6621458,2016-09-13T04:35:50,"<p>I am not aware of any tools that can directly test ICAP. </p>&#xA;&#xA;<p>However, you can test it indirectly by ""web performance tester"", via a compliant proxy, by generating the same traffic that a browser would generate through the proxy, which in turn talks ICAP to the service.</p>&#xA;&#xA;<p>Ali</p>&#xA;"
49620772,46002727,600082,2018-04-03T01:03:51,"<p>Technically, an <strong>API Gateway</strong> is the API exposed to the public (REST, etc.), and an <strong>Edge Service</strong> is a service running on the API resolving the proxying, routing, etc. There could be many edge services on the Gateway. But practically there is usually only one service, logic, on the Gateway thus API Gateway = Edge Service. </p>&#xA;"
31553183,31342583,3684882,2015-07-22T03:08:26,<p>RabbitMq is simple message queue that is fairly easy to get going with. </p>&#xA;
30674056,30648139,18706,2015-06-05T19:09:49,"<blockquote>&#xA;  <p>send all the required data from the browser, or just the id of the&#xA;  house and client and the service will handle the rest?</p>&#xA;</blockquote>&#xA;&#xA;<p>Typically, if the house and client aren't changing in this request, just send their IDs, e.g. </p>&#xA;&#xA;<pre><code>{&#xA;  sale: {&#xA;    price: ""100000.00"",&#xA;    houseID: 123,&#xA;    clientID: 456&#xA;  }&#xA;}&#xA;</code></pre>&#xA;&#xA;<blockquote>&#xA;  <p>if we have a restriction in the system that says that ""you can only sell houses to your clients"", how do we guarantee in the sales service that this agent is selling a house to his client (how can I trust the data that comes from the browser)?</p>&#xA;</blockquote>&#xA;&#xA;<p>The answer is you can't trust data from the client. If you get an incoming sale request, you have to verify it on the server. Typically you would have <a href=""http://c2.com/cgi/wiki?GuardClause"" rel=""nofollow"">guard clauses</a> or pre-filters to check any operation against constraints (along with other stuff like cleansing the data and checking the user's permission).</p>&#xA;"
42332981,42332938,155666,2017-02-19T21:16:38,"<p>How I currently see it, applications are a nice concept to group multiple services together and manage them as single unit. In context of service fabric, this is useful if you have multiple nano-services  which do not warrant them being completely standalone; instead you can package them together into microservices (SF application).</p>&#xA;&#xA;<p>Disclaimers:&#xA;- nano-service would be a REALLY small piece of code running as a stateless SF service for example (e.g. read from queue, couple of lines of code to process, write to another queue).&#xA;- in case of ""normal"" microservices, one could consider packaging them as 1 SF application = 1 SF service</p>&#xA;"
38856063,38855867,5739282,2016-08-09T16:29:22,"<p>Honestly, no matter what answers you get, none will be the perfect answer as a lot of this is subject to design and your own personal needs.</p>&#xA;&#xA;<p>The steps I take when designing a complex microservices architecture follow suit closely with Domain Driven Design (DDD) and are as follows:</p>&#xA;&#xA;<ol>&#xA;<li>Define your domain (in your example, ECommerce seems to be your domain)</li>&#xA;<li>Determine on your immediate deliverables (in your example, the ability to add items to a cart and checkout)</li>&#xA;<li>Define your contextual boundaries (in your example, Checkout could be it's own contextual boundary and is probably fine grained enough. Anymore fine grained than checkout you are probably venturing into the nano-services territory which is not where you want to be)</li>&#xA;</ol>&#xA;&#xA;<p>Now you have contextual boundaries which will be your microservices. When it comes to implementing that contextual boundary, you need to break it down into it's own individual pieces. From your example, it appears you have</p>&#xA;&#xA;<ol>&#xA;<li>Price/Tax calculations</li>&#xA;<li>Inventory management</li>&#xA;<li>Order management</li>&#xA;</ol>&#xA;&#xA;<p>From there, you can then determine what functionality you require in each piece of the contextual boundary and break them up into presentation/controller, services, and repositories.</p>&#xA;&#xA;<p>So for example, take Order Management. You would probably want something like:</p>&#xA;&#xA;<ul>&#xA;<li>Order Service to handle the business logic and data mapping for orders</li>&#xA;<li>Order Repository to pull data from your data store in regards to orders</li>&#xA;</ul>&#xA;&#xA;<p>If your Order Management needs are very complex with a lot of business rules, you might break it up into multiple services/repositories to suit those demands or maybe it would then deserve it's own contextual boundary...</p>&#xA;&#xA;<p>I can't tell you how to do your checkout (you asked about calculating stuff and controlling the transaction). That design is however you decide to do it.</p>&#xA;&#xA;<p>Once you have all that in place you can then determine your data store needs and your server needs. Initially, you probably don't have a ton of users, so each service could probably live on the same server in the same project and you could have a single database on it's own server. But make sure to write it all in a way that they can be broken out later so when you require the ability to handle more users through a distributed system, you can do so without rewriting a ton of code.</p>&#xA;&#xA;<p>That's about as good of an answer as I can give you without knowing anything about what you're doing. I wouldn't take what I put here word for word and implement it, I would instead, take the advice here and go through the steps yourself to determine the design which best suits your needs.</p>&#xA;&#xA;<p><strong>TL;DR</strong></p>&#xA;&#xA;<ol>&#xA;<li>Define your domain</li>&#xA;<li>Breakup your domain into contextual boundaries</li>&#xA;<li>Determine each piece of your contextual boundaries</li>&#xA;<li>Break those pieces up into presentation/controllers, services, and repositories.</li>&#xA;<li>Once that's all in place, determine your data store and server needs (starting out, likely a server for each of your data stores (e.g. SQL on a server and NoSQL/Blob on another), and a single server for your microservices. Then when you have more users to handle, distribute your microservices)</li>&#xA;</ol>&#xA;&#xA;<p>Good luck!</p>&#xA;"
31097694,31097306,3315474,2015-06-28T08:40:27,"<p>You may try to deploy all your services  (webapps) into one or more tomcat instances. You may not include the Tomcat into war file. Example maven/spring configuration you may find <a href=""http://blog.konstantinpavlov.net/2015/02/19/booting-spring-webapp/"" rel=""nofollow"">here</a></p>&#xA;"
41067467,41067341,1810962,2016-12-09T19:21:36,"<p>Micro services is one way to do distributed computing. There are many more distributed computing models like Map-Reduce and Bulk Synchronous Parallel.</p>&#xA;&#xA;<p>However, as you pointed out, you don't need to use micro servers for a distributed system. You can put all your services on one machine. It's like using a screw driver to hammer a nail ;). Yeah, you'll have parallel computation on a single multi-core machine, but are micro services the right way to achieve it? They might be if you plan to move those services onto separate machines. However, if those services require co-location, then micro services was the wrong tool.</p>&#xA;&#xA;<p>Distributed systems is one way to do <a href=""https://en.wikipedia.org/wiki/Parallel_computing#Classes_of_parallel_computers"" rel=""nofollow noreferrer"">parallel computing</a>. There are many different ways to achieve parallel computation, like grid computing, multi-core machines, etc. Many of them are listed in the article I linked.</p>&#xA;"
29078036,29071226,4638482,2015-03-16T13:27:59,"<p>You can put all orchestrating logic between long running transactions in Process Manager. Process manager can subscribe to events, derive commands from events and delegate this command to your another MS. All failures you can handle in Saga. Saga should perform compensation action, when your long running logic throw some exception.</p>&#xA;&#xA;<p>More about <a href=""http://www.eaipatterns.com/ProcessManager.html"" rel=""nofollow"">Process Manager </a> pattern and <a href=""http://kellabyte.com/2012/05/30/clarifying-the-saga-pattern/"" rel=""nofollow"">Saga</a> pattern.</p>&#xA;"
50223351,50221166,9754130,2018-05-07T22:28:41,"<p>It sounds like the database you're working with is storing information in a way that's easy to map directly to objects you are using directly in your business logic. </p>&#xA;&#xA;<p>If you're trying to create a .net web microservice that will give you the ability to store and retrieve information from this legacy database, I would recommend using an ORM like EntityFramework to map the tables (either Database- first or <a href=""https://msdn.microsoft.com/en-us/library/jj200620(v=vs.113).aspx"" rel=""nofollow noreferrer"">Code-First from Database</a>).</p>&#xA;&#xA;<p>I'd then recommend looking at using the <a href=""https://docs.microsoft.com/en-us/aspnet/mvc/overview/older-versions/getting-started-with-ef-5-using-mvc-4/implementing-the-repository-and-unit-of-work-patterns-in-an-asp-net-mvc-application"" rel=""nofollow noreferrer"">Repository Pattern</a> with your entities to maximize testability and isolate your data layer.</p>&#xA;"
36185177,36176403,657427,2016-03-23T17:44:13,"<p>Your question is a bit vague or too broad, but ...&#xA;Using LDAP does not scale well, consider using NoSQL alternatives as those are closer to the principles of microservices. &#xA;See:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://stackoverflow.com/questions/29644916/microservice-authentication-strategy"">Microservice Authentication strategy</a></li>&#xA;<li><a href=""http://presos.dsyer.com/decks/microservice-security.html#slide6"" rel=""nofollow noreferrer"">http://presos.dsyer.com/decks/microservice-security.html#slide6</a></li>&#xA;<li><a href=""https://www.quora.com/Microservices/How-do-I-handle-authentication-in-a-microservices-architecture-with-the-front-end-decoupled-too"" rel=""nofollow noreferrer"">https://www.quora.com/Microservices/How-do-I-handle-authentication-in-a-microservices-architecture-with-the-front-end-decoupled-too</a></li>&#xA;</ul>&#xA;"
49079510,49060040,2107667,2018-03-03T00:49:54,"<p>It is hard to answer these questions, any small detail may invalidate the answer.</p>&#xA;&#xA;<p>I had the chance to start a project from scratch and I only use DynamoDB tables, no RDS. I love DynamoDB. They are great for my evolving project because they are <code>schemaless</code> and scalable.</p>&#xA;&#xA;<p>You will need to master the partition keys, sorting keys and global secondary indexes: when and how to use them and, most important, when <strong>not</strong> to use them! There will be a strong dependency between your indexes and your queries.</p>&#xA;&#xA;<p>Should you hesitate between DynamoDB and relational databases, I should not. Go with DynamoDB or any suitable NoSQL database, scalability by design! :)</p>&#xA;"
46477120,46476437,2117651,2017-09-28T19:44:44,"<p>Endpoints is an API gateway, but it currently only routes to a single backend. On Flex, it's whatever app you deploy. The Endpoints proxy sits in front of your backend, transparently to the client, and the client requests will pass through the gateway. See <a href=""https://cloud.google.com/endpoints/docs/openapi/get-started-app-engine"" rel=""nofollow noreferrer"">the docs</a> for how to set up your Flexible environment.</p>&#xA;"
42115759,42060090,2788883,2017-02-08T14:26:06,"<p>The straight forward way is to proxy your custom API by generating a proxy in the Anypoint API Manager. &#xA;Deploy that proxy application on a Mule server in CloudHub or on a runtime on-premise.&#xA;Then you have your Spring Boot Application API under control and can apply policies, see analytics, etc.</p>&#xA;&#xA;<p><a href=""https://docs.mulesoft.com/api-manager/setting-up-an-api-proxy"" rel=""nofollow noreferrer"">MuleSoft Doc on API Proxy</a></p>&#xA;"
44509739,42836979,954643,2017-06-12T22:39:10,"<p>Heroku's default model is to map a process type to its own dyno: <a href=""https://devcenter.heroku.com/articles/procfile"" rel=""nofollow noreferrer"">https://devcenter.heroku.com/articles/procfile</a> states</p>&#xA;&#xA;<blockquote>&#xA;  <p>""Every dyno in your application will belong to one of the process&#xA;  types, and will begin executing by running the command associated with&#xA;  that process type.""</p>&#xA;</blockquote>&#xA;&#xA;<p>e.g. <code>heroku ps:scale worker=1</code> for a type of <code>worker</code>.</p>&#xA;&#xA;<p>Other people have written about <a href=""http://heyman.info/2012/dec/6/heroku-multiple-processes-single-dyno-with-foreman/"" rel=""nofollow noreferrer"">how to use <code>foreman</code></a> or <a href=""http://www.radekdostal.com/content/heroku-running-multiple-python-processes-single-dyno-using-honcho"" rel=""nofollow noreferrer""><code>Honcho</code></a> to run multiple python processes in a single dyno, which utilize a secondary Procfile and possibly other slug setup in a <code>post_compile</code> step. Presumably you could do something similar depending on your chosen language and its buildpack; the official <a href=""https://devcenter.heroku.com/articles/buildpack-api#buildpack-api"" rel=""nofollow noreferrer"">buildpack API</a> doesn't list this step though :/. That said, given Heroku's base <a href=""https://devcenter.heroku.com/articles/stack"" rel=""nofollow noreferrer"">Ubuntu stacks</a>, you may be able to get away with a <code>web: script.sh</code> to do any setup and exec of processes.</p>&#xA;&#xA;<p>For grouping multiple processes in a Docker container, you could check out <a href=""https://runnable.com/docker/rails/run-multiple-processes-in-a-container"" rel=""nofollow noreferrer"">https://runnable.com/docker/rails/run-multiple-processes-in-a-container</a>, which relies again on a custom <code>CMD [ ""/start.sh"" ]</code>. Note that it's contrary to Docker's single-­process-­per-­container philosophy, and can give you more headaches e.g. around forwarding signals to child processes, an ugly Dockerfile to setup all microservices, etc. (If you do decide to use multiple containers, Heroku has a writeup on <a href=""https://devcenter.heroku.com/articles/local-development-with-docker-compose"" rel=""nofollow noreferrer"">using Docker Compose for local dev</a>.)</p>&#xA;&#xA;<p>Also, don't forget you're <a href=""https://devcenter.heroku.com/articles/dyno-types"" rel=""nofollow noreferrer"">bounded by the performance of your dyno</a> and the <a href=""https://devcenter.heroku.com/articles/dynos#process-thread-limits"" rel=""nofollow noreferrer"">process/thread limits</a>.</p>&#xA;&#xA;<p>Of course, multiple processes in a given dyno is generally not recommended for non-toy production or long term dev maintenance. ;)</p>&#xA;"
50307383,50295779,9277971,2018-05-12T14:37:24,"<p>In my point of view, you should ask yourself: Why do you want to have such a shared microservice stub that all the other service have to use as a basis. There are good reasons I am aware of but one big strength of a microservice architecture is that each service can make use of the technology which is best suited for its business needs.</p>&#xA;&#xA;<p>Getting back to your question, you could create a Git repository with a basic JHipster setup which only contains those application parts that all microservices should have in common. Then you can create forks of this repository for each microservice.</p>&#xA;&#xA;<p>Another approach: Instead of having a single base project you could create small modules of those features that all microservices should have in common, e.g. a common logging mechanism. Then all microservice projects can use these modules as dependencies.</p>&#xA;"
39845346,39683166,6919187,2016-10-04T06:17:35,"<p>Getting started with the MEAN stack might seem very daunting at first. You might think that there is too much new technology to learn.  The truth is that you really only need to know “Javascript.” That’s right, MEAN is simply a javascript web development stack.</p>&#xA;&#xA;<p>So, how do you actually get started developing on the MEAN stack?</p>&#xA;&#xA;<p>The first step is to set up a project structure. I’ve found the following structure to make the most sense:</p>&#xA;&#xA;<p>controllers/ db/ package.json server.js public/</p>&#xA;&#xA;<p>This structure lets you keep the entire stack in a single project. Your AngularJS front end can go into the public folder while all your Express API logic goes into controller and your MongoDB collections and logic go into the db folder.</p>&#xA;&#xA;<p>Now that you’ve set up a general project structure, you need to initialize your public folder as  an Angular project. It is best to do this using a tool called Yeoman.</p>&#xA;&#xA;<p>Yeoman is a toolkit that makes it easy for you to get started with a variety of Javascript frameworks and other web frameworks like Bootstrap and foundation. You can learn more about Yeoman at Yeoman.io.</p>&#xA;&#xA;<p>You can read more about the M.E.A.N. stack here and how to get started: <a href=""http://www.citizentekk.com/mean-stack-tutorial-how-to-build-loosely-coupled-scalable-web-apps-nodejs-angularjs-applications/"" rel=""nofollow"">http://www.citizentekk.com/mean-stack-tutorial-how-to-build-loosely-coupled-scalable-web-apps-nodejs-angularjs-applications/</a></p>&#xA;"
45296187,44115310,5877779,2017-07-25T07:05:36,"<p>Finally, I resolved the problem adding an interceptor as a component: </p>&#xA;&#xA;<pre><code>@Component&#xA;public class SpringSessionClientHttpRequestInterceptor implements ClientHttpRequestInterceptor {&#xA;&#xA;@Override&#xA;public ClientHttpResponse intercept(HttpRequest request, byte[] body, ClientHttpRequestExecution execution)&#xA;    throws IOException {&#xA;&#xA;    request.getHeaders().add(""Cookie"", ""SESSION="" + sessionId);&#xA;    return execution.execute(request, body);&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>}</p>&#xA;&#xA;<p>And I created a @Bean to configure the rest template:</p>&#xA;&#xA;<pre><code>@Bean&#xA;public RestTemplate restTemplate(){&#xA;    RestTemplate rest = new RestTemplate();&#xA;    ClientHttpRequestInterceptor interceptor= new SpringSessionClientHttpRequestInterceptor();&#xA;    List&lt;ClientHttpRequestInterceptor&gt; interceptors = new ArrayList&lt;ClientHttpRequestInterceptor&gt;();&#xA;    interceptors.add(interceptor);&#xA;    rest.setInterceptors(interceptors);  &#xA;    return rest;&#xA;}&#xA;</code></pre>&#xA;"
37869695,37849739,6218003,2016-06-16T21:34:07,"<p>Your issue stems from the scopes not being configured on the client. Only scopes in that client's list of scopes can be present on the user JWT. Adding scopes to this list will not allow a user to obtain scopes they do not have, nor will it result in those scopes being present on the client credentials token for the client.</p>&#xA;&#xA;<p>If you have groups-as-scopes configured, your client needs to have each scope you expect it to use configured in its list of allowed scopes.</p>&#xA;"
28931364,28930710,6932,2015-03-08T20:13:20,"<p>1) Every high scale data solution that I can think of has something built in to handle precisely this sort of conflict. The details will depend on your final choice for data storage. In the case of a traditional relational database, this comes baked in without any add'l work on your part. Refer to your chosen technology's documentation for appropriate detail.</p>&#xA;&#xA;<p>2) Understand your data model and usage patterns. Design your datastore appropriately. Don't design for scale that you won't have. Optimize for your most common usage patterns.</p>&#xA;&#xA;<p>3) Challenge your assumptions. Do you actually <em>have</em> to mutate the same entity very frequently from multiple roles? Sometimes the answer is yes, but often you can simply create a new entity that's similar to reflect the update. IE, take a journaling/<a href=""http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying"" rel=""nofollow"">logging</a> approach instead of a single-entity approach. Ultimately high volumes of updates on a single entity will never scale.</p>&#xA;"
37446529,37445823,3225108,2016-05-25T19:59:23,"<p>You can't force something to be synchronous if it goes outside of the event loop like an ajax call. You're going to need something that looks like this:</p>&#xA;&#xA;<pre><code>PlanLineActions.calculateFlightEndDate(periodTypeId, numberOfPeriods, momentTimeUnix)&#xA;  .then(endDate =&gt; {&#xA;    this.theNextSyncFunction(..., ..., ...);&#xA;  })&#xA;</code></pre>&#xA;&#xA;<p>In order to do this, <code>calculateFlightEndDate</code> will also need to return a promise, and thus it's a good thing that promises are chainable.</p>&#xA;&#xA;<pre><code>calculateFlightEndDate(periodTypeId, numberOfPeriods, startDate) {&#xA;  let plan = new Plan();&#xA;&#xA;  // return promise!&#xA;  return plan.getFlightEndDate(periodTypeId, numberOfPeriods, startDate).then(response =&gt; {&#xA;    return response.EndDate; // must return here&#xA;  }, error =&gt; {&#xA;    log.debug(""There was an error calculating the End Date."");&#xA;  });&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>That should do it.. and one more thing: you're doubling up on promises in your server call. If something has <code>.then</code> it's already a promise, so you can just return that directly. no need to wrap in <code>new Promise</code> (a promise in a promise.. no need!)</p>&#xA;&#xA;<pre><code>callServer(path, method = ""GET"", query = {}, data, inject) {&#xA;  // just return!&#xA;  return super.callServer(uri.toString(),method,data,inject).then((data) =&gt; {&#xA;    return data;&#xA;  }).catch((data) =&gt; {&#xA;    if (data.status === 401) {&#xA;      AppActions.doRefresh();&#xA;    }&#xA;    throw data; // throw instead of reject&#xA;  });&#xA;}&#xA;</code></pre>&#xA;"
48809542,48809275,5258516,2018-02-15T14:25:25,"<p>In your application.yml or application.properties add a set for your auth url : </p>&#xA;&#xA;<pre><code>#Example for application.properties&#xA;auth.token.url = http:\\...&#xA;</code></pre>&#xA;&#xA;<p>Then in your configuration class configure your place holder so then you can read your properties values  : </p>&#xA;&#xA;<pre><code>@Configuration&#xA;public class Config {&#xA;        @Bean&#xA;        public PropertySourcesPlaceholderConfigurer propertySourcesPlaceholderConfigurer() {&#xA;            PropertySourcesPlaceholderConfigurer propertySourcesPlaceholderConfigurer = new PropertySourcesPlaceholderConfigurer();&#xA;            propertySourcesPlaceholderConfigurer.setLocations(new ClassPathResource(""application.properties""));//or application.yml&#xA;            return propertySourcesPlaceholderConfigurer;&#xA;        }&#xA;&#xA;    }&#xA;</code></pre>&#xA;&#xA;<p>Then in your AuthServletContextListener classes : add this</p>&#xA;&#xA;<pre><code>public class AuthServletContextListener implements ServletContextListener {&#xA;&#xA;  @Value(""${auth.token.url}"")&#xA;  private String authUrl ;&#xA;&#xA;    public void contextInitialized(ServletContextEvent arg0) {&#xA;        try {&#xA;            ServletContext e = arg0.getServletContext();&#xA;            Properties config = new Properties();&#xA;            this.addProp(config, e, ""auth.token.url"", authUrl);&#xA;            this.addProp(config, e, ""auth.system.username"", ""System Username"");&#xA;            this.addProp(config, e, ""cauth.system.password"", ""System Password"");&#xA;            TokenContainer.init(config);&#xA;        } catch (IOException arg3) {&#xA;            arg3.printStackTrace();&#xA;        }&#xA;&#xA;    }&#xA;&#xA;&#xA;    private void addProp(Properties config, ServletContext context, String propName, String descrip) {&#xA;        String propVal = (String) context.getAttribute(propName);&#xA;        if (StringUtils.isEmpty(propVal)) {&#xA;            propVal = context.getInitParameter(propName);&#xA;        }&#xA;&#xA;        if (StringUtils.isNotEmpty(propVal)) {&#xA;            config.put(propName, propVal);&#xA;        } else {&#xA;            throw new RuntimeException(""error: "");&#xA;        }&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
45624415,45515650,6184468,2017-08-10T22:32:57,"<p>I have a similar setup at work, with several microservices (~40) and a dozen teams. I was asked the same question several times, and my answer is <strong>the consumer is responsible for consuming</strong>. If the API works as designed and expected, there is no point in making the providing team responsible for anything.</p>&#xA;&#xA;<p>The team that provides the service (team a), <strong>may</strong> provide a client, if they want (in doubt, without warranty). The consuming team (team B) <strong>may</strong> use the client if <strong>they</strong> want (taking all the risks included).&#xA;The only contract should be the API, everything else should be a goodie a team may provide on top. If team a <strong>has</strong> to provide a client, why do they provide an api at all then?</p>&#xA;&#xA;<p>Given that both teams are loosely coupled and may use different technologies (or e.g. different spring framework versions), providing a client library to the other team proves to bring more problems than solve any. In a Java+spring-boot world e.g. you get into dependency problems very fast, especially if you include several clients from different service providing teams that evolve differently in time.</p>&#xA;&#xA;<p>And even worse: what if the client-library of team A makes the system of team B unstable and introduces bugs? Who is responsible to fix that now?</p>&#xA;&#xA;<p>If you want to reduce the work needed for your consuming teams because re-coding the client is so much work, your API may be to complex and/or your microservice may be no more microservice at all.&#xA;Imagine using HATEOAS on a restful API - writing a client for that is just a few lines of code, even with included API-Browser, Documentation and whatnot. See e.g. spring-rest-docs, hal-browser, swagger and various other technologies that make reading/browsing/documenting an API and implementing a client a breeze.</p>&#xA;&#xA;<p>Above cases are described with two teams, imagine that with 10. We had a ""client library"" provided by one team, used by 4 other teams. You can guess how fast it became a complete mess until it was just deleted :)</p>&#xA;"
47958456,47957575,5165332,2017-12-24T06:11:38,<p>Just for simplicity suppose: project is located in <code>/Users/user/Workspace/Monorepo/go-microservice</code> checked out from github repo <code>github.com/Company/Monorepo</code>.</p>&#xA;&#xA;<p>Make symlink from another side: from GOPATH to required folder. Your repo is kept in standard space and GO can see code with its usual path.</p>&#xA;&#xA;<p>Thus make in folder <code>/Users/user/go/src/github.com/Company</code> symlink <code>Monorepo</code> pointing to first path.</p>&#xA;
45294505,45293123,5165332,2017-07-25T05:21:25,"<p>Plan internal data format to suit purpose of your application first, it should be convenient for next operations. All the application should work with internal data format and only specific input-output adapters should know about specific customers formats.&#xA;Then write adapters from custom data formats A, B, C, ... to convert to basic internal format. These adapters should be as thin as possible, they should have minimal logics - just convert data and nothing more. This way it will be easier to maintain them. &#xA;You are right - if you change internal format you have to review and possibly change all adapters. That's why they should be very thin.&#xA;Think which defaults can you have?&#xA;I recommend to start with few customer formats to debug all concepts. &#xA;Wheather make the solution as a set of microservices of as modules in a single monilith - has no much sense in this way. </p>&#xA;"
45381640,45380558,6095381,2017-07-28T20:08:22,"<p>Setting the below properties helped me to fix my issue.</p>&#xA;&#xA;<pre><code>spring.cloud.stream.bindings.input.destination=test&#xA;spring.cloud.stream.bindings.input.consumer.headerMode=raw&#xA;spring.cloud.stream.bindings.input.consumer.startOffset=latest&#xA;spring.cloud.stream.bindings.input.consumer.resetOffsets=true&#xA;spring.cloud.stream.bindings.input.consumer.instanceCount=3&#xA;spring.cloud.stream.bindings.input.consumer.autoCommitOffset=false&#xA;spring.cloud.stream.kafka.bindings.input.consumer.autoCommitOffset=false&#xA;spring.cloud.stream.kafka.binder.autoCreateTopics=false&#xA;spring.cloud.stream.bindings.input.group=testGroup50&#xA;spring.cloud.stream.bindings.input.partitioned=false&#xA;</code></pre>&#xA;&#xA;<p>Thanks,</p>&#xA;&#xA;<p>BR</p>&#xA;"
44766593,44502869,6884373,2017-06-26T18:44:09,<p>I resolved this problem by creating another rest service and loop through the Zip Entries and write the particular file content in ServletOutputStream...So this can be accessed through the this rest service. Thanks to all for your suggestions.  </p>&#xA;
51406144,51392666,3094805,2018-07-18T15:46:18,"<p>Your Spring Boot app is configured to load properties from a Spring Cloud Config server specified at</p>&#xA;&#xA;<p><a href=""http://localhost:8761/config/seodin/prod/master"" rel=""nofollow noreferrer"">http://localhost:8761/config/seodin/prod/master</a> </p>&#xA;&#xA;<p>and it can't see it.</p>&#xA;&#xA;<p>As you are running in Docker either make sure that a config server is running in Docker as well, and that your Spring Boot app is configured to point at that (the URL will not be localhost; it's likely to be the container name of the config server ifyou are running that in Docker, or the URL of an external config server)</p>&#xA;"
41042345,41027573,4214037,2016-12-08T14:55:13,"<p>I'm not sure, but I hope that <code>pattern</code> parameter in <code>service</code> module will solve your problem. You can find documentation <a href=""http://docs.ansible.com/ansible/service_module.html#synopsis"" rel=""nofollow noreferrer"">here</a>. I think your Ansible task will look like this code:</p>&#xA;&#xA;<pre><code>- name: Killing enrollment-account-listener&#xA;  service:&#xA;    name: enrollment-account-listener&#xA;    state: stopped&#xA;    pattern: enrollment-account-listener&#xA;</code></pre>&#xA;&#xA;<p>Documentation says, if service with given name doesn't response to service status command, then find <code>pattern</code> in output of ps command. If the string is found, the service will be assumed to be running.</p>&#xA;&#xA;<p>If it won't work, you can still use <code>command</code> or <code>shell</code> module to find solution to this problem.</p>&#xA;"
44811693,44810758,575765,2017-06-28T20:08:53,"<p>This approach with storing the credentials in a separate database can indeed make your application more secure. It really depends on how it is implemented, and what privileges an attacker can get.</p>&#xA;&#xA;<p>One example is SQL-injection, it can be difficult to make your whole application bullet proof, but it is easy to make the only request to get the credentials safe. Usually an attacker can get the credentials whenever (s)he finds a leak, not so if the credentials are stored in a separate database.</p>&#xA;&#xA;<p>If the attacker has privileges on the server, there may be no advantage, but I cannot see any drawbacks neither.</p>&#xA;"
51238154,51238009,587285,2018-07-09T04:16:58,"<p>You should have an implementation for getting the list of suppliers. Something like <code>/suppliers/{supplierNumberList}</code>.</p>&#xA;&#xA;<p>Since you define the API for your service, there is nothing wrong to have a call like this.</p>&#xA;"
46287578,46244677,4325141,2017-09-18T20:19:28,"<p>I think you need to consider both solutions and think what are the tradeoffs. </p>&#xA;&#xA;<p><strong>Having a single service handling authorization:</strong> </p>&#xA;&#xA;<ul>&#xA;<li>Single point of failure. If someone makes a mistake, then your entire ecosystem goes down. </li>&#xA;<li>Increases encapsulation of a well known problem. </li>&#xA;<li>Reduces work on all dependent services. </li>&#xA;</ul>&#xA;&#xA;<p><strong>Implementing authorization on each service:</strong> </p>&#xA;&#xA;<ul>&#xA;<li><p>Gives more independency to each service <em>(which is the nature of ""microservices"")</em> </p></li>&#xA;<li><p>Gives flexibility on how you want to secured each of your services. Could vary depending on the intend of each of them. </p></li>&#xA;</ul>&#xA;&#xA;<p>At the end, you need to find what's your need and take a decicion based on what gives you more benefits. I just listed a few of pros and cons, but I think the idea is to invest time on this topics and then take the decision based on what you consider is best for your team. </p>&#xA;"
46287428,46283367,4325141,2017-09-18T20:09:33,"<p>I think you are saying that we could just implement the entire circuit brake logic? And you are correct. But why is better use something already proven as Hystrix? I would say: </p>&#xA;&#xA;<ol>&#xA;<li>Circuit Break logic already bullet proof. </li>&#xA;<li>Metrics out of the box, such as this <a href=""https://github.com/Netflix/Hystrix/tree/master/hystrix-dashboard"" rel=""nofollow noreferrer"">dashboard</a></li>&#xA;<li>Defines a pattern to deal with cascade failures of your interconnected services. Meaning, if one service goes down, you already had though on what to do to keep serving request on your very own service.</li>&#xA;<li>It helps developers to change its way of thinking when writing code against external dependencies <em>-design for failure-</em>, simple by making them think on what if it fails? Regularly, developers don't do that. You assume that it will work. </li>&#xA;</ol>&#xA;&#xA;<p>I think there's now magic definition under hystrix. It is a simple problem that developers don't usually thing about it. </p>&#xA;"
50125333,40972026,4325141,2018-05-01T23:49:29,"<p>For those looking into some numbers, this is a <a href=""https://engineering.opsgenie.com/comparing-api-gateway-performances-nginx-vs-zuul-vs-spring-cloud-gateway-vs-linkerd-b2cc59c65369?gi=ebd679f3d106"" rel=""nofollow noreferrer"">great post</a> of comparing the different solutions out there. </p>&#xA;&#xA;<p>I've been implementing some cross-cutting concerns at the API Gateway level and we are using Kong. However, I think some of the libraries provided by Kong to write plugins in Lua are not as mature and solid as something written in Java for example. Lack of documentation, difficult to debug, among other problems related to scripting languages.</p>&#xA;&#xA;<p>There are some good notes on the performance of using Netflix's Zuul versus the wrapper provided by Spring Cloud. So it might be good to take a look at those problems listed in there. </p>&#xA;"
46902419,40574379,2431766,2017-10-24T04:48:43,"<p>@p.magalhaes - Ideally you should not be deleting the stack and creating it again, rather you should be updating the cloudformation stack. And if you update your stack it wont delete SNS topic as there won't be any change in that. However if you have a specific need of deleting and creating the stack you can create separate stacks for all of your components using nested stack (Reference - <a href=""https://aws.amazon.com/blogs/devops/use-nested-stacks-to-create-reusable-templates-and-support-role-specialization/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/devops/use-nested-stacks-to-create-reusable-templates-and-support-role-specialization/</a> )  </p>&#xA;"
42614163,42562820,1863190,2017-03-05T21:19:45,"<p>In my opinion, you have three entities: </p>&#xA;&#xA;<ul>&#xA;<li>study</li>&#xA;<li>survey</li>&#xA;<li>person</li>&#xA;</ul>&#xA;&#xA;<p>It is pretty intuitive to see that each of these is its own aggregate root. So then we are talking about inter-root relationships. In my experience, those are meaningful entities on their own, and cleanest and most future proof by far is to treat those relationships as independent aggregate roots. </p>&#xA;&#xA;<p>The relationship between a study and a person is perhaps called TestSubject, and the relationship between a person and a survey could be called Interviewee or something similar. In another context, the person could be an employee for a company, and then the Employee would be its own aggregate root. Information that only relates to the relationship and not to the person or the study say, should be limited to this relationship specific aggregate root. This could for instance be the start date at which the subject started to take part in the test, and the end date (when he dropped out, if he or she dropped out prematurely, etc.)</p>&#xA;&#xA;<p>As for storage, all aggregate roots should define their own separate repositories as interfaces and know only those interfaces, but the implementation of those interfaces is free to choose to use the same database or different ones, or even different kinds, local or distributed, etc. So this holds for these 'relational' aggregate roots as well. But you should almost force yourself to use different databases and preferably even different technologies (e.g. one EntityFramework, the other MongoDb) when you start with this, to force yourself to make sure your interfaces are properly defined and independent of implementation.</p>&#xA;&#xA;<p>And yes, big fan of CQRS as well here, and Event/Command Sourcing as well. There are many light-weight implementations possible that allow you to upscale, but are very easy to get into and afford you almost completely linear (=predictable) complexity. </p>&#xA;"
48525633,48515460,7369238,2018-01-30T15:58:46,"<p>As you are using Database Per Service Model for Microservice, in Production perfect solution can be AWS RDS instance for database, Now you have 2 approaches :</p>&#xA;&#xA;<ul>&#xA;<li><p>You can create single RDS Instance and can have different databases for different services on same RDS insatnce, it will save cost a lot but you need to take care of database connections and load you will be having on database based on that you have to choose RDS instance type like 4xlarge etc, better the instance type more connection it will provide and more database load it can handle effectively.</p></li>&#xA;<li><p>Second solution can be creating several RDS instance and number of RDS instance will be equivalent to your microservice count as each service will be using one RDS instance for its database independently, this is not the effective solution, it will incur lot of cost and this solution will under utilize AWS RDS instances.</p></li>&#xA;</ul>&#xA;"
48525337,48505946,7369238,2018-01-30T15:45:50,"<p>Microservice based architecture never supports single database for service, Instead Each microservice have its own database and interaction of services happens over http call or event based interaction using queues.</p>&#xA;&#xA;<p>If we will be using single database then there is no point of making http call to other service because if you are making call to other microservice it means you want to fetch data from other service database or providing your service data to other service for processing, if database will be common  then we can make call directly to database for data instead of calling other service and indirectly it comes monolithic application.</p>&#xA;&#xA;<p>We call microservices : database per service model.</p>&#xA;&#xA;<p>If you mean Separate Servers with Single database = 1 RDS instance with one database then you can do that but below will be the implications :</p>&#xA;&#xA;<p>You can create single RDS Instance and that can have different databases for different services on same RDS insatnce, it will save cost a lot but you need to take care of database connections and load you will be having on database based on that you have to choose RDS instance type like 4xlarge etc, better the instance type more connection it will provide and more database load it can handle effectively.</p>&#xA;&#xA;<p>Second solution can be creating several RDS instance and number of RDS instance will be equivalent to your microservice count as each service will be using one RDS instance for its database independently, this is not the effective solution, it will incur lot of cost and this solution will under utilize AWS RDS instances.</p>&#xA;"
49420937,49395113,7369238,2018-03-22T05:06:32,"<p>Use <a href=""https://www.planttext.com/"" rel=""nofollow noreferrer"">https://www.planttext.com/</a> for creating the flowchart, You need to first understand the syntax for creating the flowchart.</p>&#xA;"
48315497,48296421,7369238,2018-01-18T07:00:00,"<p>I had created an application that is completely a micro service based architecture running on AWS ECS(Container Service), Each microservice is pushed on container as Docker image. There are 2 instances of EC2 are running for achieving High Availability and same mircoservices are running on both instances so if one instance goes down another can take care of requests.</p>&#xA;&#xA;<p>each microservice use its own database and inter microservice communication is happening using client registry on HTTP protocol and discovery, Spring Cloud Consul and Netflix Eureka can be used for service discovery and registery.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/Rejza.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rejza.jpg"" alt=""enter image description here""></a>.</p>&#xA;&#xA;<p>Please find the diagram below :</p>&#xA;"
50016031,50006860,7369238,2018-04-25T07:05:10,"<p>Application.properties or Application.yaml used for loading the configuration on startup and inject the property value in variable. Like <strong>keyStorePath</strong> variable will be injected with value defined in properties file.</p>&#xA;&#xA;<p><strong>Java Code</strong></p>&#xA;&#xA;<pre><code>@Value(""${java.keystore.path}"")&#xA;    private String keyStorePath; &#xA;</code></pre>&#xA;&#xA;<p><strong>application.properties</strong> </p>&#xA;&#xA;<pre><code>java:&#xA; keystore:&#xA;  path: /KeyStore.jks&#xA;</code></pre>&#xA;&#xA;<p>Yes, You can use centralized configurations that can be used by all microservice, Create Config Server annoted with <strong>@EnableConfigServer</strong> that will hold configurations for all microservice(instead of local application.properties all microservice on start will come to config server for configuration just we need to provide config server url so that microservices can communicate with config server on startup and have required data).</p>&#xA;&#xA;<p><strong>Config Server Main Class</strong> </p>&#xA;&#xA;<pre><code>@SpringBootApplication&#xA;@EnableConfigServer&#xA;@ComponentScan(basePackages = {""com.abc.*""})&#xA;public class ConfigApplication {&#xA;&#xA;    public static void main(String[] args) {&#xA;        SpringApplication.run(ConfigApplication.class, args);&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p><strong>Property file in config Server</strong></p>&#xA;&#xA;<pre><code> java:&#xA;     keystore:&#xA;      path: /KeyStore.jks&#xA;</code></pre>&#xA;&#xA;<p><strong>Property File in Microservice</strong> that will hold the url of Config Server</p>&#xA;&#xA;<pre><code># MicroServices Properties&#xA;spring:&#xA;  application:&#xA;     name: Microservice1&#xA;  profiles:&#xA;    active: rds&#xA;  cloud:&#xA;    config:&#xA;      uri: http://localhost:8888&#xA;      fail-fast: true&#xA;      password: test@123&#xA;      username: user&#xA;</code></pre>&#xA;"
48747881,48726644,7369238,2018-02-12T13:40:54,"<p>As you are using Spring boot, I will suggest you to use annotation for zuul gateway and create custom filter for zuul that will take care for each and every thing.</p>&#xA;&#xA;<p>Please find the code below and see whether it works for you or not :</p>&#xA;&#xA;<blockquote>&#xA;  <p>Main Class</p>&#xA;</blockquote>&#xA;&#xA;<pre><code>@EnableZuulProxy&#xA;@SpringBootApplication&#xA;@EnableScheduling&#xA;@EnableFeignClients(basePackages = { Constants.FEIGN_BASE_PACKAGE })&#xA;@ComponentScan(basePackages = { Constants.BASE_PACKAGE, Constants.LOGGING_PACKAGE })&#xA;@EntityScan(basePackages = { Constants.ENTITY_BASE_PACKAGE })&#xA;@EnableDiscoveryClient&#xA;public class GatewayInitializer {&#xA;&#xA;    /**&#xA;     * The main method.&#xA;     *&#xA;     * @param args the arguments&#xA;     */&#xA;    public static void main(String[] args) {&#xA;&#xA;        Security.setProperty(""networkaddress.cache.ttl"", ""30"");&#xA;        ConfigurableApplicationContext context = SpringApplication.run(Initializer.class, args);&#xA;&#xA;    }&#xA;</code></pre>&#xA;&#xA;<blockquote>&#xA;  <p>ZUUL Filter</p>&#xA;</blockquote>&#xA;&#xA;<pre><code>package com.sxm.aota.gateway.filters;&#xA;&#xA;import org.slf4j.Logger;&#xA;import org.slf4j.LoggerFactory;&#xA;import org.springframework.beans.factory.annotation.Autowired;&#xA;import org.springframework.context.MessageSource;&#xA;import org.springframework.util.ReflectionUtils;&#xA;&#xA;import com.fasterxml.jackson.databind.ObjectMapper;&#xA;import com.netflix.zuul.ZuulFilter;&#xA;import com.netflix.zuul.context.RequestContext;&#xA;import com.netflix.zuul.exception.ZuulException;&#xA;&#xA;&#xA;/**&#xA; * This CustomErrorFilter involve to handle ZuulException.&#xA; */&#xA;public class CustomErrorFilter extends ZuulFilter {&#xA;&#xA;    /** The logger. */&#xA;    private static final Logger logger = LoggerFactory.getLogger(CustomErrorFilter.class);&#xA;    /** The message source. */&#xA;    @Autowired&#xA;    private MessageSource messageSource;&#xA;    /** The properties. */&#xA;    @Autowired&#xA;    private Properties properties;&#xA;&#xA;    /*&#xA;     * (non-Javadoc)&#xA;     * &#xA;     * @see com.netflix.zuul.ZuulFilter#filterType()&#xA;     */&#xA;    @Override&#xA;    public String filterType() {&#xA;        return ""post"";&#xA;    }&#xA;&#xA;    /*&#xA;     * (non-Javadoc)&#xA;     * &#xA;     * @see com.netflix.zuul.ZuulFilter#filterOrder()&#xA;     */&#xA;    @Override&#xA;    public int filterOrder() {&#xA;        return -1; // Needs to run before SendErrorFilter which has filterOrder == 0&#xA;    }&#xA;&#xA;    /*&#xA;     * (non-Javadoc)&#xA;     * &#xA;     * @see com.netflix.zuul.IZuulFilter#shouldFilter()&#xA;     */&#xA;    @Override&#xA;    public boolean shouldFilter() {&#xA;        // only forward to errorPath if it hasn't been forwarded to already&#xA;        return RequestContext.getCurrentContext().containsKey(""error.status_code"");&#xA;    }&#xA;&#xA;    /*&#xA;     * (non-Javadoc)&#xA;     * &#xA;     * @see com.netflix.zuul.IZuulFilter#run()&#xA;     */&#xA;    @Override&#xA;    public Object run() {&#xA;        try {&#xA;            RequestContext ctx = RequestContext.getCurrentContext();&#xA;            Object e = ctx.get(""error.exception"");&#xA;            if (e != null &amp;&amp; e instanceof ZuulException) {&#xA;                ZuulException zuulException = (ZuulException) e;&#xA;                logger.error(""Zuul failure detected: "" + zuulException.getMessage(), zuulException);&#xA;                // Remove error code to prevent further error handling in follow up filters&#xA;                ctx.remove(""error.status_code"");&#xA;&#xA;                } else {&#xA;                    error.setMessage(messageSource.getMessage(Constants.REQUESTED_SERVICE_UNAVAILABLE, new Object[] { zuulException.getCause() }, properties.getCurrentLocale()));&#xA;                    ctx.setResponseBody(mapper.writeValueAsString(error));&#xA;                    ctx.getResponse().setContentType(""application/json"");&#xA;                    ctx.setResponseStatusCode(500); // Can set any error code as excepted&#xA;                }&#xA;            }&#xA;        } catch (Exception ex) {&#xA;            logger.error(""Exception filtering in custom error filter"", ex);&#xA;            ReflectionUtils.rethrowRuntimeException(ex);&#xA;        }&#xA;        return null;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
48823887,48802787,7369238,2018-02-16T09:42:42,"<p>Its not opensource but you can have a look that can explain you the microservice architecture in detail.</p>&#xA;&#xA;<p><a href=""https://github.com/vickygupta0017/microservice-poc"" rel=""nofollow noreferrer"">https://github.com/vickygupta0017/microservice-poc</a></p>&#xA;"
48823813,48810786,7369238,2018-02-16T09:38:28,"<p>I had written the code in java for fetching the data from sqs queue with SQSBufferedAsyncClient, advantages using this API is buffered the messages in async mode.</p>&#xA;&#xA;<pre><code>/**&#xA; * &#xA; */&#xA;package com.sxm.aota.tsc.config;&#xA;&#xA;import java.net.UnknownHostException;&#xA;&#xA;import org.springframework.beans.factory.annotation.Autowired;&#xA;import org.springframework.context.annotation.Bean;&#xA;import org.springframework.context.annotation.Configuration;&#xA;&#xA;import com.amazonaws.AmazonClientException;&#xA;import com.amazonaws.AmazonWebServiceRequest;&#xA;import com.amazonaws.ClientConfiguration;&#xA;import com.amazonaws.auth.BasicAWSCredentials;&#xA;import com.amazonaws.auth.InstanceProfileCredentialsProvider;&#xA;import com.amazonaws.regions.Region;&#xA;import com.amazonaws.regions.Regions;&#xA;import com.amazonaws.retry.RetryPolicy;&#xA;import com.amazonaws.retry.RetryPolicy.BackoffStrategy;&#xA;import com.amazonaws.services.sqs.AmazonSQSAsync;&#xA;import com.amazonaws.services.sqs.AmazonSQSAsyncClient;&#xA;import com.amazonaws.services.sqs.buffered.AmazonSQSBufferedAsyncClient;&#xA;import com.amazonaws.services.sqs.buffered.QueueBufferConfig;&#xA;&#xA;@Configuration&#xA;public class SQSConfiguration {&#xA;&#xA;    /** The properties cache config. */&#xA;    @Autowired&#xA;    private PropertiesCacheConfig propertiesCacheConfig;&#xA;&#xA;    @Bean&#xA;    public AmazonSQSAsync amazonSQSClient() {&#xA;        // Create Client Configuration&#xA;        ClientConfiguration clientConfig = new ClientConfiguration()&#xA;            .withMaxErrorRetry(5)&#xA;            .withConnectionTTL(10_000L)&#xA;            .withTcpKeepAlive(true)&#xA;            .withRetryPolicy(new RetryPolicy(&#xA;                    null, &#xA;                new BackoffStrategy() {                 &#xA;                    @Override&#xA;                    public long delayBeforeNextRetry(AmazonWebServiceRequest req, &#xA;                            AmazonClientException exception, int retries) {&#xA;                        // Delay between retries is 10s unless it is UnknownHostException &#xA;                        // for which retry is 60s&#xA;                        return exception.getCause() instanceof UnknownHostException ? 60_000L : 10_000L;&#xA;                    }&#xA;                }, 10, true));&#xA;        // Create Amazon client&#xA;        AmazonSQSAsync asyncSqsClient = null;&#xA;        if (propertiesCacheConfig.isIamRole()) {&#xA;            asyncSqsClient = new AmazonSQSAsyncClient(new InstanceProfileCredentialsProvider(true), clientConfig);&#xA;        } else {&#xA;            asyncSqsClient = new AmazonSQSAsyncClient(&#xA;                    new BasicAWSCredentials(""sceretkey"", ""accesskey""));&#xA;        }&#xA;        final Regions regions = Regions.fromName(propertiesCacheConfig.getRegionName());&#xA;        asyncSqsClient.setRegion(Region.getRegion(regions));&#xA;        asyncSqsClient.setEndpoint(propertiesCacheConfig.getEndPoint());&#xA;&#xA;        // Buffer for request batching&#xA;        final QueueBufferConfig bufferConfig = new QueueBufferConfig();&#xA;        // Ensure visibility timeout is maintained&#xA;        bufferConfig.setVisibilityTimeoutSeconds(20);&#xA;        // Enable long polling&#xA;        bufferConfig.setLongPoll(true);&#xA;        // Set batch parameters&#xA;//      bufferConfig.setMaxBatchOpenMs(500);&#xA;        // Set to receive messages only on demand&#xA;//      bufferConfig.setMaxDoneReceiveBatches(0);&#xA;//      bufferConfig.setMaxInflightReceiveBatches(0);&#xA;&#xA;        return new AmazonSQSBufferedAsyncClient(asyncSqsClient, bufferConfig);&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>then written the scheduleR which executes after every 2 secs and fetches the data from queue, process it and delete it from queue before visibility timeout otherwise it will be ready for processing again when visibility tiiimeout expires again.</p>&#xA;&#xA;<pre><code>package com.sxm.aota.tsc.sqs;&#xA;&#xA;import java.util.List;&#xA;import java.util.concurrent.CountDownLatch;&#xA;&#xA;import javax.annotation.PostConstruct;&#xA;&#xA;import org.slf4j.Logger;&#xA;import org.slf4j.LoggerFactory;&#xA;import org.springframework.beans.factory.annotation.Autowired;&#xA;import org.springframework.context.annotation.DependsOn;&#xA;import org.springframework.scheduling.annotation.EnableScheduling;&#xA;import org.springframework.scheduling.annotation.Scheduled;&#xA;import org.springframework.stereotype.Component;&#xA;&#xA;import com.amazonaws.services.sqs.AmazonSQSAsync;&#xA;import com.amazonaws.services.sqs.model.DeleteMessageRequest;&#xA;import com.amazonaws.services.sqs.model.GetQueueUrlRequest;&#xA;import com.amazonaws.services.sqs.model.GetQueueUrlResult;&#xA;import com.amazonaws.services.sqs.model.ReceiveMessageRequest;&#xA;import com.amazonaws.services.sqs.model.ReceiveMessageResult;&#xA;import com.fasterxml.jackson.databind.ObjectMapper;&#xA;&#xA;&#xA;    /**&#xA;     * The Class TSCDataSenderScheduledTask.&#xA;     * &#xA;     * Sends the aggregated Vehicle data to TSC in batches&#xA;     */&#xA;    @EnableScheduling&#xA;    @Component(""sqsScheduledTask"")&#xA;    @DependsOn({ ""propertiesCacheConfig"", ""amazonSQSClient"" })&#xA;    public class SQSScheduledTask {&#xA;&#xA;        private static final Logger LOGGER = LoggerFactory.getLogger(SQSScheduledTask.class);&#xA;        @Autowired&#xA;        private PropertiesCacheConfig propertiesCacheConfig;&#xA;        @Autowired&#xA;        public AmazonSQSAsync amazonSQSClient;&#xA;&#xA;        /**&#xA;         * Timer Task that will run after specific interval of time Majorly&#xA;         * responsible for sending the data in batches to TSC.&#xA;         */&#xA;        private String queueUrl;&#xA;        private final ObjectMapper mapper = new ObjectMapper();&#xA;&#xA;        @PostConstruct&#xA;        public void initialize() throws Exception {&#xA;            LOGGER.info(""SQS-Publisher"", ""Publisher initializing for queue "" + propertiesCacheConfig.getSQSQueueName(),&#xA;                    ""Publisher initializing for queue "" + propertiesCacheConfig.getSQSQueueName());&#xA;            // Get queue URL&#xA;            final GetQueueUrlRequest request = new GetQueueUrlRequest().withQueueName(propertiesCacheConfig.getSQSQueueName());&#xA;            final GetQueueUrlResult response = amazonSQSClient.getQueueUrl(request);&#xA;            queueUrl = response.getQueueUrl();&#xA;&#xA;            LOGGER.info(""SQS-Publisher"", ""Publisher initialized for queue "" + propertiesCacheConfig.getSQSQueueName(),&#xA;                    ""Publisher initialized for queue "" + propertiesCacheConfig.getSQSQueueName() + "", URL = "" + queueUrl);&#xA;        }&#xA;&#xA;        @Scheduled(fixedDelayString = ""${sqs.consumer.delay}"")&#xA;        public void timerTask() {&#xA;&#xA;            final ReceiveMessageResult receiveResult = getMessagesFromSQS();&#xA;            String messageBody = null;&#xA;            if (receiveResult != null &amp;&amp; receiveResult.getMessages() != null &amp;&amp; !receiveResult.getMessages().isEmpty()) {&#xA;                try {&#xA;                    messageBody = receiveResult.getMessages().get(0).getBody();&#xA;                    String messageReceiptHandle = receiveResult.getMessages().get(0).getReceiptHandle();&#xA;                    Vehicles vehicles = mapper.readValue(messageBody, Vehicles.class);&#xA;                    processMessage(vehicles.getVehicles(),messageReceiptHandle);&#xA;                } catch (Exception e) {&#xA;                    LOGGER.error(""Exception while processing SQS message : {}"", messageBody);&#xA;                    // Message is not deleted on SQS and will be processed again after visibility timeout&#xA;                }&#xA;            }&#xA;        }&#xA;&#xA;        public void processMessage(List&lt;Vehicle&gt; vehicles,String messageReceiptHandle) throws InterruptedException {&#xA;            //processing code&#xA;            //delete the sqs message as the processing is completed&#xA;            //Need to create atomic counter that will be increamented by all TS.. Once it will be 0 then we will be deleting the messages&#xA;&#xA;                    amazonSQSClient.deleteMessage(new DeleteMessageRequest(queueUrl, messageReceiptHandle));&#xA;&#xA;        }&#xA;&#xA;        private ReceiveMessageResult getMessagesFromSQS() {&#xA;            try {&#xA;                // Create new request and fetch data from Amazon SQS queue&#xA;                final ReceiveMessageResult receiveResult = amazonSQSClient&#xA;                        .receiveMessage(new ReceiveMessageRequest().withMaxNumberOfMessages(1).withQueueUrl(queueUrl));&#xA;                return receiveResult;&#xA;            } catch (Exception e) {&#xA;                LOGGER.error(""Error while fetching data from SQS"", e);&#xA;            }&#xA;            return null;&#xA;        }&#xA;&#xA;    }&#xA;</code></pre>&#xA;"
48872233,48863771,7369238,2018-02-19T18:34:09,"<p>Configuration for Ribbon Client are not Correct, I had executed the code successfully with below changes in Ribbon Client, On Client call it were throwing Null Pointer Exception as several parameters of Ribbon Client were not set successfully along with that I can see @Configuration were missing in EmployeeConfiguration Class, hence how it will initialize Ribbon client.</p>&#xA;&#xA;<p>Also Checked in the complete workable code at below location :</p>&#xA;&#xA;<p><a href=""https://github.com/abhayjohri87/RibbonClientLBWithMicroServices.git"" rel=""nofollow noreferrer"">https://github.com/abhayjohri87/RibbonClientLBWithMicroServices.git</a></p>&#xA;&#xA;<pre><code>package com.ribbon.client;&#xA;import java.util.ArrayList;&#xA;import java.util.List;&#xA;&#xA;import org.springframework.beans.factory.annotation.Autowired;&#xA;import org.springframework.boot.SpringApplication;&#xA;import org.springframework.boot.autoconfigure.SpringBootApplication;&#xA;import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;&#xA;import org.springframework.cloud.client.loadbalancer.LoadBalanced;&#xA;import org.springframework.cloud.netflix.ribbon.RibbonClient;&#xA;import org.springframework.context.annotation.Bean;&#xA;import org.springframework.context.annotation.Configuration;&#xA;import org.springframework.web.bind.annotation.RequestMapping;&#xA;import org.springframework.web.bind.annotation.RestController;&#xA;import org.springframework.web.client.RestTemplate;&#xA;&#xA;import com.netflix.client.config.DefaultClientConfigImpl;&#xA;import com.netflix.client.config.IClientConfig;&#xA;import com.netflix.loadbalancer.ConfigurationBasedServerList;&#xA;import com.netflix.loadbalancer.Server;&#xA;import com.netflix.loadbalancer.ServerList;&#xA;//import com.ribbon.Employee.configuration.EmployeeConfiguration;&#xA;import com.ribbon.client.RibbonClientApplication.UserConfig;&#xA;&#xA;&#xA;@SpringBootApplication&#xA;@RestController&#xA;@RibbonClient(name = ""employee-microservice"", configuration = UserConfig.class)&#xA;public class RibbonClientApplication {&#xA;&#xA;      @LoadBalanced&#xA;      @Bean&#xA;      RestTemplate restTemplate(){&#xA;        return new RestTemplate();&#xA;      }&#xA;&#xA;      @Autowired&#xA;      RestTemplate restTemplate;&#xA;&#xA;      @RequestMapping(""/listEmployee"")&#xA;      public List getEmployeeList() {&#xA;        List empList = this.restTemplate.getForObject(""http://employee-microservice/employees"", ArrayList.class);&#xA;        return empList;&#xA;      }&#xA;&#xA;    public static void main(String[] args) {&#xA;        SpringApplication.run(RibbonClientApplication.class, args);&#xA;    }&#xA;&#xA;&#xA;    @Configuration&#xA;    static class UserConfig {&#xA;&#xA;        private String name = ""employee-microservice"";&#xA;&#xA;        @Bean&#xA;        @ConditionalOnMissingBean&#xA;        public IClientConfig ribbonClientConfig() {&#xA;            DefaultClientConfigImpl config = new DefaultClientConfigImpl();&#xA;            config.loadProperties(this.name);&#xA;            return config;&#xA;        }&#xA;&#xA;        @Bean&#xA;        ServerList&lt;Server&gt; ribbonServerList(IClientConfig config) {&#xA;            ConfigurationBasedServerList serverList = new ConfigurationBasedServerList();&#xA;            serverList.initWithNiwsConfig(config);&#xA;            return serverList;&#xA;        }&#xA;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
48827138,48792602,7369238,2018-02-16T12:47:45,<p>Micro services follows database per service model</p>&#xA;
48826291,48805353,7369238,2018-02-16T11:57:54,"<p>If you are calling the services with ip and port then there is no need of Eureka, As you are doing inter micro service communication and eureka is already in picture for service discovery and registry. You want to do load balancing then zuul will come in picture.</p>&#xA;&#xA;<p>Change your code in consumer with mricroservice name instead of ip and port then it will work.</p>&#xA;&#xA;<pre><code>@SpringBootApplication&#xA;@EnableDiscoveryClient&#xA;public class WebclientConsumerMicroserviceApplication {&#xA;&#xA;    public static final String EMPLOYEE_SERVICE_URL = ""http://employee-microservice/"";&#xA;</code></pre>&#xA;&#xA;<p>and also change the remaining uri in employeeServicesImpl.</p>&#xA;&#xA;<pre><code>    public List&lt;Employee&gt; getEmployeeList() {&#xA;//  Employee[] employeeList =  restTemplate.getForObject(""http://employee-microservice/employees"", Employee[].class);&#xA;    Employee[] employeeList =  restTemplate.getForObject(url+""employees, Employee[].class);&#xA;</code></pre>&#xA;&#xA;<p>It will be working Just start this way :</p>&#xA;&#xA;<ol>&#xA;<li>Start your Eureka</li>&#xA;<li>Start the Producer Service</li>&#xA;<li>Start the Consumer Service and it will be workable.</li>&#xA;</ol>&#xA;&#xA;<p>Note : We need to call the services only via service name and needs to avoid ip and port(they are dynamic).</p>&#xA;&#xA;<p>If you want to run using zuul then use the zuul service name : zuul-gateway with below configuration in consumer</p>&#xA;&#xA;<pre><code>zuul-gateway:&#xA;  ribbon:&#xA;    eureka:&#xA;      enabled: false&#xA;    listOfServers: localhost:8090&#xA;    ServerListRefreshInterval: 1500&#xA;</code></pre>&#xA;"
48966729,48961984,7369238,2018-02-24T19:25:01,"<p>Use Config Server( spring cloud config server) that will maintain centralized configurations, you need to make changes to config server related to configurations, each microservices will come on startup for configurations to config server, even after start up after certain interval of time microservices can come to config server for validating any change in configurations and update accordingly.</p>&#xA;"
48854696,48850386,7369238,2018-02-18T17:42:10,"<p>You need to convert your spring boot application into docker image. Conversion of boot application can be converted using docker maven plugin or you can use docker command for this.</p>&#xA;&#xA;<p>Using docker you need dockerfile that have the steps for creating docker image.</p>&#xA;&#xA;<p>Once your image is ready you can run that docker image on docker engine, hence image after run will be a docker container. That is basically the virtualization.</p>&#xA;&#xA;<p>There set of docker commands for running/creating docker images. </p>&#xA;&#xA;<p>Install docker engine and start docker engine using </p>&#xA;&#xA;<blockquote>&#xA;  <p>service docker start</p>&#xA;</blockquote>&#xA;&#xA;<p>And then use all docker commands</p>&#xA;"
48830338,48805353,7369238,2018-02-16T15:50:23,"<p>I had successfully executed your code with below changes, removed the </p>&#xA;&#xA;<blockquote>&#xA;  <p>enablediscovery and LoadBalanced annotation</p>&#xA;</blockquote>&#xA;&#xA;<p>if this will be enabled then always rest template will go to eureka for discovery, as you are using proxy server then your consumer don't need the discovery because you are giving absolute path also remove eureka configurations from application.yml. </p>&#xA;&#xA;<p>You cannot use discovery and absolute path at the same time, only one can be used at a time.</p>&#xA;&#xA;<pre><code>package com.eureka.discovery.client;&#xA;&#xA;import org.springframework.boot.SpringApplication;&#xA;import org.springframework.boot.autoconfigure.SpringBootApplication;&#xA;import org.springframework.beans.factory.annotation.Autowired;&#xA;import org.springframework.context.annotation.Bean;&#xA;import org.springframework.web.bind.annotation.RequestMapping;&#xA;import org.springframework.web.bind.annotation.RequestParam;&#xA;import org.springframework.web.bind.annotation.RestController;&#xA;import org.springframework.web.client.RestTemplate;&#xA;&#xA;import com.eureka.discovery.client.bo.Employee;&#xA;&#xA;@SpringBootApplication&#xA;@RestController&#xA;//@EnableDiscoveryClient&#xA;public class UserApplication {&#xA;&#xA;  @Bean&#xA;//@Loadbalancer&#xA;  RestTemplate restTemplate(){&#xA;    return new RestTemplate();&#xA;  }&#xA;&#xA;  @Autowired&#xA;  RestTemplate restTemplate;&#xA;&#xA;  @RequestMapping(""/hi"")&#xA;  public String hi() {&#xA;      Employee[] employeeList = this.restTemplate.getForObject(""http://localhost:8090/emp-api"", Employee[].class);&#xA;    return String.format(""%s, %s!"", ""Testing"", ""Abhay"");&#xA;  }&#xA;&#xA;  public static void main(String[] args) {&#xA;    SpringApplication.run(UserApplication.class, args);&#xA;  }&#xA;}&#xA;</code></pre>&#xA;"
31095296,31095177,752918,2015-06-28T01:32:05,"<p>Have your primary database publish events every time a database mutation occurs, and let the replicated services subscribe to this event and apply the same mutation on their replicated data.</p>&#xA;&#xA;<p>You already use a message broker, so you can leverage your existing stack for broadcasting the events. By having replication done through events, a restore being applied to the primary database will be propagated to all other services.</p>&#xA;&#xA;<p>Depending on the scale of the backup, there will be a short period where the data on the other services will be stale. This might or might not be acceptable for your use case. Think of the staleness as some sort of eventual consistency model.</p>&#xA;"
31479208,31313170,623894,2015-07-17T15:21:05,"<p>By setting a version number in the metadata you can easily make your Svc1 fetch the latest version of Svc2,i.e. always getting the instance with the latest version number. See <a href=""https://gist.github.com/awwal/a97709c88e29406e6c59"" rel=""noreferrer"">this gist</a> as guide. </p>&#xA;"
39860939,39848800,623894,2016-10-04T20:10:22,"<p>You can have only one defaultZone. You need peer replication. So that your 8761/eureka is synchronized with 8762/eureka. When the application is registered to 8761  it is also available in 8762. To do this see <a href=""https://stackoverflow.com/users/2730527/spencergibb"">@spencergibb</a> answer <a href=""https://stackoverflow.com/questions/30288959/eureka-peers-not-synchronized"" title=""here"">here</a>.</p>&#xA;"
40053599,40036030,623894,2016-10-15T00:00:19,"<p>The problem seems to be that you are naming your client <strong><em>EurekaClient</em></strong>, There is already a bean with that name. Rename that class to something else and it should work</p>&#xA;"
39951172,39941660,623894,2016-10-10T04:06:39,"<p>You can make use of the EurekaClient#getNextServerFromEureka. You might have to create the URI yourself but that should be trivial. </p>&#xA;&#xA;<pre><code>@Autowired&#xA;EurekaClient eurekaClient;&#xA;&#xA;&#xA;public void executeMethod()     {&#xA;    InstanceInfo loadBalancedInstance = eurekaClient.getNextServerFromEureka(""myService"", false);&#xA;    //do work&#xA;}&#xA;</code></pre>&#xA;"
43660856,43659872,103190,2017-04-27T14:36:43,"<p>It looks like the return type should be a <code>Source</code> (from Akka Reactive Streams) but you are building a Java 8 <code>Stream</code>.</p>&#xA;&#xA;<p>The problem can be easily solved if you used <code>select</code> instead of <code>selectAll</code> when querying the database. Lagom's <code>CassandraSession</code> provides two families of meqthods to query the DB: (1) <code>select(...)</code> will immediately return a <code>Source&lt;Row,NotUsed&gt;</code> which is a reactive stream or (2) <code>selectAll(...)</code> which will gather all rows in memory and return a <code>List&lt;Row&gt;</code>. The latter could take your server down because will try to put all the info in memory. The former will use reactive streams to deliver items adapting the speed to your consuming end speed (backpressure) keeping a very low memory footprint.</p>&#xA;&#xA;<p>Your code can be rewritten as:</p>&#xA;&#xA;<pre><code> public ServiceCall&lt;NotUsed, Source&lt;GreetingMessage, ?&gt;&gt; getGreetings() {&#xA;    return request -&gt;&#xA;         CompletableFuture.completedFuture(&#xA;              session.select(""SELECT * FROM greetings"")&#xA;                     .map(row -&gt; new GreetingMessage(row.getString(0)))&#xA;          );&#xA; }&#xA;</code></pre>&#xA;&#xA;<p>Using <code>select</code> creates a <code>Source&lt;&gt;</code>. You can map items individually on that <code>Source&lt;&gt;</code> using the lambda you already developed.</p>&#xA;"
41139920,41041058,1020694,2016-12-14T10:11:30,"<p>Try <a href=""https://geteventstore.com/"" rel=""nofollow noreferrer"">geteventstore</a> or <a href=""https://kafka.apache.org/"" rel=""nofollow noreferrer"">Apache Kafka</a> and read  <a href=""https://stackoverflow.com/questions/17708489/using-kafka-as-a-cqrs-eventstore-good-idea"">this stack-overflow question </a></p>&#xA;"
42491305,42457900,6400278,2017-02-27T16:52:20,"<p>You don't need to copy the account data into the video / image services. When you store a video / picture, just give the accountID with it. </p>&#xA;&#xA;<p>When you for example need to get a user and it's images, you have different options to get the data:</p>&#xA;&#xA;<p>If you work with an API Gateway, you could make 2 async parallel calls from it to the services, and combine both data in a DTO and give that back.</p>&#xA;&#xA;<p>If that isn't possible because you first need specific data from the user, just make a call to the account service, which on his turn will make a call to the image service to retrieve it's data, and send the whole package back.</p>&#xA;&#xA;<p>It's up to you to see if that is possible or not.</p>&#xA;&#xA;<p>Long story short, just store account ID with each video / image entity.</p>&#xA;"
42027037,31088764,1092630,2017-02-03T14:55:09,"<p>We are starting down this same path.  Like all new hot new methodologies, you must be careful that you are actually achieving the benefits of using a Microservices approach.</p>&#xA;&#xA;<p>We have evaluated <a href=""https://azure.microsoft.com/en-us/services/service-fabric/"" rel=""nofollow noreferrer"">Azure Service Fabric</a> as one possibility.  As a place to host your applications it seems quite promising.  There is also an impressive API if you want your applications to tightly integrate with the environment.  This integration could likely answer your questions.  The caveat is that the API is still in flux (it's improving) and documentation is scarce.  It also feels a bit like ""vendor lock"".</p>&#xA;&#xA;<p>To keep things simple, we have started out by letting our microservices be simple stateless applications that communicate via REST.  The endpoints are well-documented and contain a contract version number as part of the URI.  We intend to introduce more sophisticated ways of interaction later as the need arises (ie, performance).</p>&#xA;&#xA;<p>To answer your question about ""data access layer"", my opinion would be that each microservice should persist state in whatever way is best for that service to do so.  The actual storage is private to the microservices and other services may only use that data through its public API.</p>&#xA;"
49887724,49887495,5761558,2018-04-17T21:24:41,"<p>For global data, you should simply use shared data. This has the additional benefit of being accessible across the cluster.</p>&#xA;&#xA;<p>The following code uses a <em>local</em> map:</p>&#xA;&#xA;<pre><code>SharedData sd = vertx.sharedData();&#xA;LocalMap&lt;String, String&gt; map1 = sd.getLocalMap(""mymap1"");&#xA;map1.put(""foo"", ""bar"");&#xA;</code></pre>&#xA;&#xA;<p>Reading is similarly easy:</p>&#xA;&#xA;<pre><code>String val = map1.get(""foo"");&#xA;</code></pre>&#xA;&#xA;<p>The documentation is <a href=""https://vertx.io/docs/vertx-core/java/#_using_shared_data_with_vert_x"" rel=""nofollow noreferrer"">on this page</a></p>&#xA;"
51264790,51173039,9956374,2018-07-10T12:08:17,"<p>The Orders API should be called inside </p>&#xA;&#xA;<pre><code>Run() -&gt; &#xA;Client.OnMessage((receivedMessage) =&gt;&#xA;            {&#xA;                try&#xA;                {&#xA;                   //Order API call here&#xA;                }&#xA;                catch&#xA;                {&#xA;&#xA;                }&#xA;            });&#xA;</code></pre>&#xA;&#xA;<p>Use HttpClient for triggering the API. By doing this, the API will be called whenever a message is received from the Queue.</p>&#xA;&#xA;<p>As the worker role is hosted in Azure environment, the APIs hosted somewhere public can only be called from here. It does not identify the API hosted in local machine. Try hosting the API in <a href=""https://azure.microsoft.com/en-in/services/app-service/"" rel=""nofollow noreferrer"">Azure App Service</a> or <a href=""https://azure.microsoft.com/en-in/services/functions/"" rel=""nofollow noreferrer"">Azure Function App</a>.</p>&#xA;"
50329381,50320908,2768407,2018-05-14T11:38:29,"<p>I would't worry too much about the latency, as all the calls would be asynchronous.</p>&#xA;&#xA;<p>Microservices are all about the options, selective: scalability, robustness/antifragility, deployment, etc. You cannot make the whole system robust <em>but</em> you can make some of it (the important bits).</p>&#xA;&#xA;<p>I would focus on modelling of domain models/boundary contexts, try to get Single &#xA;Responsibility Principle right, that would hopefully help you to avoid functionality replication, Death Star dependencies.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/hZnQq.png"" rel=""nofollow noreferrer"">Very basic µService Architecture</a></p>&#xA;&#xA;<p>Reading:</p>&#xA;&#xA;<p><a href=""https://www.amazon.co.uk/Building-Microservices-Sam-Newman/dp/1491950358/ref=sr_1_1?ie=UTF8&amp;qid=1526297524&amp;sr=8-1&amp;keywords=building+microservices"" rel=""nofollow noreferrer"">Building Microservices</a></p>&#xA;&#xA;<p><a href=""https://www.amazon.co.uk/Domain-Driven-Design-Tackling-Complexity-Software/dp/0321125215/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1526297564&amp;sr=1-1&amp;keywords=domain+driven+design&amp;dpID=51sZW87slRL&amp;preST=_SX218_BO1,204,203,200_QL40_&amp;dpSrc=srch"" rel=""nofollow noreferrer"">Domain-Driven Design: Tackling Complexity in the Heart of Software</a></p>&#xA;"
50656544,50651256,419863,2018-06-02T11:42:39,"<p>I would say it depends on how these services are used. Here are two options i would think of</p>&#xA;&#xA;<h3>opt 1) One Lambda per endpoint method</h3>&#xA;&#xA;<p>You can even setup <strong>one lambda function for each endpoint</strong> (so in your case 6) and have the benefit of choosing the optimal memory sizes and timeouts for each function (good for billing and performance).</p>&#xA;&#xA;<p>Additionally you can separate the permissions for each function (e.g. one IAM role per function) in this case and allow each function to access only the resources in the way which is necessary to fulfill the task.</p>&#xA;&#xA;<h3>opt 2) two lambda functions</h3>&#xA;&#xA;<p>In this case i would separate into <em>customer</em> and <em>order</em> services, because they both handle a different data model.</p>&#xA;&#xA;<p>The 2 Lambda functions share the same memory and timeout settings.</p>&#xA;&#xA;<p>...</p>&#xA;&#xA;<p>in any case you can of course combine code-wise the function handlers in one class whatsoever and you multiple handlers for each CRUD event. Or check the HTTP method and the redirect to the corresponding logic.</p>&#xA;&#xA;<p>Both options can be deployed easily using <a href=""https://github.com/awslabs/serverless-application-model"" rel=""nofollow noreferrer"">SAM</a>.</p>&#xA;&#xA;<p>Some good documentation for further information: </p>&#xA;&#xA;<h2><a href=""https://d1.awsstatic.com/whitepapers/serverless-architectures-with-aws-lambda.pdf"" rel=""nofollow noreferrer"">Serverless architecture with AWS Lambda</a></h2>&#xA;"
40467492,40467382,7105559,2016-11-07T14:23:31,"<p>The devise gem is the leader in the industry.  All of its methods are fully customizable - they can be used as a before_action (typical usage but not ideal for micro services) and can also be used as just another method in your code (inside a block, in a 'if' statement, etc).  Checkout the github page here &#xA;<a href=""https://github.com/plataformatec/devise"" rel=""nofollow noreferrer"">https://github.com/plataformatec/devise</a></p>&#xA;&#xA;<p>It has so much functionality, I could teach en entire course on this gem.  There's a lot to learn if you aren't familiar yet.  </p>&#xA;"
43477971,43132158,5338391,2017-04-18T16:53:15,"<p>The following are two valid and equally correct solutions that you can choose from, taking into account that your implementation API is a Mule app:</p>&#xA;&#xA;<ul>&#xA;<li>Create an API on API Platform</li>&#xA;</ul>&#xA;&#xA;<p>Solution A:</p>&#xA;&#xA;<ol>&#xA;<li>Configure the autogenerated proxy to use your implementation API URL</li>&#xA;<li><a href=""https://docs.mulesoft.com/api-manager/setting-up-an-api-proxy"" rel=""nofollow noreferrer"">Deploy the proxy</a> to a correctly configured API Gateway/Mule runtime&#xA;>= v3.8.0</li>&#xA;<li>Apply one or more policies to the tracked proxy</li>&#xA;</ol>&#xA;&#xA;<p>Solution B:</p>&#xA;&#xA;<ol>&#xA;<li>Add <a href=""https://docs.mulesoft.com/api-manager/api-auto-discovery"" rel=""nofollow noreferrer"">autodiscovery</a> to your implementation API, using the same API&#xA;name and API version name than your already created API on API&#xA;Platform </li>&#xA;<li>Deploy the impl app to a correctly configured API&#xA;Gateway/Mule runtime >= v3.8.0 </li>&#xA;<li>Apply one or more policies to the tracked implementation app</li>&#xA;</ol>&#xA;&#xA;<p>With solution A, you have to make sure that your implementation app is only accessible by the proxy app (eg with a firewall).</p>&#xA;&#xA;<p>If your implementation API would not be a Mule app, then Solution B would not be possible.</p>&#xA;"
47449687,29117570,1348631,2017-11-23T07:21:11,"<p>i have written few posts on this topic: </p>&#xA;&#xA;<p>Maybe these posts can also help:</p>&#xA;&#xA;<p>API Gateway pattern - Course-grained api vs fine-grained apis</p>&#xA;&#xA;<p><a href=""https://www.linkedin.com/pulse/api-gateway-pattern-ronen-hamias/"" rel=""nofollow noreferrer"">https://www.linkedin.com/pulse/api-gateway-pattern-ronen-hamias/</a>&#xA;<a href=""https://www.linkedin.com/pulse/successfulapi-ronen-hamias/"" rel=""nofollow noreferrer"">https://www.linkedin.com/pulse/successfulapi-ronen-hamias/</a></p>&#xA;&#xA;<p>Coarse-grained vs Fine-grained service API</p>&#xA;&#xA;<blockquote>&#xA;  <p>By definition a coarse-grained service operation has broader scope than a fine-grained service, although the terms are relative. coarse-grained increased design complexity but can reduce the number of calls required to complete a task. at micro-services architecture coarse-grained may reside at the API Gateway layer and orchestrate several micro-services to complete specific business operation. coarse-grained APIs needs to be carefully designed as involving several micro-services that managing different domain of expertise has a risk to mix-concerns in single API and breaking the rules described above. coarse-grained APIs may suggest new level of granularity for business functions that where not exist otherwise. for example hire employee may involve two microservices calls to HR system to create employee ID and another call to LDAP system to create a user account. alternatively client may have performed two fine-grained API calls to achieve the same task. while coarse-grained represents business use-case create user account, fine-grained API represent the capabilities involved in such task. further more fine-grained API may involve different technologies and communication protocols while coarse-grained abstract them into unified flow. when designing a system consider both as again there is no golden approach that solve everything and there is trad-off for each. Coarse-grained are particularly suited as services to be consumed in other Business contexts, such as other applications, line of business or even by other organizations across the own Enterprise boundaries (typical B2B scenarios). </p>&#xA;</blockquote>&#xA;"
47449434,42741917,1348631,2017-11-23T07:03:48,"<p>please check out scalecube that implements microservices based on swim protocol with gossip protocol improvement</p>&#xA;&#xA;<p><a href=""https://github.com/scalecube/scalecube"" rel=""nofollow noreferrer"">https://github.com/scalecube/scalecube</a></p>&#xA;&#xA;<p>you can find references:&#xA;<a href=""https://github.com/scalecube/scalecube/wiki/Distributed-Computing-Research"" rel=""nofollow noreferrer"">https://github.com/scalecube/scalecube/wiki/Distributed-Computing-Research</a></p>&#xA;&#xA;<p>in general when new node joining to the network it joins one of the already running cluster nodes (seeds or members) and the cluster gossip about the new member and creating a ""cluster"" the gossip protocol ""infects"" the cluster with the membership information.</p>&#xA;&#xA;<p>usually there is a set of nodes that serve as the entry point to the cluster called seeds and they can be a well known members or discovered using diffident methods such as dns name so when new member join the cluster it can look for a host name ""seed"" and that is resolved to its current or one of the seed ips.</p>&#xA;&#xA;<p>in microservices architecture seeds can also be the api-gateways or specific nodes that act as seeds usually its best to choose the seeds as the members that least subject to changes and upgrades.</p>&#xA;&#xA;<p>I have written a post discussing the topic&#xA;<a href=""https://www.linkedin.com/pulse/swim-cluster-membership-protocol-ronen-nachmias/"" rel=""nofollow noreferrer"">https://www.linkedin.com/pulse/swim-cluster-membership-protocol-ronen-nachmias/</a></p>&#xA;"
49445282,49443206,243991,2018-03-23T08:27:36,"<p>OSGi and microservices share the same architectural style but differ in their <em>granularity</em>. We actually used to call OSGi services microservices until the web stole that name. We now sometimes call them <em>nanoservices</em>. </p>&#xA;&#xA;<p>The principle of (micro|nano)services is to tunnel the communications between modules through a <em>gate</em> with a well defined <em>API</em>. Since an API is, or at least should be, independent of the implementations you can change one module without affecting the other modules. One of the most important benefits is that designs of even large systems can remain understandable when looking at the service diagram. In a way, a service based design captures the essence of the system, leaving the details for the modules.</p>&#xA;&#xA;<p>With web/micro services the <em>gate</em> is a communication endpoint (host:port for example) and protocol (REST for example). The API is defined informally or with something like Swagger/OpenAPI or SOAP.</p>&#xA;&#xA;<p>OSGi defines a (nano) service as an object that is made available to other modules (bundles) to use. Java is used to define the API.</p>&#xA;&#xA;<p>Since nanoservices are OSGi most important design primitive there is a lot of support to make them easy to work with. Interestingly, since the service registry is dynamic and reflective, it is quite straightforward to map a nanoservice to a microservice and vice versa. The OSGi Alliance standardized this in their model for <em>distributed OSGi</em>, the ""Remote Service Admin"". This specification allows you to take an OSGi nanoservice and map it to REST, SOAP, or other protocols.</p>&#xA;&#xA;<p>Therefore, choosing OSGi allows you not only to defer the decision to support microservices, it also allows you to add microservices to your system afterwards. Having a unified architectural style for the most basic functions as well as the highest level functions makes systems easier to understand as well scale.</p>&#xA;"
43704015,43700905,2801687,2017-04-30T07:28:31,"<p>You can use your local network to discover services, via Dhcp and whatnot. But that requires that all services are already ""registered"" within that DNS server.</p>&#xA;&#xA;<p>Microservices can find each other via service discovery, server or client side. If you choose client side service discovery, you can use tools like <a href=""http://consul.io"" rel=""nofollow noreferrer"">Consul</a>, which provides a bunch of great features. One of which is a DNS endpoint which allows queries via <code>SRV</code> records with <code>&lt;serviceName&gt;.consul.service</code> domain names.</p>&#xA;&#xA;<p>Consul has it's own DNS endpoint, you can configure your services to use that (usually on port 8600 locally, as Consul agents run locally).</p>&#xA;&#xA;<p>But you can also configure an actual DNS server to forward questions to Consul, so that you can easily mix service discovery drive by Consul with manually setup services within a <code>Bind</code> instance or similar...</p>&#xA;"
31142723,26975640,836887,2015-06-30T15:41:46,"<p>This is a huge question and probably can't be answered effectively in SO's Q&amp;A format. </p>&#xA;&#xA;<p>It depends what you are doing with it. </p>&#xA;&#xA;<p>If you are building a single product which consists of lots of small pieces of function that can be thought of as being independent then microservices maybe the way to go. </p>&#xA;&#xA;<p>If you are a large enterprise organisation where IT is not the main consideration of the board of directors as a competitive advantage and you work in a heavily regulate industry where new standards have to be applied across globally distributed projects with their own IT departments, some from new acquisitions, where you can't centrally control all the endpoints and applications within your organisation, then maybe you need an ESB. </p>&#xA;&#xA;<p>I don't want to be accused of trying to list <strong>ALL</strong> the advantages of both approaches here as they wouldn't be complete and may be out of date quickly. </p>&#xA;&#xA;<p>Having said that, in an effort to be useful to the OP:</p>&#xA;&#xA;<p>If you look up how Spotify and Netflix do microservices you can find many things they like about the approach, including but not limited to: ease of blue/green deployment of individual services, decoupled team structures, and isolation of failures. </p>&#xA;&#xA;<p>ESBs allow you to centrally administer and enforce policies, like legal requirements, audit everything in one place rather than hoping each team got the memo about logging everything, provide global statistics about load and uptime, as well as many other things. ESBs grew out of large enterprises where the driver was not customer response time on a website and speed of innovation (amongst other things) but Service Level Agreements, cost effectiveness and regulations (amongst other things).</p>&#xA;&#xA;<p>There is a lot of value in both approaches. Microservices are being written about a lot at the moment, just as ESBs were 10-15 years ago. Maybe that's a progression, maybe it's just a change, maybe it's just that consumer product companies need to market themselves and large enterprises like to keep details private. We may find out in another 10 years. For now, it depends heavily on what you are doing. As with most things in programming, I'd start out simple and only move to the more complex solution if you need to. </p>&#xA;"
51298287,51280734,2029079,2018-07-12T06:01:32,"<p>The region ""ap-south-1a"" is the incorrect region. It should be ""ap-south-1"" in my opinion. You can check the config file in .aws dir to see if region is correctly set. Also check if the environment variable AWS_DEFAULT_REGION is correct if set.</p>&#xA;&#xA;<p><a href=""https://docs.aws.amazon.com/general/latest/gr/rande.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/general/latest/gr/rande.html</a></p>&#xA;"
46818262,46813736,983064,2017-10-18T19:59:57,"<p>The whole point of microservices is about having small, independent services that are decoupled as much as possible.</p>&#xA;&#xA;<p>Sharing a common database introduces very strong coupling, and is not recommended.</p>&#xA;&#xA;<p>If two services need the same data, you could either (a) have a different database for each, and replicate the data, or (b) introduce a third service that is responsible for access to the database.</p>&#xA;&#xA;<p>If you're looking for a bigger-scale distributed transaction across microservices, then you should look into things like sagas. Typically you'll have a coordinator (""process manager"" in some literature) that tracks the various operations, and can compensate or cancel actions that have been performed if the transaction as a whole is bound to fail.</p>&#xA;&#xA;<blockquote>&#xA;  <p>3- SQL with microservices it's maybe too much slow?</p>&#xA;</blockquote>&#xA;&#xA;<p>What makes you think so?</p>&#xA;&#xA;<p>There is nothing about SQL that makes it inadequate for microservices. Microservices may vary wildly in terms of what they do and what they require. SQL will be perfectly suitable for some microservices, and possibly not so suitable for others. It depends on the service.</p>&#xA;"
48488845,48487563,7554328,2018-01-28T16:52:24,"<p>If you choose to automatically updated dependencies, I'd recommend going latest-1 to avoid the risk of any undiscovered bugs. Unless you are 100% confident in your automated tests. </p>&#xA;&#xA;<p>You can configure the maven versions plugin to do this, and could get it to re-commit back to your repo. </p>&#xA;&#xA;<p><a href=""http://www.mojohaus.org/versions-maven-plugin/examples/advancing-dependency-versions.html"" rel=""nofollow noreferrer"">http://www.mojohaus.org/versions-maven-plugin/examples/advancing-dependency-versions.html</a></p>&#xA;&#xA;<p><code>versions:use-latest-releases</code></p>&#xA;&#xA;<p>You can configure it to ignore, major/minor/patch versions. </p>&#xA;&#xA;<p><a href=""http://www.mojohaus.org/versions-maven-plugin/use-latest-releases-mojo.html"" rel=""nofollow noreferrer"">http://www.mojohaus.org/versions-maven-plugin/use-latest-releases-mojo.html</a></p>&#xA;&#xA;<p>E.g. if you didn't want to automatically advance to Spring 5.0/Spring Boot 2.0 <code>versions:use-latest-releases -DallowMajorUpdates=false</code> and/or include/exclude depenedencies you want to handle manually. </p>&#xA;"
51734663,51734002,7554328,2018-08-07T20:09:00,"<p>I wouldn't recommend doing this, or assuming that your non-web facing apps are completely secure. Realistically you should be re-validating the bearer token. </p>&#xA;&#xA;<p>What you need is a zuul filter to add a header to the request. This is mostly from memory and you could update the filter to check if it should filter or not, that the request doesn't already contain an expected header etc.</p>&#xA;&#xA;<pre><code>@Component&#xA;public class AddUserHeader extends ZuulFilter {&#xA;&#xA;    private static final Logger LOG = LoggerFactory.getLogger(AddUserHeader.class);&#xA;&#xA;    @Override&#xA;    public String filterType() {&#xA;        return ""pre"";&#xA;    }&#xA;&#xA;    @Override&#xA;    public int filterOrder() {&#xA;        return 0; &#xA;    }&#xA;&#xA;    @Override&#xA;    public boolean shouldFilter{&#xA;    return true;&#xA;    }&#xA;&#xA;    @Override&#xA;    public Object run() {&#xA;&#xA;       RequestContext.getCurrentContext().addZuulRequestHeader(""MYUSER"", SecurityContextHolder.getAuthentication().getPrincipal().getName());&#xA;       return null;&#xA;    }&#xA;</code></pre>&#xA;"
36646154,36645517,2315544,2016-04-15T11:39:27,"<p>I would avoid using 2x 'draws'.</p>&#xA;&#xA;<p>Here is a possible way to design the URI structure.&#xA;Please note, that segments should be lower case (so don't use 'Draws')</p>&#xA;&#xA;<pre><code>&lt;scheme&gt;://&lt;host&gt;[:&lt;port&gt;]/&lt;api-path&gt;/&lt;api-name&gt;/&lt;api-version&gt;/&lt;resource-path&gt;&#xA;</code></pre>&#xA;&#xA;<ul>&#xA;<li><strong>scheme</strong> (eg. http) </li>&#xA;<li><strong>host</strong> is a fully qualified hostname, a DNS alias hiding the equipment and physical location of the API implementation. It contains information about the environment when it is a non-productive environment (test, int).</li>&#xA;<li><strong>port</strong> should be the default http port (80) and thus can be omitted. Other ports can be used for non-productive environments.</li>&#xA;<li><strong>api</strong> <strong>path</strong> separates the REST API from other resources served by the server (e.g. a webapp). It is usually of the form /api. For servers serving only REST apis, it can be omitted.</li>&#xA;<li><strong>api name</strong> gathers a set of resources in a kind of package. This is the unit of publishing and versioning.</li>&#xA;<li><strong>api version</strong> is the version of the API. It has the form v[major-version-number]</li>&#xA;<li><strong>resource</strong> <strong>path</strong> is composed of the resource URI segments</li>&#xA;</ul>&#xA;"
51489029,51371678,3440152,2018-07-24T00:56:54,"<p>I found an answer that will work for our situation, using Aurora with MySQL compatibility. During my research I came across the excellent source of information at <a href=""http://microservices.io"" rel=""nofollow noreferrer"">microservices.io</a>. Specifically, the page about the <a href=""http://microservices.io/patterns/data/event-driven-architecture.html"" rel=""nofollow noreferrer"">event-driven architecture pattern</a> refers to four related patterns to guarantee atomicity of updating state and publishing events. </p>&#xA;&#xA;<ul>&#xA;<li>Event sourcing</li>&#xA;<li>Application events</li>&#xA;<li>Database triggers</li>&#xA;<li>Transaction log tailing</li>&#xA;</ul>&#xA;&#xA;<p>Event sourcing is out of the question because it is way too complex for what we want to achieve. I already argued against tx log tailing in my original question. Application events and DB triggers are very similar in that, as part of a transaction, the state is updated as well as an entry is written to an Events table: Tx successfully commits, state is persisted and Event entry shows up in that table. Tx rolls back and state is unchanged and no Event entry shows up. The only difference between the two is whether Event entries get generated by the application/service logic itself, or by DB triggers.</p>&#xA;&#xA;<p>Then an external process polls this table and publishes events for other micro-services based on the Event entries (and deletes the published ones afterwards of course). These two patterns guarantee that a state change always results in at least one event (exactly once would be a bit more complex to achieve).</p>&#xA;&#xA;<p>Now about how to implement this... my first idea was to use a Fargate container with a Node app that does the polling, thinking that I would stay serverless with this solution. However, that turned out not to be quite true: In order to guarantee order of events, there should be just one container polling and publishing. A single Fargate container is assigned to one availability zone, and if that zone ""goes away"", so does the container. Now I would have to build some kind of monitoring on top so that a new container #2 gets instantiated in a different AZ #2 if and when needed. But what if AZ #1 and container #1 come back? Then there would be two instances. This is getting way too complex.</p>&#xA;&#xA;<p>For now I settled on the following approach: A CloudWatch event triggers a polling Lambda function once a  minute (min. interval for CW). Once called, the function keeps polling the DB until a second Lambda function call takes over one minute later. In order for the two Lambda function calls to coordinate, I created a second table in my DB, Event Polling State, where the most recent Lambda function call updates a dedicated row in that table, indicating to the previous function call that it started (this is done with help of SELECT ... FOR UPDATE and TXs to prevent race conditions). Before each polling cycle, the function checks the row in Event Polling State if no other function has started in the meantime.</p>&#xA;&#xA;<p>Advantages of this approach (as I see them):</p>&#xA;&#xA;<ul>&#xA;<li>Truly serverless and AZ-, VPC, subnet-unaware.</li>&#xA;<li>Order of events is guaranteed because never more than one Lambda call polls and publishes in parallel.</li>&#xA;<li>If the polling Lambda function is terminated for whatever reason (e.g. because an AZ goes away), the gap in publishing is at most 1 min, until CloudWatch calls the Lambda function the next time. This gap is acceptable.</li>&#xA;</ul>&#xA;"
38603412,38507565,1425432,2016-07-27T03:51:41,"<p>I had the same issue. After spending hours, finally fixed the error by adding following in MicroserviceSecurityConfiguration.java:</p>&#xA;&#xA;<p>Inject org.springframework.cloud.client.discovery.DiscoveryClient</p>&#xA;&#xA;<pre><code> @Inject&#xA;private DiscoveryClient discoveryClient;&#xA;</code></pre>&#xA;&#xA;<p>In any method of MicroserviceSecurityConfiguration.java (I choose getKeyFromAuthorizationServer method), add following:</p>&#xA;&#xA;<pre><code>discoveryClient.getServices();&#xA;</code></pre>&#xA;"
46717306,46717204,4325657,2017-10-12T19:18:16,"<p>""IF""  the sequential data you mention just divided by id, 1 2 and 3,&#xA;Then Would be the best you make 3 background services as an consumer, just need 1 partition for the case (you can decided this on your own)</p>&#xA;&#xA;<p>Then make 3 topic based on the data&#xA;ex : &#xA;TOPIC 1&#xA;TOPIC 2&#xA;TOPIC 3</p>&#xA;&#xA;<p>which mean you need to make 3 kind of consumer, each of consumer would be listen to only 1 topic</p>&#xA;&#xA;<p>Then you would be spawn new process / Thread for every new stream data,&#xA;It would work in parallel</p>&#xA;"
37067693,36920620,6295321,2016-05-06T08:27:46,"<p>Based on our experience in software architecture (> 20 years), there should be 2 distinct APIs:</p>&#xA;&#xA;<ul>&#xA;<li><strong>API for Meta-data</strong> &mdash; API will provide information about images and links to images.</li>&#xA;<li><strong>API for Image Download</strong> &mdash; Separate API for downloading images. It can also support bulk download (as archive) considering network latency. This can leverage in-memory database like Redis DB or similar to provide quicker response time.</li>&#xA;</ul>&#xA;"
37067841,36988921,6295321,2016-05-06T08:35:51,"<p>Our suggestion is to use PUB/SUB model with RabbitMQ so that multiple subscribers can listen to single queue and act accordingly.&#xA;See: <a href=""https://www.rabbitmq.com/tutorials/tutorial-three-python.html"" rel=""nofollow"">https://www.rabbitmq.com/tutorials/tutorial-three-python.html</a></p>&#xA;"
47269228,47269095,4592665,2017-11-13T16:36:49,"<p>With regards to using FeignBuilder, maybe you can find an answer to your problem <a href=""http://cloud.spring.io/spring-cloud-static/Camden.SR6/#_creating_feign_clients_manually"" rel=""nofollow noreferrer"">here</a> <p>&#xA;If this doesn't work, I would suggest creating a wrapper class around the interface. <br>&#xA;When I had to deal with such a problem in the past I simply created a class that uses the Feign client interface. Spring will handle the wiring for you based on the fact that you annotated your feign client with the @FeignClient annotation. This way you can modify the behavior from the wrapper class. You'd write your fallback logic in a method and call the feign client as needed.<p>&#xA;Hope this helps</p>&#xA;"
50663842,50571546,3620633,2018-06-03T06:29:17,"<p>Lagom does not provide out-of-the-box support for Dgraph. If you have to use Lagom's Read-Side processor with Dgraph, then you have to use Lagom's <a href=""https://www.lagomframework.com/documentation/1.4.x/java/ReadSide.html#Query-the-Read-Side-Database"" rel=""nofollow noreferrer"">Generic Read Side support</a>. Like this:</p>&#xA;&#xA;<pre><code>/**&#xA; * Read side processor for Dgraph.&#xA; */&#xA;public class FriendEventProcessor extends ReadSideProcessor&lt;FriendEvent&gt; {&#xA;    private static void createModel() {&#xA;        //TODO: Initialize schema in Dgraph&#xA;    }&#xA;&#xA;    @Override&#xA;    public ReadSideProcessor.ReadSideHandler&lt;FriendEvent&gt; buildHandler() {&#xA;        return new ReadSideHandler&lt;FriendEvent&gt;() {&#xA;            private final Done doneInstance = Done.getInstance();&#xA;&#xA;            @Override&#xA;            public CompletionStage&lt;Done&gt; globalPrepare() {&#xA;                createModel();&#xA;                return CompletableFuture.completedFuture(doneInstance);&#xA;            }&#xA;&#xA;            @Override&#xA;            public CompletionStage&lt;Offset&gt; prepare(final AggregateEventTag&lt;FriendEvent&gt; tag) {&#xA;                return CompletableFuture.completedFuture(Offset.NONE);&#xA;            }&#xA;&#xA;            @Override&#xA;            public Flow&lt;Pair&lt;FriendEvent, Offset&gt;, Done, ?&gt; handle() {&#xA;                return Flow.&lt;Pair&lt;FriendEvent, Offset&gt;&gt;create()&#xA;                        .mapAsync(1, eventAndOffset -&gt; {&#xA;                                    if (eventAndOffset.first() instanceof FriendCreated) {&#xA;                                        //TODO: Add Friend in Dgraph;&#xA;                                    }&#xA;&#xA;                                    return CompletableFuture.completedFuture(doneInstance);&#xA;                                }&#xA;                        );&#xA;            }&#xA;        };&#xA;    }&#xA;&#xA;    @Override&#xA;    public PSequence&lt;AggregateEventTag&lt;FriendEvent&gt;&gt; aggregateTags() {&#xA;        return FriendEvent.TAG.allTags();&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>For <code>FriendEvent.TAG.allTags()</code>, you have to add following code in <code>FriendEvent</code> interface:</p>&#xA;&#xA;<pre><code>int NUM_SHARDS = 20;&#xA;&#xA;  AggregateEventShards&lt;FriendEvent&gt; TAG =&#xA;          AggregateEventTag.sharded(FriendEvent.class, NUM_SHARDS);&#xA;&#xA;  @Override&#xA;  default AggregateEventShards&lt;FriendEvent&gt; aggregateTag() {&#xA;    return TAG;&#xA;  }&#xA;</code></pre>&#xA;&#xA;<p>I hope this helps!</p>&#xA;"
50159587,50159354,4934937,2018-05-03T16:12:12,"<p>I'd say it depends on the technology you use, but still I'd rather say no. </p>&#xA;&#xA;<p>Assuming that the HTML part is your frontend, you want to deliver the frontend either per microservice (meaning one frontend per microservice) or as a own microservice. </p>&#xA;&#xA;<p>But it's hard to say anything absolute - it always depends on the application you're building.</p>&#xA;"
46097706,46094734,85785,2017-09-07T13:30:18,"<p>ServiceStacks recommendation is <a href=""https://stackoverflow.com/questions/12400071/servicestack-restful-resource-versioning/12413091#12413091"">evolve Services gracefully and not try to maintain multiple implementations</a> which causes undue friction in static type language.</p>&#xA;"
47079513,47049241,6380313,2017-11-02T15:53:28,"<p>Your're not the first developer with a monitoring problem! Use something already in existence like:</p>&#xA;&#xA;<ul>&#xA;<li><p>Implement a health/ready enpoint in your service</p></li>&#xA;<li><p>use <a href=""https://github.com/influxdata/telegraf"" rel=""nofollow noreferrer"">Telegraf</a> to ping that service and pump the results to a metrics storage (for example InfluxDB, or <a href=""https://github.com/influxdata/telegraf#output-plugins"" rel=""nofollow noreferrer"">anything else that is supported</a>)</p></li>&#xA;<li>then deploy a frontend like <a href=""https://grafana.com/"" rel=""nofollow noreferrer"">grafana</a> to display the aggregated data...</li>&#xA;</ul>&#xA;&#xA;<p>Bottom line - do yourself a favour and stick with something established and do not write something yourself that others have already solved for you! I've used all of these components and they saved me tonns of time and are thousands of times more powerfull that anything I could have produced myself!</p>&#xA;"
38619869,38615132,6380313,2016-07-27T17:44:49,"<p>I like what you're trying to achieve! <strong>A service is not production-ready unless it's thoroughly monitored.</strong></p>&#xA;&#xA;<p>I believe what your're describing goes into the topics of <strong>health-checking</strong> and <strong>metrics</strong>.</p>&#xA;&#xA;<blockquote>&#xA;  <p>... I would be able to say for sure, if services are usable to end-customer.</p>&#xA;</blockquote>&#xA;&#xA;<p>That however will require a little of both ;-) To ensure you're currently fulfilling your SLA, you have to make sure, that your services are all a) running and b) perform as requested. With both problems I suggest to look at the <a href=""https://github.com/etsy/statsd"" rel=""nofollow"">StatsD</a> toolchain. Initially developed by Etsy, it has become the de-facto standard for gathering metrics.</p>&#xA;&#xA;<p>To ensure all your services are running, we're relaying Kubernetes. It takes our description for what should run, be reachable from outside etc. and hosts that on our infrastructure. It also makes sure, that should things die - that they will be restarted. It helps with things like auto-scaling etc. as well! Awesome tooling and kudos to Google!&#xA;The way it ensures that is with <strong>health-checks</strong>. There are multiple ways how you can ensure your service node booted by Kubernetes is alive and kicking (namely HTTP calls and CLI scripts but this should be a modular thing should you need anything else!) If Kubernetes detects unhealthy nodes it will immediately phase them out and start another node instead.</p>&#xA;&#xA;<p>Now, making sure, all your services perform as expected you'll need to gather some <strong>metrics</strong>. For all of our services (and all individual endpoints), we gather a few metrics via StatsD like:</p>&#xA;&#xA;<ul>&#xA;<li>Requests/sec</li>&#xA;<li>number of errors returned (404, etc...)</li>&#xA;<li>Response times (Average, Median, Percentiles depending on the services SLA)</li>&#xA;<li>Payload size (Average)</li>&#xA;<li>sometimes the number of concurrent requests per endpoint, the number of instances currently running</li>&#xA;<li>general metrics like the hosts current CPU and memory usage and uptime.</li>&#xA;</ul>&#xA;&#xA;<p>We gather a lot more metrics but that's about the bottom line. Since StatsD has become more of a ""protocol specification"" than a concrete product there are a myriad of collector, front- and backends to choose from. They help you visualize your systems state and many of them feature alerts of something or some combination of metrics go beyond their thresholds.</p>&#xA;&#xA;<p>Let me know, if this was helpfull!</p>&#xA;"
38560767,38554037,6380313,2016-07-25T06:09:27,"<blockquote>&#xA;  <p>... there are calls to the socket server that needs to check that validity of that ID, thus, needs to read from DB. This two services will have to share the model of A in order to map it to an object, ...</p>&#xA;</blockquote>&#xA;&#xA;<p>Well - no, they do not need to share code! The only thing they actually need, is a common understanding of the schema of the DB (I assume you're using MongoDB). Whether that understanding is coming from shared class definitions in shared libraries or from duplicated class definitions in separate libraries does not really matter. Many developers will now start screaming at me for violating the DRY (Don't repeat yourself) principle, but with microservices many things are different to what we're used to!</p>&#xA;&#xA;<p>In her answer, Priti Singh states that:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Two microservices should not share same data model</p>&#xA;</blockquote>&#xA;&#xA;<p>which in correct in a microservice context and considered good practice! Let me show you why:</p>&#xA;&#xA;<p>In the microservice pattern, services should be independant of other sevices and have well-defined interfaces. Having two different services reading the same DB makes that DB another ""service"" (I KNOW, weird, right?!?). By definition this database now needs a well-defined interface - which is kind of difficult in a schemaless DB ;-) Another reason not to make a DB act like a service is, that changes in one service will always have some influence on the other service accessing the data. This means, changing the ""schema"" on one service might force you to change another service as well, just to keep your system running! That's a headache when you consider a full-blown microservice system with >100 services.</p>&#xA;&#xA;<p>That is why your second idea:</p>&#xA;&#xA;<blockquote>&#xA;  <p>... or should I make only service1 able to read from DB and then make the second one talks to service 1?</p>&#xA;</blockquote>&#xA;&#xA;<p>is much bettern. Keep your database hidden behind a service with a well-defined interface that can be versioned. Like this you can refactor inside service 1 as your heart desires without necessarily influencing other services. Once you need to make a breaking change on your interface - give it a new version and start migrating other services to use the new interface.</p>&#xA;&#xA;<p>The underlying controversy in your question is the one of <strong>Coupling vs. Duplication</strong>. Sharing an interface definition and database is coupling (the wakes-you-up-at-night-after-a-small-change kind of coupling) but duplicating the database schema to both services is duplication. I believe that coupling will kill you long before duplication does, but taking your second approach, having the http-service access the socks-service which then accesses the database should remove duplication as well as reduce coupling!</p>&#xA;&#xA;<p>Let me know if this was helpfull!</p>&#xA;"
38326772,38297333,6380313,2016-07-12T10:51:59,"<p>After the heated debate on the first answer, let me lend some perspective:</p>&#xA;&#xA;<p>One use case that often comes up is how to handle for example authentication information after the request hit the first service which then in turn calls other services. Now the question usually is: do I hand over the authentication-information (like usernames and groups etc.) or do I just hand over the token, that the client sent and let the next service query the authentication information again.</p>&#xA;&#xA;<p>As far as I can tell, the microservice community has not yet agreed upon an ""idiomatic"" way of solving this problem. I think there is a good reason for that and it lays in the different requirements that various application pose in this subject. Sometimes authentication can only be necessary at the first service that gets hit with an external request - then don't bother putting too much work into authentication. Still most systems I know have higher demands and thus require another level of sophistication on the subject of authentication.</p>&#xA;&#xA;<p>Let me give you my view of how this problem could be solved: The easiest way is to hand around the access-token the client has sent between the back-end services. Yes - this approach requires every service to re-inquire the user-information every time it gets hit with a request. If (and I hoe this does not happen in this amount in your system) there are 25 cross-service calls per request - this most likely means 25 hits on some kind of authentication service. Most people will now start screaming in terror of this horrible duplication - but let's think the other way: If the same system were a well-structured monolith you'd still make these calls (probably hit a DB every single time) at different places in your process. The big deal about these calls in a microservice architecture is the network overhead, and it's true - it will kill you if done wrong! I will give you the solution we took and that worked well under heavy loads for us:</p>&#xA;&#xA;<p>We developed a token service (which we'll be open-sourcing quite soon). This service does nothing else except store a combination of the token, it's expiration date and some schema-less JSON content. It has a very simple REST interface that lets you create, invalidate, extend and read tokens and their content. This service has multiple back-ends that can be configured according to the environment it run in. For development purposes it has a simple in-memory storage that is not synchronized, persisted or replicated in any way. For production environment we wrote a back-end that synchronizes these tokens between multiple instances (including all the stuff like quorums, asynchronous persistence etc.). This back-end enables us to scale this service very well; which is a premise for the solution I'm proposing: If every service nodes has to get the information associated with a token every time it receives a request - the service that provides it has to be really fast! Our implementation return tokens and their information in far less than 5 milliseconds and we're confident we can push this metric down even further.</p>&#xA;&#xA;<p>The other strategy we have is to orchestrate services that make heavier queries to the token-service (receiving the content is expensive compared to just checking a tokens validity/existence) so that their located on the same physical nodes or close by to keep network latency to a minimum.</p>&#xA;&#xA;<p>What is the more general message: Do not be afraid of cross-service calls as long as the number or these calls stay uncoupled from the amount of content that is handled (bad example <a href=""https://stackoverflow.com/q/38172510/6380313"">here</a>). The services that are called more frequently need to be engineered much more carefully and their performance needs to be very optimized to have off the last possible millisecond. DB-Hits in this kind of system-critical services for example are and absolute Nogo - but there are design patterns and architectures that can help you avoid them!!</p>&#xA;&#xA;<p>You may have detected already that I did not directly answer your question to debate. Why? I'm vehemently against having shared databases between services. Even if these data-bases are schema-less you will couple two services together without this dependency being visible. Once you decide to restructure your data in a token-service and there is another service even just reading on this database - you just screwed up two services an you might just realize it when it's too late because the dependency is not transparent. State/Data in services should only be accessed through well-defined interfaces so they can be well abstracted, developed and tested independently. In my opinion, changing the persistence technology or structure in one service should never screw-up or even require changes in another service. Exclusively accessing a service through it's API gives you the possibility to refactor, rebuild or even completely rewrite services without necessarily breaking other services relying on it. It's called decoupling!</p>&#xA;&#xA;<p>Let me know whether this is helpful or not!</p>&#xA;"
38457717,38453830,6380313,2016-07-19T11:49:26,"<p>Well your approach is not that bad!</p>&#xA;&#xA;<p>Some of the HTTP status codes are reserved for cases of malformed requests etc. but in your case, you ask Service2 to return information for a token! If that token exists, you specified correctly, that Service2 has to return <code>200 OK</code>. Now you just need to specify what happens if the token is not valid anymore or if it does not exists (or treat both cases the same...). If you specify, that Service2 has to return <code>404 Not found</code> if it does not know the token or that the token expired, there (in most cases) is no need for Service1 to go any further! Parsing the status code is cheap in almost any language/environment, but forcing the deserialization of the content in both success and error cases is in comparison very expensive. Authentication needs to be fast - so I'd go for the status code here!</p>&#xA;&#xA;<p>The key is, that this behavior has to be specified somewhere! (We went for swagger definitions!)</p>&#xA;"
38993386,38966184,6380313,2016-08-17T09:55:42,"<blockquote>&#xA;  <p>New user order request -> Rest API -> calls order service to create order -> returns the response. Then order service takes of things from there on asynchronously?</p>&#xA;</blockquote>&#xA;&#xA;<p>Doing something asynchronously that do not have to be processed synchronously is always a good idea! However it's not always possible. If for example you have to return a billing id along with the response from the order service - that has to be a synchronous usecase.</p>&#xA;&#xA;<blockquote>&#xA;  <p>New user order request -> Rest API -> calls order service to create order -> (if successful) Rest API -> (if successful) calls the billing service</p>&#xA;</blockquote>&#xA;&#xA;<p>Your thing you call <code>Rest API</code> in this description is actually a business service (Create order, then - if successfull - create billing information - is business logic). Your gateways should be very stupid and I advise you NOT to write them yourself! <a href=""http://microservices.io/patterns/apigateway.html"" rel=""nofollow"">There is plenty of gateway solution out there</a>!</p>&#xA;&#xA;<p>Assuming the gateway now talks to the order (and only the order) service, you need some way of letting the billing service know, that it needs to create billing information for this order! I assume from your question that this does not necessarily need to happen synchronously, so I agree with sean-farmar on this: Use some kind of asynchronous communication to let the billing service know you created a new order. Also consider the billing service to send a message as well, once it finished creating the billing information, so the order service can take this notification and add the ID of the bill to the order. And like this you have completely asynchronously:</p>&#xA;&#xA;<ul>&#xA;<li>created billing information</li>&#xA;<li>linked that information to the order and</li>&#xA;<li>linked the order to the billing information.</li>&#xA;</ul>&#xA;&#xA;<p>Since this happens asynchronously and your messages could be buffered somehow, one of both services can die without taking the other one down. And once they come to live again - they just need to process outstanding messages!</p>&#xA;&#xA;<p>By the way: Updating just means killing and starting a new version of your service - which means you now have a secure way to update your services without impacting the up-time of your system or your SLA!</p>&#xA;&#xA;<p>Let me know if that was helpful!</p>&#xA;"
39073680,39056199,6380313,2016-08-22T07:33:11,"<blockquote>&#xA;  <p>Assuming the above 2 are true, then it stands to reason to me that I would use Elixir 90% of the time [...]</p>&#xA;</blockquote>&#xA;&#xA;<p>Be careful when making these kinds of statements! They tempt you into choosing the thing you always choose when setting up a new service, when actually you should be thinking about what that service is supposed to do and what languages and frameworks help you get there best! That said: your two premises are true! A DB hit is the most expensive operation and concurrency is a vital tool when handling larger loads. They are true but not complete: There are other conditions you might need to think about like resource consumption, scheduling behavior of your platform etc.</p>&#xA;&#xA;<p>On the count of languages: Managed languages (like for example everything based on the JVM or .NET runtime) always imply a certain static overhead because their need to do garbage collection, or their need to compile code on the go, dynamic type deduction at runtime, reflection etc. This means, that they will need more memory and CPU cycles from your machines than other languages like C++, GO, Rust and the likes.</p>&#xA;&#xA;<p>While you have to do memory management yourself in languages like C++, languages like GO, D and Rust attempt to provide a middle ground  towards fully managed languages/runtimes like JVM or .NET.</p>&#xA;&#xA;<p>What matters at least as much as your choice in languages/runtimes is your architecture. Everything involving classic databases will probably give you troubles on the scaling side of things, Everything hitting a disk is going to kill you under load!</p>&#xA;&#xA;<p>So what's the my suggestion? Keep all the variables in mind (Request latency is not the only metric! Resource consumption can be a killer too!), choose the best language and toolchains for whatever purpose your service has to fullfill and validate different architectures!</p>&#xA;"
38968189,38964840,6380313,2016-08-16T06:43:12,"<p>It's not particularly a microservice pattern I would suggest you, but it fits perfectly into microservices and it's called <strong>Event sourcing</strong></p>&#xA;&#xA;<p>Event sourcing describes an architectural pattern in which events are generated by different sources. An event will now trigger 0 or more so called <strong>Projections</strong> which then use the data contained in the event to aggregate information in the form it is needed.</p>&#xA;&#xA;<p>This is directly applicable to your problem: Whenever the organisation service changes it's internal state (Added / removed / updated an organization) it can fire an event. If an organization is added, it will for example aggregate the contacts to this organization and store this aggregate. The search for it is now trivial: Lookup the organizations id in the aggregated information (this can be indexed) and get back the contacts associated with this organization. Of course the same works if contracts are added to the contract service: It just fires a message with the contract creation information and the corresponding projections now alter different aggregates that can again be indexed and searched quickly.</p>&#xA;&#xA;<p>You can have multiple projections responding to a single event - which enables you to aggregate information in many different forms - exactly the way you'd like to query it later. Don't be afraid of duplicated data: event sourcing takes this trade-off intentionally and since this is not the data your business-services rely on and you do not need to alter it manually - this duplication will not hurt you.</p>&#xA;&#xA;<p>If you store the events in the chronological order they happened (which I seriously advise you to do!) You can 'replay' these events over and over again. This helps for example if a projection was buggy and has to be fixed!</p>&#xA;&#xA;<p>If your're interested I suggest you read up on event sourcing and look for some kind of event store:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""http://martinfowler.com/eaaDev/EventSourcing.html"" rel=""nofollow"">event sourcing</a></li>&#xA;<li><a href=""https://geteventstore.com"" rel=""nofollow"">event store</a></li>&#xA;</ul>&#xA;&#xA;<p>We use event sourcing to aggregate an array of different searches in our system and we aggregate millons of records every day into mongodb. All projections have their own collection create their own indexes and until now we never had to resort to different systems / patterns like elastic search or the likes!</p>&#xA;&#xA;<p>Let me know if this helped!</p>&#xA;&#xA;<p><strong>Amendment</strong></p>&#xA;&#xA;<blockquote>&#xA;  <p>use the data contained in the event to aggregate information in the form it is needed</p>&#xA;</blockquote>&#xA;&#xA;<p>An event should contain all the information necessary to aggregate more information. For example if you have an organization creation event, you need to at least provide some information on what the organizations name is, an ID of some kind, creation date, parent organizations ID etc. As a rule of thumb, we send all the information we gather in the service that gets the request (don't take it directly form the request ;-) check it first, then write it to the event and send it off) because we do not know what we're gonna need in the future. Just stay cautious - payloads should not get too large!</p>&#xA;&#xA;<p>We can now have multiple projections responding to this event: One that adds the organizations to it's parents aggregate (to get an easy lookup for all children of a given organization), one that just adds it to the search set of all organizations and maybe a third that aggregates all the parents of a given child organization so the lookup for the parent organizations is easy and fast.</p>&#xA;&#xA;<p>We have the same service process these events that also process client requests. The motivation behind it is, that the schema of the data that your projections create is tightly coupled to the way it is read by the service that the client interacts with. This does not have to be that way and it could be separated into two services - but you create an almost invisible dependency there and releasing these two services independently becomes even more challenging. But if you do not mind that additional level of complexity - you can separate the two.</p>&#xA;&#xA;<p>We're currently also considering writing a generic service for aggregating information from events for things like searches, where projections could be scripted. That only makes the <em>invisible dependencies problem</em> less conspicuous, it does not solve it.</p>&#xA;"
39139869,39139313,6380313,2016-08-25T08:06:58,"<p>I believe the thing you're looking for is a <a href=""http://martinfowler.com/articles/schemaless/"" rel=""nofollow"">schemaless database</a>:</p>&#xA;&#xA;<p>On the concrete products out there - choose an established one that suits your needs. Some examples:</p>&#xA;&#xA;<ul>&#xA;<li>MongoDB</li>&#xA;<li>CouchDB</li>&#xA;<li>(many others available - just google for it!)</li>&#xA;</ul>&#xA;&#xA;<p><strong>Possible solution for your example:</strong></p>&#xA;&#xA;<p>The json you may want to store could look like this:</p>&#xA;&#xA;<pre><code>{&#xA;  ""set_id"":""someid"",&#xA;  ""owner"":""usrid"",&#xA;  ""properties"":&#xA;    {&#xA;      ""height"":2.5,&#xA;      ""name"":""somename"",&#xA;      ""birtDate"":date,&#xA;      ...&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>NOW, depending on what you'll need to do with the data in your application logic, you'll have to map the properties to something your language/program can handle you try to hande these Key/Values generic, i.e. not enforce any schema upon them - that is if that's feasable! </p>&#xA;"
38182593,38172510,6380313,2016-07-04T10:35:00,"<p>Well you ended up with the classical microservice mayhem. It's completely irrelevant what technologies you employ - the problem lays within the way you applied the concept of microservices!</p>&#xA;&#xA;<p>It is natural in this architecture, that services call each other (preferably that should happen asynchronously!!). Since I know only little about your service APIs I'll have to make some assumptions about what went wrong in your backend:</p>&#xA;&#xA;<p>I assume that a user makes a request to one service. This service will now (obviously synchronously) query another service and receive these 30k records you described. Since you probably have to know more about these records you now have to make another request per record to a third service/endpoint to aggregate all the information your frontend requires!</p>&#xA;&#xA;<p>This shows me that you probably got the whole thing with <a href=""http://martinfowler.com/bliki/BoundedContext.html"">bounded contexts</a> wrong! So much for the analytical part. Now to the solution:</p>&#xA;&#xA;<p>Your API should return all the information along with the query that enumerates them! Sometimes that could seem like a contradiction to the kind of isolation and authority over data/state that the microservices pattern specifies - but it is not feasible to isolate data/state in one service only because that leads to the problem you currently have - all other services HAVE to query that data every time to be able to return correct data to the frontend! However it is possible to duplicate it as long as the authority over the data/state is clear!</p>&#xA;&#xA;<p>Let me illustrate that with an example: Let's assume you have a classical shop system. Articles are grouped. Now you would probably write two microservices - one that handles articles and one that handles groups! And you would be right to do so! You might have already decided that the group-service will hold the relation to the articles assigned to a group! Now if the frontend wants to show all items in a group - what happens: The group service receives the request and returns 30'000 Article numbers in a beautiful JSON array that the frontend receives. This is where it all goes south: The frontend now has to query the article-service for every article it received from the group-service!!! Aaand your're screwed!</p>&#xA;&#xA;<p>Now there are multiple ways to solve this problem: One is (as previously mentioned) to duplicate article information to the group-service: So every time an article is assigned to a group using the group-service, it has to read all the information for that article form the article-service and store it to be able to return it with the <em>get-me-all-the-articles-in-group-x</em> query. This is fairly simple but keep in mind that you will need to update this information when it changes in the article-service or you'll be serving stale data from the group-service. Event-Sourcing can be a very powerful tool in this use case and <a href=""http://martinfowler.com/eaaDev/EventSourcing.html"">I suggest you read up on it</a>! You can also use simple messages sent from one service (in this case the article-service) to a message bus of your preference and make the group-service listen and react to these messages.</p>&#xA;&#xA;<p>Another very simple quick-and-dirty solution to your problem could also be just to provide a new REST endpoint on the articles services that takes an array of article-ids and returns the information to all of them which would be much quicker. This could probably solve your problem very quickly.</p>&#xA;&#xA;<p>A good rule of thumb in a backend with microservices is to aspire for a constant number of these cross-service calls which means your number of calls that go across service boundaries should never be directly related to the amount of data that was requested! We closely monitory what service calls are made because of a given request that comes through our API to keep track of what services calls what other services and where our performance bottlenecks will arise or have been caused. Whenever we detect that a service makes many (there is no fixed threshold but everytime I see >4 I start asking questions!) calls to other services we investigate why and how this could be fixed! There are some great metrics tools out there that can help you with tracing requests across service boundaries!</p>&#xA;&#xA;<p>Let me know if this was helpful or not, and whatever solution you implemented!</p>&#xA;"
39660021,39658735,6380313,2016-09-23T11:47:29,"<blockquote>&#xA;  <p>But problem is in my bank passing id's in URI prohibited due to security reason. (URI can read any one)</p>&#xA;</blockquote>&#xA;&#xA;<p>That is, if you're not using TLS/SSL which is anyway a no-go in security-critical environment like banking!! If somebody is able to read your request, he's able to read your HTTP-Traffic and thus there is no real way to protect anything sent over this wire reliably be that in the URL, in the headers or the content!</p>&#xA;&#xA;<p>If you're not able to put anything in your URI you're gonna have a hard time developing clean and concise restfull HTTP APIs!</p>&#xA;"
37433465,37403682,6380313,2016-05-25T09:48:20,"<p>As far as I can tell, there are little to no open source projects out there using this pattern!</p>&#xA;&#xA;<p>There are however many great framworks/toolchains to help you implement it:</p>&#xA;&#xA;<ul>&#xA;<li>If you like GO then you're gonna like <strong>Go-Kit</strong></li>&#xA;<li>If you like C# you might have a look at <strong>servicestack.net</strong></li>&#xA;<li>as for other languages/toolchains I'm not very well informed but there are many frameworks out there that can help you building microservices in almost every major language.</li>&#xA;</ul>&#xA;&#xA;<p>That said: The main reason for this lack of open source micro service systems is probably that most open source project keep a very narrow track of use cases that they cover to stay generic and reusable. This stands in contrast to what commercial backend systems have to provide - which is myriad of usually very specific business services which will probably never be public since they contain the companies competitive advantages and business critical knowledge! Most large exponents of the micro service pattern (like Netflix, Spotify and SoundCloud) have however open sourced the tools and frameworks they build/used to get the services orchestrated, coordinated, synchronized, health checked, list, balanced, scaled etc...</p>&#xA;&#xA;<p>To give you some general pointer towards micro services in general <a href=""https://www.google.com/search?q=martin%20fowler%20microservices&amp;oq=martin%20fowler&amp;aqs=chrome.2.69i57j0l5.3443j0j4&amp;sourceid=chrome&amp;ie=UTF-8"" rel=""nofollow"">Martin Fowler</a> has some great resources. Also a good reference are the talks of <a href=""https://www.youtube.com/watch?v=Vd4DE6D6o94"" rel=""nofollow"">Peter Bourgon on Micro services</a>.</p>&#xA;"
51962668,50927770,4088857,2018-08-22T08:24:23,"<p>What I recommend you is to <strong>Share</strong> Data Contract and Event contract through a Project Reference and have all of your project into one solution, that way the compiler will tell you instantly where you need to fix things if you change a Contract.&#xA;Of course the <strong>down side</strong> of this is that you need to deploy all services every time, since if going to be hard to know which service uses a specific Contract when you check in.</p>&#xA;&#xA;<p>I like the Idea of having a nuget package and Repository/Service, but imagen if you decide to change a Contract, you have to know <strong>in advance</strong> which Services consume that Contract, check out the code and update nuget package and check in the code, you need to repeat for all Microservices that consume that changed Contract.</p>&#xA;&#xA;<p>I have seen both solutions in production, but I think that First solution is more bullet proof since you don’t need to think of if any service that is running in any environments has old Data Contract.</p>&#xA;&#xA;<p><strong>Heads up</strong>&#xA;The only thing that you should share between Services in Contracts.</p>&#xA;"
51746955,51220369,4088857,2018-08-08T12:37:23,"<p>There is no such a thing as (C# Microservice).&#xA;Microservice does not depend on any programming language. You can have multiple microservices running on different technologies.</p>&#xA;&#xA;<p>In short, the microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. (Martin Fowler) </p>&#xA;&#xA;<p>Best Regrads&#xA;Burim</p>&#xA;"
51747071,51740873,4088857,2018-08-08T12:43:06,<p>I don't know if I understand you right?</p>&#xA;&#xA;<p>But as you describe I think you can just Add Reference to your controller and call the method that you want to test.</p>&#xA;&#xA;<p>Best Regards</p>&#xA;
51843030,51842460,4088857,2018-08-14T13:46:48,"<p>One thing you should be aware of, is that each microservice should have it's own Database, if all of your microservices share the same database, than the DB will become you central Contract and it will be always hard to make changes since you never know which Microservice uses what.</p>&#xA;&#xA;<p>When it comes to RabbitMQ, I personally  think that it is the best Message-Broker ever.&#xA;I recommend you to have one Queue/Microservice and one Exchange/Microservice.&#xA;Than you can easy configure your RabbitMQ to redirect Messages between queues based on routing key.</p>&#xA;&#xA;<p>Hope this helps&#xA;Best Regard</p>&#xA;&#xA;<p>Burim Hajrizaj</p>&#xA;"
51930934,51929079,4088857,2018-08-20T12:37:33,<p>In this case I would go for <strong>Approach 1</strong> sine you can download the list of Products you need and than run 2 other request in parallel for downloading  Prices and Reviews.</p>&#xA;&#xA;<p>After your receive response from all 3 requests than you build the model and return it.</p>&#xA;&#xA;<p>I think that Gateway API should be smart enough to make calls to different services and build the result that need to return.</p>&#xA;
51926055,51924926,4088857,2018-08-20T07:37:56,"<p>I think that the way you have drawn boundaries between services is not a problem in any case.&#xA;There are some <strong>pros and cons</strong> that I can see here</p>&#xA;&#xA;<p>Having separate SMS-Service it gives you possibility to use it even in other scenarios like sending other type of notification to user, it makes it easy if you want to change SMS provider, but this service should be all stand alone(using a message broker for comunication).</p>&#xA;&#xA;<p>User service is totally fine, but you should be aware of that each new HTTP call to another service creates ""Latency""(you should take that into consideration), like in this case when a new http call is made for releasing a TOKEN. </p>&#xA;&#xA;<p>When you have a service calling another service than you introduce coupling. In my point of view if you don't use any third party tool for handling Tokens than I think you should have both the Token handling and User within the same microservice, I think that this way you have a better Bounded-Context and it is more easy to run joins since these data will be within a same database.</p>&#xA;&#xA;<p>In case you use third party tool for handling Tokens than it is OK to have it in different Service since in that way you will create abstraction and will be easily to change tool in future.  </p>&#xA;&#xA;<p><strong>Heads-up</strong></p>&#xA;&#xA;<p>Make sure that each Microservice has it own database and you don't access any other database directly outside of the Context that service belong to.  </p>&#xA;"
51938157,51936401,4088857,2018-08-20T20:35:40,"<p>Think like this: as times goes by and you add more and more services to your system.</p>&#xA;&#xA;<p>If all of your services access the same database than you will end up having your database as a central Contact for all of your services. It's going to be very difficult to make changes since it will be hard to know what service use which tables.</p>&#xA;&#xA;<p>If you wanna do it right go for <strong>Approach 2</strong>. &#xA;Each service has it's own database,  and a microservice should not access database other than it's own.&#xA;It is perfectly fine for a service to call another service if that is needed(orchestrated method)</p>&#xA;&#xA;<p>Best regards &#xA;Burim</p>&#xA;"
51966211,51965342,2519577,2018-08-22T11:48:51,"<p>It seems selective routing of messages is not achievable with SQS as stated in the link below</p>&#xA;&#xA;<p><a href=""https://stackoverflow.com/questions/12630756/finding-certain-messages-in-sqs"">Finding certain messages in SQS</a></p>&#xA;&#xA;<p>As you can see from SQS docs <a href=""https://github.com/awsdocs/aws-doc-sdk-examples/blob/master/java/example_code/sqs/src/main/java/aws/example/sqs/SendReceiveMessages.java"" rel=""nofollow noreferrer"">https://github.com/awsdocs/aws-doc-sdk-examples/blob/master/java/example_code/sqs/src/main/java/aws/example/sqs/SendReceiveMessages.java</a>, there is no way you can add attributes to categorise messages i.e SQS is not architected for the use-case</p>&#xA;&#xA;<p>The only possible solution is using three different queues for different types of messages and have each of the microservices listen to specific type of messages which is a typical use case of Pipe and filter design pattern</p>&#xA;&#xA;<p>On the otherhand, if you consider building your own dynamic-routing queue, please refer this documentation to implement the content-based router as discussed in this enterprise design pattern document <a href=""https://www.enterpriseintegrationpatterns.com/patterns/messaging/ContentBasedRouter.html"" rel=""nofollow noreferrer"">https://www.enterpriseintegrationpatterns.com/patterns/messaging/ContentBasedRouter.html</a></p>&#xA;&#xA;<p>Hope it helps!</p>&#xA;"
43761624,43593778,2519577,2017-05-03T13:34:01,"<p>Yes you will have to write each content management system as a separate micro-service &amp; expose REST end-points so that they can scale individually without being inter-dependent on each other</p>&#xA;&#xA;<p>PS : For understanding micro-services, <a href=""http://microservices.io/"" rel=""nofollow noreferrer"">http://microservices.io/</a> is the ideal place to start. Also articles of <a href=""https://microserviceweekly.com/"" rel=""nofollow noreferrer"">https://microserviceweekly.com/</a> would be helpful in your micro-services architecture journey</p>&#xA;"
34526270,34525969,2915097,2015-12-30T09:07:03,"<p>Windows server 2012 lacks the container support that comes with Windows server 2016. As 2012 does not explicitly support docker containers, you can draw your own conclusions :-)</p>&#xA;"
43662716,43657535,688665,2017-04-27T16:03:32,"<p>Well, it depends on what is your requirements from API gateway. </p>&#xA;&#xA;<p>If you need only centralization and monitoring you can check good old ""<a href=""https://en.wikipedia.org/wiki/Enterprise_service_bus"" rel=""nofollow noreferrer"">Enterprise service bus</a>"" pattern. It basically means that you add some message queue like Kafka or Nsq as the main communication source between services.</p>&#xA;"
45366448,45174699,5557273,2017-07-28T06:35:59,"<p>""Invalid signature"" implies that the secret key that you used to encode the token doesn't match with the secret key you used for decoding it.</p>&#xA;&#xA;<p>Make sure that the secret you are using for encoding and decoding are same.</p>&#xA;&#xA;<p>For more info visit the <a href=""https://jwt.io/"" rel=""nofollow noreferrer"">JWT's site</a>.</p>&#xA;"
51340849,51227737,8253209,2018-07-14T15:29:32,"<p>The following picture shows an OpenID Connect authentication flow with a 3rd party IDP (like Google) for a web app. (It does not apply for an architecture where you host your own IDP.)</p>&#xA;&#xA;<p>I would not recommend to reuse the ID or access token from the IDP to access your own APIs (Both tokens should only be used at the authentication.) Instead you should implement an OAuth2 authorization server which issues access tokens for your API. (In this case, you have two OAuth2 flows. One for authentication and one for authorization.)</p>&#xA;&#xA;<p>The authentication flow (Authorization Code Grant) calls are marked blue. The authorization flow (Implicit Grant) calls are marked green.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/Avj58.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Avj58.png"" alt=""OpenID Connect Authentication Flow""></a></p>&#xA;&#xA;<p>1: Your web app is loaded from the app server.</p>&#xA;&#xA;<p>2a: The user clicks on your login button, your web app builds the authorization URL and opens it. (See: <a href=""https://tools.ietf.org/html/rfc6749#section-4.2.1"" rel=""nofollow noreferrer"">Authorization Request</a>)</p>&#xA;&#xA;<p>2b: Because the user hasn't authenticated and has no valid session with your authorization server, the URL he wanted to access is stored and your authorization server responds with a redirect to its login page.</p>&#xA;&#xA;<p>3: The login page is loaded from your authorization server.</p>&#xA;&#xA;<p>4a: The user clicks on ""Login with ..."".</p>&#xA;&#xA;<p>4b: Your authorization server builds the IDP authorization URL and responds with a redirect to it. (See: <a href=""http://openid.net/specs/openid-connect-core-1_0.html#AuthRequest"" rel=""nofollow noreferrer"">Authentication Request</a>)</p>&#xA;&#xA;<p>5a: The IDP authorization URL is opend.</p>&#xA;&#xA;<p>5b: Because the user hasn't authenticated and has no valid session with the IDP, the URL he wanted to access is stored and the IDP responds with a redirect to its login page.</p>&#xA;&#xA;<p>6: The login page is loaded from the IDP.</p>&#xA;&#xA;<p>7a: The user fills in his credentials and clicks on the login button.</p>&#xA;&#xA;<p>7b: The IDP checks the credentials, creates a new session and responds with a redirect to the stored URL.</p>&#xA;&#xA;<p>8a: The IDP authorization URL is opend again.</p>&#xA;&#xA;<p>(The approval steps are ignored here for simplicity.)</p>&#xA;&#xA;<p>8b: The IDP creates an authorization and responds with a redirect to the callback URL of your authorization server. (See: <a href=""http://openid.net/specs/openid-connect-core-1_0.html#AuthResponse"" rel=""nofollow noreferrer"">Authentication Response</a>)</p>&#xA;&#xA;<p>9a: The callback URL is opened.</p>&#xA;&#xA;<p>9b: Your authorization server extracts the authorization code from the callback URL. </p>&#xA;&#xA;<p>10a: Your authorization server calls the IDP's token endpoint, gets an ID and access token and validates the data in the ID token. (See: <a href=""http://openid.net/specs/openid-connect-core-1_0.html#TokenRequest"" rel=""nofollow noreferrer"">Token Request</a>)</p>&#xA;&#xA;<p>(10b: Your authorization server calls the IDP's user info endpoint if some needed claims aren't available in the ID token.)</p>&#xA;&#xA;<p>11a/b: Your authorization server queries/creates the user in your service/DB, creates a new session and responds with a redirect to the stored URL.</p>&#xA;&#xA;<p>12a: The authorization URL is opend again.</p>&#xA;&#xA;<p>(The approval steps are ignored here for simplicity.)</p>&#xA;&#xA;<p>12b/+13a/b: Your authorization server creates/gets the authorization (creates access token) and responds with a redirect to the callback URL of your web app. (See: <a href=""https://tools.ietf.org/html/rfc6749#section-4.2.2"" rel=""nofollow noreferrer"">Access Token Response</a>)</p>&#xA;&#xA;<p>14a: The callback URL is opened.</p>&#xA;&#xA;<p>14b: Your web app extracts the access token from the callback URL.</p>&#xA;&#xA;<p>15: Your web app makes an API call.</p>&#xA;&#xA;<p>16/17/18: The API gateway checks the access token, exchanges the access token with an JWT token (which contains user infos, ...) and forwards the call. </p>&#xA;"
51295349,51227737,8253209,2018-07-11T23:13:15,"<p>I'm trying to answer the sub-questions in the order they were asked. (If one question was not answered, please comment.)</p>&#xA;&#xA;<p>!!! The following answers do apply for a architecture where you use 3rd party OpenID Connect IDPs for authentication. They do not apply if you host an own OpenID Connect IDP. !!!</p>&#xA;&#xA;<p><strong>1. Authentication service could probably be built into the API gateway?</strong></p>&#xA;&#xA;<p>Yes, the authentication service can be built into the API gateway. (For example, IBM's <a href=""https://www.ibm.com/products/datapower-gateway"" rel=""nofollow noreferrer"">DataPower</a> does authentication and acts as API gateway.) If you will have heavy load on the API gateway, then it would be a good idea to separate both. You could then run several API gateway services (with caching).</p>&#xA;&#xA;<p><strong>2. Is OpenID Connect even the right tool for authentication?</strong></p>&#xA;&#xA;<p>Yes, OpenID Connect is definitely the right tool.</p>&#xA;&#xA;<blockquote>&#xA;  <p>It enables Clients to verify the identity of the End-User ..., as well as to obtain basic profile information about the End-User ... . </p>&#xA;</blockquote>&#xA;&#xA;<p>(Many of organizations have started to use/support OpenID Connect.)</p>&#xA;&#xA;<p><strong>3. Use the ID or access token (of the OpenID Connect IDP) for accessing your own APIs?</strong></p>&#xA;&#xA;<p>I wouldn't reuse the ID or access token to access your own APIs. Both tokens should only be used at the authentication. Your authentication service should be a authentication and (OAuth2) authorization service and should issue own access tokens which should only work with your own APIs.</p>&#xA;&#xA;<p>You will then have two nested OAuth2 flows. One between your client applications and your authorization service and one between your authentication service and the OpenID Connect IDP. (Your client application will not notice the second OAuth2 flow.)</p>&#xA;&#xA;<p><strong>4. Do I even need the access token (of the OpenID Connect IDP)?</strong></p>&#xA;&#xA;<p>The ID token might not contain all information you need. (Some claims are optional and some IDPs will not provide them in the ID token.) In this case you use the access token to get the information from the user info endpoint.</p>&#xA;&#xA;<p>(You need specific scopes, to get specific information from the user info endpoint.)</p>&#xA;&#xA;<p><strong>5. Is the ID token really just for the client?</strong></p>&#xA;&#xA;<p>No, the ID token is (also) used by your authentication service to authenticate the user.</p>&#xA;&#xA;<p><strong>6. Store user details, groups, roles and permissions inside JWT?</strong></p>&#xA;&#xA;<p>The use of JWT doesn't only bring a security advantage, it also can improve performance. The user details and so on have to be read only once (per request) from the database and can be used by every service. (You could also add caching to the API gateway. E.g., one minute old user details might be tolerable.)</p>&#xA;&#xA;<p>However, if you have fine grained permissions it's a better idea to only put user details, groups and roles inside the JWT and check single permissions at each service.</p>&#xA;&#xA;<p><strong>7. Passing JWT to client side.</strong></p>&#xA;&#xA;<p>Because the JWT is signed, there is no security issue. However, it isn't easy to revoke issued JWTs.</p>&#xA;&#xA;<p>It's better to use JWTs only after the API gateway and translate them at the API gateway. (Between the client and API gateway the access token should only be a random string.)</p>&#xA;&#xA;<p><strong>8. Do I store and append all my groups, roles, permissions inside the ID token?</strong></p>&#xA;&#xA;<p>You cannot append data to the ID token. You have to create a new JWT.</p>&#xA;"
44831081,41762928,1447450,2017-06-29T17:00:04,"<p>Is this any use? <a href=""https://www.npmjs.com/package/resilient"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/resilient</a></p>&#xA;&#xA;<p>It says: Resilient is conceptually similar to Ribbon, a Netflix's project.</p>&#xA;"
47987235,42372431,6298906,2017-12-27T06:14:30,<p>We used topic to resolve this issue. So instead of JMS queue use JMS topic. Have all the web applications on all cluster subscribe to this topic. We also have a writer component so that all of them can write to the topic.</p>&#xA;&#xA;<pre><code>Cluster A:&#xA;webapp1 --&gt; StatusTopic (JMS Topic)&#xA;webapp2 --&gt; StatusTopic (JMS Topic)&#xA;&#xA;Cluster B:&#xA;webapp1 --&gt; StatusTopic (JMS Topic)&#xA;webapp2 --&gt; StatusTopic (JMS Topic)&#xA;</code></pre>&#xA;
44782621,44781219,4940826,2017-06-27T14:16:03,"<p>If your services are completely independent you will have a hard time when using cassandra. You are asking for a transaction around the three services all using their own data storage. </p>&#xA;&#xA;<p>Cassandra supports only lightweight transactions by its own - see <a href=""http://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlLtwtTransactions.html"" rel=""nofollow noreferrer"">http://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlLtwtTransactions.html</a></p>&#xA;&#xA;<p>You can go and implement your own 2-Phase-Commit:&#xA;<a href=""https://en.wikipedia.org/wiki/Two-phase_commit_protocol"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Two-phase_commit_protocol</a>&#xA;<a href=""https://en.wikipedia.org/wiki/Paxos_(computer_science)"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Paxos_(computer_science)</a></p>&#xA;"
34526515,34526251,295962,2015-12-30T09:22:16,"<p>I'm sorry, your question makes no sense. It's like asking</p>&#xA;&#xA;<blockquote>&#xA;  <p>What is better? A rental car or a Ford?</p>&#xA;</blockquote>&#xA;&#xA;<p>AWS provides IaaS, Kubernetes is system to manage distributed containers. You can run Kubernetes on various platforms, <em>including</em> AWS-provided infrastructure.</p>&#xA;&#xA;<p>Do you want to pay Amazon to host your Docker containers? <a href=""https://aws.amazon.com/ecs/"" rel=""nofollow"">You can do that</a>.</p>&#xA;&#xA;<p>Or do you want to create your own ""cloud"" where you run your Docker containers? Then get yourself some bare metal and use (for example) <a href=""https://coreos.com/"" rel=""nofollow"">CoreOS</a> and <a href=""http://kubernetes.io/gettingstarted/"" rel=""nofollow"">Kubernetes</a> on top.</p>&#xA;"
33034039,33033834,821786,2015-10-09T08:58:06,"<p>No, unfortunately you do not miss anything substantial. The microservice architecture comes with its own cost. The one that caught your eye (boilerplate code) is one well-known item from the list. <a href=""http://martinfowler.com/articles/microservice-trade-offs.html"" rel=""nofollow"">This</a> is a very good article from Martin Fowler explaining the various advantages and disadvantages of the idea. It includes topics like:</p>&#xA;&#xA;<ul>&#xA;<li>added complexity</li>&#xA;<li>increased operational maintance cost </li>&#xA;<li>struggle to keep consistency (while allowing special cases to be treated in exceptional ways)</li>&#xA;</ul>&#xA;&#xA;<p>... and many more.</p>&#xA;"
49771498,49771449,5413146,2018-04-11T09:32:47,<p>User <code>getRemoteAddr()</code> method of request.</p>&#xA;&#xA;<pre><code>ipAddress = request.getRemoteAddr();&#xA;</code></pre>&#xA;
50927852,38714097,3581466,2018-06-19T11:56:15,"<p>narendramacha </p>&#xA;&#xA;<p>I think the problem here could be that you are using the Base64 of the cer certificate file on the SF endpoint, rather than Base64 of the PFX file (which is normally protected by a password).</p>&#xA;"
44328380,40216362,108826,2017-06-02T11:57:47,<p>I think the User object in Hera and User object in Zeus point to same identity but they are different objects and should be implemented in each service according to the service needs.</p>&#xA;&#xA;<p>The common library requires redeployment of ALL services that depend on it.&#xA;If you'll want to store some additional information in Hera (in new field) you will have to redeploy Zeus or start to deal with different versions of library (leads you to two different objects again).</p>&#xA;
51738585,51737931,3648282,2018-08-08T04:23:37,"<p>Writing the main application in one language only is a easier approach than dividing your application and attempting to write pieces in each language that is best suited for the task, unless you are fluent in a few languages and the ones chosen are particularly suited to specific groups of tasks that make up parts of the functionality.</p>&#xA;&#xA;<p>Because MongoDB has a <a href=""https://mongodb.github.io/mongo-java-driver/"" rel=""nofollow noreferrer"">Java Driver</a> there's nothing wrong with writing your main application in Java and relying on libraries written in other languages (MongoDB is <a href=""https://en.m.wikipedia.org/wiki/MongoDB"" rel=""nofollow noreferrer"">written</a> in C++, C and JavaScript). </p>&#xA;&#xA;<p>As long as other works you need to rely on are well maintained there's no reason to switch from your preferred language to match what any of your libraries are using.</p>&#xA;&#xA;<p>If you add artificial intelligence to your program in the future and part of the code is to run on a GPU you are forced to have a program that is a hybrid; learning a new language along with the details of the underlying algorithms is certainly more of a burden than learning the API.</p>&#xA;&#xA;<p>Decide where to draw the line, what you will write in your preferred language and what will be written by others. It's certainly better to choose libraries and programs that you interface with written in languages you understand (assuming that they are open source). If what you interface with has no source available it becomes a 'black box' which simply must work, there are occasions when that is acceptable and occasional when there is no choice.</p>&#xA;"
50161170,50064156,310298,2018-05-03T17:53:28,"<p>I can share a simplified view on how we ship code <a href=""http://codefresh.io"" rel=""nofollow noreferrer"">Codefresh</a>:</p>&#xA;&#xA;<ul>&#xA;<li>each microservice has it's own git repo and helm chart.</li>&#xA;<li>each service codebase is versioned (regardless of the chart), for example in <code>package.json</code>.</li>&#xA;<li>each helm chart is also versioned.</li>&#xA;<li>When devs update services, they must bump the version in the code.</li>&#xA;<li>We also have one Helm uber-chart that lists all of the microservices charts as <a href=""https://github.com/kubernetes/helm/blob/master/docs/chart_best_practices/requirements.md"" rel=""nofollow noreferrer"">chart requirements</a>.</li>&#xA;<li>CI pipeline on the code repo builds a docker image (tagged with the code version) and a helm chart (tagged with the code version). This doesn't deploy anything yet.</li>&#xA;<li>If a dev wants to deploy, they edit the requirements file in the uber-chart, requiring the new version.</li>&#xA;<li>CD pipeline on the uber-chart does a helm upgrade.</li>&#xA;</ul>&#xA;&#xA;<p>This is a very simplified description. In reality the pipeline also does stuff like:</p>&#xA;&#xA;<ul>&#xA;<li>auto handling of npm and github releases</li>&#xA;<li>gates based on semver scope of update</li>&#xA;<li>automatic provisioning of ad-hoc environments on PR/branch (spin up a new environment to test/share your change)</li>&#xA;<li>secret management</li>&#xA;</ul>&#xA;&#xA;<p>To be honest this is not a simple pipeline, but hey, we are a CI/CD company so this is our bread and butter.<br>&#xA;We are often asked by customers to share more about the work we've done, and even replicate it for a customer, which we happily do. Feel free to ping me.</p>&#xA;"
36698543,36573857,310298,2016-04-18T15:39:21,"<p>Since none of the existing answers address your questions directly (however valid suggestions), I'll answer your original questions for the record:</p>&#xA;&#xA;<ol>&#xA;<li>Usually you would use some kind deterministic partitioning scheme, for example range partitioning - so for example if you need to search for user 'foo' you would search the F partition (or the E-G partition) and not every partition.</li>&#xA;<li>SF reliable collections use transactions which might be able to protect you from the side effects of race conditions. Read this for more details: <a href=""https://azure.microsoft.com/en-gb/documentation/articles/service-fabric-reliable-services-reliable-collections/#persistence-model"" rel=""nofollow"">https://azure.microsoft.com/en-gb/documentation/articles/service-fabric-reliable-services-reliable-collections/#persistence-model</a></li>&#xA;</ol>&#xA;&#xA;<p>Your title talks about consistency level - all operations in Service Fabric are strongly consistent, meaning a write will be committed in all replicas before acked.</p>&#xA;"
46318199,41008507,2878746,2017-09-20T09:26:00,"<p>I have a similar problem, and it also involves filtering and sorting on data from multiple partitions.</p>&#xA;&#xA;<p>My plan is to build an indexed view in a non-partitioned stateful service by using notifications. Here I will have multiple dictionaries, with different keys, where each key is one or more properties that can be filtered on or sorted by, and the value is a sorted list of IDs.</p>&#xA;&#xA;<p>Basically, I plan to search, sort and page on these keys, then on the last step, I go with the IDs for the page that needs to be returned to the original partitions and get the full data for these IDs from there (this can be done from a different stateless service as well).</p>&#xA;&#xA;<p>This method does not provide data consistency, since the queried data can change between subsequent paging.</p>&#xA;&#xA;<p>It must be slower than the one with snapshots and continuation tokens, but maybe its worth trying out.</p>&#xA;"
27515280,27470721,1056539,2014-12-16T22:37:26,"<p>In your case, I would suggest to make a separation by concern, I mean a plugin that gathers only the domain classes if using Grails, the other plugins that take care of Services ... these would represent the application core, I think it's much easier this way if your application contain too much KLOC, using microservices will take you time too much time if you have a lot of calls between modules.</p>&#xA;&#xA;<p>Communication between functional modules aka. plugins can be made via events, see events-si or rabbit MQ plugin.</p>&#xA;"
41032153,37409153,5471259,2016-12-08T05:08:10,"<p>After did a lot of research, I find out the solution for my above problem using these below links.</p>&#xA;&#xA;<p><strong>Prepare your development environment</strong></p>&#xA;&#xA;<p><a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-get-started"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-get-started</a></p>&#xA;&#xA;<p><strong>Create a Service Fabric cluster in Azure using the Azure portal</strong></p>&#xA;&#xA;<p><a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-cluster-creation-via-portal"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-cluster-creation-via-portal</a></p>&#xA;&#xA;<p>What I did mistake for connecting secure cluster using PowerShell is before I write the above command I forget to enable the PowerShell script execution. </p>&#xA;&#xA;<p>So, first I was enabled the power shell script execution using this below command.</p>&#xA;&#xA;<pre><code>Set-ExecutionPolicy -ExecutionPolicy Unrestricted -Force -Scope CurrentUser&#xA;</code></pre>&#xA;&#xA;<p>After that  I used the below command for connecting to my secure cluster, Now  it's working fine.</p>&#xA;&#xA;<pre><code>Connect-serviceFabricCluster&#xA;</code></pre>&#xA;&#xA;<p>-Pradeep</p>&#xA;"
46320528,45990451,3046683,2017-09-20T11:12:13,"<p>We have finally found out how to deal with token refresh in our case.</p>&#xA;&#xA;<p>JWT has TTL=10min&#xA;RefreshToken has TTL=30min</p>&#xA;&#xA;<p>Javascript, embedded in each page of our site refreshes JWT each 8-9 minutes. So when User has an opened tab in his browser, the refresh procedure will happen seamlessly.</p>&#xA;&#xA;<p>A corner case is when User:</p>&#xA;&#xA;<ol>&#xA;<li>Opens tab mysite.com/scs1</li>&#xA;<li>Logs in</li>&#xA;<li>Closes tab</li>&#xA;<li>Waits 15 minutes. JWT expires, RefreshToken is still alive.</li>&#xA;<li>Opens new tab and enters mysite.com/scs1 or scs2 etc.</li>&#xA;</ol>&#xA;&#xA;<p>At this point the BackEnd receives only JWT which is expired.&#xA;So BackEnd redirects User to a dedicated web-page /try-refresh?<strong>uri</strong>=mysite.com/scs1</p>&#xA;&#xA;<ol start=""6"">&#xA;<li>try-refresh page contains ONLY javascript which tries to refresh token and in successful case redirects User back to address from <strong>uri</strong> parameter</li>&#xA;</ol>&#xA;"
34728680,34722107,5739035,2016-01-11T18:40:12,"<p>Microservices are great, but it is not good to start with too many microservices right away. If you have doubt about how to define the boundaries between microservices in your application, start by a monolith (all the time keeping the code clean and a good object-oriented with well designed layers and interfaces). When you get to a more mature state of the application, you will more easily see the right places to split to independently deployable services.</p>&#xA;&#xA;<p>The key is to keep together things that should really be coupled. When we try to decouple everything from everything, we end up creating too many layers of interfaces, and this slows us down.</p>&#xA;"
30656291,30648096,3380763,2015-06-05T00:10:27,"<p>With my exp, you have some paths.</p>&#xA;&#xA;<ol>&#xA;<li><a href=""http://readme.io/"" rel=""nofollow"">http://readme.io/</a></li>&#xA;<li>Make a wiki with JIRA, Redmine.</li>&#xA;<li>In Github create a repo for exclusive docs.</li>&#xA;<li>Google Docs.</li>&#xA;</ol>&#xA;"
41255354,41247817,3991067,2016-12-21T04:56:18,"<p>This can be a pretty complicated topic, but the simplest way is to change consul to bind to port 53 using the ports directive, and add some recursers to the consul config can pass real DNS requests on to a host that has full DNS functionality. Something like these bits:</p>&#xA;&#xA;<p>{&#xA;  ""recursors"": [&#xA;    ""8.8.8.8"",&#xA;    ""8.8.4.4""&#xA;  ],&#xA;  ""ports"": {&#xA;    ""dns"": 53&#xA;  }&#xA;}</p>&#xA;&#xA;<p>Then modify your system to use the consul server for dns with a nameserver entry in /etc/resolve.conf. Depending on your OS, you might be able to use a port in the resolv.conf file, and avoid having to deal with Consul needing root to bind to port 53.</p>&#xA;&#xA;<p>In a more complicated scenario, I know many people that use unbound or bind to do split DNS and essentially do the opposite, routing the .consul domain to the consul cluster on a non-privileged port at the org level of their DNS infrastructure.</p>&#xA;"
45921617,45886359,7600667,2017-08-28T14:40:41,"<p>The infinite <code>for</code> loops are likely starving the garbage collector: <a href=""https://github.com/golang/go/issues/15442#issuecomment-214965471"" rel=""nofollow noreferrer"">https://github.com/golang/go/issues/15442#issuecomment-214965471</a></p>&#xA;&#xA;<p>I was able to reproduce the issue by just running the publisher. To resolve, I recommend using a <code>sync.WaitGroup</code>. Here's how I updated the code linked to in the comments to get it to complete:</p>&#xA;&#xA;<pre><code>package main&#xA;&#xA;import (&#xA;    ""fmt""&#xA;    ""log""&#xA;    ""sync""&#xA;    ""time""&#xA;&#xA;    ""github.com/nats-io/go-nats""&#xA;)&#xA;&#xA;// create wait group&#xA;var wg sync.WaitGroup&#xA;&#xA;func main() {&#xA;    // add 1 waiter&#xA;    wg.Add(1)&#xA;    go createPublisher()&#xA;&#xA;    // wait for wait group to complete&#xA;    wg.Wait()&#xA;}&#xA;&#xA;func createPublisher() {&#xA;&#xA;    log.Println(""pub started"")&#xA;    // mark wait group done after createPublisher completes&#xA;    defer wg.Done()&#xA;&#xA;    nc, err := nats.Connect(nats.DefaultURL)&#xA;    if err != nil {&#xA;        log.Fatal(err)&#xA;    }&#xA;    defer nc.Close()&#xA;&#xA;    msg := make([]byte, 16)&#xA;&#xA;    for i := 0; i &lt; 100000; i++ {&#xA;        if errPub := nc.Publish(""alenSub"", msg); errPub != nil {&#xA;            panic(errPub)&#xA;        }&#xA;&#xA;        if (i % 100) == 0 {&#xA;            fmt.Println(""i"", i)&#xA;        }&#xA;        time.Sleep(time.Millisecond * 1)&#xA;    }&#xA;&#xA;    log.Println(""pub finish"")&#xA;&#xA;    errFlush := nc.Flush()&#xA;    if errFlush != nil {&#xA;        panic(errFlush)&#xA;    }&#xA;&#xA;    errLast := nc.LastError()&#xA;    if errLast != nil {&#xA;        panic(errLast)&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>I'd recommend updating the above subscriber code similarly.</p>&#xA;&#xA;<p>The main difference between <code>Subscribe</code> and <code>QueueSubscriber</code> is that in <code>Subscribe</code> all subscribers are sent all messages from. While in <code>QueueSubscribe</code> only one subscriber in a <code>QueueGroup</code> is sent each message.</p>&#xA;&#xA;<p>Some details on additional features for NATS Streaming are here:&#xA;<a href=""https://nats.io/documentation/streaming/nats-streaming-intro/"" rel=""nofollow noreferrer"">https://nats.io/documentation/streaming/nats-streaming-intro/</a></p>&#xA;&#xA;<p>We see both NATS and NATS Streaming used in a variety of use cases from data pipelines to control planes. Your choice should be driven by the needs of your use case.</p>&#xA;"
47835677,47835162,5641244,2017-12-15T15:47:16,"<p>Code coverage measures how much of your code is exercised by unit tests.  You can't get 100% coverage without writing unit tests.  Your question therefore reduces to ""how do I add unit tests to scala code"".</p>&#xA;&#xA;<p>The simplest answer to this is to choose a unit test framework (of which there are a few).  I tend to use <a href=""http://www.scalatest.org/"" rel=""nofollow noreferrer"">http://www.scalatest.org/</a>, which is reasonably well documented, and you can follow its user guide for info on how to structure tests and so forth.</p>&#xA;"
48954559,48949794,5641244,2018-02-23T18:58:18,"<p><code>Command</code>s are requests for an entity to do something.  <code>Event</code>s are essentially notifications that something has happened (typically resulting from receiving a command, though not every command needs to result in an Event).  The <code>State</code> is the result of playing all the Events that have happened (in functional terms this is a fold over the history of Events), and the <code>EventHandler</code> is a [partial] function that, if defined for a particular combination of State and Event, creates a new State.</p>&#xA;"
50697733,50688223,2966556,2018-06-05T10:21:36,"<p>Typically, a micro service should manage single entity. So in your case you can have one micro-service to manage the entity (for various operations on the entity). Now if you want to split the service again on the basis of read and write operation then you are following the CQRS pattern. In CQRS , you split your micro-service on the basis of read and write operations. So now you will have 2 services one called command service and other called query service over the same entity. I will suggest to go with one service first to manage the entity and then if required split it more for separate service for read and write operations. Again if you are going to use CQRS, then have a look at event sourcing as it nicely fits with CQRS in micro-services design.</p>&#xA;"
50123167,50039099,1199132,2018-05-01T20:12:19,"<p>The ideal way to do this for me is using a SSO service like <a href=""https://www.keycloak.org/"" rel=""nofollow noreferrer"">keycloak</a>, as your Identity Provider. That involves porting your authentication logic to it and decouple it from your monolith. But once you rely on it as your IdP, you only share tokens between your applications and IdP. Keycloak has many adapters, for Java, Javascript, NodeJs clients.. and so on. That has the huge advantage that you can keep decoupling logic from your monolith and you don't even need new modules to be written in Java. You could even integrate it with services not written by you, it's enough for them to have Open Id Connect or SAML integrations.</p>&#xA;&#xA;<p>KC supports both SAML and OIDC Auth protocols. With OIDC (the way to go with brand new applications), the adapter located in each of the apps checks wether the user has a valid web session or not. If not, it gets redirected to KC login page. The result of a valid login is a code which you can use to obtain an access token (a valid token to obtain resources from your app). Apart from that, the adapter ties that access token to a browser web session, which is stored in KC too. So if you try to access the other web page, the adapter checks for your web session, which finds to be valid.</p>&#xA;&#xA;<p>You can use it to access your apps from a mobile device too, taking care of the access token management/renewal in the mobile app yourself.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/L7TqL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L7TqL.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p><strong>See also:</strong></p>&#xA;&#xA;<ul>&#xA;<li><a href=""http://openid.net/connect/"" rel=""nofollow noreferrer"">The OpenId Connect protocol</a></li>&#xA;</ul>&#xA;"
28635258,28635179,1789724,2015-02-20T17:55:15,"<p>You could look at using <a href=""https://uwsgi-docs.readthedocs.org/en/latest/Emperor.html"" rel=""nofollow"">uWSGI in Emperor mode</a>.  It was built to handle situations like this.  Here's a quote from the docs:</p>&#xA;&#xA;<blockquote>&#xA;  <p>If you need to deploy a big number of apps on a single server, or a group of servers, the Emperor mode is just the ticket. It is a special uWSGI instance that will monitor specific events and will spawn/stop/reload instances (known as vassals, when managed by an Emperor) on demand.</p>&#xA;</blockquote>&#xA;&#xA;<p>If you're not restricted to using mod_wsgi, then I'd take a look at uWSGI.</p>&#xA;"
45088674,45088519,53788,2017-07-13T18:42:00,"<p>This is super broad, and there are a lot of options.  </p>&#xA;&#xA;<p>The simplest approach is that the JS UI just hits the frontend API as needed (either on some event or via polling), and so when a new calculation request comes in and is answered, the new calculation is logged (to some form of persistence, e.g. a database or message queue) and so the current 'state' of that model changes and the next time the client asks for the current state, it gets that update. </p>&#xA;&#xA;<p>A more complex approach is to set up some form of messaging infrastructure (e.g. pub/sub), and perhaps a websocket connection between the client and the frontend server(s).  Then the event happening on the frontend microservice would be pushed to the client w/o the client asking for it directly.  In that case, you need to account for dropped connections and so on, so that if a client misses an update b/c they're offline they can get the one(s) they missed once they reconnect.  </p>&#xA;"
34029122,34023438,742429,2015-12-01T20:04:05,"<p>It depends how complex ""project"" and ""contract"" domains are. By answering the following questions I hope you will be able to make a right decision:</p>&#xA;&#xA;<ol>&#xA;<li>Isolation of changes perspective: do you expect changes in requirements of one domain to be independent or more frequent then in another?</li>&#xA;<li>Team setup perspective: do you expect those functionality to be implemented by separate/multiple teams? Will they be able to work independently without any knowledge about the domain of another team?</li>&#xA;<li>Technology perspective: do you expect to project and contract domains to be implemented more effectively with different technologies?</li>&#xA;<li>Data consistency perspective: can you accept eventual consistency between project and contract?</li>&#xA;<li>Non-functional requirements perspective: are performance and availability requirements for those services different?</li>&#xA;<li>Technological risk perspective: do you already have distributed system and necessary expertise inside the team?</li>&#xA;<li>Cohesion perspective: try to model the services, is one of them fully independent in runtime from another? Mutual dependencies are a sign of high cohesion and bad candidates for different services</li>&#xA;<li>Service clients perspective: will those service have different clients? will both of them be accessed by another services?</li>&#xA;</ol>&#xA;&#xA;<p>If answer is ""yes"" to almost all the question then go ahead with 2 microservices. I think that most likely it is not.</p>&#xA;"
42443645,42428045,1467165,2017-02-24T16:28:52,<p>If you use Zuul from Spring Cloud Netflix as your load balancer it will automatically add the <code>X-Forwarded-Host</code> header which Spring HATEOAS will respect.  When that header is present HATEOAS will generate links using the host value in the header.  If you are not using Zuul than you will have to configure your load balancer to add that header.</p>&#xA;
40973622,40960601,1603357,2016-12-05T11:49:56,"<p>I guess this can be implemented in different ways, depending on your requirements:</p>&#xA;&#xA;<ul>&#xA;<li><p>If you want to be able to handle a host (agent) failure, I think there is no other way than using a shared filesystem such as NFS. Otherwise, if you use Marathon to schedule your Microservices, the task will be restarted on another agent (where the data isn't locally available). Also, you would then need to make sure that the same mount points are available on each agent, and use these as host volumes in your containers. Unfortunately, the POD feature availability for co-locating tasks starts to be available in Mesos 1.1.0 and Marathon 1.4 (not yet finally released), as a side note...</p></li>&#xA;<li><p>If you don't care about host (agent) failures, then you possible could co-locate the two Microservices on the same agent if you use <a href=""https://mesosphere.github.io/marathon/docs/constraints.html#cluster-operator"" rel=""nofollow noreferrer"">hostname constraints</a> in Marathon, and mount the host volumes which then can be shared across the services. I guess you'd need some orchestration to only start the analysis service only after the producing service has finished.</p></li>&#xA;</ul>&#xA;"
40811287,40807126,6067741,2016-11-25T19:27:21,"<p>Actually, you should take a look at Azure Functions, they achieve exactly that what you want and they support PowerShell/ </p>&#xA;&#xA;<p><a href=""https://david-obrien.net/2016/07/azure-functions-PowerShell/"" rel=""nofollow noreferrer"">https://david-obrien.net/2016/07/azure-functions-PowerShell/</a></p>&#xA;&#xA;<p><a href=""https://azure.microsoft.com/en-us/services/functions/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/services/functions/</a></p>&#xA;"
43838917,39020065,7978269,2017-05-08T02:30:55,"<p>You can try this.</p>&#xA;&#xA;<pre><code>{&#xA;    ""id"": ""consul-agent"",&#xA;    ""instances"": 10,&#xA;    ""constraints"": [[""hostname"", ""UNIQUE""]],&#xA;    ""container"": {&#xA;        ""type"": ""DOCKER"",&#xA;        ""docker"": {&#xA;            ""image"": ""consul"",&#xA;            ""privileged"": true,&#xA;            ""network"": ""HOST"",&#xA;            ""parameters"": [&#xA;                ""key"": ""env"",&#xA;                ""value"": ""YOUR_ENV_VAR=VALUE""&#xA;            ]&#xA;        }&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Or</p>&#xA;&#xA;<pre><code>{&#xA;    ""id"": ""consul-agent"",&#xA;    ""instances"": 10,&#xA;    ""constraints"": [[""hostname"", ""UNIQUE""]],&#xA;    ""container"": {&#xA;        ""type"": ""DOCKER"",&#xA;        ""docker"": {&#xA;            ""image"": ""consul"",&#xA;            ""privileged"": true,&#xA;            ""network"": ""HOST""&#xA;        }&#xA;    },&#xA;    ""env"": {&#xA;        ""ENV_NAME"" : ""VALUE""&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
43817759,43350853,69150,2017-05-06T07:06:42,"<p>I'm working on a similar setup and these are my finding/ conclusions at the moment:</p>&#xA;&#xA;<p>User sign up has to be the way you describe. </p>&#xA;&#xA;<p>Upon login I do believe there are two possible ways to solve this:</p>&#xA;&#xA;<ol>&#xA;<li>Store the consumer_id in your user database, </li>&#xA;<li>Store the jwt key and secret in your user database.</li>&#xA;</ol>&#xA;&#xA;<p>In scenario 1 you'll have to get the jwt key and secret from kong and generate a jwt token and use this token to do requests to your kong services. </p>&#xA;&#xA;<p>Scenario 2 is pretty much identical to scenario 1 except you don't have to do any requests to kong in order to generate a jwt token.</p>&#xA;&#xA;<p>You can add additional payload parameters to the jwt token but these are not passed down to your upstream services. It does however seem like this plugin solves this issue (I have not tested this yet):</p>&#xA;&#xA;<p><a href=""https://github.com/wshirey/kong-plugin-jwt-claims-headers"" rel=""nofollow noreferrer"">https://github.com/wshirey/kong-plugin-jwt-claims-headers</a></p>&#xA;&#xA;<p>Kong passes on the custom_id and username from the jwt consumer to the upstream service upon authorization, like this:</p>&#xA;&#xA;<pre><code>x-consumer-custom-id: [245]&#xA;x-consumer-username: ['my-test-user']&#xA;x-consumer-id: ['1e9e25dd-396f-4195-94fc-f2a8bd8447a2']&#xA;</code></pre>&#xA;&#xA;<p>It also passes on the entire authorization header</p>&#xA;"
30452220,30449278,6430,2015-05-26T07:30:09,"<p>Firstly, I'd suggest following a convention for the release tags of your components. In the simplest case that just would be the freshest git tag on each of the repositories.</p>&#xA;&#xA;<p>Then, you could create a mapping script (say, it's called <code>map_versions</code>) enumerating the release (latest) git tags for all the repositories, and storing that mapping somewhere for the SaltStack to pick it up -- to be used as the <code>revision</code>-s in the <code>git.latest</code> states.</p>&#xA;&#xA;<p>The same mapping script can be also used to prepare the <a href=""http://nvie.com/posts/a-successful-git-branching-model/"" rel=""nofollow"">develop or master</a> branches of all the components for deployment, -- all the <code>revision</code> values will be switched to <code>develop</code> or <code>master</code>.</p>&#xA;&#xA;<p>Thus, your workflow will be:</p>&#xA;&#xA;<pre><code>// In the dev environment:&#xA;&#xA;$ map_versions develop&#xA;$ salt * state.highstate&#xA;&#xA;// Do the development, until all the stable features&#xA;// are merged back into master. Then, in production:&#xA;&#xA;$ map_versions master&#xA;$ salt * state.highstate&#xA;&#xA;// Make sure everything works fine; then, manually tag&#xA;// the new release versions for all the repos.&#xA;&#xA;$ map_versions tags&#xA;$ salt * state.highstate&#xA;</code></pre>&#xA;&#xA;<p>After which, all the released components in production are tagged.</p>&#xA;&#xA;<p>You can also save some time with automatic git-tagging script for all your deployable components. The script would check if anything has changed in the <code>master</code> since the last tag, and, if it has, it would stick a new git tag on the repo; say, just the today's <code>YYYY-MM-DD</code>. Then, those tags would be picked up by the <code>map_versions tags</code>.</p>&#xA;"
51425366,46425554,412446,2018-07-19T14:33:21,"<p>When your requirements for a DTO model exactly match your entity model you are either in a very early stage of the project or very lucky that you just have a simple model. If your model is very simple, then DTOs won't give you many immediate benefits.</p>&#xA;&#xA;<p>At some point, the requirements for the DTO model and the entity model will diverge though. Imagine you add some audit aspects, statistics or denormalization to your entity/persistence model. That kind of data is usually never exposed via DTOs directly, so you will need to split the models. It is also often the case that the main driver for DTOs is the fact that you don't need all the data all the time. If you display objects in e.g. a dropdown you only need a label and the object id, so why would you load the whole entity state for such a use case?</p>&#xA;&#xA;<p>The fact that you have annotations on your DTO models shouldn't bother you that much, what is the alternative? An XML-like mapping? Manual object wiring?&#xA;If your model is used by third parties directly, you could use a subclassing i.e. keep the main model free of annotations and have annotated subclasses in your project that extend the main model.</p>&#xA;&#xA;<p>Since implementing a DTO approach correctly, I created <a href=""https://github.com/Blazebit/blaze-persistence#entity-view-usage"" rel=""nofollow noreferrer"">Blaze-Persistence Entity Views</a> which will not only simplify the way you define DTOs, but it will also improve the performance of your queries.</p>&#xA;&#xA;<p>If you are interested, I even have an example for an <a href=""https://github.com/Blazebit/blaze-storage/blob/master/rest/model/src/main/java/com/blazebit/storage/rest/model/AccountRepresentation.java"" rel=""nofollow noreferrer"">external model</a> that uses <a href=""https://github.com/Blazebit/blaze-storage/blob/master/rest/impl/src/main/java/com/blazebit/storage/rest/impl/view/AccountRepresentationView.java"" rel=""nofollow noreferrer"">entity view subclasses</a> to keep the main model clean.</p>&#xA;"
38900048,31468806,5985566,2016-08-11T15:06:25,"<p>I would suggest <strong>UAA</strong> of Cloud foundry. It is the best ""User Account and Authentication"" management server you need for your requirement.</p>&#xA;&#xA;<p>Other than the above you can use Spring OAuth2 with JWT for securing microservices.</p>&#xA;"
34067912,34043861,2058014,2015-12-03T13:57:34,"<ul>&#xA;<li>A.  You can monitor the health of your services using any of the&#xA;checks defined <a href=""https://consul.io/docs/agent/checks.html"" rel=""nofollow"">here</a></li>&#xA;<li><p>B.  To gracefully remove a service from consul, you can do <a href=""https://consul.io/docs/agent/http/agent.html#agent_service_deregister"" rel=""nofollow"">this http&#xA;call</a> to the local consul agent. I suggest adding this to the&#xA;closing logic of your service. If you want consul to give you some sort of warning when your sevice goes down, you can use <a href=""https://consul.io/docs/agent/watches.html"" rel=""nofollow"">Watches</a></p></li>&#xA;<li><p>C.  Health checks are performed on a set interval. So you decide the&#xA;intensity of the checks. For example: every 30 seconds</p></li>&#xA;</ul>&#xA;"
49891657,46131443,9661982,2018-04-18T05:15:28,"<p>Anatoly above sums it pretty clearly and nicely as to what you need to do.&#xA;If you are looking for a quick and dirty test try changing the <em>hostnames</em>  from &#xA;<a href=""http://config:8888"" rel=""nofollow noreferrer"">http://config:8888</a> to <a href=""http://localhost:8888"" rel=""nofollow noreferrer"">http://localhost:8888</a>&#xA;in the bootstrap.yml files, for the service you are trying to run.  </p>&#xA;"
42497848,42327562,29470,2017-02-27T23:44:41,"<p>When you don't have any services to bind, you should instead declare a <code>ServiceInfo</code>.</p>&#xA;&#xA;<p>In addition, for a service that only consumes from Kafka and does not publish topics of it's own, it's only necessary to mix in <code>LagomKafkaClientComponents</code> instead of <code>LagomKafkaComponents</code>. <code>LagomKafkaClientComponents</code> does not require a <code>LagomServer</code>, only a <code>ServiceInfo</code>.</p>&#xA;&#xA;<p>You can declare <code>ServiceInfo</code> for a simple consumer that does not include any services like this:</p>&#xA;&#xA;<pre><code>override lazy val serviceInfo = ServiceInfo(clientName, Map.empty)&#xA;</code></pre>&#xA;&#xA;<p>Where <code>clientName</code> is a unique, identifying string for this project that is used to name the Kafka client ID and default consumer group ID. You might use <code>""twitterConsumer""</code> in this example.</p>&#xA;&#xA;<p><a href=""http://www.lagomframework.com/documentation/1.3.x/scala/ServiceInfo.html"" rel=""nofollow noreferrer"">http://www.lagomframework.com/documentation/1.3.x/scala/ServiceInfo.html</a> has more information on <code>ServiceInfo</code>.</p>&#xA;"
47278389,47275278,29470,2017-11-14T05:36:45,"<p>Absolutely. You would do this the same way you would with any other Java or Scala project: create a sub-project that is an internal library included into your service.</p>&#xA;&#xA;<p>For an example of this look at the <a href=""https://github.com/lagom/online-auction-java"" rel=""nofollow noreferrer"">Online Auction Java</a> example Lagom project. Specifically, the <code>security</code> and <code>tools</code> sub-projects. These are included into other services using the <code>sbt</code> <code>dependsOn</code> method in <code>build.sbt</code>.</p>&#xA;&#xA;<p>For example:</p>&#xA;&#xA;<pre><code>lazy val itemApi = (project in file(""item-api""))&#xA;  .settings(commonSettings: _*)&#xA;  .settings(&#xA;    version := ""1.0-SNAPSHOT"",&#xA;    libraryDependencies ++= Seq(&#xA;      lagomJavadslApi,&#xA;      lombok&#xA;    )&#xA;  )&#xA;  .dependsOn(security, tools)&#xA;</code></pre>&#xA;&#xA;<p>In this case, it is the API project that depends on the additional libraries, but you could do the same thing with your implementation project to use libraries that are not needed by the API. Note that the implementation project also depends on its corresponding API, so any dependencies of the API are inherited by the implementation.</p>&#xA;"
42200951,41115982,101662,2017-02-13T09:42:55,"<p>So I'll add some answers to my own question here as I did a little bit of testing.</p>&#xA;&#xA;<p><strong>Server</strong></p>&#xA;&#xA;<ul>&#xA;<li>Intel Xeon - X5550</li>&#xA;<li>32GB Ram</li>&#xA;<li>Windows Server 2012 R2</li>&#xA;</ul>&#xA;&#xA;<p><strong>Application</strong></p>&#xA;&#xA;<p>Created a barebones WebAPI only ASP.Net application with a single controller and action.&#xA;When installed in IIS this is the observed memory footprint. </p>&#xA;&#xA;<ul>&#xA;<li>Memory (Idle) = ~ 5172 K</li>&#xA;<li>Memory (Running) = ~26 000 K</li>&#xA;</ul>&#xA;&#xA;<p><strong>Prep</strong></p>&#xA;&#xA;<p>I created some powershell scripts (sorry can't share it as they leverage our closed source deployment scripts) to:</p>&#xA;&#xA;<ul>&#xA;<li>Create - Unique folder for each application to prevent possible resource sharing</li>&#xA;<li>Launch - Makes a web request</li>&#xA;<li>Cleanup - Deletes all applications, pools and folders</li>&#xA;<li>Recycle - Unloads the application, sets it back to Idle state</li>&#xA;</ul>&#xA;&#xA;<p><strong>Test</strong></p>&#xA;&#xA;<p>Below are my results observed from PerfMon</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/iIMHc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iIMHc.png"" alt=""Test Results""></a></p>&#xA;&#xA;<p>As you will note I could not get all 1000 running at once. I ran into a few things:</p>&#xA;&#xA;<ul>&#xA;<li>Trying to fire a call to all 1000 so they all running simultaneously is not as easy as it sounds.</li>&#xA;<li>ASP.Net temporary internet files is on the C:\ which ran out of space </li>&#xA;<li>Things began running slowly since memory was being paged.</li>&#xA;</ul>&#xA;&#xA;<p><strong>Conclusion</strong></p>&#xA;&#xA;<p>It seems that IIS really has no limit on the number of processes. The core constraint is the resourcing on the machine. </p>&#xA;&#xA;<p>What is interesting is it is unlikely all applications would be running simulationoesly and so one can take advantage of the fact that IIS will provision mem </p>&#xA;"
30441546,29117570,697630,2015-05-25T15:27:12,"<p>The Book <a href=""https://rads.stackoverflow.com/amzn/click/1491950358"" rel=""noreferrer"">Building Microservices</a> describes in detail the styles mentioned by @RogerAlsing in his answer.</p>&#xA;&#xA;<p>On page 43 under Orchestration vs Choreography the book says:</p>&#xA;&#xA;<blockquote>&#xA;  <p>As we start to model more and more complex logic, we have to deal with&#xA;  the problem of managing business processes that stretch across the&#xA;  boundary of individual services. And with microservices, we’ll hit&#xA;  this limit sooner than usual. [...] When it comes to actually&#xA;  implementing this flow, there are two styles of architecture we could&#xA;  follow. With orchestration, we rely on a central brain to guide and&#xA;  drive the process, much like the conductor in an orchestra. With&#xA;  choreography, we inform each part of the system of its job, and let it&#xA;  work out the details, like dancers all find‐ ing their way and&#xA;  reacting to others around them in a ballet.</p>&#xA;</blockquote>&#xA;&#xA;<p>The book then proceeds to explain the two styles. The orchestration style corresponds more to the SOA idea of <a href=""http://serviceorientation.com/soamethodology/task_services"" rel=""noreferrer"">orchestration/task services</a>, whereas the choreography style corresponds to the <a href=""http://martinfowler.com/articles/microservices.html#SmartEndpointsAndDumbPipes"" rel=""noreferrer"">dumb pipes and smart endpoints</a> mentioned in Martin Fowler's article.</p>&#xA;&#xA;<p><strong>Orchestration Style</strong></p>&#xA;&#xA;<p>Under this style, the book above mentions:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Let’s think about what an orchestration solution would look like for&#xA;  this flow. Here, probably the simplest thing to do would be to have&#xA;  our customer service act as the central brain. On creation, it talks&#xA;  to the loyalty points bank, email service, and postal service [...],&#xA;  through a series of request/response calls. The&#xA;  customer service itself can then track where a customer is in this&#xA;  process. It can check to see if the customer’s account has been set&#xA;  up, or the email sent, or the post delivered. We get to take the&#xA;  flowchart [...] and model it directly into code. We could even use&#xA;  tooling that implements this for us, perhaps using an appropriate&#xA;  rules engine. Commercial tools exist for this very purpose in the form&#xA;  of business process modeling software. Assuming we use synchronous&#xA;  request/response, we could even know if each stage has worked [...]&#xA;  The downside to this orchestration approach is that the customer&#xA;  service can become too much of a central governing authority. It can&#xA;  become the hub in the middle of a web, and a central point where logic&#xA;  starts to live. I have seen this approach result in a small number of&#xA;  smart “god” services telling anemic CRUD-based services what to do.</p>&#xA;</blockquote>&#xA;&#xA;<p>Note: I suppose that when the author mentions tooling he's referring to something like <a href=""http://en.wikipedia.org/wiki/Business_process_modeling"" rel=""noreferrer"">BPM</a> (e.g. <a href=""http://activiti.org/"" rel=""noreferrer"">Activity</a>, <a href=""http://ode.apache.org"" rel=""noreferrer"">Apache ODE</a>, <a href=""http://camunda.com"" rel=""noreferrer"">Camunda</a>). As a matter of fact the <a href=""http://workflowpatterns.com"" rel=""noreferrer"">Workflow Patterns Website</a> has an awesome set of patterns to do this kind of orchestration and it also offers evaluation details of different vendor tools that help to implement it this way. I don't think the author implies one is required to use one of these tools to implement this style of integration though, other lightweight orchestration frameworks could be used e.g. <a href=""https://projects.spring.io/spring-integration/"" rel=""noreferrer"">Spring Integration</a>, <a href=""http://camel.apache.org"" rel=""noreferrer"">Apache Camel</a> or <a href=""https://www.mulesoft.com/platform/soa/mule-esb-open-source-esb"" rel=""noreferrer"">Mule ESB</a></p>&#xA;&#xA;<p>However, <a href=""http://www.oreilly.com/programming/free/software-architecture-patterns.csp"" rel=""noreferrer"">other books</a> I've read on the topic of Microservices and in general the majority of articles I've found in the web seem to <a href=""https://www.thoughtworks.com/insights/blog/scaling-microservices-event-stream"" rel=""noreferrer"">disfavor this approach</a> of orchestration and instead suggest using the next one.</p>&#xA;&#xA;<p><strong>Choreography Style</strong></p>&#xA;&#xA;<p>Under choreography style the author says:</p>&#xA;&#xA;<blockquote>&#xA;  <p>With a choreographed approach, we could instead just have the customer&#xA;  service emit an event in an asynchronous manner, saying Customer&#xA;  created. The email service, postal service, and loyalty points bank&#xA;  then just subscribe to these events and react accordingly [...] &#xA;  This approach is significantly more decoupled. If some&#xA;  other service needed to reach to the creation of a customer, it just&#xA;  needs to subscribe to the events and do its job when needed. The&#xA;  downside is that the explicit view of the business process we see in&#xA;  [the workflow] is now only implicitly reflected in our system [...]&#xA;  This means additional work is needed to ensure that you can monitor&#xA;  and track that the right things have happened. For example, would you&#xA;  know if the loyalty points bank had a bug and for some reason didn’t&#xA;  set up the correct account? One approach I like for dealing with this&#xA;  is to build a monitoring system that explicitly matches the view of&#xA;  the business process in [the workflow], but then tracks what each of&#xA;  the services does as independent entities, letting you see odd&#xA;  exceptions mapped onto the more explicit process flow. The [flowchart]&#xA;  [...] isn’t the driving force, but just one lens through&#xA;  which we can see how the system is behaving. In general, I have found&#xA;  that systems that tend more toward the choreographed approach are more&#xA;  loosely coupled, and are more flexible and amenable to change. You do&#xA;  need to do extra work to monitor and track the processes across system&#xA;  boundaries, however. I have found most heavily orchestrated&#xA;  implementations to be extremely brittle, with a higher cost of change.&#xA;  With that in mind, I strongly prefer aiming for a choreographed&#xA;  system, where each service is smart enough to under‐ stand its role in&#xA;  the whole dance.</p>&#xA;</blockquote>&#xA;&#xA;<p>Note: To this day I'm still not sure if choreography is just another name for <a href=""http://radar.oreilly.com/2015/02/variations-in-event-driven-architecture.html"" rel=""noreferrer"">event-driven architecture</a> (EDA), but if EDA is just one way to do it, what are the other ways? (Also see <a href=""https://martinfowler.com/articles/201701-event-driven.html"" rel=""noreferrer"">What do you mean by ""Event-Driven""?</a> and <a href=""https://www.youtube.com/watch?v=STKCRSUsyP0"" rel=""noreferrer"">The Meanings of Event-Driven Architecture</a>). Also it seems that things like CQRS and EvenSourcing resonate a lot with this architectural style, right?</p>&#xA;&#xA;<p>Now, after this comes the fun. The Microservices book does not assume microservices are going to be implemented with REST. As a matter of fact in the next section in the book they proceed to consider RPC and SOA-based solutions and finally REST. Important point here is that Microservices does not imply REST.</p>&#xA;&#xA;<p><strong>So, What About HATEOAS?</strong></p>&#xA;&#xA;<p>Now, if we want to follow the RESTful approach we cannot ignore HATEOAS or Roy Fielding will be very much pleased to say in his blog that our solution is not truly REST. See his blog post on <a href=""http://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven"" rel=""noreferrer"">REST API Must be Hypertext Driven</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p>I am getting frustrated by the number of people calling any HTTP-based&#xA;  interface a REST API. What needs to be done to make the REST&#xA;  architectural style clear on the notion that hypertext is a&#xA;  constraint? In other words, if the engine of application state (and&#xA;  hence the API) is not being driven by hypertext, then it cannot be&#xA;  RESTful and cannot be a REST API. Period. Is there some broken manual&#xA;  somewhere that needs to be fixed?</p>&#xA;</blockquote>&#xA;&#xA;<p>So, as you can see, Fielding thinks that without HATEOAS you are not truly building RESTful applications. For fielding HATEOAS is the way to go when it comes to orchestrate services. I am just learning all this, but to me HATEOAS does not clearly define who or what is the driving force behind actually following the links. In a UI that could be the user, but in computer-to-computer interactions, I suppose that needs to be done by a higher level service.</p>&#xA;&#xA;<p>According to HATEOAS, the only link the API consumer truly needs to know is the one that initiates the communication with the server (e.g. POST /order). From this point on, REST is going to conduct the flow, because in the response of this endpoint, the resource returned will contain the links to next possible states. The API consumer then decides what link to follow and move the application to the next state.</p>&#xA;&#xA;<p>Despite how cool that sounds, the client still needs to know if the link must be POSTed, PUTed, GETed, PATCHed, etc. And the client still needs to decide what payload to pass. The client still needs to be aware of what to do if that fails (retry, compensate, cancel, etc.).</p>&#xA;&#xA;<p>I am fairly new to all this, but for me, from HATEOAs perspective, this client, or API consumer is a high order service. If we think it from the perspective of a human, you can imagine an end user in a web page, deciding what links to follow, but still the programmer of the web page had to decide what method to use to invoke the links, and what payload to pass. So, to my point, in a computer-to-computer interaction, the computer takes the role of the end user. Once more this is what we call an orchestrations service.</p>&#xA;&#xA;<p>I suppose we can use HATEOAS with either orchestration or choreography. </p>&#xA;&#xA;<p><strong>The API Gateway Pattern</strong></p>&#xA;&#xA;<p>Another interesting pattern is suggested by Chris Richardson who also proposed what he called an <a href=""http://www.infoq.com/articles/microservices-intro"" rel=""noreferrer"">API Gateway Pattern</a>.</p>&#xA;&#xA;<blockquote>&#xA;  <p>In a monolithic architecture, clients of the application, such as web&#xA;  browsers and native applications, make HTTP requests via a load&#xA;  balancer to one of N identical instances of the application. But in a&#xA;  microservice architecture, the monolith has been replaced by a&#xA;  collection of services. Consequently, a key question we need to answer&#xA;  is what do the clients interact with?</p>&#xA;  &#xA;  <p>An application client, such as a native mobile application, could make&#xA;  RESTful HTTP requests to the individual services [...] On the surface&#xA;  this might seem attractive. However, there is likely to be a&#xA;  significant mismatch in granularity between the APIs of the individual&#xA;  services and data required by the clients. For example, displaying one&#xA;  web page could potentially require calls to large numbers of services.&#xA;  Amazon.com, for example,&#xA;  <a href=""http://highscalability.com/amazon-architecture"" rel=""noreferrer"">describes</a> how some&#xA;  pages require calls to 100+ services. Making that many requests, even&#xA;  over a high-speed internet connection, let alone a lower-bandwidth,&#xA;  higher-latency mobile network, would be very inefficient and result in&#xA;  a poor user experience.</p>&#xA;  &#xA;  <p>A much better approach is for clients to make a small number of&#xA;  requests per-page, perhaps as few as one, over the Internet to a&#xA;  front-end server known as an API gateway.</p>&#xA;  &#xA;  <p>The API gateway sits between the application’s clients and the&#xA;  microservices. It provides APIs that are tailored to the client. The&#xA;  API gateway provides a coarse-grained API to mobile clients and a&#xA;  finer-grained API to desktop clients that use a high-performance&#xA;  network. In this example, the desktop clients makes multiple requests&#xA;  to retrieve information about a product, where as a mobile client&#xA;  makes a single request.</p>&#xA;  &#xA;  <p>The API gateway handles incoming requests by making requests to some&#xA;  number of microservices over the high-performance LAN. Netflix, for&#xA;  example,&#xA;  <a href=""http://techblog.netflix.com/2013/01/optimizing-netflix-api.html"" rel=""noreferrer"">describes</a>&#xA;  how each request fans out to on average six backend services. In this&#xA;  example, fine-grained requests from a desktop client are simply&#xA;  proxied to the corresponding service, whereas each coarse-grained&#xA;  request from a mobile client is handled by aggregating the results of&#xA;  calling multiple services.</p>&#xA;  &#xA;  <p>Not only does the API gateway optimize communication between clients&#xA;  and the application, but it also encapsulates the details of the&#xA;  microservices. This enables the microservices to evolve without&#xA;  impacting the clients. For examples, two microservices might be&#xA;  merged. Another microservice might be partitioned into two or more&#xA;  services. Only the API gateway needs to be updated to reflect these&#xA;  changes. The clients are unaffected.</p>&#xA;  &#xA;  <p>Now that we have looked at how the API gateway mediates between the&#xA;  application and its clients, let’s now look at how to implement&#xA;  communication between microservices.</p>&#xA;</blockquote>&#xA;&#xA;<p>This sounds pretty similar to the orchestration style mentioned above, just with a slightly different intent, in this case it seems to be all about performance and simplification of interactions.</p>&#xA;&#xA;<p><strong>Further Reading</strong></p>&#xA;&#xA;<p>There is a great series of articles recently published in the <a href=""https://www.nginx.com/blog/"" rel=""noreferrer"">NGINX Blog</a> that I recommend to delve deeper into all these concepts:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://www.nginx.com/blog/introduction-to-microservices"" rel=""noreferrer"">Introduction to Microservices</a></li>&#xA;<li><a href=""https://www.nginx.com/blog/building-microservices-using-an-api-gateway"" rel=""noreferrer"">Building Microservices: Using an API Gateway</a></li>&#xA;<li><a href=""https://www.nginx.com/blog/building-microservices-inter-process-communication"" rel=""noreferrer"">Building Microservices: IPC in a Microservices Architecture</a></li>&#xA;<li><a href=""https://www.nginx.com/blog/service-discovery-in-a-microservices-architecture"" rel=""noreferrer"">Service Discovery in a Microservices Architecture</a></li>&#xA;<li><a href=""https://www.nginx.com/blog/event-driven-data-management-microservices"" rel=""noreferrer"">Event-Driven Data Management for Microservices</a></li>&#xA;<li><a href=""https://www.nginx.com/blog/deploying-microservices"" rel=""noreferrer"">Choosing a Microservices Deployment Strategy</a></li>&#xA;<li><a href=""https://www.nginx.com/blog/refactoring-a-monolith-into-microservices"" rel=""noreferrer"">Refactoring a Monolith into Microservices</a></li>&#xA;</ul>&#xA;"
47760947,46615008,697630,2017-12-11T20:32:37,"<p>I believe there is a misunderstanding in the question in that you assume that event-driven architectures cannot be implemented on top of HTTP.</p>&#xA;&#xA;<p>An event-driven architecture may be implemented in many different ways and (when the architecture is that of a distributed system), on top of many different protocols.</p>&#xA;&#xA;<p>It can be implemented using a message broker (i.e. Kafka, RabbitMQ, ActiveMQ, etc) as you suggested it too. However, this is just a choice and certainly not the only way to do it.</p>&#xA;&#xA;<p>For example, the seminal book <a href=""http://shop.oreilly.com/product/0636920033158.do"" rel=""nofollow noreferrer"">Building Microservices</a> by Sam Newman, in Chapter 4: Integration, under Implementing Asynchronous Event-Based Collaboration says:</p>&#xA;&#xA;<blockquote>&#xA;  <p>“Another approach is to try to use HTTP as a way of propagating&#xA;  events. ATOM is a REST-compliant specification that defines semantics&#xA;  (among other things) for publishing feeds of resources. Many client&#xA;  libraries exist that allow us to create and consume these feeds. So&#xA;  our customer service could just publish an event to such a feed when&#xA;  our customer service changes. Our consumers just poll the feed,&#xA;  looking for changes. On one hand, the fact that we can reuse the&#xA;  existing ATOM specification and any associated libraries is useful,&#xA;  and we know that HTTP handles scale very well. However, HTTP is not&#xA;  good at low latency (where some message brokers excel), and we still&#xA;  need to deal with the fact that the consumers need to keep track of&#xA;  what messages they have seen and manage their own polling schedule.</p>&#xA;  &#xA;  <p>I have seen people spend an age implementing more and more of the&#xA;  behaviors that you get out of the box with an appropriate message&#xA;  broker to make ATOM work for some use cases. For example, the&#xA;  Competing Consumer pattern describes a method whereby you bring up&#xA;  multiple worker instances to compete for messages, which works well&#xA;  for scaling up the number of workers to handle a list of independent&#xA;  jobs. However, we want to avoid the case where two or more workers see&#xA;  the same message, as we’ll end up doing the same task more than we&#xA;  need to. With a message broker, a standard queue will handle this.&#xA;  With ATOM, we now need to manage our own shared state among all the&#xA;  workers to try to reduce the chances of reproducing effort. If you&#xA;  already have a good, resilient message broker available to you,&#xA;  consider using it to handle publishing and subscribing to events. But&#xA;  if you don’t already have one, give ATOM a look, but be aware of the&#xA;  sunk-cost fallacy. If you find yourself wanting more and more of the&#xA;  support that a message broker gives you, at a certain point you might&#xA;  want to change your approach.”</p>&#xA;</blockquote>&#xA;&#xA;<p>Likewise, if your design uses a message broker for the event-driven architecture, then I'm not sure if a circuit breaker is needed, because in that case the consumer applications control the rate at which event messages are being consumed from the queues. The producer application can publish event messages at its own pace, and the consumer applications can add as many competing consumers as they want to keep up with that pace. If the server application is down the client applications can still continue consuming any remaining messages in the queues, and once the queues are empty, they will just remain waiting for more messages to arrive. But that does not put any burden on the producer application. The producer and the consumer applications are decoupled in this scenario, and all the work the circuit breaker does in other scenarios would be solved by the message broker application.</p>&#xA;&#xA;<p>Somewhat similar can be said of the service discovery feature. Since the producer and the consumer do not directly talk to each other, but only through the message broker, then the only service you need to discover would be the message broker. </p>&#xA;"
47816698,47774291,697630,2017-12-14T15:17:47,"<p>Perhaps what you should do is to embrace asyncrony and eventual consistency in this case.</p>&#xA;&#xA;<p>Your HTTP endpoint, when invoked, actually does nothing, it only accepts the UI command, and enqueues it for other backend microservices to eventually process it, right?</p>&#xA;&#xA;<p>So, what you probably should do is just accept the command, validate it, enqueue it and just return a HTTP status code <a href=""https://httpstatuses.com/202"" rel=""nofollow noreferrer"">202 Accepted</a> to the client. You could include an ETA header in your response indicating more or less when the client can poll another endpoint to check if the results are ready. If you're following a <a href=""http://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven"" rel=""nofollow noreferrer"">truly RESTful approach</a>, you may include the link to the resource the client can use to poll the results when ready.</p>&#xA;&#xA;<p>You may want to read the following articles:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://farazdagi.com/2014/rest-and-long-running-jobs/"" rel=""nofollow noreferrer"">REST and long-running jobs</a></li>&#xA;<li><a href=""https://lostechies.com/jimmybogard/2013/05/15/eventual-consistency-in-rest-apis/"" rel=""nofollow noreferrer"">Eventual consistency in REST APIs</a></li>&#xA;<li><a href=""http://danielwhittaker.me/2014/10/27/4-ways-handle-eventual-consistency-ui/"" rel=""nofollow noreferrer"">4 Ways to Handle Eventual Consistency in the UI</a></li>&#xA;</ul>&#xA;"
48132731,47938835,697630,2018-01-06T22:57:52,"<p>What you seem to be talking about is how to deal with transactions in a distributed architecture. </p>&#xA;&#xA;<p>This is a very broad topic and entire books could written about this. Your question seems to be just about retrying the transactions, but I believe that alone is probably not enough to solve the problem of <em>distributed transactional workflow</em>.</p>&#xA;&#xA;<p>I believe you could probably benefit from gaining more understanding of concepts like:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://docs.microsoft.com/en-us/azure/architecture/patterns/compensating-transaction"" rel=""nofollow noreferrer"">Compensating Transactions Pattern</a></li>&#xA;<li><a href=""https://www.atomikos.com/Blog/TransactionsForSOA"" rel=""nofollow noreferrer"">Try/Cancel/Confirm Pattern</a></li>&#xA;<li><a href=""http://www.amundsen.com/downloads/sagas.pdf"" rel=""nofollow noreferrer"">Long Running Transactions</a> </li>&#xA;<li><a href=""https://blog.bernd-ruecker.com/saga-how-to-implement-complex-business-transactions-without-two-phase-commit-e00aa41a1b1b"" rel=""nofollow noreferrer"">Sagas</a></li>&#xA;</ul>&#xA;&#xA;<p>The idea behind <strong>compensating transactions</strong> is that every ying has its yang: if you have one transaction that can place an order, then you could  undo that with a transaction that cancels that order. This latter transaction is a <em>compensating transaction</em>. So, if you carry out a number of successful transactions and then one of them fails, you can trace back your steps and compensate every successful transaction you did and, as a result, revert their side effects.</p>&#xA;&#xA;<p>I particularly liked a chapter in the book <a href=""https://rads.stackoverflow.com/amzn/click/1441983023"" rel=""nofollow noreferrer"">REST from Research to Practice</a>. Its chapter 23 (<em>Towards Distributed Atomic Transactions over RESTful Services</em>) goes deep in explaining the <strong>Try/Cancel/Confirm pattern</strong>.</p>&#xA;&#xA;<p>In general terms it implies that when you do a group of transactions, their side effects are not effective until a transaction coordinator gets a confirmation that they all were successful. For example, if you make a reservation in Expedia and your flight has two legs with different airlines, then one transaction would reserve a flight with American Airlines and another one would reserve a flight with United Airlines. If your second reservation fails, then you want to compensate the first one. But not only that, you want to avoid that the first reservation is effective until you have been able to confirm both. So, initial transaction makes the reservation but keeps its side effects <em>pending to confirm</em>. And the second reservation would do the same. Once the transaction coordinator knows everything is reserved, it can send a confirmation message to all parties such that they confirm their reservations. If reservations are not confirmed within a sensible time window, they are automatically reversed but the affected system.</p>&#xA;&#xA;<p>The book <a href=""https://rads.stackoverflow.com/amzn/click/0321200683"" rel=""nofollow noreferrer"">Enterprise Integration Patterns</a> has some basic ideas on how to implement this kind of <strong>event coordination</strong> (e.g. see <a href=""http://www.enterpriseintegrationpatterns.com/patterns/messaging/ProcessManager.html"" rel=""nofollow noreferrer"">process manager pattern</a> and compare with <a href=""http://www.enterpriseintegrationpatterns.com/patterns/messaging/RoutingTable.html"" rel=""nofollow noreferrer"">routing slip pattern</a> which are similar ideas to <a href=""https://stackoverflow.com/a/30441546/697630"">orchestration vs choreography</a> in the Microservices world).</p>&#xA;&#xA;<p>As you can see, being able to compensate transactions might be complicated depending on how complex is your distributed workflow. The process manager may need to keep track of the state of every step and know when the whole thing needs to be undone. This is pretty much that idea of <strong>Sagas</strong> in the Microservices world. </p>&#xA;&#xA;<p>The book <a href=""https://www.manning.com/books/microservice-patterns"" rel=""nofollow noreferrer"">Microservices Patterns</a> has an entire chapter called Managing Transactions with Sagas that delves in detail on how to implement this type of solution.</p>&#xA;&#xA;<p>A few other aspects I also typically consider are the following:</p>&#xA;&#xA;<p><strong>Idempotency</strong></p>&#xA;&#xA;<p>I believe that a key to a successful implementation of your service transactions in a distributed system consists in making them <a href=""https://tools.ietf.org/html/rfc7231#page-23"" rel=""nofollow noreferrer"">idempotent</a>. Once you can guarantee a given service is idempotent, then you can safely retry it without worrying about causing additional side effects. However, just retrying a failed transaction won't solve your problems.</p>&#xA;&#xA;<p><strong>Transient vs Persistent Errors</strong></p>&#xA;&#xA;<p>When it comes to retrying a service transaction, you shouldn't just retry because it failed. You must first know why it failed and depending on the error it might make sense to retry or not. Some types of errors are transient, for example, if one transaction fails due to a query timeout, that's probably fine to retry and most likely it will succeed the second time; but if you get a database constraint violation error (e.g. because a DBA added a check constraint to a field), then there is no point in retrying that transaction: no matter how many times you try it will fail.</p>&#xA;&#xA;<p><strong>Embrace Error as an Alternative Flow</strong></p>&#xA;&#xA;<p>In those cases of interservice communication (computer-to-computer interactions) , when a given step of your workflow fails,  you don't necessarily need to undo everything you did in previous steps. You can just embrace error as part of you workflow. Catalog the possible causes of error and make them an alternative flow of events that simply requires human intervention. It is just another step in the full orchestration that requires a person to intervene to make a decision, resolve an inconsistency with the data or just approve which way to go.</p>&#xA;&#xA;<p>For example, maybe when you're processing an order, the payment service fails because you don't have enough funds. So, there is no point in undoing everything else. All we need is to put the order in a state that some problem solver can address it in the system and, once fixed, you can continue with the rest of the workflow.</p>&#xA;&#xA;<p><strong>Transaction and Data Model State are Key</strong></p>&#xA;&#xA;<p>I have discovered that this type of transactional workflows require a good design of the different states your model has to go through. As in the case of Try/Cancel/Confirm pattern, this implies initially applying the side effects without necessarily making the data model available to the users.</p>&#xA;&#xA;<p>For example, when you place an order, maybe you add it to the database in a ""Pending"" status that will not appear in the UI of the warehouse systems. Once payments have been confirmed the order will then appear in the UI such that a user can finally process its shipments.</p>&#xA;&#xA;<p>The difficulty here is discovering how to design transaction granularity in way that even if one step of your transaction workflow fails, the system remains in a valid state from which you can resume once the cause of the failure is corrected.</p>&#xA;&#xA;<p><strong>Designing for Distributed Transactional Workflows</strong></p>&#xA;&#xA;<p>So, as you can see, designing a distributed system that works in this way is a bit more complicated than individually invoking distributed transactional services. Now every service invocation may fail for a number of reasons and leave your distributed workflow in a inconsistent state. And retrying the transaction may not always solve the problem. And your data needs to be modeled like a state machine, such that side effects are applied but not confirmed until the entire orchestration is successful. </p>&#xA;&#xA;<p>That‘s why the whole thing may need to be designed in a different way than you would typically do in a monolithic client–server application. Your users may now be part of the designed solution when it comes to solving conflicts, and contemplate that transactional orchestrations could potentially take hours or even days to complete depending on how their conflicts are resolved.</p>&#xA;&#xA;<p>As I was originally saying, the topic is way too broad and it would require a more specific question to discuss, perhaps, just one or two of these aspects in detail. </p>&#xA;&#xA;<p>At any rate, I hope this somehow helped you with your investigation.</p>&#xA;"
36253555,27007353,1294878,2016-03-27T22:40:34,"<p>I would see each microservice as an Object, and as like any ORM , you use those objects to pull the data and then create joins within your code and query collections, Microservices should be handled in a similar manner. The difference only here will be each Microservice shall represent one Object at a time than a complete Object Tree. An API layer should consume these services and model the data in a way it has to be presented or stored.</p>&#xA;&#xA;<p>Making several calls back to services for each transaction will not have an impact as each service runs in a separate container and all these calles can be executed parallely.</p>&#xA;&#xA;<p>@ccit-spence, I liked the approach of intersection services, but how it can be designed and consumed by other services? I believe it will create a kind of dependency for other services.</p>&#xA;&#xA;<p>Any comments please?</p>&#xA;"
32395418,32392202,5300254,2015-09-04T10:08:28,"<p>I do not see the point 'share nothing' in this scenario. As long as you will hold your De/Serializer as an Artifact in some nexus, you do not ""share"" anything, instead you are using an (somehow) external library.&#xA;If you use e.g. logging, both of your projects will use the e.g. slf4s, but they do not share it, as each uses it separately.</p>&#xA;"
45762702,45762128,1981471,2017-08-18T18:12:37,"<p>My thoughts:</p>&#xA;&#xA;<p>Your first microservice is going to be </p>&#xA;&#xA;<pre><code>GetOrders(int userId)&#xA;</code></pre>&#xA;&#xA;<p>In it you need to do a check for which company to pull by userId</p>&#xA;&#xA;<p>Your second call is going to be</p>&#xA;&#xA;<pre><code>GetTransactions(int orderId)&#xA;</code></pre>&#xA;&#xA;<p>Depending on how much security you want, you can also do:</p>&#xA;&#xA;<pre><code>GetTransactions(int orderId, int userId)&#xA;</code></pre>&#xA;&#xA;<p>To back check that the user making the call belongs to the correct company in case of API abuse.  So basically, yes you have to check that the user belongs to the company that they are getting the transaction for.</p>&#xA;"
51953179,51953109,6121503,2018-08-21T16:37:42,"<p>A simple answer would be, that a back-end developer does everything that a front-end developer does not. Back-end development would typically include building REST APIs, networking/system devops, database management, cloud infrastructure management, etc. Back-end would imply things are not seen by users but are necessary for projects to run, and front-end would imply the opposite; things are directly seen and interacted with by users.</p>&#xA;&#xA;<p>A lot of developers are full-stack developers, or somewhere in between front-end and back-end developer. They do a little front-end, and a little back-end development as well.</p>&#xA;"
40851766,40784804,2730527,2016-11-28T19:14:53,"<p>Your services should connect to an agent on localhost.</p>&#xA;&#xA;<p>Consul servers should be run in a cluster. A 3 node cluster can handle 1 node failure, a 5 node cluster can handle 2 nodes failing. See <a href=""https://www.consul.io/docs/internals/consensus.html#deployment_table"" rel=""nofollow noreferrer"">https://www.consul.io/docs/internals/consensus.html#deployment_table</a></p>&#xA;"
37685730,37679132,2730527,2016-06-07T17:32:27,"<p>@dshuld if you are using the Angel release train or version 1.0.x you need to make each instance have a unique id. See the <a href=""http://projects.spring.io/spring-cloud/docs/1.0.3/spring-cloud.html#_eureka_metadata_for_instances_and_clients"" rel=""nofollow"">docs here</a>. Something like:</p>&#xA;&#xA;<pre><code>eureka:&#xA;  instance:&#xA;    metadataMap:&#xA;      instanceId: ${spring.application.name}:${spring.application.instance_id:${server.port}}&#xA;</code></pre>&#xA;&#xA;<p>For the Brixton release train (1.1.x) it should have a sensible default.</p>&#xA;"
37842520,37839701,2730527,2016-06-15T17:56:20,"<p>Spring Cloud integrates with some http clients, like you mentioned. Ribbon has some <a href=""https://github.com/Netflix/ribbon/tree/master/ribbon-transport/src/main/java/com/netflix/ribbon/transport/netty"" rel=""nofollow"">non-http clients/transports built in</a>, but I haven't used that and AFIAK, netflix doesn't use them either.</p>&#xA;&#xA;<p>You can use the Spring Cloud <a href=""http://cloud.spring.io/spring-cloud-static/spring-cloud.html#_using_the_ribbon_api_directly"" rel=""nofollow""><code>LoadBalancerClient</code> interface directly</a>. It gives you access to a host and port, then you can use any client you want.</p>&#xA;&#xA;<pre><code>public class MyClass {&#xA;    @Autowired&#xA;    private LoadBalancerClient loadBalancer;&#xA;&#xA;    public void doStuff() {&#xA;        ServiceInstance instance = loadBalancer.choose(""myService"");&#xA;        String host = instance.getHost();&#xA;        int port = instance.getPort();&#xA;        // ... do something with the host and port&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>I also did a sample integration with <a href=""https://github.com/spring-cloud-incubator/okhttp-ribbon"" rel=""nofollow"">OkHttp</a>.</p>&#xA;"
33289093,33287495,2730527,2015-10-22T19:27:05,"<blockquote>&#xA;  <ol start=""2"">&#xA;  <li>Any issues/ complexities with running Netflix cloud on Weblogic 12c?</li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p>I don't know of anyone running Spring Cloud Netflix on Weblogic. Spring Cloud requires spring boot, so you would need to follow the <a href=""http://docs.spring.io/spring-boot/docs/current/reference/html/howto-traditional-deployment.html#howto-weblogic"" rel=""noreferrer"">docs there</a>.</p>&#xA;&#xA;<blockquote>&#xA;  <ol>&#xA;  <li>Along with WL upgrade, is orcale DB (currently 11g) upgrade required? 3. Does WL 12c supports JDBC 4.1 and 4.2 and any dependency for Netflix OSS products on these JDBC versions?</li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p>Netflix OSS does not require oracle/jdbc at all.</p>&#xA;&#xA;<blockquote>&#xA;  <ol start=""4"">&#xA;  <li>How can Eureka and Ribbon be used along with WL cluster load balancing?</li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p>Eureka is for service registration and discovery. Ribbon is a client-side load balancer. On the surface these seem at odds with WL cluster load balancing. Eureka is built for many, ephemeral nodes to be created and destroyed at any moment. The answer probably depends on your design, not just technical components.</p>&#xA;&#xA;<blockquote>&#xA;  <ol start=""5"">&#xA;  <li>Is Apache WL plugin required anymore? at-least for session stikiness?</li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p>If you continue to have WL manage sessions, then yes.</p>&#xA;"
35776050,35772518,2730527,2016-03-03T15:19:15,"<p>Currently, you would need to put config server behind a load balancer. It is stateless, so that wouldn't hurt. There is an <a href=""https://github.com/spring-cloud/spring-cloud-config/issues/342"" rel=""nofollow"">open issue</a> to configure multiple config server url's in the client, so it could do failover there.</p>&#xA;"
25601048,25600580,10272,2014-09-01T07:45:11,"<p>Regarding common code, the best practice it to use a packaging system. So if you use Java, then use maven, if you use Ruby then Gems, if python then pypi etc.&#xA;Ideally a packaging system adds little friction so you may have a (say, git) repository for a common lib (or several common-libs for different topics) and publish their artifacts through an artifact repository (e.g. private maven/gems/pypi). Then at the microservice you add dependency on the required libs.So code reuse is easy.&#xA;In some cases packaging systems do add some friction (maven for one) so one might prefer using a single git repo for everything and a multi-module project setup. That isn't as clean as the first approach but works as well and not too bad.&#xA;Other options are to use git submodule (less desired) or git subtree (better) in order to include the source code in a single ""parent"" repository.</p>&#xA;&#xA;<p>Regarding schema - if you want to play by the book, then each microservice has its own database. They don't touch each other's data. This is a very modular approach which at first seems to add some friction to your process but eventually I think you'll thank me. It will allow fast iteration over your microservices, for example you might want to replace one database implementation with another database implementation for one specific service. Imagine doing this when all your services use the same database! Good luck with that... But if each single service uses it's own database the service abstracts the database correctly (e.g. it does not accept SQL queries as API calls for example ;-)) then changing mysql to Cassandra suddenly become feasible.&#xA;There are other upsides to having completely isolated databases, for example load and scaling, finding out bottlenecks, management etc.</p>&#xA;&#xA;<p>So in short - common code (utilities, constants etc) - use a packaging system or some source code linkage such as git-tree</p>&#xA;&#xA;<p>Database - you don't touch mine, I don't touch yours. That's the better way around this.</p>&#xA;&#xA;<p>HTH, Ran.</p>&#xA;"
48637092,48635782,5519648,2018-02-06T06:54:22,"<p>Another tool might be <a href=""https://www.consul.io"" rel=""noreferrer"">Consul</a>. </p>&#xA;&#xA;<p>Eureka is mostly a service discovery tool and mostly designed to use inside AWS infrastructure. </p>&#xA;&#xA;<p>Zookeeper is a common-purpose distributed key/value store which can be used for service-discovery in conjunction with <a href=""https://curator.apache.org/curator-x-discovery/index.html"" rel=""noreferrer"">curator-x-discovery framework</a></p>&#xA;&#xA;<p>Here is a brief <a href=""http://daviddawson.me/getting/started/with/microservices/2015/06/10/service-discovery-overview.html"" rel=""noreferrer"">overview of service-discovery solutions</a></p>&#xA;&#xA;<p>You can also find comparison of <a href=""https://stackshare.io/stackups/consul-vs-eureka-vs-zookeeper"" rel=""noreferrer"">Consul vs Eureka vs Zookeeper</a> here.</p>&#xA;&#xA;<p>Although Consul is as well as zookeeper - can be used not only for discovery but as a key/value store, the advantages of Consul are cool service discovery features out of the box</p>&#xA;&#xA;<ol>&#xA;<li>DNS out of the box</li>&#xA;<li>convenient RESTful API</li>&#xA;<li>HealthCheck API out of the box</li>&#xA;</ol>&#xA;&#xA;<p>Also consul has more distributed nature: agents are installed on all service VMs and hence the system has higher availability than zookeeper.&#xA;Be aware that consul system has <a href=""https://www.consul.io/docs/guides/datacenters.html"" rel=""noreferrer"">low coupling between datacenters</a>.</p>&#xA;&#xA;<p>Zookeeper is mature, but too generic. So you can use zookeeper not only for service discovery but for storing configs, distributed locks, notifications etc. Again, it's convenient to use all this functionality with Curator Framework / Curator Recipes.</p>&#xA;&#xA;<p>Zookeeper is using master/slave communication schema between nodes in cluster. Master is elected by cluster members. Be aware that there could be edge cases (due to network issues for example) when it appears more that 1 master in the cluster. In this case restart of the cluster helps. </p>&#xA;&#xA;<p>Difference of Eureka from Zookeeper and Consul is that Eureka is narrow-purpose system - service discovery and load balancing system.</p>&#xA;&#xA;<p>All 3 systems can be integrated with Spring.</p>&#xA;"
46077596,46067333,3423520,2017-09-06T14:16:06,"<p>What you need is <a href=""https://steeltoe.io/"" rel=""nofollow noreferrer"">Steeltoe</a>.</p>&#xA;&#xA;<p>Steeltoe client libraries enable .NET Core and .NET Framework apps to easily leverage Netflix Eureka, Hystrix, Spring Cloud Config Server, and Cloud Foundry services.</p>&#xA;"
48426100,38786207,4870176,2018-01-24T15:29:55,"<p>What we do is as follows:</p>&#xA;&#xA;<p>Share common jar which contains exceptions with both microservices.</p>&#xA;&#xA;<p>1.) In microservices A convert exception to a DTO class lets say ErrorInfo.&#xA;Which will contain all the attributes of your custom exception with a String exceptionType, which will contain exception class name.</p>&#xA;&#xA;<p>2.) When it is received at microservice B it will be handled by ErrorDecoder in microservice B and It will try to create an exception object from exceptionType as below:</p>&#xA;&#xA;<pre><code>@Override&#xA;public Exception decode(String methodKey, Response response) {       &#xA;&#xA;ErrorInfo errorInfo = objectMapper.readValue(details, ErrorInfo.class);&#xA;Class exceptionClass;&#xA;&#xA;Exception decodedException;&#xA;&#xA;try {&#xA;&#xA;    exceptionClass = Class.forName(errorInfo.getExceptionType());  &#xA;&#xA;    decodedException = (Exception) exceptionClass.newInstance();&#xA;&#xA;    return decodedException;&#xA;&#xA; }&#xA;&#xA; catch (ClassNotFoundException e) {&#xA;&#xA;    return new PlatformExecutionException(details, errorInfo);&#xA;&#xA; }&#xA;  return defaultErrorDecoder.decode(methodKey, response);&#xA; }&#xA;</code></pre>&#xA;"
51835952,51835569,1166880,2018-08-14T07:30:08,"<p>If you have different microservices, as in different applications, you will not need bundles. You can keep them in different repositories, but a common practice is to use a so called mono-repository. As the name suggests, with a mono-repository you keep all of the projects in a single repository. This has the benefit that changes spanning all projects can be done more easily and in sync. The drawback is that it requires more effort when managing and might cause additional overhead when building and deploying as it will not be easy to see which service has changed so must likely you rebuild all of them. There are a few books and presentations on mono-repositories you might want to check out. In short, Symfony does not restrict how you manage your services. You can have a single repository for all projects or multiple repositories.</p>&#xA;&#xA;<p>If you want to serve all ""services"" through the same application, even without bundles, you can do so by using namespaces to separate the logic, e.g. for controllers:</p>&#xA;&#xA;<pre><code>my_app&#xA;- src&#xA;  - Controller&#xA;    - Alpha&#xA;      - IndexController&#xA;    - Beta&#xA;      - IndexController&#xA;</code></pre>&#xA;&#xA;<p>This should work out of the Box with the default configuration and even if you deviate you can make things like argument resolvers work by just pointing the configuration to the correct folder. Obviously this will require you to make sure that code is not shared between services should you ever want to extract them into their own application. There are some static code analyis tools that help you with keeping your architecture clean, i.e. make sure Alpha does not use code from Gamma and vice versa.</p>&#xA;&#xA;<p>If you want to separate the apps more clearly by doing something like this:</p>&#xA;&#xA;<pre><code>my_app&#xA;- src&#xA;  - AlphaApp&#xA;    - ...&#xA;  - BetaApp&#xA;    - ...&#xA;</code></pre>&#xA;&#xA;<p>You can still do that but it will require more manual work and the recipes will not work anymore, requiring you to do manual changes to most configurations and moving around files. How to do it depends on whether you want a shared kernel or a separate kernel for each service, but if you go that route I recommend keeping separate projects in the same repository, as it will probably yield cleaner results and be less work.</p>&#xA;"
40498760,40485481,1499936,2016-11-09T00:43:29,"<p>We have a javascript front end (so this may not be applicable to you)- which means that there's a ton of front end library files etc.  This was a nightmare to copy across to the cluster for testing and took forever.  Theres a couple of ways I've been able to speed things up.</p>&#xA;&#xA;<p>One is by keeping all the front end files in a separate project and using a build step to copy them across into the asp.net core project.  So only the bundled/minified files are copied into the cluster when deploying.</p>&#xA;&#xA;<p>Another option is to host these front-end files with a local node http-server which watches for changes etc and keep a static environment file where you can set the ip/hostname of your local cluster thats running. I use fiddler to redirect the hostname to the local ip, this way you can use the urls that you will use in production, which is handy.  You'll need to set up cors though, which wasn't a problem for us.  </p>&#xA;&#xA;<p>So yes, definitely possible.</p>&#xA;"
46692058,46689798,6679072,2017-10-11T15:22:41,"<p><code>localhost:8090/login</code> is a direct request to the UI app, no Zuul involved.</p>&#xA;&#xA;<p>If it's not loading / displaying the page, it could be an error while starting the application or <code>/login</code> doesn't exist.</p>&#xA;&#xA;<p>Do you mind sharing the logs?</p>&#xA;"
43940980,43927492,6679072,2017-05-12T15:06:40,"<p>An approach would be to use / deploy an app which maps paths / urls to json response files. I personally haven't used it but I believe <a href=""http://wiremock.org/"" rel=""nofollow noreferrer"">http://wiremock.org/</a> might help you</p>&#xA;"
43855915,43850359,6679072,2017-05-08T19:33:19,"<p>Without going into the details of choosing between a monolithic app versus a Microservice architecture, what your diagram is missing a Registration and Discovery service such as Spring Cloud Netflix Eureka or Consul.</p>&#xA;&#xA;<p>Services would register with Eureka then they get other services metadata (host, port service listen on) from Eureka.</p>&#xA;&#xA;<p>A detailed tutorial could be found at <a href=""http://tech.asimio.net/2016/11/14/Microservices-Registration-and-Discovery-using-Spring-Cloud-Eureka-Ribbon-and-Feign.html"" rel=""nofollow noreferrer"">Microservices Registration and Discovery using Spring Cloud, Eureka, Ribbon and Feign</a>, a blog post I published a few moths ago.</p>&#xA;"
42631718,42571279,6679072,2017-03-06T17:18:54,"<p>You should never break backwards compat.&#xA;If changes are going to break backwards compatibility then version the service.</p>&#xA;&#xA;<p>I have blogged about how to register and discover multiple versions of a service using <code>Spring Cloud Netflix Eureka</code> and <code>Ribbon</code> at <a href=""http://tech.asimio.net/2017/03/06/Multi-version-Service-Discovery-using-Spring-Cloud-Netflix-Eureka-and-Ribbon.html"" rel=""nofollow noreferrer"">http://tech.asimio.net/2017/03/06/Multi-version-Service-Discovery-using-Spring-Cloud-Netflix-Eureka-and-Ribbon.html</a>.</p>&#xA;"
48835900,48805353,6679072,2018-02-16T22:44:33,<p>The <code>RestTemplate</code> instance might have been configured to lookup the service <code>localhost</code> via <code>Eureka</code> instead of assuming <code>http://localhost:....</code> is already the <code>URL</code></p>&#xA;
48969995,48962157,6679072,2018-02-25T03:40:45,"<p>In order to register <code>Postgres</code>, <code>Elastic Search</code>, etc. or in-house non-<code>JVM</code> services you would have to implement the <code>Sidecar</code> pattern, a companion application to the main services that serves has a mediator between the main service and <code>Eureka</code>, for instance.</p>&#xA;&#xA;<p>To do so using <code>Docker</code> it's a little bit tricky because it's a suggested practice for a <code>Docker</code> container to run just one process but using a <code>Sidecar</code> along with the main service you would have to either run two process or make changes / provide implementation in the <code>Sidecar</code> application to support the <code>Sidecar</code> and <code>Postgres</code> to run in different <code>Docker</code> containers.</p>&#xA;&#xA;<p>I recently blogged about this exact topic at <a href=""http://tech.asimio.net/2018/02/20/Microservices-Sidecar-pattern-implementation-using-Postgres-Spring-Cloud-Netflix-and-Docker.html"" rel=""nofollow noreferrer"">Microservices Sidecar pattern implementation using Postgres, Spring Cloud Netflix and Docker</a>.</p>&#xA;&#xA;<p>I decided to run both, the <code>Sidecar</code> app and <code>Postgres</code> in the same container but I might follow up on this in the future.</p>&#xA;"
44145237,44124914,6679072,2017-05-23T21:20:33,"<p>I have blogged about this: <a href=""http://tech.asimio.net/2017/03/06/Multi-version-Service-Discovery-using-Spring-Cloud-Netflix-Eureka-and-Ribbon.html"" rel=""nofollow noreferrer"">Multi-version Service Discovery using Spring Cloud Netflix Eureka and Ribbon</a>, focussed on <code>Spring Cloud Netflix</code> components / libraries though.</p>&#xA;&#xA;<p>But the idea is to deploy a new version of the artifact / binary in a new host / VPS / Container and have the service register with a registry server (Eureka, Consul, ....) and include metadata about the API versions it supports (v1, v2, ...). Client apps would discover which host / container / ... serves the API version needed.</p>&#xA;"
44145491,44113228,6679072,2017-05-23T21:39:14,"<p><code>Zuul</code> as a <code>Proxy</code> server not only route requests but <code>Zuul</code> filters could also be used for handling crosscutting concerns like:</p>&#xA;&#xA;<ul>&#xA;<li>geolocation</li>&#xA;<li>token decryption</li>&#xA;<li>authentication</li>&#xA;<li>request / response manipulation</li>&#xA;<li>Traffic shaping</li>&#xA;</ul>&#xA;&#xA;<p>You mention <code>Feign</code> clients and that would work (to the best of my knowledge) with <code>Java</code>-based front-end apps, what if the front-end app is developed with <code>Angular</code> or <code>React</code>?</p>&#xA;"
37739156,37738759,858913,2016-06-10T02:46:12,"<p>if want you need is something to help you on rate limiting, I would recommend using a proxy rotation service, Scrapy won't be necessary if you already have your crawler ready.</p>&#xA;&#xA;<p>I would recommend <a href=""http://crawlera.com/"" rel=""nofollow"">Crawlera</a> or <a href=""http://proxymesh.com/"" rel=""nofollow"">proxymesh</a>.</p>&#xA;"
47565915,33125508,1121825,2017-11-30T04:31:25,"<p>You should also read about EUREKA</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>Amazon ELB provides the EC2 instances to your service requests based on Load balancer and the IP addresses of the EC2 instances are not consistent so you can also use EUREKA which does the same job but is based on service registry and client side load balancing in which the Application client for each region has the registry. &#xA;You can read more about it here : &#xA;<a href=""https://github.com/Netflix/eureka/wiki/Eureka-at-a-glance"" rel=""nofollow noreferrer"">https://github.com/Netflix/eureka/wiki/Eureka-at-a-glance</a></p>&#xA;"
50227690,50227462,3880225,2018-05-08T06:59:20,<p>you should check mockito or any other mock framework&#xA;just record and serialize the result e.g. with xstream and respond with the deserialized xstream result and modify it slightly by your needs.</p>&#xA;&#xA;<p>This is the quickest solution for mocking remote services.</p>&#xA;
51518876,51515839,7770145,2018-07-25T12:26:08,"<p>When connecting to a server on the same machine, use mongodb://localhost:27017.</p>&#xA;"
44386652,35751630,6662689,2017-06-06T09:46:18,"<p>I had the same problem. This solved it for me:</p>&#xA;&#xA;<p>Regarding to <a href=""http://cloud.spring.io/spring-cloud-static/spring-cloud-netflix/1.3.0.RC1/#Retrying"" rel=""nofollow noreferrer"">this article</a> ribbon is only retring when the http client is set to ribbon's restclient. On default ribbon is using the Apache http client which does not retry any request.</p>&#xA;&#xA;<p>Due the fact that ribbon's restclient is deprecated you should consider using spring-retry (<a href=""https://github.com/spring-projects/spring-retry"" rel=""nofollow noreferrer"">https://github.com/spring-projects/spring-retry</a>)</p>&#xA;&#xA;<p>Keep in mind that you also have to handle the hystrix timeouts for zuul as well when you configure your retries on ribbon.</p>&#xA;"
50280551,50280470,309793,2018-05-10T19:52:45,"<p>Tonz of possible solutions. </p>&#xA;&#xA;<p>1) Try to add the new layer like Nginx reverse proxy in front of the backends. </p>&#xA;&#xA;<p>2) Pretty much the same. Use API gateway.</p>&#xA;&#xA;<p>3) Call the backend B from the backend A directly and create new endpoints in the backend A</p>&#xA;&#xA;<p>4) Duplicate data and logic</p>&#xA;&#xA;<p>5) extract one more backend</p>&#xA;&#xA;<p>6) etc.</p>&#xA;&#xA;<p>7) etc... </p>&#xA;&#xA;<p>It really depends on many factors like the common architecture, code organization, development rules, tooling... There is no one silver bullet. </p>&#xA;"
51037235,50932766,6935841,2018-06-26T07:29:25,"<p>I guess this is a problem of the dns-lookup inside the Docker-Container, try to start a bash  ....<code>docker container exec -it &lt;yourcontainer&gt; bash</code>... and check if a nslookup at the SBA fqdn works </p>&#xA;&#xA;<p>In SBA 2.0.1 the property has changed ... it is spring.boot.admin.<strong>client</strong>.url &#xA;NOT spring.boot.admin.url </p>&#xA;"
46171272,46131443,5449250,2017-09-12T08:15:49,"<p>If you build and run project modules manually (e.g. from IDE) you should rename all ""config"" hostnames in <strong>bootstrap.yml</strong> files to ""localhost"". </p>&#xA;&#xA;<p>The issue is, this project <a href=""https://github.com/sqshq/PiggyMetrics#how-to-run-all-the-things"" rel=""nofollow noreferrer"">uses Docker Compose</a> to run things</p>&#xA;&#xA;<blockquote>&#xA;  <p>Development mode</p>&#xA;  &#xA;  <p>If you'd like to build images yourself (with some changes in the code, for example), you have to clone all repository and build artifacts with maven. Then, run docker-compose -f docker-compose.yml -f docker-compose.dev.yml up</p>&#xA;  &#xA;  <p>docker-compose.dev.yml inherits docker-compose.yml with additional possibility to build images locally and expose all containers ports for convenient development.</p>&#xA;</blockquote>&#xA;&#xA;<p>and Docker Compose <a href=""https://docs.docker.com/compose/networking/"" rel=""nofollow noreferrer"">allows you to use service name as hostname</a>.</p>&#xA;"
51317141,51304242,9779490,2018-07-13T03:12:21,"<p>I found way to do this finally:</p>&#xA;&#xA;<pre><code>@GetMapping(params = { ""id"" })&#xA;public Limit getLimitByid(@RequestParam Long id, @DateTimeFormat(iso = DateTimeFormat.ISO.DATE) @RequestParam(name = ""asOfDate"") Optional&lt;LocalDate&gt; opAsOfDate) throws EntityNotFoundException, ApiException {&#xA;    LocalDate asOfDate = opAsOfDate.orElse(LocalDate.now());&#xA;    return limitService.getLimitById(id, asOfDate);&#xA;}&#xA;&#xA;@GetMapping({""/{pid}""})&#xA;public Limit getLimitByPid(@PathVariable Long pid) throws EntityNotFoundException, ApiException {&#xA;    return limitService.getLimitByPid(pid);&#xA;}&#xA;</code></pre>&#xA;"
47197911,47191332,3950370,2017-11-09T09:15:18,"<p>If I understand your question correctly, you are talking about a shared environment of multiple projects, that may not be related in that sense, that they are built together. For such an environment you should indeed develop a plugin with a common codebase.</p>&#xA;&#xA;<p>The easy solution for such a plugin is a so-called <em>script plugin</em>, which is basically a Gradle script just like <code>build.gradle</code>. It can be included via:</p>&#xA;&#xA;<pre><code>apply from: 'path/to/script.gradle'&#xA;// or&#xA;apply from: 'http://my.domain.tld/script.gradle'&#xA;</code></pre>&#xA;&#xA;<p>The more advanced but therefor more complicated solution would be the development of a <em>binary plugin</em>. You could provide the plugin via any repository and include it like any other Gradle plugin.</p>&#xA;"
45598949,45598647,3888162,2017-08-09T19:16:49,"<p>In my opinion building a microservice for this is unnecessary complication of your architecture.</p>&#xA;&#xA;<p>For decoupling you can simply use a <code>Serilog</code> sink for <code>Seq</code></p>&#xA;&#xA;<p>There are sinks for <code>Serilog</code> for <code>Seq</code>:<br>&#xA;<a href=""https://github.com/serilog/serilog-sinks-seq"" rel=""nofollow noreferrer"">https://github.com/serilog/serilog-sinks-seq</a><br>&#xA;<a href=""https://www.nuget.org/packages/Serilog.Sinks.Seq/"" rel=""nofollow noreferrer"">https://www.nuget.org/packages/Serilog.Sinks.Seq/</a></p>&#xA;&#xA;<p>There are also <code>Seq</code> appenders for <code>log4net</code> and <code>NLog</code> here <a href=""https://github.com/datalust/seq-client"" rel=""nofollow noreferrer"">https://github.com/datalust/seq-client</a></p>&#xA;"
50526957,50506101,5790498,2018-05-25T10:20:46,"<p>As mentioned by @g00glen00b in comments Eureka is not used for communication between microservices. Its for service discovery.&#xA;There are two ways that I know ofthrough which you can communicate with other Microservices :</p>&#xA;&#xA;<ol>&#xA;<li>RestTemplate</li>&#xA;<li>Feign Client</li>&#xA;</ol>&#xA;&#xA;<p>RestTemplate is very simple to use. It does not require configurations.</p>&#xA;&#xA;<p>e.g.</p>&#xA;&#xA;<pre><code>   ResponseType obj=  new RestTemplate().getForObject(URL, ResponseType.class, params);&#xA;</code></pre>&#xA;&#xA;<p>url - the URL</p>&#xA;&#xA;<p>responseType - the type of the return value</p>&#xA;&#xA;<p>params- the variables to expand the template</p>&#xA;&#xA;<p>Spring Doc <a href=""https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/web/client/RestTemplate.html"" rel=""nofollow noreferrer"">link</a> for your reference</p>&#xA;"
50677219,50673143,5790498,2018-06-04T09:19:50,"<p>To print error logs and info/debug logs in different files. You have to add tow different configuration your log4j/log4j2/logback file. Create different appender/logger for logging different levels of logs.</p>&#xA;&#xA;<p>e.g. for Log4j :</p>&#xA;&#xA;<pre><code>##############For errors######################&#xA;# Define the root logger with appender file&#xA;log4j.rootLogger = ERROR, FILE, ALERT&#xA;# Define the file appender&#xA;log4j.appender.FILE=org.apache.log4j.FileAppender&#xA;# Set the name of the file&#xA;log4j.appender.FILE.File=D:\\application.log&#xA;# Set the immediate flush to true (default)&#xA;log4j.appender.FILE.ImmediateFlush=true&#xA;# Set the threshold to debug mode&#xA;log4j.appender.FILE.Threshold=debug&#xA;# Set the append to false, overwrite&#xA;log4j.appender.FILE.Append=false&#xA;# Define the layout for file appender&#xA;log4j.appender.FILE.layout=org.apache.log4j.PatternLayout&#xA;log4j.appender.FILE.layout.conversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n&#xA;&#xA;##############For Alerts######################&#xA;log4j.appender.ALERT=org.apache.log4j.FileAppender&#xA;log4j.appender.ALERT.File=D:\\alert.log&#xA;log4j.appender.ALERT.Threshold=fatal&#xA;log4j.appender.ALERT.layout=org.apache.log4j.PatternLayout&#xA;log4j.appender.ALERT.layout.ConversionPattern=%-4r [%t] %-5p %c %x - %m%n&#xA;</code></pre>&#xA;&#xA;<p>Above configuration is from my practice project. Update above configs according to your need. You can refer this link as well : <a href=""https://stackoverflow.com/questions/39103771/spring-boot-multiple-log-files"">Spring boot multiple log files</a></p>&#xA;"
50692275,50678338,5790498,2018-06-05T04:52:14,"<p>Manmay saying is correct. You should go with first approach for long term gain.&#xA;If you still want alternative, then you can combine both of these approach by configuring your API gateway in such a way that,  It will route your request </p>&#xA;&#xA;<ul>&#xA;<li>/api/users -> /api/data/users</li>&#xA;<li>/api/user_types -> /api/dictionary/user_types</li>&#xA;<li>/api/roles -> /api/data/roles</li>&#xA;<li>/api/role_types -> /api/dictionary/role_types</li>&#xA;</ul>&#xA;&#xA;<p>By this approach, you will not have to compromise any of the concerns like maintenance or client side changes.</p>&#xA;"
50443399,50310975,5790498,2018-05-21T06:48:05,"<p>As I'm from java background I don't know the libraries and bounded context related to .Net mentioned above. But I will try to answer this question in generic way. &#xA;To solve this issue you can store address related tables in different database which can be accessed by all these three microservices. </p>&#xA;&#xA;<p>As lookup tables for each microservice will have same tables/data, so you can use same lookup tables for all microservices. If your address table have different columns then you can store address for each microservice in different table else you can store data in same table by storing address_type for each microservice.</p>&#xA;"
50442608,50406887,5790498,2018-05-21T05:41:29,"<blockquote>&#xA;  <p>To answer to your all three cases, it will purely depend on&#xA;  microservices architecture.</p>&#xA;</blockquote>&#xA;&#xA;<p><strong>First case</strong> -> According to this case, your client have to make consolidated response by making call to different microservices. This is the overhead for client which is bad architectural approach.</p>&#xA;&#xA;<p><strong>Second case</strong> -> This is good approach compared to first one. If you have independent database/tables (without any direct relation) then this would be good approach. Just you have to store references for orderId/userId in respective tables to get these details. Your total request time will group but your user module will work independently of order modules. With this approach you will achieve loose coupling.</p>&#xA;&#xA;<p><strong>Third case</strong> -> If you don't have different databases for each microservices then this would be your best approach as it will reduce number of calls to database as well as to other microservice. You can get direct details by implementing service methods for each required model.</p>&#xA;&#xA;<blockquote>&#xA;  <p>which case is most used?</p>&#xA;</blockquote>&#xA;&#xA;<p><strong>Ans.</strong>&#xA;I don't think any one uses first approach as its not good practice. If you have different databases for different microservices then second case would be your answer. If you have single database then you go ahead with third case as well. But your service layer code will be duplicated over different microservices.</p>&#xA;&#xA;<blockquote>&#xA;  <p>In case of microservice and web api application - each microservice contains only one or maybe two controllers?</p>&#xA;</blockquote>&#xA;&#xA;<p>There is no standard for number of controllers in each microservice. It will depend on size of microservice and responsibilities performed by that microservice.</p>&#xA;"
47182866,47181503,4284706,2017-11-08T14:58:05,"<p>I think an else block can do that</p>&#xA;&#xA;<pre><code> @DeleteMapping(""{id}"") fun delete(@PathVariable id: Long): ResponseEntity&lt;Unit&gt; { &#xA;   try {&#xA;             if (dogRepository.exists(id)) { &#xA;                dogRepository.delete(id)&#xA;                return ResponseEntity.ok().build()&#xA;            } else {&#xA;                return ResponseEntity.notFound().build()&#xA;            } &#xA;    } catch (e: Exception) { return ResponseEntity.notFound().build() }&#xA; }&#xA;</code></pre>&#xA;"
48005414,47145930,3826210,2017-12-28T10:04:01,"<p>Are you sure that she was not talking about incorporating the version into the <strong>name</strong> of the service or into the <strong>name</strong> of the endpoint?&#xA;Service called OrderProcessing_2_4_1 with a versioned endpoint of get_order_2_4_1  is a very bad idea. OrderProcessing_2_4 with a versioned endpoint of get_order_2_4 is a little less evil but still problematic.</p>&#xA;&#xA;<p>if your calls to microservices have to address endpoints that have a version number in the name, you will have a maintenance (and production) nightmare every time you change a service. Every other service and client will have to be checked to change any reference to your updated service.</p>&#xA;&#xA;<p>That is different from having versions for your API, code pr services. Those are required if you are going to actually get many of the benefits of microservices.</p>&#xA;&#xA;<p>Your orchestration function has to be able to find the right version of the service that matches the client's version requirements (Version 2.6.2 of client ""Enter New Order"" app requires service ""OrderProcessing"" that is at least version 2.4.0 in order to support the NATO product classification). </p>&#xA;&#xA;<p>You may have multiple versions of a service in production at the same time (2.4.0 being deprecated but still servicing some clients, 2.4.1 being introduced, version 3.0.0 in beta for customers who are testing the newest UI and features prior to GA). </p>&#xA;&#xA;<p>This is particularly true if you run 24/7 and have to dynamically update services.&#xA;The orchestration function is the place where services and endpoints are matched up and when you introduce a new version of a service, you update the orchestration database to describe the versions of other services that are required. (My new Version 2.4.1 of OrderProcessing requires the a version 2.2.0 or later of the ProductManager because that was when the NATO Product Classification was added to the product data).</p>&#xA;"
31321082,31317470,290338,2015-07-09T14:53:05,"<p>An authentication layer as its own application fits pretty well in SOA design. There is an HTTP endpoint with no direct access to the micro-service database which what SOA best practice is:</p>&#xA;&#xA;<blockquote>&#xA;  <p>For us service orientation means encapsulating the data with the&#xA;  business logic that operates on the data, with the only access through&#xA;  a published service interface. No direct database access is allowed&#xA;  from outside the service, and there’s no data sharing among the&#xA;  services.</p>&#xA;  &#xA;  <p>-- Werner Vogels, Amazon CTO</p>&#xA;</blockquote>&#xA;&#xA;<p>Reference to <a href=""http://martinfowler.com/microservices/"" rel=""nofollow"">http://martinfowler.com/microservices/</a></p>&#xA;&#xA;<p>What is an authentication layer or service and how does one server confirms the authentication has been established yet? One kind of client based persistence is HTTP cookie which hooked strictly to a domain name, therefore it is not easy to reuse same cookie among multiple domains without an explicit authentication step.</p>&#xA;&#xA;<p>If you are able to pass a certain key or header <em>http_request</em> can provide unobtrusive authentication, this module became a built in Nginx core since version 1.5.4: <a href=""http://nginx.org/en/docs/http/ngx_http_auth_request_module.html"" rel=""nofollow"">http://nginx.org/en/docs/http/ngx_http_auth_request_module.html</a></p>&#xA;&#xA;<pre><code>location /upload {&#xA;    auth_request /auth;&#xA;    ...&#xA;}&#xA;&#xA;location = /auth {&#xA;    internal;&#xA;    proxy_pass http://auth_service.localhost;&#xA;    proxy_pass_request_body off;&#xA;    proxy_set_header Content-Length """";&#xA;    proxy_set_header X-Original-URI $request_uri;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>The endpoint accessible through <a href=""http://auth_service.localhost"" rel=""nofollow"">http://auth_service.localhost</a> (choose your own URL) is isolated and has its own database and does only one thing - to authenticate user or not. A mechanism can rely on a certain key or header or even IP address. To suppress to much subsequent request you can cache the response.</p>&#xA;&#xA;<p>SOA is hard but I recommend to read this thoroughly: <a href=""https://www.nginx.com/blog/introduction-to-microservices/"" rel=""nofollow"">https://www.nginx.com/blog/introduction-to-microservices/</a></p>&#xA;"
35136015,35135884,229154,2016-02-01T16:56:29,"<p>Well if you want to avoid long-running transactions, but want to have a logical equivalent of them which is still (possibly) long-running, then you are effectively pushing the ""transaction"" abstraction to the application level.</p>&#xA;&#xA;<p>The simplest solution is to just accept that and invent some column for your data indicating that the records are dirty, then make them non-dirty in step (6).</p>&#xA;"
48176393,29644916,2857250,2018-01-09T20:45:05,"<p>API gateway pattern should be used to implement this using OpenID Connect. User will be authenticated by IDP and will get the JWT token from authorization server. Now API gateway system can store this token in Redis database and set the cookie on the browser. API gateway will use the cookie to validate the user request and will send the token to the Microservices.</p>&#xA;&#xA;<p>API Gateway acts as a single entry point for all types of clients apps like public java script client app, traditional web app, native mobile app and third party client apps in the Microservice architecture. </p>&#xA;&#xA;<p>You can find more details about it on <a href=""http://proficientblog.com/microservices-security/"" rel=""nofollow noreferrer"">http://proficientblog.com/microservices-security/</a></p>&#xA;"
47134244,47133571,1224482,2017-11-06T10:00:53,<p>The problem was because Lumen and Laravel are using the same Machine and Sharing the same environment so when i did a call the environment variables they were overrides. </p>&#xA;
51699776,26616962,1162274,2018-08-06T02:11:19,"<p>It is a very general questions. I will try to keep it that way</p>&#xA;&#xA;<p><strong>Smart endpoints</strong> </p>&#xA;&#xA;<p>Smart endpoints just meaning actual business rules and any other validations happens behind those endpoints which are not visible to anyone to the consumers of those endpoints think of it as a place where actual Magic happens. </p>&#xA;&#xA;<p><strong>Dumb pipelines</strong> </p>&#xA;&#xA;<p>Dumb pipelines means any communication means where no further actions e.g validations are taken place, it simply carries the data across that particular channel and it may also be replaceable if need be. </p>&#xA;"
36087103,36080524,1359650,2016-03-18T14:35:48,"<p>I just set up RabbitMQ messaging within my application and will be implementing for decoupled (multiple, distributed) applications in the next day or so. I found <a href=""http://codetunes.com/2014/event-sourcing-on-rails-with-rabbitmq/"" rel=""noreferrer"">this</a> article very helpful (and the <a href=""https://www.rabbitmq.com/getstarted.html"" rel=""noreferrer"">RabbitMQ tutorials</a>, too). All the code below is for RabbitMQ and assumes you have a RabbitMQ server up and running on your local machine.</p>&#xA;&#xA;<p>Here's what I have so far - that's working for me:</p>&#xA;&#xA;<pre><code>  #Gemfile&#xA;  gem 'bunny'&#xA;  gem 'sneakers'&#xA;</code></pre>&#xA;&#xA;<p>I have a <code>Publisher</code> that sends to the queue:</p>&#xA;&#xA;<pre><code>  # app/agents/messaging/publisher.rb&#xA;  module Messaging&#xA;    class Publisher&#xA;      class &lt;&lt; self&#xA;&#xA;        def publish(args)&#xA;          connection = Bunny.new&#xA;          connection.start&#xA;          channel = connection.create_channel&#xA;          queue_name = ""#{args.keys.first.to_s.pluralize}_queue""&#xA;          queue = channel.queue(queue_name, durable: true)&#xA;          channel.default_exchange.publish(args[args.keys.first].to_json, :routing_key =&gt; queue.name)&#xA;          puts ""in #{self}.#{__method__}, [x] Sent #{args}!""&#xA;          connection.close&#xA;        end&#xA;&#xA;      end&#xA;    end&#xA;  end&#xA;</code></pre>&#xA;&#xA;<p>Which I use like this:</p>&#xA;&#xA;<pre><code>  Messaging::Publisher.publish(event: {... event details...})&#xA;</code></pre>&#xA;&#xA;<p>Then I have my 'listener':</p>&#xA;&#xA;<pre><code>  # app/agents/messaging/events_queue_receiver.rb&#xA;  require_dependency ""#{Rails.root.join('app','agents','messaging','events_agent')}""&#xA;&#xA;  module Messaging&#xA;    class EventsQueueReceiver&#xA;      include Sneakers::Worker&#xA;      from_queue :events_queue, env: nil&#xA;&#xA;      def work(msg)&#xA;        logger.info msg&#xA;        response = Messaging::EventsAgent.distribute(JSON.parse(msg).with_indifferent_access)&#xA;        ack! if response[:success]&#xA;      end&#xA;&#xA;    end&#xA;  end&#xA;</code></pre>&#xA;&#xA;<p>The 'listener' sends the message to <code>Messaging::EventsAgent.distribute</code>, which is like this:</p>&#xA;&#xA;<pre><code>  # app/agents/messaging/events_agent.rb&#xA; require_dependency  #{Rails.root.join('app','agents','fsm','state_assignment_agent')}""&#xA;&#xA;  module Messaging&#xA;    class EventsAgent&#xA;      EVENT_HANDLERS = {&#xA;        enroll_in_program: [""FSM::StateAssignmentAgent""]&#xA;      }&#xA;      class &lt;&lt; self&#xA;&#xA;        def publish(event)&#xA;          Messaging::Publisher.publish(event: event)&#xA;        end&#xA;&#xA;        def distribute(event)&#xA;          puts ""in #{self}.#{__method__}, message""&#xA;          if event[:handler]&#xA;            puts ""in #{self}.#{__method__}, event[:handler: #{event[:handler}""&#xA;            event[:handler].constantize.handle_event(event)&#xA;          else&#xA;            event_name = event[:event_name].to_sym&#xA;            EVENT_HANDLERS[event_name].each do |handler|&#xA;              event[:handler] = handler&#xA;              publish(event)&#xA;            end&#xA;          end&#xA;          return {success: true}&#xA;        end&#xA;&#xA;      end&#xA;    end&#xA;  end&#xA;</code></pre>&#xA;&#xA;<p>Following the instructions on Codetunes, I have:</p>&#xA;&#xA;<pre><code>  # Rakefile&#xA;  # Add your own tasks in files placed in lib/tasks ending in .rake,&#xA;  # for example lib/tasks/capistrano.rake, and they will automatically be available to Rake.&#xA;&#xA;  require File.expand_path('../config/application', __FILE__)&#xA;&#xA;  require 'sneakers/tasks'&#xA;  Rails.application.load_tasks&#xA;</code></pre>&#xA;&#xA;<p>And:</p>&#xA;&#xA;<pre><code>  # app/config/sneakers.rb&#xA;  Sneakers.configure({})&#xA;  Sneakers.logger.level = Logger::INFO # the default DEBUG is too noisy&#xA;</code></pre>&#xA;&#xA;<p>I open two console windows. In the first, I say (to get my listener running):</p>&#xA;&#xA;<pre><code>  $ WORKERS=Messaging::EventsQueueReceiver rake sneakers:run&#xA;  ... a bunch of start up info&#xA;  2016-03-18T14:16:42Z p-5877 t-14d03e INFO: Heartbeat interval used (in seconds): 2&#xA;  2016-03-18T14:16:42Z p-5899 t-14d03e INFO: Heartbeat interval used (in seconds): 2&#xA;  2016-03-18T14:16:42Z p-5922 t-14d03e INFO: Heartbeat interval used (in seconds): 2&#xA;  2016-03-18T14:16:42Z p-5944 t-14d03e INFO: Heartbeat interval used (in seconds): 2&#xA;</code></pre>&#xA;&#xA;<p>In the second, I say:</p>&#xA;&#xA;<pre><code>  $ rails s --sandbox&#xA;  2.1.2 :001 &gt; Messaging::Publisher.publish({:event=&gt;{:event_name=&gt;""enroll_in_program"", :program_system_name=&gt;""aha_chh"", :person_id=&gt;1}})&#xA;  in Messaging::Publisher.publish, [x] Sent {:event=&gt;{:event_name=&gt;""enroll_in_program"", :program_system_name=&gt;""aha_chh"", :person_id=&gt;1}}!&#xA;  =&gt; :closed &#xA;</code></pre>&#xA;&#xA;<p>Then, back in my first window, I see:</p>&#xA;&#xA;<pre><code>  2016-03-18T14:17:44Z p-5877 t-19nfxy INFO: {""event_name"":""enroll_in_program"",""program_system_name"":""aha_chh"",""person_id"":1}&#xA;  in Messaging::EventsAgent.distribute, message&#xA;  in Messaging::EventsAgent.distribute, event[:handler]: FSM::StateAssignmentAgent&#xA;</code></pre>&#xA;&#xA;<p>And in my RabbitMQ server, I see:</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/kwMFK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/kwMFK.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>It's a pretty minimal setup and I'm sure I'll be learning a lot more in coming days. </p>&#xA;&#xA;<p>Good luck!</p>&#xA;"
45698759,45547556,3794466,2017-08-15T18:02:12,"<p>Typically for B/G testing you wouldn't use different dns for new functions, but define rules, such as every 100th user gets send to the new function or only ips from a certain region or office have access to the new functionality, etc. </p>&#xA;&#xA;<p>Assuming you're using AWS, you should be able to create an ALB in front of the ELBs for context based routing in which you should be able define rules for your routing to either B or G. In this case you have to separate environments functioning independently (possibly using the same DB though).</p>&#xA;&#xA;<p>For more complicated rules, you can use tools such as leanplum or omniture inside your spring boot application. With this approach you have one single environment hosting old and new functionality and later you'd remove the code that is outdated. </p>&#xA;"
50359491,50265973,5695522,2018-05-15T21:36:54,"<p>When thinking about microservices, it helps to remember the concepts of <a href=""https://www.martinfowler.com/bliki/BoundedContext.html"" rel=""nofollow noreferrer"">Bounded contexts</a></p>&#xA;&#xA;<blockquote>&#xA;  <p><em>Martin Fowler</em> : <strong>Bounded Contexts</strong> have both unrelated concepts (such as a support ticket only existing in a customer support context) but also <em>share concepts</em> (such as products and customers). <em>Different contexts may have completely different models of common concepts</em> with mechanisms to map between these polysemic concepts for integration. </p>&#xA;</blockquote>&#xA;&#xA;<p>The concepts of Student and Course can be very different in each microservice of your application. For example, a Student can be defined as a person with an identifier, a first name, a last name, and an address in the StudentInformationManagement microservice, and have only an identifier, and a list of courses in the CourseAttendance microservice.</p>&#xA;&#xA;<p>That is to say I wouldn't use a shared database for these microservices, instead I would create separate databases with the needed entities. Only the identifier is ""shared"" between the services.</p>&#xA;&#xA;<p>Try thinking about your process when designing your data models : which information do I get when a student registers ? Which when he enrolls in a class ?</p>&#xA;"
50359674,50338845,5695522,2018-05-15T21:53:18,"<p>Both are perfectly valid ways of doing it, but I would go the Estimate microservice way to avoid putting too much logic in the API Gateway. </p>&#xA;&#xA;<p>Maybe in the future your estimation calculation will change, and it wouldn't make much sense to me to update the gateway every time. </p>&#xA;"
50359802,50324219,5695522,2018-05-15T22:05:22,"<p>The language you use doesn't really matter that much for CQRS. Commands and Queries are really simple objects, so you can use PHP if you want. Choose what the developers are familiar with.</p>&#xA;&#xA;<p>When using microservices, CRQS can be really useful when combined with Event Sourcing : microservice A handles Commands and stores Events in an Event Store,  while microservice B handle events, updates the query database and handle Queries. That way your services can be scaled independently, and your business logic is easier to manage.</p>&#xA;"
49228278,49227957,1768226,2018-03-12T04:44:37,"<p>There are many ways to tackle this problem but one easy way is using a custom <code>QueryDTO</code> with the all the query properties like below. </p>&#xA;&#xA;<pre><code>public class QueryDTO {&#xA;    private String firstName;&#xA;    private String lastName;&#xA;&#xA;    // Getters and Setters&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>And in your Repository you can extend <a href=""https://docs.spring.io/spring-data/commons/docs/current/api/org/springframework/data/repository/query/QueryByExampleExecutor.html"" rel=""nofollow noreferrer"">QueryByExampleExecutor</a></p>&#xA;&#xA;<pre><code>public interface UserRepository extends CrudRepository&lt;User, Long&gt;, QueryByExampleExecutor&lt;User&gt; {...}&#xA;</code></pre>&#xA;&#xA;<p>And your controller method should do the following;</p>&#xA;&#xA;<pre><code>@Autowired&#xA;private UserRepository userRepository;&#xA;&#xA;@GetMapping&#xA;public ResponseEntity&lt;User&gt; get(@RequestParam(value = ""q"") String query) {&#xA;    QueryDTO queryDto = new ObjectMapper().readValue(query, QueryDTO.class);&#xA;&#xA;    // You can write a converter to convert QueryDTO to User Example  &#xA;    User user = new User();&#xA;    user.setFirstName(queryDto.getFirstName());&#xA;    user.setLastName(queryDto.getLastName());&#xA;    Example&lt;User&gt; example = Example.of(user);&#xA;&#xA;    return new ResponseEntity&lt;User&gt;(userRepository.findOne(example), HttpStatus.OK);&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Now it doesn't matter how many new fields you have you can just modify <code>QueryDTO</code> to add new fields and enable searching by those fields.</p>&#xA;&#xA;<p><strong>UPDATE</strong></p>&#xA;&#xA;<p>If you need to query by Ranges or with custom values you can extend <a href=""https://docs.spring.io/spring-data/commons/docs/current/api/org/springframework/data/querydsl/QuerydslPredicateExecutor.html"" rel=""nofollow noreferrer"">QueryDslPredicateExecutor</a> in your <code>UserRepository</code>. But as the name suggests <code>QueryByExampleExecutor</code> only supports matching values.</p>&#xA;"
29670952,29669180,1768226,2015-04-16T09:43:34,"<blockquote>&#xA;  <p>a/</p>&#xA;</blockquote>&#xA;&#xA;<p>No, Service Registries like Zookeeper are in-memory guaranteeing high throughput and low latency.</p>&#xA;&#xA;<blockquote>&#xA;  <p>b/</p>&#xA;</blockquote>&#xA;&#xA;<p>In Service Registries like Zookeeper you can make filesystem path like registering of services. For example /App1/Service1 and /App1/Service2</p>&#xA;&#xA;<blockquote>&#xA;  <p>c/</p>&#xA;</blockquote>&#xA;&#xA;<p>Not really clear what is the problem is.</p>&#xA;&#xA;<blockquote>&#xA;  <p>d/</p>&#xA;</blockquote>&#xA;&#xA;<p>The pattern recommended by somebody is the <a href=""https://spring.io/understanding/HATEOAS"" rel=""nofollow noreferrer"">HATEOAS</a> pattern which is recommended for API response.</p>&#xA;&#xA;<blockquote>&#xA;  <p>e/</p>&#xA;</blockquote>&#xA;&#xA;<p>AMQP is only required when communication required in between Services. Should never directly call the API of another service directly from one service.</p>&#xA;&#xA;<p><strong>EDIT</strong></p>&#xA;&#xA;<blockquote>&#xA;  <p>c/</p>&#xA;</blockquote>&#xA;&#xA;<p>In that case you need to implement fallback logic. For example if a service is not available, times out or any other failure some actions need to be taken. Tools like Netflix's <a href=""https://github.com/Netflix/Hystrix"" rel=""nofollow noreferrer"">Hystrix</a> helps to achieve this.</p>&#xA;&#xA;<blockquote>&#xA;  <p>e/</p>&#xA;</blockquote>&#xA;&#xA;<p>The communication pattern must be symmetric not asymmetric. Just like below, <img src=""https://i.stack.imgur.com/xjTrF.png"" alt=""enter image description here""> </p>&#xA;&#xA;<p>This pattern will enable loose coupling between microservices thus enabling flexibility.</p>&#xA;"
41733831,41731704,2425123,2017-01-19T04:35:32,"<p>You can have your CI build and push images for each service you want to run, and have the production environment run all 3 containers.</p>&#xA;&#xA;<p>Then, your production <code>docker-compose.yml</code> would look like this:</p>&#xA;&#xA;<pre><code>lb:&#xA;  image: nginx&#xA;  depends_on:&#xA;    - rails&#xA;    - express&#xA;  ports: 80:80&#xA;&#xA;  rails:&#xA;    image: yourorg/railsapp&#xA;&#xA;  express:&#xA;    image: yourorg/expressapp&#xA;</code></pre>&#xA;&#xA;<p><s>Be noted that <code>docker-compose</code> isn't recommended for production environments; you should be looking at using <a href=""https://github.com/docker/docker/blob/master/experimental/docker-stacks-and-bundles.md"" rel=""nofollow noreferrer"">Distributed Application Bundles</a> (this is still an experimental feature, which will be released to core in version 1.13)</s></p>&#xA;&#xA;<p>Alternatively, you can orchestrate your containers with a tool like <code>ansible</code> or a bash script; just make sure you create a docker network and attach all three containers to it so they can find each other.</p>&#xA;&#xA;<p>Edit: since Docker v17 and the deprecation of <a href=""https://github.com/docker/docker/blob/master/experimental/docker-stacks-and-bundles.md"" rel=""nofollow noreferrer"">DAB</a>s in favour of the <a href=""https://docs.docker.com/compose/compose-file/#compose-file-structure-and-examples"" rel=""nofollow noreferrer"">Compose file v3</a>, it seems that for single-host environments, <code>docker-compose</code> is a valid way for running multi-service applications. For multi-host/HA/clusterised scenarios you may want to look into either <a href=""https://docs.docker.com/engine/swarm/"" rel=""nofollow noreferrer"">Docker Swarm</a> for a self-managed solution, or <a href=""https://docs.docker.com/docker-cloud/"" rel=""nofollow noreferrer"">Docker Cloud</a> for a more PaaS approach. In any case, I'd advise you to try it out in <a href=""http://play-with-docker.com"" rel=""nofollow noreferrer"">Play-with-Docker</a>, the official online sandbox where you can spin out multiple hosts and play around with a swarm cluster without needing to spin out your own boxes.</p>&#xA;"
26601742,25824957,1120035,2014-10-28T05:57:05,"<p>""Service"" behaviors usually translate to <strong>Factory</strong> pattern implementations no matter the semantic.</p>&#xA;&#xA;<p>Api Side:</p>&#xA;&#xA;<pre><code>POST: yourapi.com/v1/inventory/add/&#xA;DATA: {&#xA;  userid: 1,&#xA;  products: [&#xA;     'Canned Chicken Gizzards',&#xA;     'Snake Oil Extract',&#xA;     'Evaporated Water'&#xA;  ]&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Factory class side:</p>&#xA;&#xA;<pre><code>InventoryFactory-&gt;addProduct((object) product)&#xA;&#xA;-- also have a method to add products under a user --&#xA;&#xA;InventoryFactory-&gt;addProducts((array) products, userid)&#xA;</code></pre>&#xA;"
43438102,42769855,3163427,2017-04-16T14:10:45,"<blockquote>&#xA;  <p>Dynamically binding port for webapp and api.&#xA;  Load balance within them.</p>&#xA;</blockquote>&#xA;&#xA;<p>The NodeJS/Express app just binds to a static port of the container. In your task definition you only specify the container port, that way the host port will be assigned at random. You <em>have</em> to use a loadbalancer at this point because there's multiple containers running. If you use an Application Load Balancer you can set ECS up in such a way that a target group automatically gets new ports registered and deregistered. Have a look at <a href=""http://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-load-balancing.html"" rel=""nofollow noreferrer"">the docs</a> for details.</p>&#xA;&#xA;<p>In resume: don't change your express.js code.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Configure autoscalling to make them happen automatically.</p>&#xA;</blockquote>&#xA;&#xA;<p>You can now setup 'Service Auto scaling' when you configure a service in ECS. This works in response to Cloudwatch alarms. You can e.g. monitor the number of requests on your loadbalancer in relation to the number of services, or the memory usage of your tasks. </p>&#xA;&#xA;<blockquote>&#xA;  <p>Additionally: Can this be achieved using Ansible? How?</p>&#xA;</blockquote>&#xA;&#xA;<p>I would suggest to use a scripted infrastructure tool (like Cloudformation or Terraform) instead of Ansible because ultimately you setup a certain state, and the cluster/AWS will take care of the scaling. You don't want to intervene at runtime in your AWS ECS cluster, the whole point of ECS is that it will manage itself after you give it parameters/strategies. </p>&#xA;&#xA;<blockquote>&#xA;  <p>I need to scale containers, not clusters or instances.</p>&#xA;</blockquote>&#xA;&#xA;<p>Not sure why you would <em>not</em> want to scale instances. If you don't scale instances, why not just always run the maximum number of containers? If you <em>do</em> want to scale instances, this is possible using an Auto Scaling Group, also in combination with Cloudwatch alarms. </p>&#xA;"
41944953,38023093,3163427,2017-01-30T20:36:16,"<p>There's a distinction between marking a container as essential and routing between containers. </p>&#xA;&#xA;<p>Assuming you're looking for a routing solution (a.k.a. service discovery); there's nothing anything build into ECS. You can go for the AWS solution which is to use Application Load Balancers (ALBs) that are nicely integrated with dynamic port binding in your ECS cluster. The basic idea is simple: a request from service A to service B goes first to a loadbalancer (ALB) that routes the request to a target group that contains all ports of running/healthy instances of service B. </p>&#xA;&#xA;<p>Downside of the AWS solution is that it's fairly expensive for small setups, because you need at least one ALB per 10 (internal) services. </p>&#xA;&#xA;<p>Alternatively you can setup a 3rd party solution for service discovery, e.g. Consul is one that also plays nicely with AWS.</p>&#xA;"
28116209,28112290,1857571,2015-01-23T18:16:48,"<p>Not sure if I properly understand what you're trying to do, but I <em>think</em> you might want to consider creating a number of <a href=""http://docs.cloudfoundry.org/services/catalog-metadata.html#plan-metadata-fields"" rel=""nofollow"">Service Plans</a> for the service you want to implement. Each plan would create service instances with different (predefined) parameters. You would need to create your own <a href=""http://docs.cloudfoundry.org/services/api.html"" rel=""nofollow"">service broker</a> for this.</p>&#xA;"
43862221,43850359,685091,2017-05-09T06:02:55,"<p>Since you are using spring cloud, you and easily integrate with Hystrix circuit breaker for handle microsrvice failover scenarios.<br>&#xA;use the hystrix commands for service break down. </p>&#xA;&#xA;<pre><code>@HystrixCommand(fallbackMethod = ""reliable"")&#xA;  public List&lt;String&gt; readingList() {&#xA;    URI uri = URI.create(""http://product-service/product"");&#xA;&#xA;    return this.restTemplate.getForObject(uri, String.class);&#xA;  }&#xA;&#xA;  public String reliable() {&#xA;    return ""no product available""; // or send empty array. Arrays.asList();&#xA;  }&#xA;</code></pre>&#xA;&#xA;<p>Please find the example in,</p>&#xA;&#xA;<p><a href=""https://spring.io/guides/gs/circuit-breaker/"" rel=""nofollow noreferrer"">https://spring.io/guides/gs/circuit-breaker/</a></p>&#xA;"
31446929,31404820,1018659,2015-07-16T06:39:24,"<p>The DNS protocol for discovery is a good option - this is why tools like [SkyDNS] (for etcd) <a href=""https://github.com/skynetservices/skydns"" rel=""nofollow"">1</a> and <a href=""https://www.consul.io/docs/agent/dns.html"" rel=""nofollow"">Consol</a> support it for exactly this purpose.</p>&#xA;&#xA;<p>It is better to use these tools that just DNS since they provide additional capabilities like leader election, healthcheck etc. </p>&#xA;"
41781988,41779458,1018659,2017-01-21T16:52:52,"<p>You didn't provide a lot of details about your setup (except not using docker) -so the answer is also quite generic. It does seem, however, you can benefit from Mesos (or its more user friendly incarnation DC/OS)</p>&#xA;"
43379577,43378165,1018659,2017-04-12T21:00:58,"<p>You can still use pub/sub models but you need to set the multiple instances so that only one would get a message. the specific depends on the pub/sub mechanism </p>&#xA;&#xA;<p>for instance in AMQP systems (like RabbitMQ) you'd publish the events on an exchange. the consumer service would bing a queue to that exchange and all the instances of the same service would read from the same queue (so only one would handle any given message)</p>&#xA;&#xA;<p>Another example is Kafka - in Kafka all the instances of the same service would use the same <a href=""https://cwiki.apache.org/confluence/display/KAFKA/Consumer+Group+Example"" rel=""nofollow noreferrer"">consumer group</a> - so, again, only one instance of each subscribing service would get the message</p>&#xA;&#xA;<p>Other pub/sub systems would have similar solutions.</p>&#xA;&#xA;<p>Point to point to specific instances is not a great solution as it would couple the instances of the different services </p>&#xA;"
43471569,43460615,1018659,2017-04-18T12:00:39,"<p>Specifically around your problem with Rabbit MQ - you can create a <code>rabbitmq.config</code>  file and copy that over when creating the docker image.</p>&#xA;&#xA;<p>In that file you can specify both a <code>default_user</code> and <code>default_pass</code> that will be created when an the database is set from scratch see <a href=""https://www.rabbitmq.com/configure.html"" rel=""nofollow noreferrer"">https://www.rabbitmq.com/configure.html</a></p>&#xA;&#xA;<p>AS for the general problem - you can change the entry point to a script that runs whatever you need and the service you want instead of the run script of the service </p>&#xA;"
43379724,43379048,1018659,2017-04-12T21:11:56,<p>You can pass the  consul IP &amp; Port as environment variable(s) to your dockers (<code>-e</code> on the <code>docker run</code> command)</p>&#xA;
51531598,51520654,1018659,2018-07-26T05:28:37,"<p>Distributed transactions are problematic for most circumstances and they are bad for services </p>&#xA;&#xA;<ul>&#xA;<li>Service Boundary – service boundary is a trust boundary. Atomic&#xA;transactions require holding locks and holding them on behalf of&#xA;foreign service is opening a security hole (makes it much easier to&#xA;do a denial of service attack) You cannot assume atomicity between&#xA;two different entities or resources. Esp. when these resources belong&#xA;to different businesses. </li>&#xA;<li>Transactions introduce tight coupling both temporal and operational </li>&#xA;<li>Transactions hinder scalability – It isn’t that you can’t scale&#xA;but it is much harder</li>&#xA;</ul>&#xA;&#xA;<p>Sagas (which, by the way, do not necessitate orchestration) emerged as a solution for coordination because they allow services to be more flexible - and are in fact closer to how real life work. Another pattern you can combine with Sagas to help with delaying effects can be <a href=""https://www.infoq.com/news/2009/09/reservations"" rel=""nofollow noreferrer"">reservation</a>.</p>&#xA;&#xA;<p>Another option you have might be reconsidering how you partitioned your services. It might be that the service boundaries you have now are not correct and a redesign will contain the needed transaction into one service</p>&#xA;"
43205307,43179951,1018659,2017-04-04T10:59:35,"<p>The solution is to aggregate data from different services into a central reporting database - this is feasible if the data collected is versioned by time - i.e you can go to the reporting data and get a point-in-time data which is correct (for that time)</p>&#xA;&#xA;<p>Getting that data to the service can be via events published by the various service or periodic imports, ""log"" aggregation or combinations of them.</p>&#xA;&#xA;<p>I call this pattern <a href=""http://arnon.me/soa-patterns/aggregated-reporting/"" rel=""nofollow noreferrer"">aggregated reporting</a></p>&#xA;&#xA;<p>Note that in addition to that you still to get data from individual services for things that needs to be up-to-date as an aggregation solution has inherent delay (reduced freshness)</p>&#xA;&#xA;<p>Edit:&#xA;Considering the edits you've made and the comments you've made (ad-hoc queries) I'd say you need to treat this as a journey , that is, you want to get to option 4 so start by pulling data from sources you have to answer you current ad-hoc needs, convert to messages as you move forward with development and add more sources.</p>&#xA;&#xA;<p>Also you may want to think about the difference between services (that don't share internal data structures between them and have strict boundaries) and aspects (semi-independent parts of service that can use the same source) </p>&#xA;&#xA;<p>PS &#xA;I also wrote that <a href=""https://www.infoq.com/articles/BI-and-SOA"" rel=""nofollow noreferrer"">InfoQ piece on BI &amp; SOA</a> Tom mentioned in the comments - which essentially talks about this idea - this article is from 2007, i.e. I've successfully applied it this for more than a decade now (different technologies , moving from schema on write to schema on read etc. but same principles) </p>&#xA;"
51232172,51224374,1018659,2018-07-08T12:50:28,"<p>One of the motivations for breaking something to small(er) services is service autonomy, in this case the question is, when the comments service is down should you display the issue or not- if they are always coupled anyway, they probably shouldn't reside in two services, if they aren't then making two calls will let you get this decoupling</p>&#xA;&#xA;<p>That said, you may still need an API Gateway to solve CORS issues with your client</p>&#xA;&#xA;<p>Lastly, comments/byissueid is not a good REST interface the issueId should be a parameter /comments/?issueId=..</p>&#xA;"
44417988,44416904,1018659,2017-06-07T16:17:45,<p>It can route the Graphql as any other HTTP request but it doesn't parse graphql to route each bit to a a specific service</p>&#xA;
45151694,45111662,1018659,2017-07-17T18:57:57,"<p>Microservices - like, services, should be as autonomous as possible. sharing a database table hinders that as it means that the services coupled to the availability and more importantly the internal structure of each other.</p>&#xA;&#xA;<p>That said, sometimes it makes sense to break a single service to multiple executables to gain advantages related to development speed, technology suitability or any other reason that makes sense. I'd like to call these <a href=""http://arnon.me/2017/03/services-aspects/"" rel=""nofollow noreferrer"">aspects</a> (as in differnt aspects of the same service).</p>&#xA;&#xA;<p>It still very important to understand (and respect) service boundaries or else you'd get a distributed mess but once you identify a boundary you can also build several components (aspects) that make out the service. For instance in the system I am working on these days we have a service that has completely different requirements for handling incoming data (streaming in real-time) and for consuming its data (graphql based queries). The first aspect is implemented in Scala and is highly distributed and the latter is implemented in node.js (for using friendly libs like express-graphql) both of the aspects use the same DB but that DB is isolated from other service in the system (e.g the users service, which uses its own service and DB) </p>&#xA;"
45289186,45275679,1018659,2017-07-24T20:07:49,"<p>What do you mean Microservices shouldn't share code. Will you deliberately go and search for 2 different TCP stacks ? If you wrote one in Java do you have to write the other in Go? </p>&#xA;&#xA;<p>Services can share code and it doesn't matter if that is 3rd party code or 1st party code. What you don't want to do, as you do in other aspects of the services,  is ensure autonomy. Autonomy is what gives you flexibility to change. In terms of shared code, autonomy  means that if you updated a shared component, say the bit that handles authorization you don't automatically need to update that in the other service(s) .</p>&#xA;&#xA;<p>Also as time passes and needs change- the services will evolve. As need change  they may diverge in the code they use and maybe you'd be able to still reuse bits. Just note that reuse and shared code is not an end in itself and you'd be fine</p>&#xA;"
45312151,45302887,1018659,2017-07-25T19:39:09,"<p>Maybe you have some additional requirements you're not listing but it seems to me that in this case an event based solution that would replicate lots of data from other services is way-way over engineering.</p>&#xA;&#xA;<p>If the UFS gets location when there are changes in the location service-  then it makes sense to  tie its invocation to an event coming from the location service on changes in locations </p>&#xA;&#xA;<p>In regard to the scores service, I'd leave that synchronous but make its interface accept a list of customers rather than making it an n ""get customer score"" calls to it. </p>&#xA;"
36750559,36719045,1167907,2016-04-20T17:04:25,"<p>Scalability and resiliency - with a Microservices architecture you will have to provide your own patterns for a scalable, resilient architecture. You say you can build them yourself with ""more work"" which is true, but distributed systems are hard so in the end do you have the time and resources?</p>&#xA;"
45443991,45441593,1364771,2017-08-01T17:13:50,"<p>There are a huge variety of factors to consider here which ultimately means there is no right answer to this question and it will be very specific to the nature of the application you're trying to build. As such, broad statements attempting to offer ""general"" advice and patterns should be taken with a huge grain of salt. With Cosmos a few of the many high level things to consider when making your decisions are as follows:</p>&#xA;&#xA;<p><strong>Partitioning</strong>: Cosmos collections support almost infinite scale based on the selection of an appropriate partition key. So, for example you could have a single collection and separate your services such that they each write to a distinct partition key. This would provide you with a form of service multi-tenancy which might be perfectly appropriate for your particular application. However, throughput is also scaled at the collection level so if certain services have much higher read and/or write requirements this may not work for you and could be an indication that that particular service should use it's own collection which can be scaled independently. </p>&#xA;&#xA;<p><strong>Cost</strong>: You're billed per collection with a minimum throughput requirement. Depending on the number and nature of your micro services this could result in exponentially higher costs for little gain. </p>&#xA;&#xA;<p><strong>Isolation</strong>: Again, depending on the nature of your application you might have a hard business requirement that data from different services be physically separate from each other which would force you to use separate collections.</p>&#xA;&#xA;<p>The point that I'm trying to make here is that there is absolutely no right answer to this question. You need to weight the pros/cons very carefully in the context of the solution you are trying to build and select the approach that is right for you.</p>&#xA;"
44394545,44394119,2615075,2017-06-06T15:48:03,"<p>This is just a conflation of terminology. </p>&#xA;&#xA;<p>In the context of monolithic apps vs. microservices, ""application"" refers to a web application, or in this case more specifically a WSGI application. A Django project is usually deployed as a WSGI application. So a monolithic app would be a deployment of a huge Django project, while microservices would be multiple smaller Django projects that are deployed separately.</p>&#xA;&#xA;<p>""Monolithic app"" may sometimes be used to refer to a Django application, i.e. a python module that is in <code>INSTALLED_APPS</code>. However, in that case you wouldn't be talking about microservices.</p>&#xA;"
46760110,46746885,1176867,2017-10-15T21:19:54,"<p><strong>TL;DR</strong>: no. </p>&#xA;&#xA;<p>Micro services should talk to each other using APIs. </p>&#xA;&#xA;<p>Two reasons:</p>&#xA;&#xA;<ul>&#xA;<li>Micro services as aggregate roots define clear transactional boundaries. Reusing code means potentially taking a shortcut that dodges preconditions, postconditions and invariant checks.</li>&#xA;<li>Secondly, code sharing forces you to integrate changes to the model with all the depending micro services, potentially defeating the purpose of adopting micro services. Having different API version will help you gradually manage that.</li>&#xA;</ul>&#xA;"
46838655,46813736,1176867,2017-10-19T20:48:12,"<blockquote>&#xA;  <p>1- Every service must have access to entirely database?</p>&#xA;</blockquote>&#xA;&#xA;<p>No. A microservice has its own schema related to the Aggregate Root / Service that it offers. If a service needs data of another entity, it invokes the APIs provided by another micro service.</p>&#xA;&#xA;<blockquote>&#xA;  <p>2- Is a good idea do a service exclusive to do transactionals&#xA;  operations?</p>&#xA;</blockquote>&#xA;&#xA;<p>No. Each microservice is a transaction boundary in its own right. Distributed transactions, particularly using 2PC, do not perform particularly well.</p>&#xA;&#xA;<blockquote>&#xA;  <p>3- SQL with microservices it's maybe too much slow?</p>&#xA;</blockquote>&#xA;&#xA;<p>I am not totally clear as to why you make such a statement.</p>&#xA;"
46759831,46759745,1176867,2017-10-15T20:48:13,"<p>I would not model a query as a Command first of all, and I would not target the EventStore, as you correctly pointed out.</p>&#xA;&#xA;<p>The query targets one of the many possible read models that you generate from the event store.</p>&#xA;&#xA;<p>If the query issues a command, it is not a query anymore, it becomes as service; therefore, in that case, you should be invoking a service.</p>&#xA;&#xA;<p>Hope that makes sense and good luck with your project.</p>&#xA;"
48827847,48808722,3156004,2018-02-16T13:27:20,"<p>JWT is the self-contained token, which can store scope of access for REST-service users. JWT is issued by the Authorization server.In your Resource servers, you should validate every request from REST-service users.  If your Resource server(s) runs on spring-boot you can use the following annotation:</p>&#xA;&#xA;<p><code>@PreAuthorize(""hasRole('ROLE_VIEW')"")</code></p>&#xA;&#xA;<p>As for the part where your services call each other, there no need to use JWT because no REST-service users involved. &#xA;Microservices can be secured by simple BasicAuth, CORS or locate them in one network without access from an external network</p>&#xA;"
51934420,51932616,9187876,2018-08-20T15:54:05,"<p>Like Simon Martinelli already said, the microservices dashboard is not compatible with Spring Boot 2.&#xA;We are aware of this issue and already did some research on the matter.</p>&#xA;&#xA;<p>Because most of us are not working at a company which has its main focus on open source, we are forced to work on this project during our free time.&#xA;Unfortunately due to passed and recent events this project didn't receive the work/love it deserves to have.&#xA;However, it <strong>still</strong> is our intention to continue the work on this project.</p>&#xA;"
36195313,36188740,489764,2016-03-24T07:44:49,"<p>There is no main class to run on the root project, and that's the reason for the exception. To start all your Lagom services use <code>runAll</code>, see <a href=""http://www.lagomframework.com/documentation/1.0.x/GettingStarted.html#Running-Lagom-services"" rel=""nofollow"">the Lagom getting started documentation</a>.</p>&#xA;&#xA;<p>By the way, we have <a href=""https://github.com/lagom/lagom/issues/21"" rel=""nofollow"">a ticket</a> for making <code>run</code> an alias of <code>runAll</code> when the task is executed on the root project.</p>&#xA;"
30736593,30732982,1870884,2015-06-09T15:33:39,"<p>I generally manage all of my client-side dependencies with <a href=""http://bower.io/"" rel=""nofollow noreferrer"">Bower</a> but you could use any package manager.</p>&#xA;&#xA;<p>This is how I would do it with <a href=""http://bower.io/"" rel=""nofollow noreferrer"">Bower</a>, which just uses Git to pull down your dependencies. This solution would require you to know how to use <a href=""https://git-scm.com/"" rel=""nofollow noreferrer"">Git</a>.</p>&#xA;&#xA;<p>Install Bower—see above link.</p>&#xA;&#xA;<p>Then, create a file called <code>bower.json</code> at the root of your project that points to the Git repository of your project:</p>&#xA;&#xA;<pre><code>{&#xA;    ""name"": ""my-php-app"",&#xA;    ""version"": ""0.0.0"",&#xA;    ""dependencies"": {&#xA;        ""my-angular-app"": ""git@bitbucket.org:MY_BITBUCKET_ACCOUNT/my-angular-app.git#master""&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Then you can run the following command in the root of your PHP project:</p>&#xA;&#xA;<pre><code>bower install&#xA;</code></pre>&#xA;&#xA;<p>This will create a directory called <code>bower_components</code> at the root. You can <a href=""https://stackoverflow.com/questions/14079833/how-to-change-bowers-default-components-folder"">configure the default directory</a>.</p>&#xA;&#xA;<p>Your application should be self-contained there. You can import it and all of its dependencies with PHP on the required page.</p>&#xA;"
51297878,40807355,9211684,2018-07-12T05:24:53,"<p>I found this <a href=""https://medium.com/@twobuckchuck/rabbitmq-scheduling-messages-with-sneakers-18089e8aa7d2"" rel=""nofollow noreferrer"">link</a> interesting. The solution is to create a delayed exchange in RabbitMQ and after the delay the message is send to the queue. It's not native in RabbitMQ though but it's a plugin.</p>&#xA;"
37738299,35113957,320036,2016-06-10T00:39:42,"<p>You can do this by combining services from multiple files using the <a href=""https://docs.docker.com/compose/compose-file/#extends"" rel=""noreferrer""><code>extends</code></a> feature of <code>docker-compose</code>. Put your projects in some well-defined location, and refer to them using relative paths:</p>&#xA;&#xA;<pre><code>../&#xA;├── foo/&#xA;│   └── docker-compose.yml&#xA;└── bar/&#xA;    └── docker-compose.yml&#xA;</code></pre>&#xA;&#xA;<h3>foo/docker-compose.yml:</h3>&#xA;&#xA;<pre><code>base:&#xA;    build: .&#xA;&#xA;foo:&#xA;    extends:&#xA;        service: base&#xA;    links:&#xA;        - db&#xA;&#xA;db:&#xA;    image: postgres:9&#xA;</code></pre>&#xA;&#xA;<p>If you wanted to test this project by itself, you would do something like:</p>&#xA;&#xA;<pre><code>sudo docker-compose up -d foo&#xA;</code></pre>&#xA;&#xA;<blockquote>&#xA;  <p>Creating foo_foo_1</p>&#xA;</blockquote>&#xA;&#xA;<h3>bar/docker-compose.yml:</h3>&#xA;&#xA;<pre><code>foo:&#xA;    extends:&#xA;        file: ../foo/docker-compose.yml&#xA;        service: base&#xA;    links:&#xA;        - db&#xA;&#xA;bar:&#xA;    build: .&#xA;    extends:&#xA;        service: base&#xA;    links:&#xA;        - db&#xA;        - foo&#xA;&#xA;db:&#xA;    image: postgres:9&#xA;</code></pre>&#xA;&#xA;<p>Now you can test both services together with:</p>&#xA;&#xA;<pre><code>sudo docker-compose up -d bar&#xA;</code></pre>&#xA;&#xA;<blockquote>&#xA;  <p>Creating bar_foo_1 <br>&#xA;  Creating bar_bar_1</p>&#xA;</blockquote>&#xA;"
48072658,48071720,8254500,2018-01-03T07:01:08,<p>The problem is that your update command is defined as a constructor. The command should go to the already existing aggregate instance. </p>&#xA;&#xA;<p>Changing the command handler to:</p>&#xA;&#xA;<pre><code>@CommandHandler&#xA;public void handle(SeatReserveUpadateCommand upadateCommand) {...}&#xA;</code></pre>&#xA;&#xA;<p>should fix the issue.</p>&#xA;
47573866,47572543,4060708,2017-11-30T12:46:00,"<p>As per the error below, your project is missing a jar file</p>&#xA;&#xA;<pre><code>java.lang.NoClassDefFoundError: com/netflix/eventbus/spi/EventBus &#xA;</code></pre>&#xA;&#xA;<p>Have you tried by including <code>netflix-eventbus</code> jar file in your pom?</p>&#xA;"
48124895,48124856,3209508,2018-01-06T06:31:04,<p>This is more of a deployment-related concern. The simple solution is to block the internal service ports like <code>8001</code> and <code>8002</code> to the outside and expose only API gateway to the outside world. And the way to do this depends on the platform as a service(PAAS) or infrastructure as a service (IAAS) that you are using. If you are using a service like Kubernetes to deploy your microservices you can use Kubernetes networking model.</p>&#xA;
46961090,46956866,542757,2017-10-26T18:10:13,"<p>For tokens to be correctly decrypted by all your micro-services, you need to make sure that the key ring containing the master keys (that are derived by ASP.NET Core Data Protection to create encryption and validation keys) is correctly synchronized. The procedure is described here: <a href=""https://docs.microsoft.com/en-us/aspnet/core/security/data-protection/configuration/overview"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/aspnet/core/security/data-protection/configuration/overview</a>.</p>&#xA;&#xA;<p>Here's an example of how it could be done using a shared folder:</p>&#xA;&#xA;<pre><code>public void ConfigureServices(IServiceCollection services)&#xA;{&#xA;    services.AddDataProtection()&#xA;        .PersistKeysToFileSystem(new DirectoryInfo(@""\\server\share\directory\""))&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>You'll also need to configure the two applications to use the same ""application discriminator"":</p>&#xA;&#xA;<pre><code>public void ConfigureServices(IServiceCollection services)&#xA;{&#xA;    services.AddDataProtection()&#xA;        .PersistKeysToFileSystem(new DirectoryInfo(@""\\server\share\directory\""))&#xA;        .SetApplicationName(""Your application name"");&#xA;}&#xA;</code></pre>&#xA;"
29830516,29830038,750117,2015-04-23T17:31:07,"<p>There is a very famous approach called ELK: </p>&#xA;&#xA;<ul>&#xA;<li><strong>Elasticsearch</strong>: Search and analyse data in real time</li>&#xA;<li><strong>Logstash</strong>: Collect, parse and enrich data from each machine</li>&#xA;<li><strong>Kibana</strong>: Explore and visualise your data graphically</li>&#xA;</ul>&#xA;&#xA;<p>So all the information is collected by Logstash, stored in Elasticsearch and visualised using Kibana UI.</p>&#xA;&#xA;<p>With this stack no matters who is generating the information (or what technology is using).</p>&#xA;"
45178100,45176450,1078565,2017-07-18T22:24:02,"<h2><a href=""https://spring.io/guides/tutorials/spring-boot-oauth2/"" rel=""nofollow noreferrer"">Spring Boot and OAuth2</a></h2>&#xA;&#xA;<p>This tutorial demonstrates how to create a Spring Boot app that leverages OAuth2 authorization via Facebook and GitHub.</p>&#xA;&#xA;<h2><a href=""https://spring.io/guides/tutorials/spring-security-and-angular-js/#_sso_with_oauth2_angular_js_and_spring_security_part_v"" rel=""nofollow noreferrer"">Spring Boot + Spring Security + OAuth2 + Microservices</a></h2>&#xA;&#xA;<p>This tutorial demonstrates how to create a microservices architecture with a Spring Boot stack that separates your concerns into the following apps: UI, Auth, Resource.</p>&#xA;"
37531235,37528335,1078565,2016-05-30T17:47:25,"<h2>The Benefits of Microservices</h2>&#xA;&#xA;<ul>&#xA;<li>Deploy &amp; Scale Independently</li>&#xA;<li>Easier to 'Reason About'</li>&#xA;<li>Separation of Concerns</li>&#xA;<li>Single Responsibility</li>&#xA;<li>(Micro)Service-Oriented Architecture</li>&#xA;</ul>&#xA;&#xA;<p>I would suggest splitting your services based on domain.  This is a logical and efficient approach which makes it an easy starting point.  Your monolithic package structure may already be organized in this manner, which simplifies the refactoring even more.</p>&#xA;&#xA;<h2>API Gateway</h2>&#xA;&#xA;<p>The typical Spring Cloud approach for this would be to use a Zuul Proxy on the edge of your network which receives the requests from your clients (web, mobile, etc.) and routes them to the microservices located behind your firewall.  The client only interfaces with a single domain, and it handles CORS out of the box.</p>&#xA;&#xA;<p>Resources:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""http://spring.io/guides/tutorials/spring-security-and-angular-js/#_the_api_gateway_pattern_angular_js_and_spring_security_part_iv"" rel=""noreferrer"">API Gateway Pattern</a></li>&#xA;<li><a href=""http://spring.io/guides/gs/routing-and-filtering/"" rel=""noreferrer"">Routing and Filtering</a></li>&#xA;</ul>&#xA;"
36997985,36988921,564005,2016-05-03T07:17:07,"<p>You may have a look at <a href=""https://www.rabbitmq.com/tutorials/amqp-concepts.html"" rel=""nofollow"">this page</a> which explains how AMQP model works and thus RabbitMQ. </p>&#xA;&#xA;<p>Your message is published to an Exchange not to Queue. The exchange will route the message to the binded queues according to the Exchange type and to the binded key and routing key. </p>&#xA;&#xA;<p>Generally speaking, publishers do not mind of the routing behind an exchange. It's the consumers concern to create and bind queue to the exchange according to what they want to consume and how.</p>&#xA;&#xA;<p>Thus if your producer publishes its messages to the ExchangeA using for example a <a href=""https://www.rabbitmq.com/tutorials/amqp-concepts.html#exchange-direct"" rel=""nofollow"">direct exchange</a>, Service A/B/C can create and bind their queues to this exchange and they will receive <strong>each</strong> the published message. If a new service E is spawned, it should create and bind a queue too to this exchange. Thus you do not need to modify the publisher. </p>&#xA;"
41751488,40734086,309797,2017-01-19T20:53:09,"<p>You can have a look here for an example: <a href=""https://github.com/weidazhao/Hosting/tree/master/Microsoft.ServiceFabric.AspNetCore.Gateway"" rel=""nofollow noreferrer"" title=""Service Fabric gateway prototype"">Service Fabric gateway prototype</a></p>&#xA;&#xA;<p>This is specific for Service Fabric, but the concept is generically valid.</p>&#xA;&#xA;<p>My experience with it is positive, anyway I've switched to <a href=""https://docs.microsoft.com/en-us/azure/application-gateway/application-gateway-introduction"" rel=""nofollow noreferrer"">Azure Application Gateway</a> because all my microservices are hosted on Azure.</p>&#xA;"
37453805,37389481,851749,2016-05-26T07:05:33,"<p>After searching for me and looking for various DevOps tools, I discovered that such class of software is refereed as job Schedulers or Workload Automation. Most of the solutions are rather complex commercial system that provides not only such functionality. However there are some open source solutions available as well.</p>&#xA;&#xA;<p>List of available software (it is not complete):</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://en.wikipedia.org/wiki/List_of_job_scheduler_software"" rel=""nofollow"">https://en.wikipedia.org/wiki/List_of_job_scheduler_software</a></li>&#xA;</ul>&#xA;"
30890030,29761872,2086,2015-06-17T11:18:46,"<p>It's OK for services to have read-only replicated copies of certain reference data from other services.</p>&#xA;&#xA;<p>Given that, when trying to refactor a monolithic database into microservices (as opposed to rewrite) I would </p>&#xA;&#xA;<ul>&#xA;<li>create a db schema for the service</li>&#xA;<li>create versioned* views** in that schema to expose data from that schema to other services</li>&#xA;<li>do joins against these readonly views</li>&#xA;</ul>&#xA;&#xA;<p>This will let you independently modify table data/strucutre without breaking other applications.</p>&#xA;&#xA;<p>Rather than use views, I might also consider using triggers to replicate data from one schema to another.</p>&#xA;&#xA;<p>*the views can be extended. If a breaking change is required, create a v2 of the same view and remove the old version when it is no longer required.&#xA;**or Table-Valued-Functions, or Sprocs.</p>&#xA;"
47349174,47348422,727078,2017-11-17T11:03:18,"<p>Assuming that you have three services A (relates directly on B), b (relates directly on C) and C and user calls your service A, but it can be applied to any number of services. </p>&#xA;&#xA;<p>I will briefly describe you the approach</p>&#xA;&#xA;<p>If error occurs in the service C (exception thrown), your code should catch Exception and return a meaningful response. Simplest I can think of would be a <code>@ControllerAdvice</code> </p>&#xA;&#xA;<pre><code>@ControllerAdvice&#xA;public class GlobalExceptionHandler {&#xA;&#xA;      @ExceptionHandler(YourException.class)&#xA;      public ResponseEntity&lt;ApiErrorDto&gt; handleYourException(YourException e) {&#xA;      return ResponseEntity&#xA;            .status(HttpStatus.BAD_REQUEST) //or any other suitable &#xA;            .body(new ApiError(e.getMessage());&#xA;     }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p><code>ApiError</code> could be just a basic POJO class with a single field holding the message, or any other field meaningful for your exception handling. </p>&#xA;&#xA;<p>Now when your service B receives response it should check whether it is expected status code or an error status code and act accordingly. </p>&#xA;&#xA;<p>Your <code>ApiError</code> class should be shared between all services so that it can be easily serialized/deserialized everywhere. That way once you receive error you can decide what to do next, one of the scenarios could be throwing another exception to be caught by ExceptionHandler in service B. </p>&#xA;&#xA;<p>It's a starter, you can decide whether you want to return a string message in your <code>ApiError</code>, or maybe some kind of a meaningful code etc. Bottom line is that you should include that information in the response of the service where error occurs and interpret that information in the calling service. </p>&#xA;"
42238330,42237967,1318694,2017-02-14T23:47:56,"<p>Take a look at the authentication providers that do this sort of thing for a living. <a href=""https://auth0.com/"" rel=""nofollow noreferrer"">auth0</a> and <a href=""https://stormpath.com/"" rel=""nofollow noreferrer"">Stormpath</a> are two examples, but there are many more.</p>&#xA;&#xA;<p>Use their API's to setup an app and plug it into express. After going through this process you'll have a better idea of what features you might need and what you would need to implement in an API to use them. </p>&#xA;&#xA;<p>At the end of the review, either use one of the services or implement a cut down version of the features you need with <a href=""http://www.passportjs.org/"" rel=""nofollow noreferrer"">Passport.js</a>.</p>&#xA;"
39120389,39118038,1318694,2016-08-24T10:10:56,"<p>You could poll the required services to confirm they are responding before running the tests.</p>&#xA;&#xA;<p><code>curl</code> has inbuilt <a href=""https://curl.haxx.se/docs/manpage.html#--retry"" rel=""nofollow noreferrer"">retry logic</a> or it's fairly trivial to build retry logic around some other type of service test.</p>&#xA;&#xA;<pre><code>#!/bin/bash&#xA;&#xA;await(){&#xA;  local url=${1}&#xA;  local seconds=${2:-30}&#xA;  curl --max-time 5 --retry 60 --retry-delay 1 \&#xA;       --retry-max-time ${seconds} ""${url}"" \&#xA;  || exit 1&#xA;}&#xA;&#xA;docker-compose up -d&#xA;await http://container_ms1:3000&#xA;await http://container_ms2:3000&#xA;run-ze-tests&#xA;</code></pre>&#xA;&#xA;<p>The alternate to polling is an event based system. &#xA;If all your services push notifications to an external service, <a href=""https://stackoverflow.com/a/39118838/1318694"">scaeda gave the example of a log file</a> or you could use something like Amazon SNS. Your services emit a ""started"" event. Then you can subscribe to those events and run whatever you need once everything has started. </p>&#xA;&#xA;<p>Docker 1.12 did add the <a href=""https://docs.docker.com/engine/reference/builder/#/healthcheck"" rel=""nofollow noreferrer""><code>HEALTHCHECK</code></a> build command. Maybe this is available via <a href=""https://docs.docker.com/engine/reference/commandline/events/"" rel=""nofollow noreferrer"">Docker Events</a>?</p>&#xA;"
50654613,46453981,7298086,2018-06-02T07:36:14,"<p>It seems to me that you are struggling because you don't have all the information needed at a time.</p>&#xA;&#xA;<p>Why not use a JWT (JSON Web Token)?</p>&#xA;&#xA;<p>They can store a payload (such as user authority, permissions etc) and you can easily check if what they are claiming is true.</p>&#xA;&#xA;<p><a href=""https://auth0.com/learn/json-web-tokens/"" rel=""nofollow noreferrer"">https://auth0.com/learn/json-web-tokens/</a> </p>&#xA;"
34864744,34841789,484902,2016-01-18T21:54:22,"<p>If you look for a minimal setup, you can wrap the values you receive from Consul via <a href=""https://github.com/Netflix/ribbon"" rel=""nofollow"">ribbon</a>, Netflix' client based load balancer.</p>&#xA;&#xA;<p>You will find it as a <a href=""http://cloud.spring.io/spring-cloud-consul/"" rel=""nofollow"">module for Spring Cloud.</a> </p>&#xA;&#xA;<p>I didn't find an up-to-date standalone example, only this link to <a href=""https://github.com/chrisgray/dropwizard-consul"" rel=""nofollow"">chrisgray's dropwizard-consul implementation</a> that is using it in a Dropwizard context. But it might serve as a starting point for you.</p>&#xA;"
38872713,38872460,3437606,2016-08-10T11:53:24,<p>run <code>docker run -v $PWD/lib:/usr/src/app -p 3010:3000 -i davesrepo/dynamo bash</code> and take a look at the /usr/src/app deriectory.</p>&#xA;&#xA;<p>I guess you have not mounted the lib folder successfully or there is a permission error.</p>&#xA;
52063793,51821786,1672486,2018-08-28T17:57:17,"<p>I have tried simplifying your pom and I am not getting any issue.<br/></p>&#xA;&#xA;<pre><code>        &lt;dependency&gt;&#xA;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&#xA;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&#xA;        &lt;/dependency&gt;&#xA;&#xA;        &lt;dependency&gt;&#xA;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&#xA;            &lt;artifactId&gt;spring-cloud-starter-zuul&lt;/artifactId&gt;&#xA;        &lt;/dependency&gt;&#xA;&#xA;        &lt;dependency&gt;&#xA;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&#xA;            &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&#xA;        &lt;/dependency&gt;&#xA;</code></pre>&#xA;&#xA;<p>Let me know after putting these dependency, it is still not working. Can you also provide githhb link for your source code</p>&#xA;"
44063627,44050579,3515174,2017-05-19T07:09:11,"<p>Make sure that your any injected classes which make use of Entity Framework have the same lifetime as your <code>DbContext</code>. Default would be <code>Scoped</code>. Easy mistake to make, don't sweat!</p>&#xA;&#xA;<p>See this section and the big red warning near the bottom: &#xA;<a href=""https://docs.microsoft.com/en-us/aspnet/core/fundamentals/dependency-injection#registering-your-own-services"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/aspnet/core/fundamentals/dependency-injection#registering-your-own-services</a></p>&#xA;"
47885548,47885225,3783718,2017-12-19T11:04:19,"<p>It seems to me you are describing microservice architecture and it is a good way to go. You are likely to need OAuth2 for your authentication. I used <a href=""http://www.baeldung.com/rest-api-spring-oauth2-angularjs"" rel=""nofollow noreferrer"">this tutorial</a> from Baeldung as a good starting point, although I personally changed the front end from Angular to React but that is down to you.</p>&#xA;"
29889816,29888108,466215,2015-04-27T07:36:43,"<p>I would use ASP.NET Web API client libraries.  It works with any REST API, whether its coded using .NET or some other framework.</p>&#xA;&#xA;<p>Look here for details: <a href=""http://www.asp.net/web-api/overview/advanced/calling-a-web-api-from-a-net-client"" rel=""nofollow"">http://www.asp.net/web-api/overview/advanced/calling-a-web-api-from-a-net-client</a></p>&#xA;&#xA;<p><strong>Nuget package:</strong> Microsoft.AspNet.WebApi.Client</p>&#xA;"
42026596,42025000,733735,2017-02-03T14:32:42,"<p>Well, in the book they suggest starting with the simplest success case possible. For an authentication service that would probably be a successful authentication. </p>&#xA;&#xA;<p>So your first acceptance test could look something like that:</p>&#xA;&#xA;<p>When: receiving valid user data</p>&#xA;&#xA;<p>Then: authentication should be successful </p>&#xA;&#xA;<p>That may seem awfully small for an acceptance test that is supposed to test a whole system, but your system is also very small and there aren't many user stories to handle. Basically only authentication success, fail and maybe a test that covers the case when a user has tried to log in too many times without success.&#xA;Your unit tests then can go more into detail about the actual authentication mechanism, but the acceptance test should always be about the user story.</p>&#xA;&#xA;<p>I guess one could also argue that you don't need to write acceptance tests for your authentication service at all, since it is only a part of your system and you should rather write acceptance tests for your whole system, meaning when you have brought all the microservices together or for each individual website that will rely on that service. The main reason for this kind of argument is that acceptance testing is about testing from the outside in and your authentication service is already a rather deep component of your system.</p>&#xA;"
49910569,49910421,8600103,2018-04-18T23:39:40,"<p>First of all, what you showed us dont looks like microsservice at all.</p>&#xA;&#xA;<p>You can say that you have an application that uses microsservices architecture when it is formed by microsservices(oh rly?) with independent functionalities and that can be scalable. Scale one service, means that you will run multiple instances (possible in multiple hosts) and it will be transparent for other services.</p>&#xA;&#xA;<p>A good example to ilustrate that is a web store microsservice based composed by 4 microsservices:</p>&#xA;&#xA;<ul>&#xA;<li>Sale Microsservice </li>&#xA;<li>Product Microsservice </li>&#xA;<li>Messaging Microsservice</li>&#xA;<li>Authentication Microsservice</li>&#xA;</ul>&#xA;&#xA;<p>In a blackfriday event, for example, which theoretically will occur a lot of purchases, you can scale only the Sale Microsservice, saving resources from the other three (of course this means using a bunch of other technologies like proxy, LB ...). If you were using a monolithic architecture would need to scale all your application.</p>&#xA;&#xA;<p>If you are using correctly a microsservice architecture, yes, you can say that your application is horizontally scalable.</p>&#xA;"
49218898,49216360,934439,2018-03-11T10:18:59,"<p>The simplest solution would be to <code>yarn run build &amp; micro</code> (note the single <code>&amp;</code> and not <code>&amp;&amp;</code>).</p>&#xA;&#xA;<p>As mentioned by others, <a href=""https://www.npmjs.com/package/parallelshell"" rel=""nofollow noreferrer"">parallelshell</a> is another good hack (<a href=""https://www.npmjs.com/package/parallelshell#motivation"" rel=""nofollow noreferrer"">probably more robust</a> than <code>&amp;</code>).</p>&#xA;"
31104530,31046924,934439,2015-06-28T20:40:29,"<p><a href=""http://self-issued.info/docs/draft-ietf-jose-json-web-signature.html"">JWS</a> (the signed version of a JWT) is a perfect example as it was thought for similar scnearios:</p>&#xA;&#xA;<ul>&#xA;<li>you have an <strong>authentication</strong> app: every login goes through that (<code>signin.domain.com</code>), and once you verify the credentials of a user you issue the token, generated through <strong>private keys</strong></li>&#xA;<li>each service (<code>service1.domain.com</code>, <code>service2.domain.com</code>) can implement a middleware that instead does <strong>authorization</strong>: all your services will receive the <strong>public key</strong> and will be able to verify the authenticity of the token through that key. They don't need a DB since what they need to verify is that the token is valid, not that the user exists etc etc.</li>&#xA;</ul>&#xA;&#xA;<p>To clarify my last statement: you should probably issue very short-lived tokens. At that point, say that:</p>&#xA;&#xA;<ul>&#xA;<li>user X logs in</li>&#xA;<li>his token will be valid for ten minutes</li>&#xA;<li>user X deletes his account but still has a valid token</li>&#xA;<li>he then hits <code>service.domain.com</code></li>&#xA;</ul>&#xA;&#xA;<p>On <code>service.domain.com</code> you will still consider him logged in until you, for example, need to interact with an API that actually hits the DB (ie. add a new user address). At that point the service that is responsible for writing into the DB will throw an exception saying the user doesnt exist and you can probably trap it and log the user out. All of this can be tweaked / fine-tuned but you get a rough idea of how it could work.</p>&#xA;&#xA;<p>Getting back to JWTs and their usage, I don't know if you are familiar with PHP but this is a <a href=""https://github.com/namshi/jose#usage"">pretty straightforward example</a>.</p>&#xA;&#xA;<p>If you want to get fancy you could use nginx as a middleware and have something like the <a href=""http://nginx.org/en/docs/http/ngx_http_auth_request_module.html"">auth module</a> doing authorization for you.</p>&#xA;&#xA;<p>Last but not least, we've only covered authentication here: to do authorization you will probably either want to, in each service, either read the user's roles from the token (assuming you saved them there once the user logs in -- but this is a bit flawed as if a user loses a role then his token would still list it) or simply call <code>signin.domain.com/users/me</code> from each service to retrieve an up-to-date list of user roles, and then check that he's allowed to perform certain operations on that specific service.</p>&#xA;&#xA;<p>Oh, and remember that you should never put sensitive data in a JWT / JWS as they can be decoded. So yes, you can add user roles to a JWT but, for example, never save passwords or other plaintext tokens there.</p>&#xA;&#xA;<p>Hope this helps!</p>&#xA;"
47637105,47630168,4280615,2017-12-04T15:50:52,"<p>I am not an expert on OAuth, but I have done a fair bit of work with microservices. When working with microservices, it is often a good idea to separate your services in such a way that:</p>&#xA;&#xA;<ul>&#xA;<li>They each know as little as possible about the concepts/concerns that they delegate to other services</li>&#xA;<li>Their dependency graph is <a href=""https://en.wikipedia.org/wiki/Acyclic_dependencies_principle"" rel=""nofollow noreferrer"">acyclic</a>, whether the services are <a href=""http://chi.pl/2017/01/08/Microservices-and-modularization.html"" rel=""nofollow noreferrer"">microservices or part of a well-designed monolith</a></li>&#xA;</ul>&#xA;&#xA;<p>Take an example of <em>Accounts</em> and <em>Orders</em>. You could have the Accounts service know about users, authentication and authorization, sessions, and contact information. If a user wants to view an order, the accounts service could take all requests, ensure that the user is authorized to do so and then request the order directly from the Orders Service. </p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/RfoKdt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RfoKdt.png"" alt=""passthrough""></a></p>&#xA;&#xA;<p>The downsides to this approach are:</p>&#xA;&#xA;<ul>&#xA;<li>The Accounts Service must pass all orders data through to the user client, which may result in code duplication and complexity</li>&#xA;<li>Changes to the Orders Service API may require changes to the Accounts Service to pass through new data</li>&#xA;</ul>&#xA;&#xA;<p>Upsides are:</p>&#xA;&#xA;<ul>&#xA;<li>The Accounts Service can authenticate with the Orders Service directly using a service-level authentication mechanism like an api token</li>&#xA;<li>The Orders Service may live on a private network</li>&#xA;</ul>&#xA;&#xA;<p>Another approach might be to have a third service responsible for identity. The client would make requests directly to the Accounts Service and Orders Service, which would each then ask a third service, let's call it an Identity Service, if a session is valid. If not, it would forward the user to authenticate (sign on) with the Identity Service. Then on each request from the client to the Accounts Service or Orders service, these services would check with the identity service if the session is still valid. </p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/k9ExUm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/k9ExUm.png"" alt=""identity service""></a></p>&#xA;&#xA;<p>Advantages to this approach are:</p>&#xA;&#xA;<ul>&#xA;<li>The Accounts and Orders Services do not need to know about usernames and passwords </li>&#xA;<li>Each service simply provides the data it is responsible for directly to the client</li>&#xA;</ul>&#xA;&#xA;<p>Downsides are:</p>&#xA;&#xA;<ul>&#xA;<li>This architecture is a bit more difficult to set up</li>&#xA;</ul>&#xA;&#xA;<p>A third approach is to implement either of these approaches into a single service. In the case of accounts and orders, I might make an argument that they are very closely related and that splitting them out into separate services may not improve your architecture.</p>&#xA;&#xA;<p>As I said, I am certainly not an expert in OAuth, but I have worked a fair bit with services and microservices.</p>&#xA;"
45546649,45546510,301607,2017-08-07T12:13:40,"<ol>&#xA;<li><p>You need a services orchestration tool for SOA architecture. E.g. <a href=""http://highscalability.com/zookeeper-reliable-scalable-distributed-coordination-system"" rel=""nofollow noreferrer"">ZooKeeper</a> but there are another solutions and you can choose what is better for you.</p></li>&#xA;<li><p>A common approach is to use SSO (single signon) solution. And again there are some tools and protocols e.g. <a href=""http://projects.spring.io/spring-security-oauth/"" rel=""nofollow noreferrer"">Spring oauth</a></p></li>&#xA;</ol>&#xA;&#xA;<p>In simple words you logged in and get a toke which is used for each call of your services.</p>&#xA;"
43209063,43208872,301607,2017-04-04T13:45:05,"<p>I would use the option 2 with <code>OncePerRequestFilter</code>. You can parse it once and store the results in a <code>ThreadLocal</code> or <code>InheritableThreadLocal</code> if you create new threads.</p>&#xA;&#xA;<p>Then the thread local data can be accessed from your controllers or even deeper from service layer.</p>&#xA;&#xA;<p><a href=""https://docs.oracle.com/javase/7/docs/api/java/lang/ThreadLocal.html"" rel=""nofollow noreferrer""><strong>ThreadLocal</strong></a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Each thread holds an implicit reference to its copy of a thread-local variable as long as the thread is alive and the ThreadLocal instance is accessible; after a thread goes away, all of its copies of thread-local instances are subject to garbage collection (unless other references to these copies exist).</p>&#xA;</blockquote>&#xA;"
45758154,45757001,301607,2017-08-18T13:42:05,"<p>My attempt to implement something like this <a href=""https://github.com/StanislavLapitsky/SpringSOAProxy"" rel=""nofollow noreferrer"">https://github.com/StanislavLapitsky/SpringSOAProxy</a> </p>&#xA;&#xA;<p>The idea is to check if a spring bean cannot be found locally then automatically create a Proxy which uses RestTemplate internally to call the same service remotely.</p>&#xA;&#xA;<p>You need to define contract  - services interfaces plus DTO and define URL resolver to specify which URL should be used for each service.</p>&#xA;"
45695249,45695032,1223719,2017-08-15T14:41:10,"<p>There are a ton of frameworks and services out there that will do this for you.  Dropwizard, statsd, New Relic, App Dynamics.  Just do some research. I'd say don't roll your own because you'll spend more time creating and adding features to your metrics system than you will be gaining value from it.  Use something out of the box to get up and running quickly with a host of features already developed.</p>&#xA;&#xA;<p>-- Update --</p>&#xA;&#xA;<p>Here are the things I'd look for when deciding on a cross service metrics generation / collection system.</p>&#xA;&#xA;<ul>&#xA;<li>Minimal to no source code manipulation.  Lots of libraries can inspect runtimes without actually modifying source code.  There are also APIs you can use in conjunction with the instrumentation to add specific metric gathering code if you need it later on.</li>&#xA;<li>Easy collection.  Pointing your metrics gathering services to a collector should be easy to configure.</li>&#xA;<li>Easy viewing / aggregation.  Once you have the metrics collected, it should be easy to see where the data came from, and what the data points are.  </li>&#xA;</ul>&#xA;&#xA;<p>Basically, choose one that gets you the most for minimal work.  You may find the base solution will meet your needs.  If not, spend time adding in extra collectors / dashboards.</p>&#xA;"
32304222,31190685,4836903,2015-08-31T04:56:54,"<p>You can read my article to understand design principles <a href=""https://techietweak.wordpress.com/2015/07/05/mdp/"" rel=""nofollow"">https://techietweak.wordpress.com/2015/07/05/mdp/</a>.<br>&#xA;I would also suggest the book ""building microserices"".</p>&#xA;&#xA;<p>Microservices:</p>&#xA;&#xA;<p>Micro services are small autonomous systems that provide a solution that is unique, distinct within the eco-system. It runs as a full-stack module and collaborates with  other micro-services that are part of the eco-system.  Sam Newman defines micro services are “Small , Focused and doing one thing very well” in his book “Building Microservices”.</p>&#xA;&#xA;<p>Micro services are created by slicing and dicing a single large monolithic system into many independent autonomous systems.  It can also be a plug-gable add-on component to work along with the existing system as a new component or as a green field project.</p>&#xA;"
32304339,30910817,4836903,2015-08-31T05:11:30,"<p>This is one of the main drawback of microservices architecture.  I can think of:</p>&#xA;&#xA;<ol>&#xA;<li>Implement a feedback loop machanism using a shared database.  For example, if service A, sends an update to B, C &amp; D.  Once B,C &amp; D comples the operation they should update the DB.  If the update is not done within a timeout period, based on the status update, service A could trigger a reverse operation or any other corrective action decided by the business logic.</li>&#xA;</ol>&#xA;&#xA;<p>We have the above implementation to solve the above problem.  </p>&#xA;&#xA;<p>You can read my article to understand microservices design principles :</p>&#xA;&#xA;<p><a href=""https://techietweak.wordpress.com/2015/07/05/mdp/"" rel=""nofollow"">https://techietweak.wordpress.com/2015/07/05/mdp/</a></p>&#xA;&#xA;<p>Thanks.</p>&#xA;"
50313596,50312750,9690151,2018-05-13T07:03:43,"<p>Both the approaches mentioned above are feasible approaches. </p>&#xA;&#xA;<ol>&#xA;<li><p>In case of 1st approach, you need to consider the design using Observer design pattern where the Interview Scheduler (webapp1) is a subject subscribed by observers in webapp2. This will be instantaneous as the interview scheduler will immediately inform the other webapp about the interviews getting scheduled. You can have one service endpoint in webapp 2 for notification purpose being called by webapp1 as soon as the interview is scheduled. The cons of this approach will be additional programming effort on both the webapps.</p></li>&#xA;<li><p>The solution is feasible in many databases and will depend on which database are you using. But, the trouble you will face as per my understanding is that it is not a notification. Even if the trigger updates the database, no one is being notified that it has changed. You will need to implement a listener anyways in order for webapp2 to know if there are some changes in the database. </p></li>&#xA;</ol>&#xA;&#xA;<p>This is not an exact solution you had hoped for but I suppose it will help you in drilling down more on the possible solutions.</p>&#xA;"
35003219,34973135,4849191,2016-01-25T21:59:26,"<p>Either way could work for this scenario but it doesn't sound like you have a need for features of the Actor pattern here, so I would suggest starting with a Reliable Service and storing the users in a Reliable Dictionary. Keep in mind that Actors are a specific pattern implemented on top of Reliable Services so in some ways you'll be limited to the constraints of that pattern, which might be a problem later on if not planned for carefully. For example, running a query over a set of actors doesn't work very well, so if you decide at some point later on you need to run a query over your user base, you'll be much better off if you use a Reliable Dictionary where it's easy to do queries.</p>&#xA;&#xA;<p>For your event service, yes you can certainly partition by geo-coordinate. One way I've done this in the past is to convert geo-coordinates to <a href=""https://msdn.microsoft.com/en-us/library/bb259689.aspx"" rel=""nofollow"">quadkeys</a>, which are a convenient way to represent 2D spatial data in a one-dimensional key. However, keep in mind you may get local hotspots which could cause some clustering in your partitions (are most of your users centered around major cities? If so, those paritions will have more data than others).</p>&#xA;"
38257195,38197629,4849191,2016-07-08T00:15:47,"<p>Yeah there's definitely a problem when referencing a ""classic"" Class Library project from a Core 1.0 project when you only have an x64 build configuration. This isn't specific to Service Fabric but it tends to come up there because SF is x64-only. </p>&#xA;&#xA;<p>For now, you can work around this by adding an AnyCPU build configuration to your solution, and make sure the build output path of the Class Library project's x64 configuration is <strong>bin\&lt;configuration&gt;</strong> (e.g., bin\Debug). </p>&#xA;"
37620681,37617556,4849191,2016-06-03T17:53:40,"<p>Yes S2 can use HttpClient to call S1. First S2 needs to resolve an address for S1. Service Fabric has APIs for that. Here's some documentation that gives you an overview of how this works:</p>&#xA;&#xA;<p><a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-connect-and-communicate-with-services/"" rel=""nofollow"">https://azure.microsoft.com/en-us/documentation/articles/service-fabric-connect-and-communicate-with-services/</a></p>&#xA;&#xA;<p><a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-reliable-services-communication/"" rel=""nofollow"">https://azure.microsoft.com/en-us/documentation/articles/service-fabric-reliable-services-communication/</a></p>&#xA;&#xA;<p>And a simple sample application that has one service that resolves the address of a second service (it's resolving a stateful service but it works the same way for stateless)</p>&#xA;&#xA;<p><a href=""https://github.com/Azure-Samples/service-fabric-dotnet-getting-started/tree/master/Services/WordCount/WordCount.WebService"" rel=""nofollow"">https://github.com/Azure-Samples/service-fabric-dotnet-getting-started/tree/master/Services/WordCount/WordCount.WebService</a></p>&#xA;"
37079545,36948775,4849191,2016-05-06T18:55:50,"<p>Ideally each microservice instance is self-contained so that each instance can scale independently of others while also encapsulating its state so that others can only access it through a well-defined API. So not only do you need to figure out how to scale your database(s) that your microservices use to store state, you also have this encapsulation problem to solve if you really want to nail this architectural pattern.</p>&#xA;&#xA;<p>Have you looked at <a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-overview/"" rel=""nofollow"">Service Fabric</a> to solve this? Service Fabric has a concept of <a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-reliable-services-introduction/"" rel=""nofollow"">stateful services</a> where data is actually stored inside each microservice instance. The platform handles replication and disk persistence automatically for HA and also has data partitioning built-in for distribution across machines. The idea is basically to ditch the central database, instead co-locating your compute and data within a microservice instance. Now your services are self-contained and suddenly the solution fits this architectural pattern nicely, because now each microservice instance can be scaled out and upgraded independently and you have full encapsulation of your data inside the service. The trade-off of course is that you don't get the feature set of a full-blown RDBMS, but if you're considering NoSQL stores anyway that shouldn't be a huge deal. </p>&#xA;&#xA;<p>My thought on this has always been that a central store like a database is somewhat of an anti-pattern in a shared-nothing microservices architecture. Full disclosure though: I work on Service Fabric so my opinion may be a little biased!</p>&#xA;"
34645755,34593341,4849191,2016-01-07T01:10:31,"<p>Simon Houlton covered the trade-offs around microservices pretty well in his answer, so here are some answers specifically to your Service Fabric questions:</p>&#xA;&#xA;<ol>&#xA;<li><p>Applications are a grouping construct for services. In theory, services that work together to produce a cohesive set of functionality or accomplish a single business goal are usually grouped into an application. In practice: </p>&#xA;&#xA;<ul>&#xA;<li>Upgrades are applied at the application level. Services that generally need to be upgraded together should be in the same application. But you can still upgrade service within an application individually too.</li>&#xA;<li>Health status is rolled up at the application level so you get an all-up health overview of services within an application.</li>&#xA;<li>Visual Studio tooling makes it pretty easy to work with an application that has multiple services.</li>&#xA;</ul></li>&#xA;<li><p>Usually a service is host to multiple <em>instances</em> of one actor <em>type</em>. In other words, you usually map one actor <em>type</em> to one service type. Then you can instantiate multiple instances of that actor type in the service. </p></li>&#xA;<li>Depends on what you want here really. What's the stateless service doing? If you need a single ingress point for your users, then you should have one stateless web api for the whole project. That's pretty standard. Maybe you have a second stateless service listening on a different port just for admins or something. If you have a stateless web api service for each stateful service, then you have to worry about routing users to the right stateless service. </li>&#xA;</ol>&#xA;"
34685801,34676168,4849191,2016-01-08T20:48:24,"<p>It's possible but it probably won't perform very well because the grouping actor will have to make an individual request to each actor for its geocoordinate, and these requests aren't cheap because they involve data serialization and network requests.</p>&#xA;&#xA;<p>It would probably be easier and better performing to just use a <a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-reliable-services-reliable-collections/"" rel=""nofollow"">Reliable Dictionary</a> within a <a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-reliable-services-introduction/"" rel=""nofollow"">Reliable Service</a>. Each coordinate and any associated data can be an entry in the dictionary. You can enumerate, perform LINQ queries, and generally program like you would with a normal dictionary.</p>&#xA;&#xA;<p>With millions of items, you'll probably want to <a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-concepts-partitioning/"" rel=""nofollow"">partition</a> the service to spread it over multiple nodes. You get full control over key distribution across partitions, so you can do things like partition by region by hashing geocoordinates or using quad keys. </p>&#xA;"
40594822,40542220,4849191,2016-11-14T17:49:22,"<p>You can use an Internet-facing load balancer and just configure it to only accept and forward traffic on the ports used by your public-facing services. Don't expose ports 19000 and 19080 (or whatever ports you configured your cluster with) through the load balancer if you don't want the management endpoints exposed. Hopefully you've <a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-cluster-security/#security-recommendations"" rel=""nofollow noreferrer"">secured your cluster</a> so that even if an unauthorized user did get through to the management endpoints somehow, they would not have access to do anything.</p>&#xA;"
40686784,40685900,4849191,2016-11-18T22:15:29,"<p>You should be using a <a href=""https://f5.com/resources/white-papers/load-balancing-101-nuts-and-bolts"" rel=""nofollow noreferrer"">load balancer</a> in front of your cluster to distribute traffic to your front-end service instances. The load balancer should be configured to take nodes out of the load balancer's rotation if they are not responding to user requests. This helps protect your services and your users from outages.</p>&#xA;&#xA;<p>If your users are within a trusted boundary (e.g., Intranet users), then the alternative to a load balancer would require the client application to first do a <a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-connect-and-communicate-with-services"" rel=""nofollow noreferrer"">service address look-up with Service Fabric's naming service</a>, which will give you a list of endpoints of each stateless service instance that is available. Again, you don't really want to expose this to untrusted users because they can easily abuse it or overload it. If you do this you're relying on your client application to provide load balancing by picking a random instance to connect to. But really what you should do is get a load balancer.</p>&#xA;"
40594642,40564979,4849191,2016-11-14T17:37:25,"<p>If it's a standalone web application with a self-hosted web server (e.g., Katana, not IIS), then you can simply run it as a <a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-deploy-existing-app/"" rel=""nofollow noreferrer"">Guest Executable</a>.</p>&#xA;&#xA;<p>If it's not self-hosted and requires a separate web server to run, like IIS, then you can look at running it in a <a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-deploy-container/"" rel=""nofollow noreferrer"">Container</a>.</p>&#xA;"
39047675,38975554,4849191,2016-08-19T21:06:17,"<p>This was answered correctly earlier by another user but it got deleted by a mod for whatever reason, so for posterity: </p>&#xA;&#xA;<p><strong>Yes</strong> this is possible. You just have to specify the listener name when you create the service proxy:</p>&#xA;&#xA;<pre><code>var proxy = ServiceProxy.Create&lt;ICategoryQueryService&gt;(new Uri(""fabric:/Taxonomy/TaxonomyService""), listenerName: ""SqlCategoryQueryService"");&#xA;</code></pre>&#xA;"
44296434,44198061,4849191,2017-06-01T00:00:16,"<p>What is it about your current setup that isn't meeting your requirements? What do you hope to gain from a more complex architecture? </p>&#xA;&#xA;<p>Microservices aren't a magic bullet. You mainly get four benefits:</p>&#xA;&#xA;<ol>&#xA;<li>You can scale and distribute pieces of your overall system independently. Service Fabric has <em>very</em> sophisticated tools and advanced capabilities for this.</li>&#xA;<li>You can deploy and upgrade pieces of your overall system independently. Service Fabric again has advanced capabilities for this.</li>&#xA;<li>You can have a polyglot system - each service can be written in a different language/platform.</li>&#xA;<li>You can use conflicting dependencies - each service can have its own set of dependencies, like different framework versions.</li>&#xA;</ol>&#xA;&#xA;<p>All of this comes at a cost and introduces complexity and new ways your system can fail. For example: your fast, compile-time checked in-proc method calls now become  slow (by comparison to an in-proc function call) failure-prone network calls. And these are <strong>not</strong> specific to Service Fabric, btw, this is just what happens you go from in-proc method calls to cross-machine I/O - doesn't matter what platform you use. The <em>decision path</em> here is a pro/con list specific to your application and your requirements.</p>&#xA;&#xA;<p>To answer your Service Fabric questions specifically:</p>&#xA;&#xA;<ul>&#xA;<li>Which programming model do you go for? Start with stateless services with ASP.NET Core. It's going to be the simplest translation of your current architecture that doesn't require mucking around with your data layer.</li>&#xA;<li>Stateful has a lot of great uses, but it's not necessarily a replacement for your RDBMS. A good place to start is hot data that can be stored in simple key-value pairs, is accessed frequently and needs to be low-latency (you get local reads!), and doesn't need to be datamined. Some examples include user session state, cache data, a ""snapshot"" of the most recent items in a data stream (like the most recent stock quote in a stream of stock quotes). </li>&#xA;<li>Currently the only way to see or query your data is programmatically directly against the Reliable Collection APIs. There is no viewer or ""management studio"" tool. You have to write (and secure) an API in each service that can display and query data.</li>&#xA;<li>Finally, the actor model is a <em>very</em> niche model. It serves specific purposes but if you just treat it as a data store it will not work for you. Like in your example, a listing per actor probably wouldn't work because you can't query across that list, or even have multiple users reading the same listing simultaneously.</li>&#xA;</ul>&#xA;"
42403878,42395345,4849191,2017-02-22T22:57:22,"<p>Upgrading a stateful service does <strong>not</strong> remove data in a Reliable Collection by design. The entire Service Fabric platform is built around solving this very problem with rolling in-place upgrades of stateful service. </p>&#xA;&#xA;<p>My guess is you are either not actually performing rolling upgrades, and instead you are deleting and re-creating the service. That's what the default ""Remove Application"" setting in Visual Studio Application Debug Mode does - it deletes the application when you stop debugging (thus deleting state). The ""Auto Upgrade"" option will not delete the application and will automatically perform a rolling upgrading for you the next time you press F5. If your state is gone in this case, then I'm willing to bet there is a bug in your service code. Try it with a brand new stateful service project.</p>&#xA;"
36211690,36157778,4849191,2016-03-24T23:27:50,"<p>Judging by your second stack trace:</p>&#xA;&#xA;<pre><code>System.ArgumentException: the provided uri scheme 'http' is invalid; expected 'net.tcp'.&#xA;</code></pre>&#xA;&#xA;<p>It looks like you're trying to use ServiceProxy to connect to a service that is listening on an HTTP endpoint. ServiceProxy expects a Service Remoting Listener which uses a binary protocol, not HTTP.</p>&#xA;&#xA;<p>The first exception is still a bit of a mystery:</p>&#xA;&#xA;<pre><code>System.Fabric.FabricException: Invalid partition key/ID '{0}'  for selector {1}&#xA;</code></pre>&#xA;&#xA;<p>This will only happen if you attempt to resolve a uniform int64 or named partitioned service without providing a partition key. In your config it appears you have the target service set up as a singleton partition, in which case you should not see this error, so can you double-check that the service you are trying to connect to is in fact created as a singleton partition service?</p>&#xA;"
48801991,48801894,5902888,2018-02-15T07:31:44,"<p>I would suggest to use some bus implementation instead of plain web service call. For example you might use NServiceBus for such purposes. This will make component communication more transparent and robust (you will get load balancing, fallback mechanisms and lot more out of the box). Or you could use some queue implementation (if you won't purchase NServiceBus). RabbitMQ, ZeroMQ or anything else like that would be a good choise (base on your requirements of course). This approach will give you a lot of advantages as well.</p>&#xA;"
46724453,46724191,2445864,2017-10-13T07:16:14,"<p>Non-official, non-scientific reasons for microservices based on my experience:</p>&#xA;&#xA;<ul>&#xA;<li><strong>Horizontal scalability</strong>: If your services do not scale evenly, you might want to have 10 servers with service A, but only 1 with service B.</li>&#xA;<li><strong>Faster deployment</strong>: Having a separate deployment process per service allows you to deploy smaller pieces on-demand instead of deploying a big thing just because of a small change.</li>&#xA;<li><strong>Independent development style and lifecycle</strong>: When a project grows bigger, you might want to have other people take it over. This obviously works better with a microservice that has less dependencies than a module. Contributors and teams can develop how they want with whatever they want. Every team can use a different language, different pipelines, testing tools, editors etc. The only common denominator is formats, protocols and conventions of the resulting API.</li>&#xA;<li><strong>Simpler dependency management</strong>: Upgrading or changing libraries that depend on other libraries can lead to complications, because obviously not all of your dependencies will be updated at the same time and there will be incompatibilities.</li>&#xA;<li><strong>Clearer API</strong>: Documenting all public classes and functions can be a nightmare. On the other side having APIs with incomplete or outdated documentation is a reason to abandon projects. Microservices do not offer anything out of the box in that regard, but the interfaces are clearly defined, usually as URL endpoints and HTTP methods. This gets easier if you're following the <a href=""https://en.wikipedia.org/wiki/Representational_state_transfer"" rel=""nofollow noreferrer"">REST</a> paradigm.</li>&#xA;</ul>&#xA;&#xA;<p>In your particular case it's hard to say what to use because you do not give enough context. Some of the relevant questions:</p>&#xA;&#xA;<ul>&#xA;<li>Are you developing with a team?</li>&#xA;<li>Is this part of a framework?</li>&#xA;<li>What are the module's dependencies?</li>&#xA;<li>Is it designed with parallel access to the DB in mind?</li>&#xA;</ul>&#xA;"
46576330,46575898,2445864,2017-10-05T01:00:43,"<h2>Microservices in my own, hopefully simple terms</h2>&#xA;&#xA;<h3>Monoliths</h3>&#xA;&#xA;<p>Traditionally web applications are big. You write one piece of software that runs on a server and answers requests in form of HTML, XML or JSON. If you want your web application to do something new, you add that functionality to the existing application. Such big systems are called ""monolithic"" (a monolith is a very big rock).</p>&#xA;&#xA;<p>Monoliths are problematic, because they usually grow in size and complexity over time. This is a problem when developing something in a team. Developers are adding new code to the system and can't change or re-use the existing code, because there is many dependencies between the code pieces. They are also too afraid of removing old code because it might be used somewhere.</p>&#xA;&#xA;<p>When delivering such code to clients, e.g. by putting it on the internet, we call that ""deploying"". Deploying and the usual testing after deployment is difficult, because within a big system there is a lot of things that can break. Finding out what is going wrong and who should fix it, is very difficult and requires people to know the whole thing.</p>&#xA;&#xA;<p>Another disadvantage is the scalability. By that we mean ""how can we serve more users at the same time?"" A single web server computer can only handle a certain amount of users accessing it in parallel. Upgrading that computer to better hardware makes it serve more users, but you will soon hit the boundaries of what is possible with hardware. This upgrading is called vertical scaling. We could also put our web application on two or more servers, so that we can handle more users. This is called horizontal scaling. Monolithic applications are traditionally made only with vertical scaling in mind.</p>&#xA;&#xA;<h3>Microservices</h3>&#xA;&#xA;<p>In order to simplify the workflow with big applications, we can split it into smaller parts. Each part serves one particular purpose. We call that a ""(web) service"". These web services are very flexible to use. You can use them from within your existing monolithic application, either in the server part, or in the client part. You can also have a web service that uses other web services.</p>&#xA;&#xA;<p>The split into single web services allows you to loosely couple your application. This means that as a user of the service you only depend on the service being up, available and working. You no longer need to take care of its dependencies, its compilation, deployment or testing.</p>&#xA;&#xA;<p>You can give that responsibility to a different developer or team. You can't break their web service because you do not access it through the source code. They can even use a different programming language and you could still use their service.</p>&#xA;&#xA;<p>This independence is made possible by deciding on using a common format and common <a href=""https://en.wikipedia.org/wiki/Communications_protocol"" rel=""noreferrer"">protocols</a> (a protocol is a way of communicating). For web services the most popular formats are <a href=""https://en.wikipedia.org/wiki/JSON"" rel=""noreferrer"">JSON</a> and <a href=""https://en.wikipedia.org/wiki/XML"" rel=""noreferrer"">XML</a>. The protocol used the most is <a href=""https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol"" rel=""noreferrer"">HTTP</a>, because it's simple, well-supported by all existing software and your browser is using it, too.</p>&#xA;&#xA;<p>The word ""micro"" in ""microservices"" just emphasises the idea to make these web services as small as possible. If you need a more complex service, it is usually better to create a new service that depends on one or more others.</p>&#xA;&#xA;<h2>Example</h2>&#xA;&#xA;<p>Let's say you have an application where users can create virtual post cards. This is how such an application could be realised using microservices architecture.</p>&#xA;&#xA;<ul>&#xA;<li>A thin static web page that consists of only HTML, CSS and JavaScript</li>&#xA;<li>A microservice ""card library"" that offers a list of card templates including their dimensions and intention.</li>&#xA;<li>A microservice ""thumbnailer"" that, given a card template name, offers you a small preview of the card.</li>&#xA;<li>A microservice ""renderer"" that expects a template and the text to fill in. It then renders a card image and returns that.</li>&#xA;</ul>&#xA;&#xA;<p>How they're wired:</p>&#xA;&#xA;<ol>&#xA;<li>Web page has the URLs to the card library and to the renderer. The user's browser is calling these services as issued by JavaScript code.</li>&#xA;<li>The library is using the thumbnailer to return a list of cards including their thumbnails</li>&#xA;<li>When the user has selected one, the browser sends that template with the user input to the renderer and the browser shows the returned image.</li>&#xA;</ol>&#xA;"
51658637,44458709,5922344,2018-08-02T16:47:29,<p>Alternativly you can exclude netty from being included by either one of your dependencies. To do so you just need to add the following to the dependency from which you don't want to use the subdependencies:</p>&#xA;&#xA;<pre><code>&lt;exclusions&gt;&#xA;    &lt;exclusion&gt;&#xA;        &lt;groupId&gt;io.netty&lt;/groupId&gt;&#xA;        &lt;artifactId&gt;netty-common&lt;/artifactId&gt;&#xA;    &lt;/exclusion&gt;&#xA;    &lt;exclusion&gt;&#xA;        &lt;groupId&gt;io.netty&lt;/groupId&gt;&#xA;        &lt;artifactId&gt;netty-transport&lt;/artifactId&gt;&#xA;    &lt;/exclusion&gt;&#xA;&lt;/exclusions&gt;&#xA;</code></pre>&#xA;
50367107,50363538,1008424,2018-05-16T09:28:52,"<p>In API manager 3.0.0 release you can discover micro-services deployed in kubernetes. </p>&#xA;&#xA;<p><a href=""https://docs.wso2.com/display/AM300/Discovering+Service+Endpoints"" rel=""nofollow noreferrer"">https://docs.wso2.com/display/AM300/Discovering+Service+Endpoints</a></p>&#xA;"
38002018,37662379,1009448,2016-06-23T21:17:55,"<p>I've run on the same issue on my OSX. Currently I've managed to workaround this through a virtual-machine on linux(debian/ubuntu) using the jhipster-generator version 3.4.0 . Newer version will not create the entities even for me.</p>&#xA;&#xA;<p>Please try to delete all stuff installed from bottom to top from the instalation guide <a href=""https://jhipster.github.io/installation/"" rel=""nofollow noreferrer"">here</a> until nodejs part.</p>&#xA;&#xA;<p>So yo will done the following:</p>&#xA;&#xA;<pre><code>npm uninstall -g generator-jhipster&#xA;npm uninstall -g gulp&#xA;npm uninstall -g bower&#xA;npm uninstall -g yo&#xA;</code></pre>&#xA;&#xA;<p>Then follow steps to remove nodejs from <a href=""https://stackoverflow.com/a/11178106/1009448"">here</a></p>&#xA;&#xA;<p>After uninstalling start again installing nodejs and commponents as suggested on jhipster guide with the only difference at the last step:</p>&#xA;&#xA;<pre><code>npm install -g generator-jhipster@3.4.0&#xA;</code></pre>&#xA;"
30460406,30449278,1413416,2015-05-26T13:45:43,"<p>I think the tool you're looking for is a git hook.</p>&#xA;&#xA;<p>Personally I'd probably set up a server side post-receive hook [0] in your repository which takes the semantic tag and either automatically updates the software version in the pillar data, or creates a Salt event which triggers either an update or a deploy using the provided data.</p>&#xA;&#xA;<p>There's also the option of an external pillar data source [1], where it can automatically get the most recent tag or commit on git's master branch.</p>&#xA;&#xA;<p>In either case, I'd keep the git merge and tag a manual step.</p>&#xA;&#xA;<p>[0] <a href=""http://www.git-scm.com/book/en/v2/Customizing-Git-Git-Hooks"" rel=""nofollow"">http://www.git-scm.com/book/en/v2/Customizing-Git-Git-Hooks</a></p>&#xA;&#xA;<p>[1] <a href=""http://docs.saltstack.com/en/latest/topics/development/external_pillars.html"" rel=""nofollow"">http://docs.saltstack.com/en/latest/topics/development/external_pillars.html</a></p>&#xA;"
50655492,50651256,1479976,2018-06-02T09:37:03,"<p>We usually cut our services on aggregate boundaries. Domain Driven Design fits very well with microservices, as it helps designing the aggregates with loose coupling. I would recommend doing that first and never reference the order in the customer and vice versa. Only communicate via Domain Events. This way the decision to run one aggregate on another process or server is just an implementation detail and can be done later. </p>&#xA;&#xA;<p>If you split them up in two services you will have to implement some form of communication. This is usually more expensive to implement then running them on the same process, but you get more flexibility in terms of scaling. In your case with only two aggregates, I would keep them on one service. One big plus of microservices should be that they are so small, that you can just delete, rewrite and replace it. I think with two aggregates this is still possible and therefore not worth the hassle.</p>&#xA;&#xA;<p>But again, doing a mircorservice architecture should be an implementation detail. Your domain has to be designed well first or otherwise cutting aggergates into services will be a nightmare.</p>&#xA;&#xA;<p>The only plus for creating microservices up front is, that you already start designing with the fact in mind, that you can not just reference another aggregate and read some propeties to decide something. Wich can be valuable if your team is not used to DDD or loose coupling.</p>&#xA;"
50655948,50645878,1479976,2018-06-02T10:30:02,"<p>I think there are two main reasons services should be stateless</p>&#xA;&#xA;<ol>&#xA;<li>A statefull service can not be scaled as easily. If you want to deploy more units on peek times a stateless service is way easier to handle</li>&#xA;<li>Statless services allow you to not worry about other services while developing. Just the possibility that one service has to behave differently because another services is in a specific state is very confusing and will eventually get a ""distributed monolith""</li>&#xA;</ol>&#xA;"
50641534,50634072,1479976,2018-06-01T10:35:47,"<p>I think an Atom Feed is what you are looking for on a microservice level</p>&#xA;&#xA;<p><a href=""https://martinfowler.com/bliki/AtomFeeds.html"" rel=""nofollow noreferrer"">https://martinfowler.com/bliki/AtomFeeds.html</a></p>&#xA;&#xA;<p>We use this pattern to distribute DomainEvents in our Domain. The other services subscribe on this interface and get notified as soon as the providing service publishes a DomainEvent. This works quite well and there is no polling or stuff like that included.</p>&#xA;&#xA;<p>We code in .net, so on the first search this seems like promising java implementation:</p>&#xA;&#xA;<p><a href=""https://github.com/rometools/rome"" rel=""nofollow noreferrer"">https://github.com/rometools/rome</a></p>&#xA;&#xA;<p>or an how to for c#</p>&#xA;&#xA;<p><a href=""https://docs.microsoft.com/en-gb/dotnet/framework/wcf/feature-details/how-to-create-a-basic-atom-feed"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-gb/dotnet/framework/wcf/feature-details/how-to-create-a-basic-atom-feed</a></p>&#xA;"
51172411,51071363,1818664,2018-07-04T11:18:32,"<p>The solution for financial transactions is:&#xA;1.Any transaction on account A is a put into a queue that must have account A in the transaction. Let us call this queue QueueA. There is some code that will process transaction on QueueA, maybe a separate thread.&#xA;2.If there is any new transaction request, check the accounts involved in the transaction. If there is a queue with one of the accounts added to it, then add that transaction into that queue, if not create a new queue and add the transaction there.</p>&#xA;&#xA;<p>The idea is to linearlize processing of transactions on the same account. This prevents database connection from blocking and timing out. Also, use optimistic database locking by adding a version column on each table. This insures data integrity in financial systems as opposed to pessimistic locking which might cause database connections from timing out.</p>&#xA;"
47357236,47357038,6603816,2017-11-17T18:40:27,"<p>In the microservices world, services should be divided according to the bussiness domain that each service represents and not by specific technical functionality as you are proposing. This is an architectural design that I don't recommend bypassing just to workaround some specific technical funtionality.</p>&#xA;&#xA;<p>The approach to solve your problem is not by splitting the microservice into two services that are in the same business domain. </p>&#xA;&#xA;<p>Your problem is a performance issue. Performance issues are generally solved by scaling. It is totally normal to replicate your serivices by deploying multiple containers. Docker gives you this option by default where when deploying a service, you can specify the number of replicas to deploy.</p>&#xA;"
46379860,46377039,6603816,2017-09-23T13:01:16,"<blockquote>&#xA;  <p>what is the Docker</p>&#xA;</blockquote>&#xA;&#xA;<p>The <a href=""https://www.docker.com/what-docker"" rel=""nofollow noreferrer"">Docker docs</a> are the best place to learn about Docker. </p>&#xA;&#xA;<blockquote>&#xA;  <p>how Docker help to build Micro-services?</p>&#xA;</blockquote>&#xA;&#xA;<p>Docker doesn't help in building microservices, it helps in packaging, deploying, and shipping microservices. However, Docker is a great tool to grab all the dependencies needed to develop services in general, from database to cloud components ...</p>&#xA;&#xA;<blockquote>&#xA;  <p>Is it Docker is the only way to host Micro-services</p>&#xA;</blockquote>&#xA;&#xA;<p>No Docker is not the only way to deploy microservices, however it is probably the most popular way nowadays to package and deploy services.</p>&#xA;"
48063531,48062134,6603816,2018-01-02T15:24:52,"<p>The problem is you are trying to connect from one service to the other using <code>localhost</code>. This won't work as each container has it own IP and localhost will just point back to the caller of the request.</p>&#xA;&#xA;<p>The stardard Docker way to connect containers, is to connect them to a Docker network.</p>&#xA;&#xA;<pre><code>docker network create mynet&#xA;docker run --network mynet --name container-1 ...&#xA;docker run --network mynet --name container-2 ...&#xA;</code></pre>&#xA;&#xA;<p>Now container1 can communicate with container2 using <code>http://container-2:8080</code>.</p>&#xA;"
51383960,51282761,9231144,2018-07-17T14:28:53,"<p>Make sure you are exposing your pods through a service (ingress or loadBalancer) and use <a href=""https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"" rel=""nofollow noreferrer"">HPA</a>. You'll have to monitor your pods to see which metric is primarily used. If the pods tend to run out memory first, set HPA to base off of memory and vice versa.</p>&#xA;"
51598031,51573236,1044264,2018-07-30T16:04:46,"<p>to have an overview of the role of an API Gateway in a microservice based solution, I suggest you to have a look to this <a href=""https://www.youtube.com/watch?v=hgqqPaVEZpk"" rel=""nofollow noreferrer"">presentation</a> I did couple of months ago. That should clarify the things a little bit.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Is my understanding of this correct?</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes, you got it. If you have a look to the video, you can actually see that concept in practice as well.</p>&#xA;&#xA;<p>For the storage, that kind of depends.</p>&#xA;&#xA;<p>Express Gateway offers a subset of identity services such as users, applications and credentials. They're good enough for most application usages but you might find yourself with the needs of using an external service, such as Auth0.</p>&#xA;&#xA;<p>Now, features aside — where store the data it's up to you. You can keep some of the data in Express Gateway and some of it in your own database — or entirely in EG. There's not a good or bad strategy here.</p>&#xA;"
39306954,39305118,6722089,2016-09-03T12:35:55,"<p>In any case I'd go with stateful services. If you want to go a tad hands off, have a look at <a href=""https://azure.microsoft.com/en-us/services/service-fabric/"" rel=""nofollow"">Azure Service Fabric</a>.</p>&#xA;&#xA;<p>And as in my case, I had my own set of microservices, in a scenario like this I did the basic create operation on db first (Changing the email). I had an event entity and pushed back an event in that collection (in this case mongodb). A stateful service was polling the database and processing the events in batch.</p>&#xA;&#xA;<p>Now in your case, if your web app process is persistent you can opt to enqueue the message right away and keep a field in the event that states whether it was actually processed later by any service or not. I used mongodb for database and Azure Service Bus as a message broker. I think Amazon SQS would be similiar. </p>&#xA;&#xA;<p>Now, if your web app is a vanilla asp.net Web api or mvc process, you only should enlist the event in database and leave as in that way you dont have to create a mesasge broker listener every time you getting a request. One service can poll the db, use the message broker to let the other services know. </p>&#xA;&#xA;<p>If you want a total event driven paradigm, you might need a look in <a href=""https://azure.microsoft.com/en-us/services/event-hubs/"" rel=""nofollow"">Event Hubs</a></p>&#xA;&#xA;<p>I strongly suggest keeping a tab on whether any resource has been processed or not from the Message Bus just to make sure it's reliable. </p>&#xA;&#xA;<p>Hope it helps. :)</p>&#xA;"
40499707,40496999,1547817,2016-11-09T03:10:08,"<p>First of all, spring boot is a great project. Spring boot use default config but no longer mass of config files. This feature is support by <code>auto-config</code> project. And, spring boot use embedded containers, so that you can just start up a service single. That is your self config container such tomcat\jetty is no longer needed. Finally, microservices is just a concept of a service, you can split you service in any way. i.e. every controller class as a microservices is fine, every module as a microservice is fine. For me, use spring boot to build a microservice is a way to quick deploy a service at any service enveriment, I can start a service just copy a jar to a new service, and, run it just use <code>java -jar</code> </p>&#xA;"
46682724,46682651,1943849,2017-10-11T07:51:11,"<p>Yes, look at the <strong>scratch</strong> keyword (<a href=""https://docs.docker.com/engine/userguide/eng-image/baseimages/#create-a-simple-parent-image-using-scratch"" rel=""nofollow noreferrer"">docs</a>):</p>&#xA;&#xA;<blockquote>&#xA;  <p>You can use Docker’s reserved, minimal image, scratch, as a starting&#xA;  point for building containers.</p>&#xA;</blockquote>&#xA;&#xA;<p>Also you may find useful using <a href=""https://docs.docker.com/engine/userguide/eng-image/multistage-build/"" rel=""nofollow noreferrer"">multi-stage builds</a>.</p>&#xA;&#xA;<p>An example:</p>&#xA;&#xA;<pre><code>FROM scratch&#xA;ADD hello /&#xA;&#xA;FROM fedora&#xA;RUN yum -y update &amp;&amp; yum clean all&#xA;RUN yum -y install nginx&#xA;</code></pre>&#xA;"
47276206,47265946,5813801,2017-11-14T01:23:33,"<p>Currently I am working on microservices too, according my experience  we have designed microservices  as step below,</p>&#xA;&#xA;<ul>&#xA;<li><p><strong>Maven</strong></p>&#xA;&#xA;<ul>&#xA;<li>You should create the project with different project. But actually you can separate your project to submodule. So you will be easy to manage your project, the  submodule you can use with other project too.</li>&#xA;<li>Build the Jar Library put your local repository. it can save your time, you have just find the same component or your functionality then build the jar file put in your local repository , so every project that use this function call point to download this repository, you don't have to write many project same same.</li>&#xA;</ul></li>&#xA;</ul>&#xA;&#xA;<p>So finally I would like you to create different springboot project, but just create submodule and build local repository.</p>&#xA;"
52027825,52001694,9231401,2018-08-26T15:43:21,"<p>In the snippet given the issue is that Quart has no <code>werkzeug</code> logger, it has <code>quart.app</code> and <code>quart.serving</code> so the likely equivalent is to do &#xA;<code>logging.getLogger('quart.serving').setLevel(logging.ERROR)</code></p>&#xA;&#xA;<p>Note, this question was asked as a Quart <a href=""https://gitlab.com/pgjones/quart/issues/130"" rel=""nofollow noreferrer"">issue</a>, and I am the Quart author.</p>&#xA;"
48971416,48949396,2750505,2018-02-25T07:53:46,"<p>I suppose that you have some sort of microservice to handle client authentication into his SaaS account and generate a token to consume the SaaS microservices (like a ""private key"") correct?&#xA;It's the perfect case for microservices architecture:</p>&#xA;&#xA;<ol>&#xA;<li>Create a microservice that domains resources about the client's environment configuration</li>&#xA;<li>This microservice receives requests with the client's private key</li>&#xA;<li>Then requests the authentication service to validate the passed private key</li>&#xA;<li>Get the response of the authentication service and some sort of client's unique key</li>&#xA;<li>Responds with the environment configuration corresponding to that client's unique key (or 404 if the  auth token doesn't match with any client)</li>&#xA;</ol>&#xA;&#xA;<p>Now having this microservice (I'll call ""environment microservice""), any other microservice of your SaaS just needs request the environment microservice to get  client's configurations (database connection string, storage system and etc). From this point, you can implement some caching policy at each service to keep the private keys mapping to a set of configurations (and persistent database connections if your model permits). Just ensure that this cache has an interval to validate the tokens and configurations against the environment microservice.</p>&#xA;"
47363721,47343439,1773866,2017-11-18T07:34:24,<p>This feature is available in edgware release train. That corresponds to version 1.3.x of sleuth </p>&#xA;
34439413,34439201,1773866,2015-12-23T15:57:35,"<p>Here you have an example of a brewery system - <a href=""https://github.com/spring-cloud-samples/brewery"" rel=""nofollow"">https://github.com/spring-cloud-samples/brewery</a>. One of the files is a docker-compose file for CONSUL.</p>&#xA;&#xA;<p><a href=""https://github.com/spring-cloud-samples/brewery/blob/master/docker-compose-CONSUL.yml"" rel=""nofollow"">https://github.com/spring-cloud-samples/brewery/blob/master/docker-compose-CONSUL.yml</a></p>&#xA;&#xA;<p>Check out all the <code>application-consul.yaml</code> files that are inside the codebase to see how to set up the Spring Boot apps to talk to consul.</p>&#xA;&#xA;<p>Example: <a href=""https://github.com/spring-cloud-samples/brewery/blob/master/aggregating/src/main/resources/application-consul.yaml"" rel=""nofollow"">https://github.com/spring-cloud-samples/brewery/blob/master/aggregating/src/main/resources/application-consul.yaml</a></p>&#xA;&#xA;<p>In case of any issues write here or go to spring-cloud gitter <a href=""https://gitter.im/spring-cloud/spring-cloud"" rel=""nofollow"">https://gitter.im/spring-cloud/spring-cloud</a></p>&#xA;"
43395893,43395580,1773866,2017-04-13T15:10:01,"<p>Have you considered not doing end to end testing and do contract testing instead? <a href=""https://cloud.spring.io/spring-cloud-contract/"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-contract/</a></p>&#xA;"
51762524,51755653,1773866,2018-08-09T08:44:54,"<p>You'd have to make a conversion from Cucumber to Groovy DSL. You can read the documentation here <a href=""https://cloud.spring.io/spring-cloud-static/Finchley.SR1/single/spring-cloud.html#_custom_contract_converter"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-static/Finchley.SR1/single/spring-cloud.html#_custom_contract_converter</a> to see how to add a Contract Converter</p>&#xA;"
51580557,51578263,1773866,2018-07-29T12:58:11,<p>If you read the docs or any information starting from edgware you would see that we've removed that support. You should use native zipkin rabbit / kafka dependencies. Everything is there in the docs.</p>&#xA;
51289779,51284332,1773866,2018-07-11T15:56:45,"<p>It's not about Sleuth and MDC. It's about Sleuth being Zipkin compatible. <a href=""https://zipkin.io/pages/existing_instrumentations.html"" rel=""nofollow noreferrer"">https://zipkin.io/pages/existing_instrumentations.html</a> there's a list of available instrumentations for different languages and frameworks. If you use them then you will propagate the tracing context and things will work fine.</p>&#xA;"
46178242,46178051,1773866,2017-09-12T13:51:28,"<p>You can use Spring Boot &amp; Spring Cloud Config and standard Slf4j logging mechanism. You can check out this answer for more information - <a href=""https://stackoverflow.com/questions/28198647/managing-logging-level-using-configserver"">Managing logging.level using ConfigServer</a></p>&#xA;"
45731900,45729153,1773866,2017-08-17T09:52:49,"<p>Add the <code>spring.application.name</code> to <code>bootstrap.yaml</code>. If that doesn't work just follow the example of <code>logback</code> configuration from the docs <a href=""http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_json_logback_with_logstash"" rel=""nofollow noreferrer"">http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_json_logback_with_logstash</a></p>&#xA;"
48006082,47934052,1773866,2017-12-28T10:49:29,"<p>The code you've provided is not related to Sleuth but opentracing. In Sleuth you would call <code>Tracer.createSpan(""name"")</code> and that way a child span od your current trace would be created.</p>&#xA;"
47323502,47318596,1773866,2017-11-16T07:02:11,"<p>I have no knowledge of it being impossible. Maybe you should first try doing it and then asking a question? Also, if for some reason it turns out you can't use it, then if you just google <code>zipkin scala</code> you'll see things like <a href=""https://github.com/lloydmeta/zipkin-futures"" rel=""nofollow noreferrer"">https://github.com/lloydmeta/zipkin-futures</a> , <a href=""https://github.com/bizreach/play-zipkin-tracing"" rel=""nofollow noreferrer"">https://github.com/bizreach/play-zipkin-tracing</a> etc.</p>&#xA;"
31840297,31047564,1773866,2015-08-05T18:38:11,<p>You can also use Twitter Zipkin and hopefully quite soon Spring Cloud Sleuth to trace it further on.</p>&#xA;
49024819,49013500,731178,2018-02-28T08:14:03,"<p>For your single page application, use the Implicit grant, which is designed for browser applications - they cannot hold any secrets and with the Implicit grant, the tokens stay in the browser (because it's in the hash part of the redirect URL).</p>&#xA;&#xA;<p>The mobile app, take a look at the <a href=""https://tools.ietf.org/html/rfc8252"" rel=""nofollow noreferrer"">OAuth 2.0 for Native Apps</a>, it recommends the use of the Auth code grant. It also describes implementation details for common platforms and security considerations.</p>&#xA;&#xA;<p>There is a new grant described in the <a href=""https://tools.ietf.org/html/draft-ietf-oauth-token-exchange-12"" rel=""nofollow noreferrer"">OAuth 2.0 Token Exchange RFC</a> that would suit your needs for chained calls between services:</p>&#xA;&#xA;<blockquote>&#xA;  <p>... An OAuth resource server, for example, might assume&#xA;     the role of the client during token exchange in order to trade an&#xA;     access token, which it received in a protected resource request, for&#xA;     a new token that is appropriate to include in a call to a backend&#xA;     service.  The new token might be an access token that is more&#xA;     narrowly scoped for the downstream service or it could be an entirely&#xA;     different kind of token.</p>&#xA;</blockquote>&#xA;&#xA;<p>But I don't know whether Auth0 supports it. If it doesn't, I would probably pass the original access token from <code>serviceA</code> to <code>serviceB</code> and <code>serviceC</code>. The internal services could be secured at the network level too (e.g. they could called just from other services).</p>&#xA;"
38872720,38872460,5321002,2016-08-10T11:53:46,"<p>You can share directory between several containers using <em>data-only containers</em>.</p>&#xA;&#xA;<p>You can find details <a href=""http://container42.com/2014/11/18/data-only-container-madness/"" rel=""nofollow"">here</a>, <a href=""http://www.offermann.us/2013/12/tiny-docker-pieces-loosely-joined.html"" rel=""nofollow"">here</a> or even better, <a href=""https://docs.docker.com/engine/tutorials/dockervolumes/"" rel=""nofollow"">here</a>.</p>&#xA;"
45854054,45853546,6839338,2017-08-24T06:03:54,"<p>Short answer, No. Spring, is just a programming framework. It does nothing that  Java inherently does't do. Cross process communication is one such thing. As suggested, you can store that is_running flag in some shared database and have both services access it before running their logic. There can still be race conditions and you need to consider your application's appetite for that.<BR> However, the logic might not just be that simple where you're not even checking what result is being stored with the given status. There must be some business logic around that, which would give you leverage with the synchronization logic to implement. The current logic mentioned will always depend on point in time value of is_running flag. Doesn't really look right.</p>&#xA;"
44783208,44774754,2062634,2017-06-27T14:42:02,"<p>well, you need to define the scaling type for your module. here you can find an overview</p>&#xA;&#xA;<p><a href=""https://cloud.google.com/appengine/docs/standard/python/an-overview-of-app-engine"" rel=""nofollow noreferrer"">https://cloud.google.com/appengine/docs/standard/python/an-overview-of-app-engine</a></p>&#xA;&#xA;<p>Please try to configure manual or basic scaling for your module. Only for this scaling types background thread are allowed</p>&#xA;"
50347993,49384819,3256618,2018-05-15T10:31:09,"<p>In your case the easiest approach would be to do service orchestration.&#xA;The domain services wouldn't talk with each other and the aggregation service would be the orchestrator that would call the two domain services, combine the results and return them to the client. The gateway should only contact the orchestrator that would be also a facade that would hide to the client the complexity of the internal services.</p>&#xA;&#xA;<p>The alternative is service choreography in which the communication is decentralised and each service decides which other services it should contact. However it is a lot more complex and it is only worthy when you have tons of microservices. For example you have to prevent cycles, it is more difficult to deploy separately because you have to consider the dependencies...</p>&#xA;"
44363406,35751630,966698,2017-06-05T07:02:14,"<p>Ribbon uses the registered serviced in eureka, so it is up to eureka to update service status and let caller knows the available servers.</p>&#xA;&#xA;<p>In my understanding, when one server is down, there are 2 ways to know:<br>&#xA;1. wait for eureka server to update service status. But this update will take some time, 30 seconds as default.<br>&#xA;2. try to call and mark it as down, (maybe will confirm with eureka server later)</p>&#xA;&#xA;<p>So, in you question, you said after the first request failed, subsequent request succeeds. I think it is right behavior.</p>&#xA;"
40906233,40890804,5752362,2016-12-01T08:54:52,"<blockquote>&#xA;  <p>Can anyone tell me how to register a service fabric microservice application with WebApi exposed using Owin. i have difficulties registering the reply url and sign on url as the urls are dynamic(for statefull partitionid and replica id).</p>&#xA;</blockquote>&#xA;&#xA;<p>The <strong>client credential flow</strong> is used for the service or daemon app. There is not need to use the redirect_url when we use the <strong>client credential flow</strong> to acquire the token. You can register any validate redirect_url. Here is an example that using the client credential:</p>&#xA;&#xA;<pre><code>POST https://login.microsoftonline.com/&lt;tenantId&gt;/oauth2/token HTTP/1.1&#xA;Content-Type: application/x-www-form-urlencoded&#xA;&#xA;grant_type=client_credentials&#xA;&amp;client_id=&lt;clientId&gt;&#xA;&amp;client_secret=&lt;clientSecret&gt;&#xA;&amp;resource=&lt;app id uri of your web api &gt;&#xA;</code></pre>&#xA;&#xA;<p>And it is same that to integrate with Azure AD with web API using Azure service fabric. Here is an example for your reference:</p>&#xA;&#xA;<p>1 . register an web app(app1) which used to protect the web API on Azure portal</p>&#xA;&#xA;<p>2 . register an web app(app2) as the client to request the web API</p>&#xA;&#xA;<p>3 . grant the the app1 to app2 from portal</p>&#xA;&#xA;<p>4 . create Service Fabric application with <strong>Stateless Web API</strong> template</p>&#xA;&#xA;<p>5 . config the app.config of Service Fabric application </p>&#xA;&#xA;<pre><code>&lt;add key=""ida:Audience"" value=""app id Uri of app1"" /&gt;&#xA;&lt;add key=""ida:Tenant"" value=""tenantId"" /&gt;&#xA;</code></pre>&#xA;&#xA;<p>6 . install the package <code>Microsoft.Owin.Security.ActiveDirectory</code></p>&#xA;&#xA;<pre><code>Install-Package Microsoft.Owin.Security.ActiveDirectory&#xA;</code></pre>&#xA;&#xA;<p>7. modify the startup code like below:( <strong>Note: the method</strong> <code>appBuilder.UseWindowsAzureActiveDirectoryBearerAuthentication</code> <strong>is before</strong> <code>appBuilder.UseWebApi(config)</code>.</p>&#xA;&#xA;<pre><code>public static void ConfigureApp(IAppBuilder appBuilder)&#xA;        {&#xA;            // Configure Web API for self-host. &#xA;            HttpConfiguration config = new HttpConfiguration();&#xA;&#xA;            config.Routes.MapHttpRoute(&#xA;                name: ""DefaultApi"",&#xA;                routeTemplate: ""api/{controller}/{id}"",&#xA;                defaults: new { id = RouteParameter.Optional }&#xA;            );&#xA;&#xA;            appBuilder.UseWindowsAzureActiveDirectoryBearerAuthentication(&#xA;               new WindowsAzureActiveDirectoryBearerAuthenticationOptions&#xA;               {&#xA;                   Audience = ConfigurationManager.AppSettings[""ida:Audience""],&#xA;                   Tenant = ConfigurationManager.AppSettings[""ida:Tenant""],&#xA;                   TokenValidationParameters = new System.IdentityModel.Tokens.TokenValidationParameters&#xA;                   {&#xA;                       ValidateIssuer = false&#xA;                   }&#xA;               });&#xA;&#xA;            appBuilder.UseWebApi(config);&#xA;        }&#xA;</code></pre>&#xA;&#xA;<ol start=""8"">&#xA;<li>run the Service Fabric Application</li>&#xA;<li>acquire the token using the client credential flow mentioned above( clientId and clientSecret is from app2)</li>&#xA;<li>request the service public by Service Fabric Application with the access token and it works well</li>&#xA;</ol>&#xA;"
28945103,28942614,266795,2015-03-09T14:59:05,"<p>My take is do not use an in-container process supervisor (forever, pm2) and instead use docker restart policy via the <code>--restart=always</code> (or one of the other flavors of that option). This is more inline with the overall docker philosophy, and should operate very similarly to in-container process supervision since docker containers start running very quickly.</p>&#xA;&#xA;<p>The strongest advocate for running in-container process supervision I've seen is in the <a href=""https://github.com/phusion/baseimage-docker#docker_single_process"">phusion baseimage-docker README</a> if you want to explore the other position on this topic.</p>&#xA;"
42715091,39450504,799275,2017-03-10T09:43:08,"<p>I came up with my own solution to this problem. I abstracted the integration points of my application so that I have specific Integration services (API, S3, SNS, etc.) which respond to events and then process those events and delegate them to separate microservices. I wrote an <a href=""https://serverless.zone/an-abstracted-serverless-microservices-architecture-33706fabc516#.dcwre0psp"" rel=""nofollow noreferrer"">article</a> on it, with code examples.</p>&#xA;"
33815134,33780962,5583403,2015-11-19T21:59:54,"<p>The application upgrade workflow will, as you mention, replace v1 with v2.  However, if you wish for v1 and v2 to coexist for some time (to allow you to do testing, gradually route some users to the new version, etc.) you can accomplish this with a slightly more involved workflow:</p>&#xA;&#xA;<ol>&#xA;<li>Provision and create a new service instance with version v2.</li>&#xA;<li>Perform your testing against this instance (v2).</li>&#xA;<li>Once you are satisfied with the testing, point your dependent services at the new instance (v2).</li>&#xA;<li>Once the v1 instance is no longer being used, delete it.</li>&#xA;</ol>&#xA;&#xA;<p>This of course may give you extra headaches due to needing to manage this process yourself, deal with your dependency chains, et cetera.  That's the tradeoff- if you want more control of the upgrade/rollout process, you necessarily have to handle more details yourself.</p>&#xA;&#xA;<p>Alternatively, you can look into standing up a ""staging"" cluster where you can deploy and test new versions of your services without impacting your production services, and only roll the deployments out to your production cluster once they have been verified in stage.  Some of our customers have gone this route, as it makes the deployment and verification workflow much simpler at the cost of additional resources.</p>&#xA;"
35883457,35882330,2587435,2016-03-09T05:16:39,"<p>The problem is, with this configuration</p>&#xA;&#xA;<pre><code>bind(MyConfiguration.class).to(MyConfiguration.class);&#xA;</code></pre>&#xA;&#xA;<p>HK2 will create a new instance of the <code>MyConfiguration</code>. It will not be the same instance populated by DW. What you <em>can</em> do though, is use the instance created by DW, by simply binding that <em>same instance</em> in your HK2 configuration</p>&#xA;&#xA;<pre><code>public class MyApplication extends Application&lt;MyConfiguration&gt; {&#xA;&#xA;    @Override&#xA;    public void run(final MyConfiguration config, Environment env) {&#xA;        env.jersey().register(new AbstractBinder() {&#xA;            @Override&#xA;            protected void configure() {&#xA;                bind(config).to(MyConfiguration.class);&#xA;            }&#xA;        });&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
33767266,33763271,4296747,2015-11-17T21:23:40,"<p>you need to have a consul agent on every VM that provides a service so it can communicate info with the consul server</p>&#xA;&#xA;<p>This <a href=""https://aws.amazon.com/fr/blogs/compute/service-discovery-via-consul-with-amazon-ecs/"" rel=""noreferrer"">blog post</a> has nice information :</p>&#xA;&#xA;<p><strong>Consul Architecture</strong>&#xA;Every node that provides services to Consul runs a Consul agent. The agent is responsible for checking the health of the services on the node as well as for the node itself. The agents talk to one or more Consul servers</p>&#xA;&#xA;<p><strong>Registrator Agent</strong>&#xA;Registrator agent can automatically register/deregisters services for ECS tasks or services based on published ports and metadata from the container environment variables defined in the ECS task definition</p>&#xA;&#xA;<p>so the 2 are complementary and needed to be deployed for each service (name) you will deploy</p>&#xA;"
40094636,40092551,4296747,2016-10-17T19:57:55,"<p>From the <a href=""https://github.com/redhat-developer-tooling/openshift-vagrant#prerequisites"" rel=""nofollow"">Prerequisites</a> you need </p>&#xA;&#xA;<blockquote>&#xA;  <p><strong>Active VPN connection during the creation and provisioning of the VM</strong></p>&#xA;</blockquote>&#xA;&#xA;<p>If you're not connected to the internal Red Hat network, you will not be able to download the box</p>&#xA;&#xA;<p>You should use another box / Vagrantfile </p>&#xA;"
47768360,36570654,7773227,2017-12-12T08:43:11,"<p>You can use the way that @ebnius says where you have little pipeline jobs and a parent which is orchestrating the complete workflow and calling the different pipelines.</p>&#xA;&#xA;<p>Or you can use the Shared Library plugin (<a href=""https://jenkins.io/doc/book/pipeline/shared-libraries/"" rel=""nofollow noreferrer"">https://jenkins.io/doc/book/pipeline/shared-libraries/</a>) where you define a step per groovy file for example and you have the entire structure modularized.</p>&#xA;"
42639121,42546246,7651883,2017-03-07T02:31:53,"<p>that depends what kind of network isolation you need?&#xA;    1.layer 7:you can use network policy to change iptables rules.&#xA;    2.layer 2:you can use <a href=""https://github.com/openstack/kuryr"" rel=""nofollow noreferrer"">kuryr</a> to use neutron's vlan or vxlan network.</p>&#xA;"
48985471,48983199,8307755,2018-02-26T09:46:12,"<p>You have multiple options, you achieving this : </p></p>&#xA;&#xA;<ol>&#xA;<li><a href=""http://wiremock.org/docs/stubbing/"" rel=""nofollow noreferrer"">WireMock</a>  </li>&#xA;<li><a href=""http://rest-assured.io/"" rel=""nofollow noreferrer"">RESTAssured</a></li>&#xA;<li><a href=""https://spring.io/guides/gs/testing-web/"" rel=""nofollow noreferrer"">MockMvc</a></li>&#xA;<li><a href=""https://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/test/web/client/MockRestServiceServer.html"" rel=""nofollow noreferrer"">MockRestServer</a></li>&#xA;</ol>&#xA;"
51948080,51946603,8513835,2018-08-21T11:52:16,"<p>Based on your question I assume there is no ability to leverage the <a href=""https://martinfowler.com/eaaDev/EventCollaboration.html"" rel=""nofollow noreferrer"">event-based/reactive approach</a>, and architectural decision is already made with tradeoffs as considered <a href=""https://medium.com/capital-one-developers/microservices-when-to-react-vs-orchestrate-c6b18308a14c"" rel=""nofollow noreferrer"">here</a> (note, in this source the approach proposed below is referenced as a 'Hybrid'). </p>&#xA;&#xA;<p><strong>Orchestration</strong></p>&#xA;&#xA;<p>Under these conditions, the pattern you're looking for is called <a href=""http://soapatterns.org/compound_patterns/orchestration"" rel=""nofollow noreferrer""><strong>Orchestration</strong></a>. Check out <a href=""https://stackoverflow.com/questions/29117570/orchestrating-microservices"">this great answer</a> for wider overview. </p>&#xA;&#xA;<p>As a quick recap, you can use something like <a href=""https://docs.spring.io/spring-integration/docs/latest-ga/reference/html/"" rel=""nofollow noreferrer"">Spring Integration</a> to implement the following key points:</p>&#xA;&#xA;<ul>&#xA;<li>While processing the request to A, execute calls to B &amp; C concurrently where possible to achieve quickest response time from A</li>&#xA;<li>Accumulate, transform and aggregate the results of concurrent calls into complete response entity</li>&#xA;<li>Leverage thread pools to limit concurrently-running requests to B and C to prevent amplification of cascading failures </li>&#xA;<li>Fail fast: early cancel the subsequent bunch of calls if some of requests fails (i.e. do not call C if call to B was not successful)</li>&#xA;<li>Cut-off: involve the maximal processing time you can wait for completion of currently-running bunch of calls to B &amp; C and respond with error by A upon elapsed</li>&#xA;</ul>&#xA;&#xA;<p><strong>Update - rely on implementation of <a href=""https://en.wikipedia.org/wiki/Reactor_pattern"" rel=""nofollow noreferrer"">Reactor pattern</a> on client side</strong></p>&#xA;&#xA;<p>If you can use Spring 5/Spring Boot 2.x, you can also make the calls to B &amp; C in a reactive way using <a href=""https://docs.spring.io/spring/docs/current/spring-framework-reference/web-reactive.html"" rel=""nofollow noreferrer"">Spring WebFlux</a> based on <a href=""https://projectreactor.io/docs"" rel=""nofollow noreferrer"">Project Reactor</a> to achieve above points.</p>&#xA;&#xA;<p>Schematically, you can do something like:</p>&#xA;&#xA;<pre><code>@Service&#xA;public class MyService {&#xA;&#xA;    private final WebClient webClient;&#xA;&#xA;    ...&#xA;&#xA;    public Mono&lt;Details&gt; someRestCall(String name) {&#xA;        return this.webClient.get().url(""{name}/details"", name)&#xA;                        .retrieve().bodyToMono(ResponseEntity.class);&#xA;    }&#xA;&#xA;}&#xA;&#xA;...&#xA;&#xA;&#xA;Mono&lt;ResponseEntity&gt; b1 = myService.someRestCall(""serviceB"");&#xA;Mono&lt;ResponseEntity&gt; c1 = myService.someRestCall(""serviceC"");&#xA;Mono&lt;ResponseEntity&gt; b2 = myService.someOtherRestCall(""serviceB"");&#xA;&#xA;ResponseEntity response = Flux&#xA;       .parallel(NUM_CPUS)&#xA;       .merge(b1, c1, b2)&#xA;       .limitRequest(MAX_REQUESTS)&#xA;       .onErrorReturn(ERR_RESPONSE_ENTITY)&#xA;       .blockLast(CUTOFF_TIMEOUT);&#xA;</code></pre>&#xA;&#xA;<p>(based on <a href=""https://stackoverflow.com/questions/50203875/how-to-use-spring-webclient-to-make-multiple-calls-simultaneously"">this example</a>)</p>&#xA;"
38210255,38186942,5932,2016-07-05T18:30:02,"<p>I've had a lot of success using <a href=""https://serilog.net"" rel=""nofollow"">serilog</a> and <a href=""https://getseq.net"" rel=""nofollow"">seq</a> and using an enricher to add properties to log messages. </p>&#xA;&#xA;<p>In my services I call ServiceLogger.CreateLogger(this) to get the Log enriched with all the state about the service. If you want a correlation token then you should be able to add that relatively easily, but it's not something I've done yet.</p>&#xA;&#xA;<p>I think I've copied the relevant code below!</p>&#xA;&#xA;<pre><code>public class ServiceFabricEnricher&lt;T&gt; : ILogEventEnricher where T : ServiceContext&#xA;{&#xA;    protected T Context { get; }&#xA;    private LogEventProperty _nodeName;&#xA;&#xA;    public ServiceFabricEnricher(T context)&#xA;    {&#xA;        Context = context;&#xA;    }&#xA;&#xA;    public virtual void Enrich(LogEvent logEvent, ILogEventPropertyFactory propertyFactory)&#xA;    {&#xA;        if (_nodeName == null) _nodeName = propertyFactory.CreateProperty(""NodeName"", Context.NodeContext.NodeName);&#xA;        logEvent.AddPropertyIfAbsent(_nodeName);&#xA;    }&#xA;}&#xA;&#xA;&#xA;public class ServiceEnricher&lt;T&gt; : ServiceFabricEnricher&lt;T&gt; where T : ServiceContext&#xA;{&#xA;    private LogEventProperty _serviceName;&#xA;    private LogEventProperty _partitionId;&#xA;    private LogEventProperty _applicationName;&#xA;&#xA;    public ServiceEnricher(T context) : base(context)&#xA;    {&#xA;    }&#xA;&#xA;    public override void Enrich(LogEvent logEvent, ILogEventPropertyFactory propertyFactory)&#xA;    {&#xA;        base.Enrich(logEvent, propertyFactory);&#xA;&#xA;        if (_serviceName == null) _serviceName = propertyFactory.CreateProperty(""ServiceName"", Context.ServiceName);&#xA;        if (_partitionId == null) _partitionId = propertyFactory.CreateProperty(""PartitionId"", Context.PartitionId);&#xA;        if (_applicationName == null) _applicationName = propertyFactory.CreateProperty(""ApplicationName"", Context.CodePackageActivationContext.ApplicationName);&#xA;&#xA;        logEvent.AddPropertyIfAbsent(_serviceName);&#xA;        logEvent.AddPropertyIfAbsent(_partitionId);&#xA;        logEvent.AddPropertyIfAbsent(_applicationName);&#xA;    }&#xA;}&#xA;&#xA;public static class ServiceFabricLogger&#xA;{&#xA;    private static ILogger CreaterDefaultLogger()&#xA;    {&#xA;        var configurationProvider = new FabricConfigurationProvider(""SeqConfig"");&#xA;&#xA;        var loggerConfiguration = new LoggerConfiguration();&#xA;        if (configurationProvider.HasConfiguration)&#xA;        {&#xA;            var seqServer = configurationProvider.GetValue(""SeqServer"");&#xA;            loggerConfiguration =&#xA;                loggerConfiguration&#xA;                .WriteTo.Seq(seqServer, period: TimeSpan.FromMilliseconds(500))&#xA;                ;&#xA;&#xA;            var level = configurationProvider.GetValue(""MinimumLevel"");&#xA;            LogEventLevel minimumLevel;&#xA;            if (!string.IsNullOrWhiteSpace(level) &amp;&amp; Enum.TryParse&lt;LogEventLevel&gt;(level, true, out minimumLevel))&#xA;            {&#xA;                loggerConfiguration = loggerConfiguration.MinimumLevel.Is(minimumLevel);&#xA;            }&#xA;        }&#xA;        else&#xA;        {&#xA;            loggerConfiguration =&#xA;                loggerConfiguration&#xA;                .MinimumLevel.Error()&#xA;                ;&#xA;        }&#xA;&#xA;        Log.Logger = loggerConfiguration.CreateLogger();&#xA;        return Log.Logger;&#xA;    }&#xA;&#xA;    public static ILogger Logger { get; } = CreaterDefaultLogger();&#xA;}&#xA;&#xA;public static class ServiceLogger&#xA;{&#xA;    public static ILogger CreateLogger(this StatefulServiceBase service) =&gt;&#xA;        ServiceFabricLogger.Logger.ForContext(new[] { new StatefulServiceEnricher(service.Context) });&#xA;&#xA;    public static ILogger CreateLogger(this StatelessService service) =&gt;&#xA;        ServiceFabricLogger.Logger.ForContext(new[] { new StatelessServiceEnricher(service.Context) });&#xA;}&#xA;</code></pre>&#xA;"
42571807,42562820,6763052,2017-03-03T06:08:15,"<p>The context for microservices is distributed systems. In any other situation it would probably be overkill. Shared kernel will eventually split. That is usually the case. You may start from it. Nothing wrong with that. However, it will not stay there.</p>&#xA;"
45489392,45474338,151084,2017-08-03T16:04:34,"<p>It sounds like an OrderUpdated event is not granular enough to meet your needs.  Based on the information you provided, you likely want to have a more specific event like OrderCustomerChanged that contains both the new and the old customer Id.</p>&#xA;"
35940200,35926145,2862892,2016-03-11T12:22:18,"<p>You are right. In many cases, your data model has a very interconnected structure. There is nothing wrong about it since your problem asks for things to have such structure. It is an old discussion related to NoSQL. Obviously, you can't throw away relational databases to solve relational problems.</p>&#xA;&#xA;<p>In your case you need to ask yourself why you want to use microservices. Is your code too complex to maintain? Do you want to try to evolve part of your system independently in a way that cannot be simply injected as a module on a monolith? Do you have separate development teams that should work independently in order to be more productive? Do you have a scalability issue?</p>&#xA;&#xA;<p>When breaking things into microservices, one rule of thumb you can use, at least initially, is to try to avoid having more than one microservice accessing the same tables on the same database. A common pattern if for each microservice to have its own database.</p>&#xA;&#xA;<p>So in your example your system keeps relational data about people. Then you can make this system you described a microservice for another system that needs to read and write data about people. Note that this is not the same as two systems accessing the same database as the second system would access it indirectly via a REST interface for instance.</p>&#xA;"
32018126,31962345,2862892,2015-08-14T20:25:44,"<p>If I got your question straight, you have a web interface in which files are uploaded to an S3 bucket and you need to make sure that in a certain back end API (such as REST) all file upload commands will have authentication and authorization.</p>&#xA;&#xA;<p>The answer is highly dependent on your architecture but generally speaking, all Javascript calls are nothing but HTTP calls. So you need HTTP authentication/authorization. In general, the most straightforward method for REST over HTTP is the basic authentication, in which the client sends a credential in every single request. This may sound odd at first but it is quite standard since HTTP is supposed to be stateless.</p>&#xA;&#xA;<p>So the short answer, at least for the scenario I just described, would be to ask the user to provide the credentials that Javascript will keep in the client side, then send basic authentication which the REST interface can understand. The server-side processes will then get such information and decide whether a certain file can be written in a certain S3 bucket.</p>&#xA;"
34655763,34652782,2862892,2016-01-07T12:57:26,"<p>There is a saying where I come from ""a dog with too many owners dies of starvation"", meaning, when everyone things someone else is in charge, no one is really in charge even if people have agreed to share the responsibility.</p>&#xA;&#xA;<p>There are many ways to address your situation but in all of them someone should be in charge. Some ways to address this issue:</p>&#xA;&#xA;<ol>&#xA;<li>A single person should be responsible for the project. If this person moves to a new team, he or she should appoint the new project manager. If this person quits the company, someone else should be chosen by the remaining staff.</li>&#xA;<li>Periodically check who is in charge of each project. If no one is in responsible, then act fact and assign someone.</li>&#xA;<li>Everyone who was just appointed to a project should necessarily check if he or she can keep evolving the project. For instance, if the documentation is badly written or absent, or if the source code is too hard to read, then the newly appointed person should ask the previously responsible people during project handover.</li>&#xA;</ol>&#xA;&#xA;<p>There is no silver bullet but there are principles you can follow that will avoid some future headaches.</p>&#xA;"
34727577,34722107,2862892,2016-01-11T17:29:05,"<p>Having a database service is a completely valid pattern. In fact, this is one of the key examples of where to start to export aspects of a monolith to a micro service in the <a href=""http://shop.oreilly.com/product/0636920033158.do"" rel=""nofollow"">Building Microservices book</a>.</p>&#xA;&#xA;<p>How to organize your code around such idea is a different issue. Yes, from the db client programmer's stand point, having the same DAO layer on each DB client makes a lot of sense.</p>&#xA;&#xA;<p>The DAO pattern may be suitable to bind your DB to one programming language that you use. But then you need to ask yourself why you are exposing your database as a web service if all access to it will be mediated by the same DAO infrastructure. Or are you going to create one DAO pattern for each client programming language binding?</p>&#xA;&#xA;<p>If all database clients are going to be written on the same programming language, then are you sure you really need to wrap your DB as a microservice? After all, the DB is usually already a remote service with a well-defined network protocol optimized to transfer data fast and reliably. Why adding HTTP on top of it? What are you expecting to gain from adding such complexity?</p>&#xA;&#xA;<p>Another problem with using the DAO pattern is that the DAO structure does not necessarily follow the evolution of the web service. The web service may evolve in a way that does not make old clients incompatible. You may have different clients using different features of the micro service. In this case you are not sharing the same DAO layer structure on each client.</p>&#xA;&#xA;<p>Make sure you are not using RPC-style programming over web services, which does not make much sense. You will be basically throwing away one of the key advantages of micro services, which is the decoupling between service and client.</p>&#xA;"
32594200,32574103,2862892,2015-09-15T19:34:40,"<p>Short answer: check OAUTH, and manage caches of credentials in each microservice that needs to access other microservices. By ""manage"" I mean, be careful with security. Specially, mind who can access those credentials and let the network topology be your friend. Create a DMZ layer and other internal layers reflecting the dependency graph of your microservices.</p>&#xA;&#xA;<p>Long answer, keep reading. Your question is a good one because there is no simple silver bullet to do what you need although your problem is quite recurrent.</p>&#xA;&#xA;<p>As with everything related with microservices that I saw so far, nothing is really new. Whenever you need to have a distributed system doing things on behalf of a certain user, you need distributed credentials to enable such solution. This is true since mainframe times. There is no way to violate that.</p>&#xA;&#xA;<p><a href=""http://www.linuxproblem.org/art_9.html"" rel=""noreferrer"">Auto SSH</a> is, in a sense, such a thing. Perhaps it may sound like a glorified way to describe something simple, but in the end, it enables processes in one machine to use services in another machine.</p>&#xA;&#xA;<p>In the Grid world, the <a href=""http://toolkit.globus.org/"" rel=""noreferrer"">Globus Toolkit</a>, for instance, bases its <a href=""http://toolkit.globus.org/toolkit/security/"" rel=""noreferrer"">distributed security</a> using the following:</p>&#xA;&#xA;<ul>&#xA;<li>X.509 certificates;</li>&#xA;<li>MyProxy - manages a repository of credentials and helps you define a chain of certificate authorities up to finding the root one, which should be trusted by default;</li>&#xA;<li>An extension of OpenSSH, which is the de facto standard SSH implementation for Linux distributions.</li>&#xA;</ul>&#xA;&#xA;<p><a href=""https://en.wikipedia.org/wiki/OAuth"" rel=""noreferrer"">OAUTH</a> is perhaps what you need. It is a way provide authorization with extra restrictions. For instance, imagine that a certain user has read and write permission on a certain service. When you issue an OAUTH authorization you do not necessarily give full user powers to the third party. You may only give read access.</p>&#xA;&#xA;<p>CORS, mentioned in another answer, is useful when the end client (typically a web browser) needs single-sign-on across web sites. But it seems that your problem is closer to a cluster in which you have many microservices that are managed by you. Nevertheless, you can take advantage of solutions developed by the Grid field to ensure security in a cluster distributed across sites (for high availability reasons, for instance).</p>&#xA;&#xA;<p>Complete security is something unattainable. So all this is of no use if credentials are valid forever or if you do not take enough care to keep them secret to whatever received them. For such purpose, I would recommend partitioning your network using layers. Each layer with a different degree of secrecy and exposure to the outside world.</p>&#xA;&#xA;<p>If you do not want the burden to have the required infrastructure to allow for OAUTH, you can either use basic HTTP or create your own tokens.</p>&#xA;&#xA;<p>When using <a href=""https://en.wikipedia.org/wiki/Basic_access_authentication"" rel=""noreferrer"">basic HTTP authentication</a>, the client needs to send credentials on each request, therefore eliminating the need to keep session state on the server side for the purpose of authorization.</p>&#xA;&#xA;<p>If you want to create your own mechanism, then change your login requests such that a token is returned as the response to a successful login. Subsequent requests having the same token will act as the basic HTTP authentication with the advantage that this takes place at the application level (in contrast with the framework or app server level in basic HTTP authentication).</p>&#xA;"
33959184,33952306,2862892,2015-11-27T14:10:05,"<p>AMQP (any MOM, actually) provides a way for processes to communicate without having to mind about actual IP addresses, communication security, routing, among other concerns. That does not necessarily means that any process can trust or even has any information about the processes it communicates with.</p>&#xA;&#xA;<p>Message queues do solve half of the process: how to reach the remote service. But they do not solve the other half: which service is the right one for me. In other words, which service:</p>&#xA;&#xA;<ul>&#xA;<li>has the resources I need</li>&#xA;<li>can be trusted (is hosted on a reliable server, has a satisfactory service implementation, is located in a country where the local laws are compatible with your requirements, etc)</li>&#xA;<li>charges what you want to pay (although people rarely discuss cost when it comes to microservices)</li>&#xA;<li>will be there during the whole time window needed to process your service -- keep in mind that servers are becoming more and more volatile. Some servers are actually containers that can last for a couple minutes.</li>&#xA;</ul>&#xA;&#xA;<p>Those two problems are almost linearly independent. To solve the second kind of problems, you have resource brokers in Grid computing. There is also resource allocation in order to make sure that the last item above is correctly managed.</p>&#xA;&#xA;<p>There are some alternative strategies such as multicasting the intention to use a service and waiting for replies with offers. You may have reverse auction in such a case, for instance.</p>&#xA;&#xA;<p>In short, the rule of thumb is that if you do not have an a priori knowledge about which service you are going to use (hardcoded or in some configuration file), your agent will have to negotiate, which includes dynamic service discovery.</p>&#xA;"
36140619,36137802,2862892,2016-03-21T20:10:03,"<p>What you are facing is an instance of the <a href=""https://en.wikipedia.org/wiki/Two_Generals%27_Problem"" rel=""nofollow"">Two General's Problem</a>. Basically, you want to have two entities on a network agreeing on something but <a href=""https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing"" rel=""nofollow"">the network is not fail safe</a>. Leslie Lamport proved that this is impossible.</p>&#xA;&#xA;<p>So no matter how much you add new entities to your network, the message queue being one, you will never have 100% certainty that agreement will be reached. In fact, the opposite takes place: the more entities you add to your distributed system, the less you can be certain that an agreement will eventually be reached.</p>&#xA;&#xA;<p>A practical answer to your case is that 2PC is not that bad if you consider adding even more complexity and single points of failures. If you absolutely do not want a single point of failure and wants to assume that the network is reliable (in other words, that the network itself cannot be a single point of failure), you can try a P2P algorithm such as <a href=""https://en.wikipedia.org/wiki/Distributed_hash_table"" rel=""nofollow"">DHT</a>, but for two peers I bet it reduces to simple 2PC.</p>&#xA;"
35730229,35730116,2862892,2016-03-01T18:09:53,"<p>Yes, this is one use case of AWS Lambda:</p>&#xA;&#xA;<blockquote>&#xA;  <p>As an event-driven compute service where AWS Lambda runs your code in&#xA;  response to events, such as changes to data in an Amazon S3 bucket or&#xA;  an Amazon DynamoDB table.</p>&#xA;</blockquote>&#xA;&#xA;<p><a href=""http://docs.aws.amazon.com/lambda/latest/dg/welcome.html"" rel=""nofollow"">http://docs.aws.amazon.com/lambda/latest/dg/welcome.html</a></p>&#xA;&#xA;<p>Since it runs your code, you are free to write something that places a request to a microservice.</p>&#xA;"
49443255,49292843,2870572,2018-03-23T05:58:21,"<p>I used Zuul and that solved my problem &#xA;This is how my app would be deployed &#xA;<a href=""https://i.stack.imgur.com/ENBwT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ENBwT.png"" alt=""app deployment""></a></p>&#xA;&#xA;<p>I added this in my <code>pom.xml</code></p>&#xA;&#xA;<pre><code>&lt;dependencies&gt;&#xA;        ....&#xA;        &lt;dependency&gt;&#xA;            &lt;groupId&gt;io.springfox&lt;/groupId&gt;&#xA;            &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt;&#xA;            &lt;version&gt;2.6.1&lt;/version&gt;&#xA;        &lt;/dependency&gt;&#xA;        &lt;dependency&gt;&#xA;            &lt;groupId&gt;io.springfox&lt;/groupId&gt;&#xA;            &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt;&#xA;        &lt;/dependency&gt;&#xA;&lt;/dependencies&gt;&#xA;</code></pre>&#xA;&#xA;<p>My main class looks like this </p>&#xA;&#xA;<pre><code> @EnableZuulProxy&#xA; @SpringBootApplication&#xA; @EnableSwagger2&#xA; public class Application {&#xA;     public static void main(String[] args) {&#xA;         SpringApplication.run(Application.class, args);&#xA;     }&#xA;&#xA;    @Bean&#xA;    UiConfiguration uiConfig() {&#xA;        return new UiConfiguration(""validatorUrl"", ""list"", ""alpha"", ""schema"",&#xA;            UiConfiguration.Constants.DEFAULT_SUBMIT_METHODS, false, true, 60000L);&#xA;    }&#xA; }&#xA;</code></pre>&#xA;&#xA;<p>I created the aggregator for swagger document </p>&#xA;&#xA;<pre><code>@Component&#xA;@Primary&#xA;@EnableAutoConfiguration&#xA;public class SwaggerAggregatorController implements SwaggerResourcesProvider {&#xA;    @Override&#xA;    public List&lt;SwaggerResource&gt; get() {&#xA;        List&lt;SwaggerResource&gt; resources= new ArrayList&lt;&gt;();&#xA;        SwaggerResource swaggerResource = new SwaggerResource();&#xA;        swaggerResource.setName(""cust-service"");&#xA;        swaggerResource.setLocation(""/cust/v2/api-docs"");&#xA;        swaggerResource.setSwaggerVersion(""2.0"");&#xA;&#xA;        resources.add(swaggerResource);&#xA;        return resources;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>I can add more microservices in this field. (Can be improved to be read from config file)</p>&#xA;&#xA;<p>My <code>application.properties</code> looks like following </p>&#xA;&#xA;<pre><code>...&#xA;server.port=8001&#xA;&#xA;zuul.routes.cust.path=/cust/**&#xA;zuul.routes.cust.url=http://1.1.1.2:8002/cust-service/&#xA;...&#xA;</code></pre>&#xA;"
45421864,43612866,3330348,2017-07-31T17:20:59,"<p>In general a microservice should be responsible for it's own data. That's a perfect world scenario. </p>&#xA;&#xA;<p>In practice some of the services may be highly related to each other. E.g. CustomerShippingDetails and CustomerShoppingCheckout services may both access the same data - customer address. How would you then solve a problem of providing customer address to the customer checkout service. If the checkout service queries the shopping details directly then you break loose coupling between services. Other option is to introduce a shared database. </p>&#xA;&#xA;<p>There will always have to be some kind of compromise on the architecture. What is sacreficed is an architectural decision that highly depends on the big picture (the design of the whole system).</p>&#xA;&#xA;<p>Not having too many details about your system I would go with a mixed approach. That is, having a shared database for services that take care of similar business logic. So CustomerShippingDetails and CustomerShoppingCheckout can share a database. But a StoreItemsDetails would have a separate database.</p>&#xA;&#xA;<p>You can find more about shared database pattern for microservices at <a href=""http://microservices.io/patterns/data/shared-database.html"" rel=""nofollow noreferrer"">Microservice Architecture</a>.</p>&#xA;"
45604579,45534252,8443180,2017-08-10T04:37:04,<p>Try removing the hyphen from <code>key-auth</code> under <code>policies:</code> in your config file - this worked for me. It should just be <code>keyauth</code>.</p>&#xA;
49359712,49353369,7932461,2018-03-19T09:36:09,"<p>it depends on your design , but any microservice should autonomous,  so it should keep it's own logic and data.</p>&#xA;&#xA;<p>keeping data depends also if you will log something in case of failure or even if you will implement your own retry mechanism</p>&#xA;"
49359861,49349235,7932461,2018-03-19T09:44:17,<p>accessing some shared database between microservices is highly not recommended as this will violate the basic rule of microservices architecture.</p>&#xA;&#xA;<p>microservice must be autonomous and keep it own logic and data </p>&#xA;&#xA;<p>also to achive a good microservice design you  should losely couple your microservices</p>&#xA;
49595896,49584617,7932461,2018-04-01T07:39:00,"<p>I don't think that this is a valid design, using a database per client which is a Multi-tenant architecture practice, and database per microservice is a microservice architecture practice. you are mixing things up.</p>&#xA;&#xA;<p>if you will use microservice architecture you better design it as Bounded contexts and each Context has its own database to achieve microservices main rule <strong>Autonomy</strong> </p>&#xA;"
49595990,49576208,7932461,2018-04-01T07:52:57,"<p>let's assume that each microservice has it own Bounded Context, so it has its own business logic and database totally separated and all your microservices are loosely coupled.</p>&#xA;&#xA;<p>your shopping cart is responsible for calculating the price as it is the main responsibility of that microservice so it is the service that will apply the promotion too as it is the main responsibility of the shopping cart.</p>&#xA;&#xA;<p>but to recalculate the price after promotion it has to keep and maintain some data related to the promotions. if you are using Event-driven design, you can send an event when you add/update or delete a promotion to the shopping cart.</p>&#xA;&#xA;<p>or you can call an endpoint at promotion microservice to check the discount rate or percentage at runtime but at that point, your shopping cart microservice will be coupled with the promotion microservice.</p>&#xA;&#xA;<p>there are some pros and cons with each approach and it depends on your design</p>&#xA;"
49610636,49607242,7932461,2018-04-02T11:42:42,"<p>it is not mandatory to use Docker in microservices architecture. you can design your system/Application and use microservices architecture and the final deployment can be pure hardware.</p>&#xA;&#xA;<p>At the end, a microservice can be treated as a process that needs a host to run. this host can be docker host or any operating system that can run this process.</p>&#xA;&#xA;<p>for legacy application as yours, you can use Docker too. instead of your normal infrastructure, you can containerize your application and ship to Docker</p>&#xA;"
26731682,26705784,200237,2014-11-04T09:30:53,"<p>It depends on what you put in front of the app, in the past I've used nginx and HA Proxy, and the config files for those services need to be adjusted to be aware of the additional instances you're now running. Hopefully that's automated as part of the same process that created those new instances.</p>&#xA;"
41862674,41640621,2953261,2017-01-25T22:16:25,"<p>Check the section <strong>Possible Solution</strong> on my question:</p>&#xA;&#xA;<p>The idea is to use the gateway as an api gateway request processing that will cache some HTTP response from MS1 and MS2 and use them as a response to MS2 SDK and MS1 SDK. This way no communication (SYNC OR ASYNC) is made directly between MS1 and MS2 and data duplication is also avoided.</p>&#xA;&#xA;<p>Inspiration: <a href=""https://aws.amazon.com/api-gateway/"" rel=""nofollow noreferrer"">https://aws.amazon.com/api-gateway/</a> and <a href=""https://getkong.org/"" rel=""nofollow noreferrer"">https://getkong.org/</a></p>&#xA;"
42355702,42354108,4009517,2017-02-20T22:56:28,"<p>There are three general approaches to consider:</p>&#xA;&#xA;<ol>&#xA;<li>Don't integration test. Just mock everything and unit test.</li>&#xA;<li>Integration test against a sandbox (non-PROD) deployment of the microservice using real data</li>&#xA;<li>Integration test against a PROD deploy using fake data (known by the other service as fake)</li>&#xA;</ol>&#xA;&#xA;<p>There are pros/cons with each. </p>&#xA;&#xA;<p>3) Requires some cross-team coordination to ensure that every microservice has some known inputs that will be regarded as ""test-only"", but most accurately simulates a real use case.</p>&#xA;&#xA;<p>2) Doesn't require as much cross-team coordination, and doesn't have the possibility of disruptive service or adding garbage data on accident.</p>&#xA;&#xA;<p>1) is the most fragile, and only ensures that your application can handle the expected (at the time) input/output data. </p>&#xA;&#xA;<p>Go for the best strategy that your organizations DevOps and Development maturity will support. </p>&#xA;&#xA;<p>To answer your detailed questions:</p>&#xA;&#xA;<ul>&#xA;<li>a. You only need to mock the integration with #1. #2 and #3 are real.</li>&#xA;<li>b. Setup and teardown for #2 and #3 should simply be injecting the&#xA;endpoint </li>&#xA;<li>c. Test all of your use cases. Ensure that you hit each&#xA;method call you consume.</li>&#xA;</ul>&#xA;"
48661988,48660912,739373,2018-02-07T10:53:01,"<p>Client Certificates are the best way to go.</p>&#xA;&#xA;<p><a href=""https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-client-side-ssl-authentication.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-client-side-ssl-authentication.html</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>You can use API Gateway to generate an SSL certificate and use its&#xA;  public key in the backend to verify that HTTP requests to your backend&#xA;  system are from API Gateway. This allows your HTTP backend to control&#xA;  and accept only requests originating from Amazon API Gateway, even if&#xA;  the backend is publicly accessible.</p>&#xA;</blockquote>&#xA;&#xA;<p>Think of it as inverting the typical role of an SSL certificate (which currently verifies that the HTTP <em>response</em> you're viewing came from StackOverflow.com.)</p>&#xA;&#xA;<p>Instead, you'll use this client certificate to verify in your microservices' HTTP layers that the <em>request</em> came from your API Gateway. To do this, generate a <a href=""https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-client-side-ssl-authentication.html"" rel=""nofollow noreferrer"">Client Certificate for your API gateway</a>. Then, retrieve the PEM-encoded public key from the same interface, and <strong>configure your HTTP server to only accept connections which are encrypted with this client certificate.</strong></p>&#xA;&#xA;<p>This article appears to describe how to configure Tomcat to accept / enforce client certificates: <a href=""http://www.maximporges.com/2009/11/18/configuring-tomcat-ssl-clientserver-authentication/"" rel=""nofollow noreferrer"">http://www.maximporges.com/2009/11/18/configuring-tomcat-ssl-clientserver-authentication/</a></p>&#xA;&#xA;<pre><code>&lt;Connector&#xA;   clientAuth=""true"" port=""8443"" minSpareThreads=""5"" maxSpareThreads=""75""&#xA;   enableLookups=""true"" disableUploadTimeout=""true""&#xA;   acceptCount=""100"" maxThreads=""200""&#xA;   scheme=""https"" secure=""true"" SSLEnabled=""true""&#xA;   keystoreFile=""/Users/mporges/Desktop/tomcat-ssl/final/server.jks""&#xA;   keystoreType=""JKS"" keystorePass=""password""&#xA;   truststoreFile=""/Users/mporges/Desktop/tomcat-ssl/final/server.jks""&#xA;   truststoreType=""JKS"" truststorePass=""password""&#xA;   SSLVerifyClient=""require"" SSLEngine=""on"" SSLVerifyDepth=""2"" sslProtocol=""TLS""&#xA;/&gt;&#xA;</code></pre>&#xA;&#xA;<p><strong>A less robust option</strong>, rather than SSL Client Certificates, might be to add ""secret"" API keys, e.g. as request headers or query parameters, to all requests from your API Gateway. Your microservices can then look for this API key as indication that the request came from your API Gateway and should be trusted / accepted.</p>&#xA;"
36795052,29460485,1987373,2016-04-22T13:22:26,"<p>I usually use both approaches. Sometimes i have another service which is sitting on top on x other services and combines the data. I don't really like this approach because it is causing dependencies and coupling between services. So in general, within my last projects we tried to stick to polyglot persistence.</p>&#xA;&#xA;<p>Also think about, if you need to have x sub http requests for combining data in some kind of middleware service, it will lead you to higher latency. We always try to cut down the amount of requests for one task and handle everything what is possible through asynchronous queues. ( especially data sync )</p>&#xA;"
36798750,30908725,1987373,2016-04-22T16:16:30,"<p>Another solution would be that you store all the necessary information in both services. E.g., if you need data of the user within a billing, then just store all the data in the billings datastorage as well. The sync  between both services you would do through a queue ( subscribe / publish ). This comes with pros and cons but in the end you end up having one <strong>synchronous</strong> http call, if you want receive the data for a specific billing.</p>&#xA;"
35502444,35501600,1811501,2016-02-19T10:02:01,"<p>The <code>172.17.0.4</code> IP-address is the IP-address of the <code>authserver</code> container on the <strong>internal</strong> (container-container) network, which is <em>not</em> reachable from outside the docker host (Virtual Machine).</p>&#xA;&#xA;<p>This may be tricky, because (in this case) you need to provide the IP-address of the Virtual Machine that docker runs on, which may <em>change</em>, and definitely will be different in production.</p>&#xA;&#xA;<p>If you change <code>${AUTHSERVER_PORT_9999_TCP_ADDR:localhost}</code> to <code>192.168.99.100</code>, it should work.</p>&#xA;&#xA;<p>I suggest to make the IP-address (or domain) configurable using an environment-variable that you provide in the docker-compose.yml, so something like:</p>&#xA;&#xA;<pre><code>${DOMAIN_NAME:192.168.99.100}&#xA;</code></pre>&#xA;&#xA;<p>Which defaults to the ""standard"" IP-address of the Virtual Machine. In production you can then pass the actual domain-name, or IP-address of the server your project runs on.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Note that the ""link"" environment variables are marked deprecated, and only will&#xA;  be used on the default (bridge) network. The new linking feature won't create&#xA;  these variables, but you can simply link to other containers by <em>name</em>. See&#xA;  <a href=""https://docs.docker.com/engine/userguide/networking/work-with-networks/#linking-containers-in-user-defined-networks"" rel=""nofollow"">https://docs.docker.com/engine/userguide/networking/work-with-networks/#linking-containers-in-user-defined-networks</a></p>&#xA;</blockquote>&#xA;"
44397565,42392789,5679917,2017-06-06T18:43:19,"<p>In the example you provided (and provided in the documentation), <code>priority</code> would be the field within your collection that you're interested in searching for and <code>$1</code> is the next value provided in the select function, i.e. <code>parseInt(request.params.priority)</code>. <code>request.params.priority</code> is the value of the query parameter ""priority"", which in this case is being parsed into an int for the sake of comparison.</p>&#xA;&#xA;<p>From what I've found you can do the all the basic comparisons (<code>=</code>,<code>!=</code>,<code>&gt;(=)</code>,<code>&lt;(=)</code>).</p>&#xA;"
36088078,36080524,5227053,2016-03-18T15:18:44,"<p>Rails is a lot of things. Parts of it handle web requests. Other parts (ActiveRecord) don't care if you are a web request or a script or whatever. Rails itself does not even come with a production worthy web server, you use other gems (e.g., <code>thin</code> for plain old web browsers, or <code>wash_out</code> for incoming SOAP requests) for that. Rails only gives you the infrastructure/middleware to combine all the pieces regarding servers.</p>&#xA;&#xA;<p>Unless your queue can call out to your application in some fashion of HTTP, for example in the form of SOAP requests, you'll need something that listens to your queueing system, whatever that may be, and translates new ""tickets"" on your queue into controller actions in your Rails world.</p>&#xA;"
45340865,43700824,8373549,2017-07-27T03:27:32,"<p>do not use ""_"" in ""spring.application.name"".you can change to ""registration-service""</p>&#xA;"
44107228,44105840,6223149,2017-05-22T07:38:28,"<p>Since your resources are part of your jar container, you cannot use file path but URLs provided by your classloader. Depending on whether you start it from the IDE or the deployed jar, this will be either a <code>file://...</code> or <code>jar:...</code> URL. The resource folder is not part of the spring boot structure, so you must not use it. Using a URLDataSource you can just do:</p>&#xA;&#xA;<pre><code>URLDataSource source = new URLDataSource(this.getClass().getResource(""/images/header.png""));&#xA;</code></pre>&#xA;"
51242272,50964458,2438446,2018-07-09T09:30:32,"<p>As far as I understand, in case of a service not responding, a missing route, etc. you can setup the /error endpoint to deliver a custom response to the user.&#xA;For example:</p>&#xA;&#xA;<pre><code>@Controller&#xA;public class CustomErrorController implements ErrorController {&#xA;&#xA;    @RequestMapping(value = ""/error"", produces = ""application/json"")&#xA;    public @ResponseBody&#xA;    ResponseEntity error(HttpServletRequest request) {&#xA;        // consider putting these in a try catch&#xA;        Integer statusCode = (Integer)request.getAttribute(""javax.servlet.error.status_code"");&#xA;        Throwable exception = (Throwable)request.getAttribute(""javax.servlet.error.exception"");&#xA;&#xA;        // maybe add some error logging here, e.g. original status code, exception, traceid, etc.&#xA;&#xA;        // consider a better error to the user here&#xA;        return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(""{'message':'some error happened', 'trace_id':'some-trace-id-here'}"");&#xA;    }&#xA;&#xA;    @Override&#xA;    public String getErrorPath() {&#xA;        return ""/error"";&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
44115679,42353292,8029230,2017-05-22T14:35:49,"<p>Each microservice is expected to be functionally independent. For interacting with other microservices, it should rely on rest api calls only. Service Discovery plays a role to keep the services fairly loose coupled with each other. Also due to dynamic nature of the service urls, the hard dependencies are removed. Hope this helps</p>&#xA;"
46398450,46329712,5343790,2017-09-25T05:55:56,"<p>Apparently this happens because of different line endings in Linux and Windows. Windows systems use CR+LF while Unix and Unix-like systems use LF. </p>&#xA;&#xA;<p>To solve your issue, perform</p>&#xA;&#xA;<pre><code>sed -i -e 's/\r$//' entrypoint.sh&#xA;</code></pre>&#xA;&#xA;<p>You might need to do this if any other <code>.sh</code> files where <code>sed -i -e 's/\r$//' *.sh</code> helps.</p>&#xA;&#xA;<p>Then go ahead with Service Fabric Cluster stuff!</p>&#xA;&#xA;<p>People also play with <code>unix2dos</code> and <code>dos2unix</code> kind of things over such issues for which ample alternatives are listed <a href=""https://kb.iu.edu/d/acux"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;"
31135842,31135664,4236078,2015-06-30T10:30:56,"<p>Seems like a work in progress to me. I haven't use it myself, but you could check the <a href=""http://doc.akka.io/api/akka-stream-and-http-experimental/1.0-RC4/?_ga=1.108323586.1208307473.1416240750#akka.http.scaladsl.testkit.package"" rel=""nofollow"">API documentation</a> along with <a href=""https://github.com/akka/akka/blob/releasing-akka-stream-and-http-experimental-1.0-RC4/akka-http-testkit/src/test/scala/akka/http/scaladsl/testkit/ScalatestRouteTestSpec.scala"" rel=""nofollow"">some tests</a>. </p>&#xA;&#xA;<p>My advice is: continue using Spray until akka-http is stable. Then you can use the <a href=""http://doc.akka.io/docs/akka-stream-and-http-experimental/1.0-RC4/scala/http/migration-from-spray.html"" rel=""nofollow"">migration guide</a> (which is also a work in progress ;))</p>&#xA;"
47379504,47379427,4108590,2017-11-19T17:01:45,"<p>This can be done. Spring provides <code>org.springframework.web.client.RestTemplate</code> which is an HTTP client. You may refer this <a href=""https://spring.io/guides/gs/consuming-rest/"" rel=""nofollow noreferrer"">article</a> for more details. Also it allows you just to deal with java objects, instead of JSON data. It is easier to deal with Java objects. Spring takes care on serializing and deserializing between objects and JSON data.</p>&#xA;&#xA;<p>For an example, let's say you need to submit a simple <code>GET</code> request to the downstream rest endpoint and obtain the response as a DTO class. The following code would do that.</p>&#xA;&#xA;<pre><code>private RestTemplate restTemplate = new RestTemplate();&#xA;ResponseDTO responseDTO = restTemplate.getForEntity(""/api/endpoint"", ResponseDTO.class).getBody();&#xA;</code></pre>&#xA;"
42062655,42062199,4108590,2017-02-06T07:45:33,"<p>Well, Reactive Programming means you are doing all your IO bound tasks such as network calls asynchronously. For an instance say your application calls an external REST API or a database, you can do that invocation asynchronously. If you do so your current thread does not block. You can serve lots of requests by merely spawning one or few threads. If you follow blocking approach you need to have one thread to handle each and every request. You may refer my multi part blog post <a href=""http://ravindraranwala.blogspot.com/2016/12/introducing-java-reactive-extentions-in.html"" rel=""noreferrer"">part one</a>, <a href=""http://ravindraranwala.blogspot.com/2017/01/calling-exterenal-nosql-database.html"" rel=""noreferrer"">part two</a> and <a href=""http://ravindraranwala.blogspot.com/2017/01/combine-emissions-of-multiple_16.html"" rel=""noreferrer"">part three</a> for further details.</p>&#xA;&#xA;<p>Other than that you may use callbacks to do the same. You can do asynchronous invocation using callbacks. But if you do so sometimes you may ended up with callback hell. Having one callback inside another leads to very complex codes which are very hard to maintain. On the other hand RxJava lends you write asynchronous code which is much more simple, composable and readable. Also RxJava provides you a lots of powerful operators such as Map, Zip etc which makes your code much more simple while boosting the performance due to parallel executions of different tasks which are not dependent on each other.</p>&#xA;&#xA;<p>RxJava is not another Observer implementation with set of operators rather it gives you good error handling and retry mechanisms which are really handy. </p>&#xA;&#xA;<p>But I have not conducted any bench marking of RxJava with imperative programming approach to commend you statistically. But I am pretty much sure RxJava should yield good performance over blocking mechanisms.</p>&#xA;&#xA;<p><strong>Update</strong></p>&#xA;&#xA;<p>Since I gathered more experience over time, I thought of adding more points to my answer. </p>&#xA;&#xA;<p>Based on the <a href=""http://reactivex.io/intro.html"" rel=""noreferrer"">article</a>, ReactiveX is a library for composing asynchronous and event-based programs by using observable sequences. I reckon you to go through this introductory article in the first place.</p>&#xA;&#xA;<p>These are some properties of reactive systems: Event Driven, Scalable, Resilient, Responsive</p>&#xA;&#xA;<p>When it comes to RxJava it offers two main facilities to a programmer. First it offers a nice composable API using a rich set of operators such as zip, concat, map etc. This yields more simple and readable code. When it comes to code, readability and simplicity are the uttermost important properties. Second, it provides excellent abstractions, that enable concurrency to become declarative. </p>&#xA;&#xA;<p>A popular misconception is that Rx is multithreaded by default. To be honest, Rx is single-threaded by default. If you want to do things asynchronously, then you have to tell it explicitly using <code>subscribeOn</code> and <code>observeOn</code> operators by passing relevant schedulers. RxJava gives you thread pools to do asynchronous tasks. There are many schedulers such as IO, Computation and so forth. IO scheduler as the name suggests is best suited for IO intensive tasks such as network calls etc. In contrary Computation scheduler is good for more CPU intensive computation tasks. You can also hook up your own Executor services with RxJava too. The built in schedulers mainly helps you to get rid of maintaining your own Executor services, making your code more simple.</p>&#xA;&#xA;<p><strong>Finally a word on subscribeOn and observeOn</strong></p>&#xA;&#xA;<p>In the Rx world, there are generally two things you want to control the concurrency model for:</p>&#xA;&#xA;<ol>&#xA;<li>The invocation of the subscription </li>&#xA;<li>The observing of notifications</li>&#xA;</ol>&#xA;&#xA;<p>SubscribeOn: specify the Scheduler on which an Observable will operate.</p>&#xA;&#xA;<p>ObserveOn: specify the Scheduler on which an observer will observe this Observable</p>&#xA;"
50586868,50583823,441902,2018-05-29T14:26:57,"<p>I am assuming you want to run these services 24x7 and not on demand. With container orchestration it is possible. One way of doing it with Rancher is as below:</p>&#xA;&#xA;<p>Create 5 micro instances. 4 for the services and 1 for Rancher and put all 5 in 1 VPC. Now install Rancher in the 5th instance and add the other 4 hosts in Rancher, so that all your 4 hosts show up in Rancher's infrastructure. </p>&#xA;&#xA;<p>Now label all the 4 hosts in Rancher uniquely - for ex: 'zuulserver' , 'database' , 'configserver' , 'eurekaserver'</p>&#xA;&#xA;<p>Now edit your docker compose to add those rancher host labels to each of your services.</p>&#xA;&#xA;<p><code>io.rancher.scheduler.affinity:host_label: key1=value1</code></p>&#xA;&#xA;<pre><code> wordpress:&#xA;  labels:&#xA;    # Make wordpress a global service&#xA;    io.rancher.scheduler.global: 'true'&#xA;    # Make wordpress only run containers on hosts with a key1=value1 label&#xA;    io.rancher.scheduler.affinity:host_label: key1=value1&#xA;    # Make wordpress only run on hosts that do not have a key2=value2 label&#xA;    io.rancher.scheduler.affinity:host_label_ne: key2=value2&#xA;  image: wordpress&#xA;  links:&#xA;    - db: mysql&#xA;  stdin_open: true&#xA;</code></pre>&#xA;&#xA;<p>In Rancher create a stack with your docker compose and start the stack.</p>&#xA;&#xA;<p>Rancher will deploy those services to the corresponding hosts according to the host affinity labels.</p>&#xA;&#xA;<p><a href=""https://rancher.com/docs/rancher/v1.1/en/cattle/scheduling"" rel=""nofollow noreferrer"">https://rancher.com/docs/rancher/v1.1/en/cattle/scheduling</a></p>&#xA;&#xA;<p><a href=""https://rancher.com/docs/rancher/v1.2/en/hosts/"" rel=""nofollow noreferrer"">https://rancher.com/docs/rancher/v1.2/en/hosts/</a></p>&#xA;"
43659271,43313424,441902,2017-04-27T13:27:06,"<p>You have mentioned you are using a docker container to run the microservice. Are you trapping the SIGTERM in your entrypoint script in docker container ? If a SIGTERM is sent, the boot application will get it and you will see the below log showing that the microservice is deregistering with Consul.</p>&#xA;&#xA;<pre><code>2017-04-27 09:20:19.854  INFO 6852 --- [on(6)-127.0.0.1] inMXBeanRegistrar$SpringApplicationAdmin : Application shutdown requested.&#xA;2017-04-27 09:20:19.857  INFO 6852 --- [on(6)-127.0.0.1] ationConfigEmbeddedWebApplicationContext : Closing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@afb5821: startup date [Thu Apr 27 09:20:00 EDT 2017]; parent: org.springframework.context.annotation.AnnotationConfigApplicationContext@130c12b7&#xA;2017-04-27 09:20:19.859  INFO 6852 --- [on(6)-127.0.0.1] o.s.c.support.DefaultLifecycleProcessor  : Stopping beans in phase 2147483647&#xA;2017-04-27 09:20:19.863  INFO 6852 --- [on(6)-127.0.0.1] o.s.c.support.DefaultLifecycleProcessor  : Stopping beans in phase 0&#xA;2017-04-27 09:20:19.863  INFO 6852 --- [on(6)-127.0.0.1] o.s.c.c.s.ConsulServiceRegistry          : Deregistering service with consul: xxxxxxxxxxxxx&#xA;</code></pre>&#xA;&#xA;<p><a href=""http://martin.podval.eu/2015/06/java-docker-spring-boot-and-signals.html"" rel=""nofollow noreferrer"">This blog post discusses this.</a></p>&#xA;"
37751415,37750547,5247279,2016-06-10T14:59:29,<p>add set the classpath to the dir where the driver is installed</p>&#xA;&#xA;<pre><code>--classpath=&lt;value&gt; Classpath containing migration files and JDBC Driver.&#xA;</code></pre>&#xA;
45039580,45031529,143919,2017-07-11T16:06:12,"<p>You could create a stateful service with 32 partitions. Each partition in the service would read from a single partition in your IoT hub. You could also do this as a stateless service that is a background worker (not a web api) that has 32 instances. You would need some way of coordinating which instance/partition is communicating with each IoT partition.</p>&#xA;&#xA;<p>If you insist on have 32 instances of the service then you would just need to make sure that each instance of the service has a unique name. You could put these services in the  section of your ApplicationManifest:</p>&#xA;&#xA;<pre><code>&lt;DefaultServices&gt;&#xA;    &lt;Service Name=""Service01""&gt;&#xA;        &lt;StatelessService ServiceTypeName=""MyServiceType"" InstanceCount=""1""&gt;&#xA;            &lt;SingletonPartition /&gt;&#xA;        &lt;/StatelessService&gt;&#xA;    &lt;/Service&gt;&#xA;    &lt;Service Name=""Service02""&gt;&#xA;        &lt;StatelessService ServiceTypeName=""MyServiceType"" InstanceCount=""1""&gt;&#xA;            &lt;SingletonPartition /&gt;&#xA;        &lt;/StatelessService&gt;&#xA;    &lt;/Service&gt;&#xA;    ...&#xA;&lt;/DefaultServices&gt;&#xA;</code></pre>&#xA;"
42495563,36792713,537647,2017-02-27T20:53:18,"<p>Below is the steps can be achieved.</p>&#xA;&#xA;<ul>&#xA;<li>use systemjs loader to load components in your router ``</li>&#xA;<li>in your systemjs config file provide the path of the component</li>&#xA;</ul>&#xA;&#xA;<p>these <a href=""https://vsavkin.com/angular-router-declarative-lazy-loading-7071d1f203ee#.slllslx7r"" rel=""nofollow noreferrer""> examples</a> explans more in detail.</p>&#xA;"
51495571,40936597,3296815,2018-07-24T09:50:59,"<p>It was also not working for me, then I made some changes in my pom.xml and then it worked</p>&#xA;&#xA;<p><strong>Non working pom.xml</strong></p>&#xA;&#xA;<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;&#xA;&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""&#xA;    xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;&#xA;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&#xA;&#xA;    &lt;groupId&gt;com.sample.eurekaserver&lt;/groupId&gt;&#xA;    &lt;artifactId&gt;EurekaServer&lt;/artifactId&gt;&#xA;    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&#xA;    &lt;packaging&gt;jar&lt;/packaging&gt;&#xA;&#xA;    &lt;name&gt;EurekaServer&lt;/name&gt;&#xA;    &lt;description&gt;Demo project for Spring Cloud&lt;/description&gt;&#xA;&#xA;    &lt;parent&gt; &#xA;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&#xA;        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;&#xA;        &lt;version&gt;2.0.3.RELEASE&lt;/version&gt;&#xA;        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;&#xA;    &lt;/parent&gt;&#xA;&#xA;    &lt;properties&gt;&#xA;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;&#xA;        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;&#xA;        &lt;java.version&gt;1.8&lt;/java.version&gt;&#xA;    &lt;/properties&gt;&#xA;    &lt;dependencyManagement&gt;&#xA;        &lt;dependencies&gt;&#xA;            &lt;dependency&gt;&#xA;                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&#xA;                &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;&#xA;                &lt;version&gt;Finchley.RELEASE&lt;/version&gt;&#xA;                &lt;type&gt;pom&lt;/type&gt;&#xA;                &lt;scope&gt;import&lt;/scope&gt;&#xA;            &lt;/dependency&gt;&#xA;        &lt;/dependencies&gt;&#xA;    &lt;/dependencyManagement&gt;&#xA;    &lt;dependencies&gt;&#xA;        &lt;dependency&gt;&#xA;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&#xA;            &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;&#xA;        &lt;/dependency&gt;&#xA;&#xA;        &lt;dependency&gt;&#xA;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&#xA;            &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;&#xA;        &lt;/dependency&gt;&#xA;&#xA;        &lt;dependency&gt;&#xA;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&#xA;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;&#xA;            &lt;scope&gt;test&lt;/scope&gt;&#xA;        &lt;/dependency&gt;&#xA;    &lt;/dependencies&gt;&#xA;&#xA;    &lt;build&gt;&#xA;        &lt;plugins&gt;&#xA;            &lt;plugin&gt;&#xA;                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&#xA;                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;&#xA;            &lt;/plugin&gt;&#xA;        &lt;/plugins&gt;&#xA;    &lt;/build&gt;&#xA;&#xA;&#xA;&lt;/project&gt;&#xA;</code></pre>&#xA;&#xA;<p><strong>After changing above pom.xml to below one, it worked</strong></p>&#xA;&#xA;<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;&#xA;&lt;project xmlns=""http://maven.apache.org/POM/4.0.0""&#xA;    xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""&#xA;    xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;&#xA;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&#xA;&#xA;    &lt;groupId&gt;com.sample.eurekaserver&lt;/groupId&gt;&#xA;    &lt;artifactId&gt;EurekaServer&lt;/artifactId&gt;&#xA;    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&#xA;    &lt;packaging&gt;jar&lt;/packaging&gt;&#xA;&#xA;    &lt;name&gt;EurekaServer&lt;/name&gt;&#xA;    &lt;description&gt;Demo project for Spring Cloud&lt;/description&gt;&#xA;&#xA;    &lt;parent&gt;&#xA;        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&#xA;        &lt;artifactId&gt;spring-cloud-starter-parent&lt;/artifactId&gt;&#xA;        &lt;version&gt;Angel.SR6&lt;/version&gt;&#xA;        &lt;relativePath /&gt;&#xA;    &lt;/parent&gt;&#xA;&#xA;    &lt;properties&gt;&#xA;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;&#xA;        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;&#xA;        &lt;java.version&gt;1.8&lt;/java.version&gt;&#xA;    &lt;/properties&gt;&#xA;&#xA;&#xA;    &lt;dependencies&gt;&#xA;    &lt;dependency&gt;&#xA;      &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&#xA;      &lt;artifactId&gt;spring-cloud-starter-eureka-server&lt;/artifactId&gt;&#xA;    &lt;/dependency&gt;&#xA;    &lt;dependency&gt;&#xA;      &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&#xA;      &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;&#xA;      &lt;scope&gt;test&lt;/scope&gt;&#xA;    &lt;/dependency&gt;&#xA;  &lt;/dependencies&gt;&#xA;&#xA;    &lt;build&gt;&#xA;    &lt;plugins&gt;&#xA;      &lt;plugin&gt;&#xA;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&#xA;        &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;&#xA;        &lt;configuration&gt;&#xA;          &lt;requiresUnpack&gt;&#xA;            &lt;dependency&gt;&#xA;              &lt;groupId&gt;com.netflix.eureka&lt;/groupId&gt;&#xA;              &lt;artifactId&gt;eureka-core&lt;/artifactId&gt;&#xA;            &lt;/dependency&gt;&#xA;            &lt;dependency&gt;&#xA;              &lt;groupId&gt;com.netflix.eureka&lt;/groupId&gt;&#xA;              &lt;artifactId&gt;eureka-client&lt;/artifactId&gt;&#xA;            &lt;/dependency&gt;&#xA;          &lt;/requiresUnpack&gt;&#xA;        &lt;/configuration&gt;&#xA;      &lt;/plugin&gt;&#xA;      &lt;plugin&gt;&#xA;        &lt;artifactId&gt;maven-deploy-plugin&lt;/artifactId&gt;&#xA;        &lt;configuration&gt;&#xA;          &lt;skip&gt;true&lt;/skip&gt;&#xA;        &lt;/configuration&gt;&#xA;      &lt;/plugin&gt;&#xA;      &lt;plugin&gt;&#xA;        &lt;groupId&gt;com.heroku.sdk&lt;/groupId&gt;&#xA;        &lt;artifactId&gt;heroku-maven-plugin&lt;/artifactId&gt;&#xA;        &lt;version&gt;1.0.0&lt;/version&gt;&#xA;        &lt;configuration&gt;&#xA;          &lt;appName&gt;${heroku.appName}&lt;/appName&gt;&#xA;          &lt;includeTarget&gt;false&lt;/includeTarget&gt;&#xA;          &lt;includes&gt;&#xA;            &lt;include&gt;target/${project.build.finalName}.jar&lt;/include&gt;&#xA;          &lt;/includes&gt;&#xA;        &lt;/configuration&gt;&#xA;      &lt;/plugin&gt;&#xA;    &lt;/plugins&gt;&#xA;  &lt;/build&gt;&#xA;&#xA;&#xA;&lt;/project&gt;&#xA;</code></pre>&#xA;&#xA;<p>May this will help</p>&#xA;&#xA;<p><strong>--Edit--</strong>&#xA;my application.yml file looks like:</p>&#xA;&#xA;<pre><code>server:&#xA;  port: 8010&#xA;&#xA;eureka:  &#xA;  instance:&#xA;    hostname: localhost    &#xA;  client:&#xA;    registerWithEureka: true&#xA;    fetchRegistry: true&#xA;    serviceUrl:&#xA;      defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/&#xA;</code></pre>&#xA;"
51040968,51035215,348975,2018-06-26T10:45:07,"<p>In the article it had</p>&#xA;&#xA;<p><code>CMD java -jar HelloWorld.jar</code></p>&#xA;&#xA;<p>I want you to imagine the logical extension of hello world that takes arguments and can output ""Hello Steve"" instead.</p>&#xA;&#xA;<p>change that line to </p>&#xA;&#xA;<p><code>ENTRYPOINT [""java"", ""--jar"", ""HellowWorld.jar""]</code></p>&#xA;&#xA;<p>and</p>&#xA;&#xA;<p><code>CMD [""World""]</code></p>&#xA;"
51532388,51532187,5330223,2018-07-26T06:33:45,"<p>As you have trained the model using <code>Keras</code> I suggest you convert the model into <code>tensorflow</code> frozen model (<code>pb</code> file). You can use this <a href=""https://github.com/amir-abdi/keras_to_tensorflow"" rel=""nofollow noreferrer"">library</a> to convert the <code>h5</code> format <code>keras</code> model to <code>tensorflow</code> <code>pb</code> model.</p>&#xA;&#xA;<p>Once you have a ready <code>tensorflow</code> model you have many matured libraries to deploy the model. <a href=""https://www.tensorflow.org/serving/"" rel=""nofollow noreferrer"">Tensorflow-serving</a> is the famous one which has many handy built-in features like having a restful output from the model, a faster parallel prediction and many more. </p>&#xA;&#xA;<p>Here is a <a href=""https://medium.com/@mr.acle/exporting-deep-learning-models-from-keras-to-tensorflow-serving-7d4a6e49ce3"" rel=""nofollow noreferrer"">post</a> showing to deploy <code>keras</code> model in <code>tensorflow-serving</code>. After deploying int <code>tensorflow-serving</code> you can containerize it using <code>nvidia-docker</code> and then consume the service using any <code>java</code> <code>spring-boot</code> application.</p>&#xA;"
34657227,34637868,752175,2016-01-07T14:09:22,"<p>I would put the Azure API management in front of the mobile apps, on the microsoft integration roadmap, there was an announcement that API management will be built into App Services (and thus mobile services) by the end of 2016:&#xA;<a href=""https://www.microsoft.com/en-us/download/details.aspx?id=50408"" rel=""nofollow"">https://www.microsoft.com/en-us/download/details.aspx?id=50408</a></p>&#xA;&#xA;<p>The first preview is in ""Power Apps"" <a href=""https://azure.microsoft.com/nl-nl/documentation/articles/powerapps-configure-apis/"" rel=""nofollow"">https://azure.microsoft.com/nl-nl/documentation/articles/powerapps-configure-apis/</a>&#xA;where API management policies are incorporated directly</p>&#xA;&#xA;<p>For more info on creating an API management layer on top of mobile services:&#xA;<a href=""http://giventocode.com/azure-api-management-and-azure-mobile-services"" rel=""nofollow"">http://giventocode.com/azure-api-management-and-azure-mobile-services</a></p>&#xA;"
44564382,44554638,3768367,2017-06-15T09:52:24,"<blockquote>&#xA;  <p>The question is how to prevent cases where communication between two&#xA;  microservices is broken due to a broken contract between them?</p>&#xA;</blockquote>&#xA;&#xA;<ol>&#xA;<li><p><strong>Design of Contract</strong>: While you are designing the contract, the server [also called service provider or producer] can not dictate the contract and just say here is the contract for my offering and now go and consume the service. If server has multiple clients (which is usually the case), then multiple clients will give their 'demands' for contracts and server will then implement the minimal common aggregate as service offering.</p></li>&#xA;<li><p><strong>Change In Contract</strong>: Service provider should strive to keep the contract as backword compatible in most of the times. However, if contract requires significant change, then it can be handled with spinning up new version of endpoint. In this case, decommissioning of old endpoint of service(e.g. say v1) will not be done immediately. Consumers of service are given notice of this change and given some time to switch over to newer version of contract. (eg. say v2)</p></li>&#xA;</ol>&#xA;&#xA;<p>You can get more infomration on Martin Fowler's bliki: <a href=""https://martinfowler.com/articles/consumerDrivenContracts.html"" rel=""nofollow noreferrer"">https://martinfowler.com/articles/consumerDrivenContracts.html</a> </p>&#xA;&#xA;<blockquote>&#xA;  <p>What is the best strategy to handle such issues?</p>&#xA;</blockquote>&#xA;&#xA;<p>I think,I have answered the strategies above. However, in terms of tooling, following are some tools that will facilitate the scenario:</p>&#xA;&#xA;<ol>&#xA;<li>Pact: <a href=""https://docs.pact.io/"" rel=""nofollow noreferrer"">https://docs.pact.io/</a></li>&#xA;<li>Consul:This is service discovery tool, however, if you are adopting microservices, then this will be really useful for dealing with numerous services. <a href=""https://www.consul.io/"" rel=""nofollow noreferrer"">https://www.consul.io/</a></li>&#xA;</ol>&#xA;"
44564677,44535853,3768367,2017-06-15T10:05:44,"<blockquote>&#xA;  <p>Can I cluster the the API Gateway and have the load balancer in front&#xA;  of it.</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes, you can. Most of the good Api Gateway solutions will provide the ability to do clustering. e.g. <a href=""https://getkong.org/docs/0.9.x/clustering/"" rel=""nofollow noreferrer"">https://getkong.org/docs/0.9.x/clustering/</a> or you can use cloud based Api Gateway: Azure API Management or AWS API Gateway</p>&#xA;&#xA;<blockquote>&#xA;  <p>What do I need to consider with respect to managing authentication?</p>&#xA;</blockquote>&#xA;&#xA;<p>These specifics depends on your selection of API Gateway solution.</p>&#xA;"
46618823,46615008,3768367,2017-10-07T09:48:10,"<p>I. <code>circuit breaker</code> and <code>service discovery</code> are patterns. When we say Pattern they can be implemented with any programming language. 'HTTP' protocol is for transfer of data.</p>&#xA;&#xA;<p><strong><code>circuit breaker</code></strong> can be implemented within Java. You can find many implementations (of course, with varying capabilities and interpretation of pattern) on github. </p>&#xA;&#xA;<p>Some of the well-known, built for purpose implementations are :</p>&#xA;&#xA;<ol>&#xA;<li><p><a href=""https://github.com/Netflix/Hystrix"" rel=""nofollow noreferrer"">Hysterix from NetflixOSS</a> For using Hysterix: You can follow Spring Guide - <a href=""https://spring.io/guides/gs/circuit-breaker/"" rel=""nofollow noreferrer"">Spring Circuit Breaker</a></p></li>&#xA;<li><p><a href=""https://polygene.apache.org/java/latest/library-circuitbreaker.html"" rel=""nofollow noreferrer"">Apache Polygene</a> - which has example of JMX circuit breaker</p></li>&#xA;<li><p><a href=""https://github.com/resilience4j/resilience4j"" rel=""nofollow noreferrer"">Resilience4j</a></p></li>&#xA;</ol>&#xA;&#xA;<p>II. About,</p>&#xA;&#xA;<blockquote>&#xA;  <p>Given this event-driven nature, how can service-discovery and circuit&#xA;  breakers be achieved/ implemented, given that these are commonly&#xA;  applicable for services communicating via HTTP?</p>&#xA;</blockquote>&#xA;&#xA;<p>It seems you need bit more research on topic of Microservices interactions.&#xA;There are two ways to which microservices interactions are possible. You have to choose one over the other. You can/should not mix both.</p>&#xA;&#xA;<ol>&#xA;<li><p><strong>Orchestration</strong>: An interaction style that has an intelligent controller that dispatches events to processes. Please note the word 'processes' which is representing business processes here. Orchestration style was preferred in old SOA implementations as well.</p></li>&#xA;<li><p><strong>Choreography</strong>: An interaction style that allows processes to subscribe to events and handle them independently or through integration with other processes without the need for a central controller.</p></li>&#xA;</ol>&#xA;&#xA;<p>These topics are greatly covered under &#xA;<a href=""https://stackoverflow.com/questions/4127241/orchestration-vs-choreography"">Orchestration vs. Choreography</a></p>&#xA;&#xA;<p><strong>Need of Service Discovery</strong>:</p>&#xA;&#xA;<p>With choreography, two or more microservices can coordinate their activities and processes to share information and value. </p>&#xA;&#xA;<p>But, these microservices may not be aware of each other's existence i.e. There are no hard-coded or service references of dependency endpoints configured or coded into them. Why we do this, is for avoiding any kind of coupling between services. So, the question remains is how one service, if required will find another services' endpoint? This is where <code>service discovery</code> mechanism is used.</p>&#xA;&#xA;<p>Another perspective is, with microservices deployment with containers etc, microservices endpoints will not be even tied to any hosts etc. [due to spin-up and spin-down of containers]. So, for this case as well, we need 'service discovery' mechanism.</p>&#xA;&#xA;<p>So, In service discovery mechanism, a centralized service discovery tool helps services to register themselves and to discover other services via a DNS or HTTP interface. </p>&#xA;&#xA;<p>Service discovery can be implemented with&#xA;1. <a href=""http://microservices.io/patterns/server-side-discovery.html"" rel=""nofollow noreferrer"">Server-side service discovery</a>&#xA;2. <a href=""http://microservices.io/patterns/client-side-discovery.html"" rel=""nofollow noreferrer"">Client Side service discovery</a></p>&#xA;&#xA;<p>Consul,etcd, zookeeper are some of the key-tools names within service discovery space. </p>&#xA;"
45534475,44482448,5213999,2017-08-06T17:04:20,"<p>The issue I faced was not directly from firebase or its routing. The issue was with my express app configuration. I fixed the same and it all worked fine. </p>&#xA;&#xA;<p>Moreover, I've created microservices (cloud functions) from multiple projects and been able to deploy them independently using the <code>firebase deploy --only functions:fun1,fun2</code> and the scoping seem to work just fine. I've been using different version of the js libraries in different microservices and I didn't face any conflicts as of now!</p>&#xA;&#xA;<p>Though I don't know the internals of the firebase deployment strategies. I could assert from my experience so far that, each deployment of the microservices are somehow sandboxed so that the other microservice projects have no conflict!</p>&#xA;&#xA;<p>Only glitch is that the routing need to be handled in one single project, and this is understandable because of the complexity of maintaining multiple routing rules and possible collisions if they support the re-write rules be supported from multiple projects. </p>&#xA;&#xA;<p>Someone with more insightful knowledge may correct my understanding. </p>&#xA;"
35140751,35140042,238639,2016-02-01T21:32:48,"<h2>Microservices is an architecture, not a <em>web</em>* framework choice</h2>&#xA;&#xA;<p>I don't think whether your app turns out to be a monolith or a microservice depends on the web framework that you are using (e.g. Django or Flask)</p>&#xA;&#xA;<p>*Unless we're talking about frameworks specifically built to deal with managing microservices (e.g. to facilitate service discovery) web frameworks like Django or Flask are pretty equivalent (input HTTP request, output HTTP response)</p>&#xA;&#xA;<p>Maybe their <em>perceived popularity</em> in a given use case (e.g. Django monolith or Flask microservice) may suggest they are better suited for a particular deployment style, but you could a microservice with Django just as easily as you can a monolith with Flask.</p>&#xA;&#xA;<p>For instance, if I select Flask as my web framework, and in a single Flask app/process I implement all of the services (auth, admin, reporting, payroll, marketing, billing, messaging, search etc), where everything communicates on the code level, then I'd end up with a monolith.</p>&#xA;&#xA;<p>On the other hand, if for instance I take Django, and divide my services into several small Django deploys, and make it so they only communicate through well defined HTTP contracts, and can be updated/deployed independently, then I'd end up with a microservice.</p>&#xA;&#xA;<p>Bottom line is, a microservice is more than just the web framework you use, so use whichever one you like the most, until you have more specific requirements and e.g. 3rd party libs that are useful to you and are only available for a given framework but not the other.</p>&#xA;"
44888590,44886715,1877295,2017-07-03T14:50:45,"<p>What I usually do is keep them separate. Account information (first name, last name, contact data, affiliation, sex etc) is not related to authentication/authorization. Also, an account can have multiple authentication methods (i.e. OAuth, uname-pass, private key), which isn't really related to account data. So, I take them as separate entities. I know auth and account data seem the same, but they represent two very different things, with very different responsibilities, so I keep them separate. If one user should have to see some other user's first and last name, I wouldn't like to get other user's credentials out of the database (a lot can go wrong). </p>&#xA;&#xA;<p>If you are thinking of UserService from Spring Security, it goes with Auth server.</p>&#xA;&#xA;<p>From security stand point, having a single point of truth (auth server) and be able to fix an issue in one place is a huge advantage. </p>&#xA;&#xA;<p>Anyhow, IMHO, account and auth can share some properties, but they are two different things - hence I keep them separate.</p>&#xA;&#xA;<p>Hope this helps. </p>&#xA;"
35933932,35304113,777263,2016-03-11T06:49:33,"<p>Yes, you are right when you say Microservices are more about independent service (processes) that can be deployed in one or more cloud machines. Each service can communicate to other using non-http protocol like Message brokers, Thrift, Remote Procedure call (RPC) etc. </p>&#xA;&#xA;<p>As the architecture point of view, services should mostly be decoupled enough to handle complexity of distributed computing. see the image on <a href=""http://nordicapis.com/microservices-architecture-the-good-the-bad-and-what-you-could-be-doing-better/"" rel=""nofollow noreferrer"">Microservices Architecture link</a>&#xA;There's a concept of API Gateway which could be used for authentication and service registration and discovery purpose.</p>&#xA;&#xA;<p>Coming back to your question, you can test microservices on single cloud (by running each service on different port) and use API Gateway to discover the service path for references here are the links which are worth to look at these. </p>&#xA;&#xA;<p>For concept see links: <a href=""http://microservices.io/patterns/apigateway.html"" rel=""nofollow noreferrer"">Microservices.io</a> and <a href=""https://stackoverflow.com/questions/29669180/microservice-service-registry-api-gateway-and-data-sharing"">stackoverflow question</a></p>&#xA;&#xA;<p>For Implementation: <a href=""http://blog.arungupta.me/zookeeper-microservice-registration-discovery/"" rel=""nofollow noreferrer"">zookeeper</a> and <a href=""https://auth0.com/blog/2015/09/13/an-introduction-to-microservices-part-2-API-gateway/"" rel=""nofollow noreferrer"">Auth0</a> (this is what i'm using)</p>&#xA;&#xA;<p>If you are java lover great to look at <a href=""http://www.infoq.com/articles/Ratpack-and-Spring-Boot"" rel=""nofollow noreferrer"">infoQ article</a>&#xA;Some of the free source that might can help in building and testing microservices are: <a href=""https://cloud.google.com/solutions/microservices-on-app-engine"" rel=""nofollow noreferrer"">Google App Engine</a>, <a href=""https://hook.io/examples"" rel=""nofollow noreferrer"">hook.io</a></p>&#xA;"
35969684,31044380,777263,2016-03-13T11:30:57,"<p>It is possible to create separate Auth service to provide access_token like you showed in step 1. But in API Gateway each service will needed to call that auth service to validate token. Its best to apply oauth process within API Gateway which I'm using as well for my product and this approach is also explained in many articles. lets look at the image below.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/Evdzz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Evdzz.png"" alt=""inlined image""></a></p>&#xA;&#xA;<p>In technical perspective, It could simply a part of code (function) which handles request header to verify the token provided as oauth authentication, which might handled within code or by accessing own database before it forwards requests to service's endpoint. </p>&#xA;&#xA;<p>You can follow one method to provide authentication, security and request dispatching to endpoint either by service of enhanced API Gateway. There's question already asked at stackoverflow <a href=""https://stackoverflow.com/questions/29669180/microservice-service-registry-api-gateway-and-data-sharing"">here</a>, But what i found easy to follow are 3 or 4 series of tutorials you'll find <a href=""https://auth0.com/blog/2015/09/13/an-introduction-to-microservices-part-2-API-gateway/"" rel=""nofollow noreferrer"">here</a></p>&#xA;&#xA;<p>Get clear picture of your API Gateway usage before you focus on microservices to work on.</p>&#xA;"
38063749,37897058,777263,2016-06-27T21:44:56,"<p>Buddy! its hell of info you are trying to perceive here. Let me try to explain in steps.</p>&#xA;&#xA;<p>Documentation centric means transitions between services and Yes, it should be called as semantically sharing information (or understanding as data types) on the web. </p>&#xA;&#xA;<p><strong>Step:1</strong>&#xA;Protocols (http) used for services with data types metadata and standard data types could be any form of hypermedia i.e. HTML, XML, JSON, HAL etc.&#xA;For example JSON shown below, which is a root document with links. Both 'todos' and 'profile' are just hypermedia links which is HAL based and HAL only augments your current APIs.</p>&#xA;&#xA;<pre><code>{ ""_links"" : {&#xA;    ""todos"" : {&#xA;      ""href"" : ""http://localhost:8080/todos""&#xA;    },&#xA;    ""profile"" : {&#xA;      ""href"" : ""http://localhost:8080/alps""&#xA;    }&#xA;  }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Note that its just links with possible resource linking that can point to semantics of resources. HAL main focus is just linking resources/templates either by links, properties and/or embedding.&#xA;Affordances explained above are mainly links shared data types at protocol level.</p>&#xA;&#xA;<p><strong>Step:2</strong>&#xA;ALPS are the Application level affordances which mean in above JSON i know what the Todo is but how to interact with it? To Interact with Todo, need to have Application level state transitions.&#xA;Consider below the 'todo' JSON which navigated from link and shows detailed extra keywords such as 'descriptors' and 'Type' (SEMANTIC, SAFE, UNSAFE, etc).</p>&#xA;&#xA;<p>'id' properties becomes representation identifiers. These are set or Rules to apply independent ALPS state and transitions.</p>&#xA;&#xA;<pre><code>{ ""version"" : ""1.0"",&#xA;  ""descriptors"" : [ {&#xA;    ""id"" : ""todo-representation"",&#xA;    ""descriptors"" : [ {&#xA;      ""name"" : ""description"",&#xA;      ""doc"" : {&#xA;        ""value"" : ""Details about the TODO item"",&#xA;        ""format"" : ""TEXT""&#xA;      },&#xA;      ""type"" : ""SEMANTIC""&#xA;    }, {&#xA;      ""name"" : ""title"",&#xA;      ""doc"" : {&#xA;        ""value"" : ""Title for the TODO item"",&#xA;        ""format"" : ""TEXT""&#xA;      },&#xA;      ""type"" : ""SEMANTIC""&#xA;    }, {&#xA;      ""name"" : ""id"",&#xA;      ""type"" : ""SEMANTIC""&#xA;    } ]&#xA;  }, {&#xA;    ""id"" : ""get-todos"",&#xA;    ""name"" : ""todos"",&#xA;    ""type"" : ""SAFE"",&#xA;    ""rt"" : ""#todo-representation""&#xA;  }, {&#xA;    ""id"" : ""create-todos"",&#xA;    ""name"" : ""todos"",&#xA;    ""type"" : ""UNSAFE"",&#xA;    ""rt"" : ""#todo-representation""&#xA;  }, {&#xA;    ""id"" : ""delete-todo"",&#xA;    ""name"" : ""todo"",&#xA;    ""type"" : ""IDEMPOTENT"",&#xA;    ""rt"" : ""#todo-representation""&#xA;  }, {&#xA;    ""id"" : ""patch-todo"",&#xA;    ""name"" : ""todo"",&#xA;    ""type"" : ""UNSAFE"",&#xA;    ""rt"" : ""#todo-representation""&#xA;  }, {&#xA;    ""id"" : ""get-todo"",&#xA;    ""name"" : ""todo"",&#xA;    ""type"" : ""SAFE"",&#xA;    ""rt"" : ""#todo-representation""&#xA;  } ]&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Some links are worth to check in detail <a href=""http://www.slideshare.net/InfoQ/generic-hypermedia-and-domainspecific-apis-resting-in-the-alps"" rel=""nofollow"">slides about ALPS</a> and <a href=""https://spring.io/blog/2014/07/14/spring-data-rest-now-comes-with-alps-metadata"" rel=""nofollow"">Rest Example</a>. Hope this helped you in understanding.</p>&#xA;"
49706030,45655728,4845615,2018-04-07T09:48:51,"<h2>Microservices</h2>&#xA;&#xA;<p>A <a href=""https://en.wikipedia.org/wiki/Microservices"" rel=""nofollow noreferrer"">microservice architecture</a> is about slicing an application logic into small pieces or ""components"" that can act between them and/or be exposed through an API.</p>&#xA;&#xA;<h2>API</h2>&#xA;&#xA;<p>A (web) application need to design the business logic with all set of object entities (model) and possible operations on them.&#xA;An (Application Programming Interface][<a href=""https://en.wikipedia.org/wiki/Application_programming_interface"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Application_programming_interface</a>) is a way of making the requests to an application by exposing specific entry-points that are in charge of invoking the appropriate application operations.</p>&#xA;&#xA;<p><a href=""https://en.wikipedia.org/wiki/Representational_state_transfer"" rel=""nofollow noreferrer"">REST(ful) APIs</a>&#xA; (""REST"" as in Representational State Transfer) are APIs complying with at least these <a href=""https://www.ics.uci.edu/~fielding/pubs/dissertation/top.htm"" rel=""nofollow noreferrer"">5 constraints</a>: </p>&#xA;&#xA;<ul>&#xA;<li>User-interface is distinct from data storage and manipulation (Client-Server architecture)</li>&#xA;<li>No client context is stored on the server (""stateless"")</li>&#xA;<li>Server responses must, implicitly or explicitly, define themselves as cacheable or not</li>&#xA;<li>Client does not have to be aware of the layers between him and the server</li>&#xA;<li>Response/request messages must be: be self-descriptive; allow to identify a resource; use representations allowing to manipulate the resources; announce available actions and resources (""Uniform interface"").  </li>&#xA;</ul>&#xA;&#xA;<h2>""The real difference""</h2>&#xA;&#xA;<p>So, while these notions are obviously related, they are clearly distinct concepts:</p>&#xA;&#xA;<ul>&#xA;<li><p>Being RESTful or not, an <strong>API</strong> exposes operations provided by a server that might (but not necessarily) be shelled into smaller components (<strong>microservices</strong>).</p></li>&#xA;<li><p>Also, while a typical web (REST)API uses the HTTP protocol between the client and the server, components within a microservice architecture might communicate using other protocol(s) (e.g. <a href=""http://wamp-proto.org/spec/"" rel=""nofollow noreferrer"">WAMP</a>, <a href=""https://www.amqp.org/about/what"" rel=""nofollow noreferrer"">AMQP</a>, <a href=""https://en.wikipedia.org/wiki/JSON-RPC"" rel=""nofollow noreferrer"">JSON-RPC</a>, <a href=""https://en.wikipedia.org/wiki/XML-RPC"" rel=""nofollow noreferrer"">XML-RPC</a>, <a href=""https://en.wikipedia.org/wiki/SOAP"" rel=""nofollow noreferrer"">SOAP</a>, ...)</p></li>&#xA;</ul>&#xA;"
38577075,38576865,527152,2016-07-25T20:41:32,"<p>Like requesting that information in a controller and then outputting it as a string in an ERB template?</p>&#xA;&#xA;<p>Is there some other requirement that I'm missing?</p>&#xA;&#xA;<p><strong>Controller</strong></p>&#xA;&#xA;<pre><code>def index&#xA;  @content = body_of_request_to_service&#xA;end&#xA;</code></pre>&#xA;&#xA;<p><strong>View</strong></p>&#xA;&#xA;<pre><code>...&#xA;&lt;%= @content %&gt;&#xA;...&#xA;</code></pre>&#xA;&#xA;<p>But, I wouldn't recommend this style of microservices using Ruby <em>(unless you have a dev-team that understands how to handle async Ruby IO)</em>. A good rule of thumb is to only have one blocking service request per client request.</p>&#xA;"
43608013,42140285,986160,2017-04-25T10:22:09,"<p>I think the proper way for implementing Event Sourcing is by having Kafka be filled directly from events pushed by a plugin that reads from the RDBMS binlog e.g using Confluent BottledWater (<a href=""https://www.confluent.io/blog/bottled-water-real-time-integration-of-postgresql-and-kafka/"" rel=""nofollow noreferrer"">https://www.confluent.io/blog/bottled-water-real-time-integration-of-postgresql-and-kafka/</a>) or more active Debezium (<a href=""http://debezium.io/"" rel=""nofollow noreferrer"">http://debezium.io/</a>). Then consuming Microservices can listen to those events, consume them and act on their respective databases being eventually consistent with the RDBMS database.</p>&#xA;&#xA;<p>Have a look here to my full answer for a guideline:&#xA;<a href=""https://stackoverflow.com/a/43607887/986160"">https://stackoverflow.com/a/43607887/986160</a></p>&#xA;"
50893240,50891382,1707312,2018-06-17T02:37:51,"<p>Finally found response here. looks like go has a test binary that can be used &#xA;<a href=""https://www.elastic.co/blog/code-coverage-for-your-golang-system-tests"" rel=""nofollow noreferrer"">https://www.elastic.co/blog/code-coverage-for-your-golang-system-tests</a></p>&#xA;"
45721620,45667346,5183024,2017-08-16T19:33:21,"<p>If you can't use additional service like RabbitMQ, add host-id (which C instance) in the message header before you broadcast. &#xA;C instances check the header, if its id is present in the header it will process.</p>&#xA;&#xA;<p>This mechanism requires that each instance has accurate information of other running instances.</p>&#xA;&#xA;<p>You can further improve by load balancing among C instances.</p>&#xA;"
42294730,30286443,1515568,2017-02-17T09:53:40,"<p>git submodules and subtrees have their own problems too. Facebook and google? have a single repository for all their microservices. There is no clear answer to this problem but things such as refactoring, dependency management and project set up get benefits from a mono repository. Again, the coupling of the services and how different teams interact with the repo is key to choose one or another.</p>&#xA;"
51394950,45789168,1599792,2018-07-18T06:27:51,"<p><strong>API</strong> - It a way of exposing functionality over web. Imagine you have developed some functionality in .Net but not you are developing some software in a different language. Would you develop the same functionality again? No. So, just expose it via web service.Web services are not tied to any one operating system or programming language. For example, an application developed in Java can communicate with the one developed in C#, Android, etc., and vice versa. </p>&#xA;&#xA;<p><strong>Microservice</strong> - They are used to break a complex software into small pieces of individually deployable, testable, loosely coupled sub-modules. Micro Services are designed to cope with failure and breakdowns of large applications. Since multiple unique services are communicating together, it may happen that a particular service fails, but the overall larger applications remain unaffected by the failure of a single module.</p>&#xA;&#xA;<p><strong>API Vs Microservice</strong> - Now that we have broken our complex software into loosely couple sub-modules. These sub-modules communicate with each other via an API. Therefore, Microservices and an API solve different problems but works together!</p>&#xA;&#xA;<p><strong>More Details:</strong></p>&#xA;&#xA;<p><a href=""https://www.tatvasoft.com/blog/the-difference-between-micro-services-and-web-services/"" rel=""nofollow noreferrer"">The Difference between Web Services and Micro Services</a></p>&#xA;&#xA;<p><a href=""https://medium.com/@ericjwhuang/restful-api-vs-microservice-eea903ac3e73"" rel=""nofollow noreferrer"">RESTful API vs Microservice</a></p>&#xA;"
50199140,50184065,8951344,2018-05-06T11:25:37,"<p>Let's assume that you have a class like the following: </p>&#xA;&#xA;<pre><code>@Value&#xA;public class QuestionOptions {&#xA;     private Question question;&#xA;     private List&lt;Option&gt; options;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>(the <a href=""https://projectlombok.org/features/Value"" rel=""nofollow noreferrer"">@Value</a> annotation is from <a href=""https://projectlombok.org"" rel=""nofollow noreferrer"">Lombok</a>)</p>&#xA;&#xA;<p>You can retrieve a question with its options like this:</p>&#xA;&#xA;<pre><code>Flux&lt;String&gt; categoryIds = Flux.just(""1"", ""2"", ""3"");&#xA;Flux&lt;QuestionOptions&gt; questionOptions = &#xA;    categoryIds.flatMap(categoryId -&gt; &#xA;         // retrieve questions for each category&#xA;         questionService.getQuestions(categoryId)&#xA;              // get options for each question &#xA;              .flatMap(question -&gt; optionService.getOptions(question.getId())&#xA;              .collectList()&#xA;              .map(optionList -&gt; new QuestionOptions(question, optionList))&#xA;         ))&#xA;    .subscribeOn(Schedulers.elastic()); // retrieve each question on a different thread.&#xA;</code></pre>&#xA;&#xA;<p>Note that, if the order of categories might be different than the order you requested. If that's a deal breaker for you, you might consider using <a href=""https://projectreactor.io/docs/core/release/api/reactor/core/publisher/Flux.html#concatMap-java.util.function.Function-"" rel=""nofollow noreferrer""><code>concatMap()</code></a> instead of <a href=""https://projectreactor.io/docs/core/release/api/reactor/core/publisher/Flux.html#flatMap-java.util.function.Function-"" rel=""nofollow noreferrer""><code>flatMap()</code></a>, though each request will run sequentially then.</p>&#xA;"
50266052,50264214,891440,2018-05-10T05:27:02,"<p>In <a href=""https://github.com/ugexe/zef/issues/241#issuecomment-383823558"" rel=""nofollow noreferrer"">this answer in zef's issues</a>, they state that ""installations are immutable"". &#xA;It's probably a better option if you download Cro from its source, patch it and install again so that your application picks up the new version.</p>&#xA;&#xA;<p>It might also happen that 'application/json' does not admit that charset declaration, or that there should be no space behind the ;. But the main issue here is that you shouldn't edit modules once installed. </p>&#xA;"
51786928,51786390,6864688,2018-08-10T12:49:14,"<p>That's a lot of fundamental questions, which definitely need good answers before you proceed with your project. I recommend reading ""<a href=""http://shop.oreilly.com/product/0636920033158.do"" rel=""nofollow noreferrer"">Building microservices""</a> from Sam Newman. </p>&#xA;"
32719193,32715464,3146801,2015-09-22T14:10:06,"<p>I think you are heading the right way. You can write your own <code>@RibbonClient</code> which is basically a <code>@Configuration</code> class for loadbalancing per service.</p>&#xA;&#xA;<p><code>@RibbonClient(name = ""yourServiceName"", configuration = RibbonConfigForThatService.class)</code> .</p>&#xA;&#xA;<p>In that you can place your own <code>IRule</code> - and if needed any other component - for that service. With this you can filter the instances available for loadbalancing based on their metadata.</p>&#xA;&#xA;<p><code>RoundRobinRule</code> does a simple round robin loadbalancing, good example for further needs.</p>&#xA;"
32865020,32863771,3146801,2015-09-30T11:33:10,"<p>When posting the <code>String</code> to the backend service you have to specify the <code>Content-Type</code> header so Spring knows which <code>HttpMessageConverter</code> to use to deserialize the <code>Data</code> object.</p>&#xA;&#xA;<p>With <code>RestTemplate</code> you can specify the header like this: </p>&#xA;&#xA;<pre><code>HttpHeaders headers = new HttpHeaders();&#xA;headers.setContentType(MediaType.APPLICATION_JSON);&#xA;&#xA;HttpEntity&lt;String&gt; entity = new HttpEntity&lt;String&gt;(data, headers);&#xA;restTemplate.postForEntity(url, entity, responseType);&#xA;</code></pre>&#xA;"
32482744,32481469,3146801,2015-09-09T14:47:17,<p>Depending on how much method needs to be linked I would construct the links manually or go with sharing api-s between apps and building links from those descriptors. I wouldn't introduce a dependency for a few links.</p>&#xA;&#xA;<p>The more interesting question is what are you planning to set as host of the link? The actual host or the eureka id of the service? I recommend setting the id of the service and then setting up a zuul instance and deal with loadbalancing and proxying in that.</p>&#xA;&#xA;<p>P.S: When linking services i always found myself in trouble when linking them too much. Deployment speed and resiliency can greatly suffer.</p>&#xA;
37686214,26616962,6436657,2016-06-07T17:58:56,"<p>According to Martin Fowler: ""The second approach in common use is messaging over a lightweight message bus. The infrastructure chosen is typically dumb (dumb as in acts as a message router only)"".</p>&#xA;&#xA;<p>The rationale for using smart end points is implied by: ""The key property of a component is the notion of independent replacement and upgradeability - which implies we look for points where we can imagine rewriting a component without affecting its collaborators."".</p>&#xA;&#xA;<p>To support the latter a micro service needs to be tolerant to its consumer. E.g. adding a mandatory input argument later on would break the interface, and therefore should be avoided. Instead one should use compensation strategies, like defaults, or support some sort of internal ""routing"" so that the microservice is still able to give a valid response. This is a kind-of smart ""end-point"".</p>&#xA;"
37877602,34586306,2188593,2016-06-17T09:13:04,"<p>oData is entirely valid as a method for exposing a microservice; exposing a explicit table however isn't microservices. So I don't agree completely with jgauffin. There is no reason why an API cannot be made available using oData. Where I do agree with JGauffin is that an API should have a small, and planar footprint that is decoupled from the detailed data structures of the source or destination. Therefore it is up to the service calling it to transform the API, but means that the generic format of the API can be reused as long as the business need is there, and technical platforms switched as required. </p>&#xA;"
27840731,27839789,1612849,2015-01-08T12:58:32,"<p>Some alternatives (just brainstorming):</p>&#xA;&#xA;<ol>&#xA;<li>Just drop HTTP and use pub-sub with a queue. Have the producer queueing items, a number of consumers processing them (and triggering state changes, in this case with HTTP if you fancy it).</li>&#xA;<li><p>If you really want to HTTP, I think there are a couple of missing pieces. If your items' states are <em>pending</em> and <em>processed</em>, there's a hidden/implicit state in your state machine: <em>in_progress</em> (or whatever). Once you think of it, picture becomes clearer: your <code>GET /items</code> is not idempotent (because it changes the state of items from pending to in progress) and hence should not be a GET in the first place.</p>&#xA;&#xA;<p>a. an alternative could be adding a new entity (e.g. batch) that gets created via POST and groups some items under it and sends them. Items already returned won't be part of future batches, and then you can mark as done whole batches (e.g. <code>PUT /batches/X/done</code>). This gets crazy very fast, as you will start reimplementing features (acks, timeouts, errors) already present both in queueing systems and plain/explicit (see c) HTTP.</p>&#xA;&#xA;<p>b. a slightly simpler alternative: just turn <code>/items</code> in a <code>POST</code>/<code>PUT</code> (weird in both cases) endpoint that marks items as being processed (and doesn't return them anymore because it only returns pending items). The same issue with errors and timeouts apply though.</p>&#xA;&#xA;<p>c. have the producer being explicit and requesting the processing of an item to the other service via PUT. you can either include all needed data in the body, or use it as a ping and have the processor requesting the info via GET. you can add asynchronous processing in either side (but probably better in the processor).</p></li>&#xA;</ol>&#xA;&#xA;<p>I would honestly do 1 (unless compelling reason).</p>&#xA;"
40277594,29117570,7078706,2016-10-27T06:25:41,"<p>If the <strong>State</strong> needs to be managed then the Event Sourcing with CQRS is the ideal way of communication. Else, an Asynchronous messaging system (AMQP) can be used for inter microservice communication.</p>&#xA;&#xA;<p>From your question, it is clear that the ES with CQRS should be the right mix. If using java, take a look at Axon framework. Or build a custom solution using Kafka or RabbitMQ.</p>&#xA;"
41841846,41837933,269106,2017-01-25T01:16:11,"<p>The URL you are using to access the dashboard is an endpoint on the API Server. By default, <code>kubeadm</code> deploys the API server on port <code>6443</code>, and not on <code>443</code>, which is what you would need to access the dashboard through <code>https</code> without specifying a port in the URL (i.e. <code>https://&lt;kubernetes-master&gt;/ui</code>)</p>&#xA;&#xA;<p>There are various ways you can expose and access the dashboard. These are ordered by increasing complexity:</p>&#xA;&#xA;<ul>&#xA;<li>If this is a dev/test cluster, you could try making <code>kubeadm</code> deploy the API server on port <code>443</code> by using the <code>--api-port</code> flag <a href=""https://github.com/kubernetes/kubernetes/blob/master/cmd/kubeadm/app/cmd/init.go#L83"" rel=""noreferrer"">exposed</a> by <code>kubeadm</code>.</li>&#xA;<li>Expose the dashboard using a <a href=""https://kubernetes.io/docs/user-guide/services/"" rel=""noreferrer"">service</a> of type <code>NodePort</code>.</li>&#xA;<li>Deploy an <a href=""https://kubernetes.io/docs/user-guide/ingress/"" rel=""noreferrer"">ingress</a> controller and define an ingress point for the dashboard.</li>&#xA;</ul>&#xA;"
44782347,44781219,1982514,2017-06-27T14:03:03,<p>You can use Batch Query for this. There is a parameter in yaml file to configure the batch size.&#xA;batch_size_fail_threshold_in_kb : 50 is default.</p>&#xA;
51944204,40737349,985395,2018-08-21T08:01:42,"<p>After looking for almost two years I have found my answer in GraphQL, It allows to be do the aggregation and view selection with minimum fuss.</p>&#xA;&#xA;<p>It is an open source implementation from multiple companies like Apollo, Facebook and is available in multiple languages.</p>&#xA;"
52001247,51999232,6236211,2018-08-24T09:18:42,<p>You server application needs to understand the incoming request type. can you define the Content-Type=application/json and give a try.</p>&#xA;
52003473,51980596,6236211,2018-08-24T11:26:58,"<p>You can use ModelMapper. This library is used to perform this entity-DTO conversion.</p>&#xA;&#xA;<ol>&#xA;<li>Add below maven dependancy : </li>&#xA;</ol>&#xA;&#xA;<blockquote>&#xA;<pre><code>&lt;dependency&gt;&#xA;    &lt;groupId&gt;org.modelmapper&lt;/groupId&gt;&#xA;    &lt;artifactId&gt;modelmapper&lt;/artifactId&gt;&#xA;    &lt;version&gt;0.7.4&lt;/version&gt;&#xA;&lt;/dependency&gt;&#xA;</code></pre>&#xA;</blockquote>&#xA;&#xA;<ol start=""2"">&#xA;<li>Autowire the ModelMapper - </li>&#xA;</ol>&#xA;&#xA;<blockquote>&#xA;<pre><code>@Bean&#xA;public ModelMapper modelMapper() {&#xA;    return new ModelMapper();&#xA;}&#xA;</code></pre>&#xA;</blockquote>&#xA;&#xA;<ol start=""3"">&#xA;<li>Sample Code transforming DTO to entity : </li>&#xA;</ol>&#xA;&#xA;<blockquote>&#xA;<pre><code>private Employee convertToDto(EmployeeDto employeeDto) {&#xA;    Employee employee = modelMapper.map(employeeDto, Employee.class);&#xA;    return employee;&#xA;}&#xA;</code></pre>&#xA;</blockquote>&#xA;"
52014488,51953129,6236211,2018-08-25T06:02:03,<p>Try updating the spring boot version and build again</p>&#xA;
52001670,51993175,6236211,2018-08-24T09:41:31,<p>Vault is deployed on 8200 port. Can you please check if its running before running the inventory service.</p>&#xA;
51611752,51610138,6236211,2018-07-31T10:56:16,"<p><a href=""http://client_host/actuator/refresh"" rel=""nofollow noreferrer"">http://client_host/actuator/refresh</a>"" will only update the static configurations where as /actuator/bus-refresh or /bus/refresh will update all the dynamic configurations and it will trigger all the cloud client to refresh their instances.</p>&#xA;"
51612158,51578263,6236211,2018-07-31T11:18:12,"<p>Spring has deprecated the Zipkin UI and Zipkin server from Spring Boot. You will need to setup external Zipkin server.</p>&#xA;&#xA;<p>You can refer to below URL : </p>&#xA;&#xA;<p><a href=""http://onlyfullstack.blogspot.com/2018/07/setting-up-zipkin-server.html"" rel=""nofollow noreferrer"">http://onlyfullstack.blogspot.com/2018/07/setting-up-zipkin-server.html</a></p>&#xA;"
50451714,50448620,8078899,2018-05-21T15:09:49,"<p>Well, you are right <code>Annotations</code> is like <code>Labels</code>. But I saw that We could customize to config with <code>Annotations</code> for example:</p>&#xA;&#xA;<pre><code>apiVersion: extensions/v1beta1&#xA;kind: Ingress&#xA;metadata:&#xA;  name: cafe-ingress-with-annotations&#xA;  annotations:&#xA;    nginx.org/proxy-connect-timeout: ""30s""&#xA;    nginx.org/proxy-read-timeout: ""20s""&#xA;    nginx.org/client-max-body-size: ""4m""&#xA;spec:&#xA;  rules:&#xA;  - host: cafe.example.com&#xA;    http:&#xA;      paths:&#xA;      - path: /tea&#xA;        backend:&#xA;          serviceName: tea-svc&#xA;          servicePort: 80&#xA;      - path: /coffee&#xA;        backend:&#xA;          serviceName: coffee-svc&#xA;          servicePort: 80&#xA;</code></pre>&#xA;&#xA;<p>Nginx config can customize according to given <code>Annotation</code>.  So how to do this. I couldn't find a tutorial.</p>&#xA;"
34410805,34406896,219187,2015-12-22T07:45:05,<ol>&#xA;<li>Create a library. </li>&#xA;<li>Version the library with semantic versioning and create a package using the package management functionality of your environment (e.g. Nuget if you're on .NET). </li>&#xA;<li>Include the package as dependency in the micro services.</li>&#xA;</ol>&#xA;
33775792,33772776,219187,2015-11-18T09:10:15,"<p>A message bus is typically used in applications to support <strong>asynchronous processing</strong>. A very simple example of this would be sending emails in response to a state change that happened in the application.</p>&#xA;&#xA;<p><strong>In this regard, 100ms is quite fast.</strong></p>&#xA;&#xA;<p>If you're trying to keep synchronous operations in your application fast, you won't get happy with making a message bus part of it.</p>&#xA;&#xA;<p>Note that the above statement refers to external message buses. In-process message delivery mechanisms can be built with much less latency, but this is probably not what you need in the context of a microservices architecture.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Should I be looking at different tools?</p>&#xA;</blockquote>&#xA;&#xA;<p>No, you have appropriate tools for a microservices architecture. But you should ask yourself the following questions:</p>&#xA;&#xA;<ul>&#xA;<li>Is a microservices architecture the right choice for my application?</li>&#xA;<li>If yes, do I have suitable service boundaries?</li>&#xA;</ul>&#xA;"
39722851,39721791,219187,2016-09-27T10:56:33,"<p>What your model currently expresses is </p>&#xA;&#xA;<blockquote>&#xA;  <p>Every employee works for exactly one Company.</p>&#xA;</blockquote>&#xA;&#xA;<p>With that, it's absolutely clear that there must be a Company before you can create an Employee. </p>&#xA;&#xA;<p>I see two different solutions that could fit here: </p>&#xA;&#xA;<ul>&#xA;<li><p>Pass the Company into <code>buildEmployee</code>. This communicates the fact creating an Employee <em>requires</em> a Company best.</p></li>&#xA;<li><p>Pass in the GUID of the company. This may be suitable for cases where you already have this information, but you don't have the whole Company object available (e.g. creating a co-worker for an existing employee).</p></li>&#xA;</ul>&#xA;&#xA;<p>No matter which approach you take, <strong>you should avoid loading data through a repository from within the factory.</strong> It's better to leave this for the calling app service, because the app service might require the Company anyway (e.g. to do some kind of input validation).</p>&#xA;&#xA;<p>Keeping all repository interaction in the app service will make your application more robust and easier to reason about. After all, repository access usually involves network interaction and delay.</p>&#xA;&#xA;<p><strong>So to sum up, there's nothing wrong with having a dependency on Company from <code>buildEmployee</code>, but you should avoid calling the repository.</strong> </p>&#xA;&#xA;<p>If depending on Company still feels wrong, then you should reconsider your model.</p>&#xA;"
39806514,39791667,219187,2016-10-01T12:18:16,"<p><strong>Yes, this is a bad idea.</strong></p>&#xA;&#xA;<p>Your design would result in a circular dependency between the two BCs. As in many other areas of software development, <strong>circular dependencies are almost always a bad idea</strong>.</p>&#xA;&#xA;<p>If your use cases force you to do this, then you should reconsider your context map. Ask yourself the following questions:</p>&#xA;&#xA;<ul>&#xA;<li>Are the two BCs really separate BCs, or should they rather be one? </li>&#xA;<li>Or should a part of one of the BCs that lead to the circular dependency in fact be in a third BC?</li>&#xA;</ul>&#xA;&#xA;<p>Finding answers to these questions <strong>in the context of your domain</strong> will probably lead you to a cleaner design.</p>&#xA;"
33826499,33805449,219187,2015-11-20T12:15:43,"<p><strong>If you don't want to re-invent master-master replication (which will be highly non-trivial), I suggest you choose a DB system that supports this out of the box.</strong></p>&#xA;&#xA;<p>This will not solve all your problems out of the box, but at least it solves the hard part of the problem. What you still will need to do is e.g. defining and implementing a conflict resolution strategy.</p>&#xA;&#xA;<p>A good choice for a master-master DB system is <a href=""http://couchdb.apache.org/"" rel=""nofollow"">CouchDB</a>. It is open source and there are also service providers available, in case you don't want to host the DB by yourself. I'm sure there are other DB systems that provide master-master replication as well.</p>&#xA;"
40305061,40296598,219187,2016-10-28T12:24:57,"<h2>Use some form of caching</h2>&#xA;&#xA;<p>No matter how you twist and turn this, you will need some form of caching. If this is the only place you need to cache data and you don't have a full-blown caching system like Redis in place yet, try to keep it simple.</p>&#xA;&#xA;<p>One simple approach would be to use in-memory caching in the application that does the high frequency computation. Depending on your application this may or may not be an option.</p>&#xA;&#xA;<p>If you go this way, introduce a <strong>proxy</strong> for the data you're loading and make sure you have a clean interface, i.e. don't leak the fact that the data is cached to the client. Then, in the proxy implementation, cache the data appropriately, <strong>e.g. by keeping it for X seconds before asynchronously refreshing the cache</strong>.</p>&#xA;"
33213217,33202053,219187,2015-10-19T11:27:24,"<h3>Micro Service Versioning</h3>&#xA;&#xA;<p>First of all ensure <a href=""http://semver.org/"" rel=""nofollow noreferrer"">Semantic Versioning (SemVer)</a> is strictly followed by the micro services. Not doing this will sooner or later lead to incompatibility problems.</p>&#xA;&#xA;<p>Capture only API changes in that version, don't mix it up with micro service internal versioning (e.g. DB schema versioning for a service that has a DB).</p>&#xA;&#xA;<h3>Product Versioning</h3>&#xA;&#xA;<p>Introduce a version for the product as you already suggested. Following SemVer makes sense here, too, but may need to be loosened to meet marketing needs (e.g. allow to make a major version increment even though SemVer would require only a minor version increment). In extreme cases, use dedicated ""technical versions"" and ""marketing versions"". This is however more complicated for the customers, too.</p>&#xA;&#xA;<p>Also note that you will need to define what the SemVer version means in terms of your application, since the application as a whole has no ""API"".</p>&#xA;&#xA;<h3>Dependency Management</h3>&#xA;&#xA;<p>Now a specific product version is a list of micro services of specific versions. Note that this is essentially dependency management in the same sense as <code>apt</code>, <code>npm</code>, <code>bower</code>, etc implement. How sophisticated your solution needs to be is difficult to say, but I recommend to at least support the notion of ""minimum required versions"". If docker has a built-in mechanism, try to use that one (I don't know docker very well, so I can't tell).</p>&#xA;&#xA;<p>With this, you are e.g. able to specify that your product at version <code>4.8.12</code> requires service A at version <code>1.12.0</code> and service B at <code>3.0.4</code>. </p>&#xA;&#xA;<p>The update mechanism should then follow a strategy that adheres to SemVer. This means that installing a specific product version automatically installs the newest services <em>with the same major version</em>. In the example above, this could e.g. install <code>1.12.2</code> of service A and <code>3.3.0</code> of service B. Providing a mechanism to keep already installed services that meet the dependency requirement may be a good idea, so that users don't get annoyed by the update mechanism.</p>&#xA;"
32232567,26331854,352051,2015-08-26T17:05:29,"<p>Microsoft have released <a href=""https://azure.microsoft.com/en-gb/documentation/articles/service-fabric-overview/"" rel=""noreferrer"">Service Fabric</a> which gives you a platform for Microservices.</p>&#xA;&#xA;<p>It is an Azure offering but will also be available on-premise with <a href=""http://www.microsoft.com/en-us/server-cloud/roadmap/public-preview.aspx"" rel=""noreferrer"">Windows Server 2016</a>.</p>&#xA;&#xA;<p>You can install Service Fabric on a dev machine running VS2015 and try it out using the <a href=""https://azure.microsoft.com/en-gb/documentation/articles/service-fabric-get-started/"" rel=""noreferrer"">SDK</a>. </p>&#xA;"
49019068,49013500,352051,2018-02-27T22:34:19,"<p>If serviceB and serviceC are internal and will never be called from an external client then the Client Credentials Grant would be a good candidate. As the client is also a resource server too.</p>&#xA;&#xA;<p>You could also look at passing the same bearer token between services, providing the SPA (which requests the token initially) obtains consent for all scopes which may be used by the other services and the ""audience"" of the token must allow for all the possible resource servers (services). </p>&#xA;&#xA;<p>I don't think either are best practice and there are tradeoffs with both ways. </p>&#xA;"
51295138,50775206,2847283,2018-07-11T22:44:23,"<p>press F5, choose Node, replace launch.json file to:</p>&#xA;&#xA;<pre><code>{&#xA;    // Use IntelliSense to learn about possible attributes.&#xA;    // Hover to view descriptions of existing attributes.&#xA;    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387&#xA;    ""version"": ""0.2.0"",&#xA;    ""configurations"": [&#xA;        {&#xA;            ""type"": ""node"",&#xA;            ""request"": ""launch"",&#xA;            ""name"": ""Debug Nest Framework"",&#xA;            ""args"": [""${workspaceFolder}/src/main.ts""],&#xA;            ""runtimeArgs"": [""--nolazy"", ""-r"", ""ts-node/register""],&#xA;            ""sourceMaps"": true,&#xA;            ""cwd"": ""${workspaceRoot}"",&#xA;            ""protocol"": ""inspector""&#xA;        }&#xA;    ]&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>So put a breackpoint and hit F5 again. </p>&#xA;"
45569462,45498517,2918963,2017-08-08T13:17:44,"<p>For usecases when you adopt dumb pipes (as described on <a href=""https://martinfowler.com/articles/microservices.html#SmartEndpointsAndDumbPipes"" rel=""nofollow noreferrer"">martinfowler.com</a>) AWS API Gateway is a reasonable option.&#xA;For AWS API Gateway I'd suggest to describe/design your API first with RAML or OpenAPI/Swagger and then import into AWS using <a href=""https://github.com/awslabs/aws-apigateway-importer"" rel=""nofollow noreferrer"">AWS API Importer</a>.</p>&#xA;&#xA;<p>As soon as you plan to move logic in there, such as dynamic routing, detailed monitoring, alerting, etc, I'd suggest considering other approaches, such as:</p>&#xA;&#xA;<ul>&#xA;<li>Apigee</li>&#xA;<li>Mulesoft</li>&#xA;<li>WSO2</li>&#xA;</ul>&#xA;&#xA;<p>You can also host them on an EC2 within your VPC or opt-in for the hosted version. (which does have a significant pricetag in some cases)</p>&#xA;&#xA;<p>For describing APIs you can use RAML (for Mulesoft) or OpenAPI (ex-Swagger, for Apigee and WSO2). You can also convert between them using APIMATIC which enables you to migrate your specification across various API Gateways (even AWS).</p>&#xA;"
43531327,43313424,372019,2017-04-20T23:08:30,"<p>Consul doesn't automatically deregister services.</p>&#xA;&#xA;<p>See <a href=""https://groups.google.com/forum/#!topic/consul-tool/slV5xfWRpEE"" rel=""nofollow noreferrer"">https://groups.google.com/forum/#!topic/consul-tool/slV5xfWRpEE</a> for the hint about the same question. According to that thread you need to either update the config or perform an <a href=""https://www.consul.io/api/agent.html#agent_service_register"" rel=""nofollow noreferrer"">Agent API call</a>. Since the agent is the source of truth, you shouldn't try to update via Catalog API. See <a href=""https://github.com/hashicorp/consul/issues/1188#issuecomment-185977469"" rel=""nofollow noreferrer"">GitHub</a> for details. They also mention at the Google group that you don't necessarily have to deregister services if the node goes down gracefully, but that doesn't seem to be your use case.</p>&#xA;&#xA;<p>Please have a look at <a href=""https://stackoverflow.com/questions/32259323/consul-not-deregistering-zombie-services"">Consul not deregistering zombie services</a> for hints about automating the service de-registration using either the api or tools like <a href=""https://github.com/gliderlabs/registrator"" rel=""nofollow noreferrer"">registrator</a>.</p>&#xA;"
47908821,47908754,1426227,2017-12-20T14:58:06,"<p>You simply cannot have two or more claims with the same name:</p>&#xA;&#xA;<blockquote>&#xA;  <p><a href=""https://tools.ietf.org/html/rfc7519#section-4"" rel=""nofollow noreferrer""><strong>4.  JWT Claims</strong></a></p>&#xA;  &#xA;  <p>The JWT Claims Set represents a JSON object whose members are the&#xA;    claims conveyed by the JWT.  The Claim Names within a JWT Claims Set&#xA;    MUST be unique; JWT parsers MUST either reject JWTs with duplicate&#xA;    Claim Names or use a JSON parser that returns only the lexically last&#xA;    duplicate member name [...]</p>&#xA;</blockquote>&#xA;&#xA;<p>However you can use another claim for the user id:</p>&#xA;&#xA;<pre class=""lang-java prettyprint-override""><code>return Jwts.builder()&#xA;           .setId(UUID.randomUUID().toString())&#xA;           .setSubject(jwtUser.getUsername())&#xA;           .signWith(SignatureAlgorithm.HS512, encodedSecret)&#xA;           .claim(""user-id"", ""id goes here"")&#xA;           .compact();&#xA;</code></pre>&#xA;&#xA;<hr>&#xA;&#xA;<blockquote>&#xA;  <p>Can I add an object instead of a integer? Like JSON?</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes, a claim value can be any arbitrary JSON value, but a claim name must be a string:</p>&#xA;&#xA;<blockquote>&#xA;  <p><a href=""https://tools.ietf.org/html/rfc7519#section-2"" rel=""nofollow noreferrer""><strong>2.  Terminology</strong></a></p>&#xA;  &#xA;  <p>[...]</p>&#xA;  &#xA;  <p>Claim Name<br>&#xA;        The name portion of a claim representation. A Claim Name is&#xA;        always a string.**</p>&#xA;  &#xA;  <p>Claim Value<br>&#xA;        The value portion of a claim representation. A Claim Value can be&#xA;        any JSON value.</p>&#xA;  &#xA;  <p>[...]</p>&#xA;</blockquote>&#xA;&#xA;&#xA;&#xA;<blockquote>&#xA;  <p><a href=""https://tools.ietf.org/html/rfc7519#section-3"" rel=""nofollow noreferrer""><strong>3.  JSON Web Token (JWT) Overview</strong></a></p>&#xA;  &#xA;  <p>JWTs represent a set of claims as a JSON object that is encoded in a&#xA;    JWS and/or JWE structure.  This JSON object is the JWT Claims Set.&#xA;    [...] the JSON object consists of&#xA;    zero or more name/value pairs (or members), where the names are&#xA;    strings and the values are arbitrary JSON values.  These members are&#xA;    the claims represented by the JWT. [...]</p>&#xA;</blockquote>&#xA;&#xA;<p>So you could define a class like:</p>&#xA;&#xA;<pre class=""lang-java prettyprint-override""><code>class UserDetails {&#xA;&#xA;    private Integer id;&#xA;    private List&lt;String&gt; roles;&#xA;&#xA;    public UserDetails(Integer id, String... roles) {&#xA;        this.id = id;&#xA;        this.roles = Arrays.asList(roles);&#xA;    }&#xA;&#xA;    public Integer getId() {&#xA;        return id;&#xA;    }&#xA;&#xA;    public UserDetails setId(Integer id) {&#xA;        this.id = id;&#xA;        return this;&#xA;    }&#xA;&#xA;    public List&lt;String&gt; getRoles() {&#xA;        return roles;&#xA;    }&#xA;&#xA;    public UserDetails setRoles(List&lt;String&gt; roles) {&#xA;        this.roles = roles;&#xA;        return this;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>And then use as follows:</p>&#xA;&#xA;<pre class=""lang-java prettyprint-override""><code>return Jwts.builder()&#xA;           .setId(UUID.randomUUID().toString())&#xA;           .setSubject(jwtUser.getUsername())&#xA;           .signWith(SignatureAlgorithm.HS512, encodedSecret)&#xA;           .claim(""user-details"", new UserDetails(1, ""ROLE_1"", ""ROLE_2""))&#xA;           .compact();&#xA;</code></pre>&#xA;&#xA;<p>It will produce the following payload:</p>&#xA;&#xA;<pre class=""lang-json prettyprint-override""><code>{&#xA;  ""jti"": ""d65b83fd-fafb-4df4-9782-c4700c3c93ff"",&#xA;  ""sub"": ""joe.doe"",&#xA;  ""user-details"": {&#xA;    ""id"": 1,&#xA;    ""roles"": [&#xA;      ""ROLE_1"",&#xA;      ""ROLE_2""&#xA;    ]&#xA;  }&#xA;}&#xA;</code></pre>&#xA;"
47201212,47196334,1426227,2017-11-09T11:54:55,"<p>Resources and providers won't be automatically scanned when using an embedded Tomcat. You must register them manually either in an <a href=""https://docs.oracle.com/javaee/7/api/javax/ws/rs/core/Application.html"" rel=""nofollow noreferrer""><code>Application</code></a> subclass or in the <a href=""http://docs.jboss.org/resteasy/docs/3.1.4.Final/userguide/html/Installation_Configuration.html#configuration_switches"" rel=""nofollow noreferrer""><code>resteasy.resources</code></a> context parameter using comma separated values.</p>&#xA;&#xA;<p>First of all, ensure you have the following dependencies in your project:</p>&#xA;&#xA;<pre class=""lang-xml prettyprint-override""><code>&lt;properties&gt;&#xA;    &lt;tomcat.version&gt;8.5.23&lt;/tomcat.version&gt;&#xA;    &lt;resteasy.version&gt;3.1.4.Final&lt;/resteasy.version&gt;&#xA;    &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;&#xA;    &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;&#xA;    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;&#xA;&lt;/properties&gt;&#xA;&#xA;&lt;dependencies&gt;&#xA;    &lt;dependency&gt;&#xA;        &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt;&#xA;        &lt;artifactId&gt;tomcat-embed-core&lt;/artifactId&gt;&#xA;        &lt;version&gt;${tomcat.version}&lt;/version&gt;&#xA;    &lt;/dependency&gt;&#xA;    &lt;dependency&gt;&#xA;        &lt;groupId&gt;org.jboss.resteasy&lt;/groupId&gt;&#xA;        &lt;artifactId&gt;resteasy-jaxrs&lt;/artifactId&gt;&#xA;        &lt;version&gt;${resteasy.version}&lt;/version&gt;&#xA;    &lt;/dependency&gt;&#xA;&lt;/dependencies&gt;&#xA;</code></pre>&#xA;&#xA;<p>Create a JAX-RS resource class:</p>&#xA;&#xA;<pre class=""lang-java prettyprint-override""><code>@Path(""hello"")&#xA;public class HelloWorldResource {&#xA;&#xA;    @GET&#xA;    @Produces(MediaType.TEXT_PLAIN)&#xA;    public Response helloWorld() {&#xA;        return Response.ok(""Hello World!"").build();&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Create an <a href=""https://docs.oracle.com/javaee/7/api/javax/ws/rs/core/Application.html"" rel=""nofollow noreferrer""><code>Application</code></a> subclass and register the resource class create above:</p>&#xA;&#xA;<pre class=""lang-java prettyprint-override""><code>public class MyApplication extends Application {&#xA;&#xA;    @Override&#xA;    public Set&lt;Class&lt;?&gt;&gt; getClasses() {&#xA;        return Collections.singleton(HelloWorldResource.class);&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Create a class to launch Tomcat and deploy your application:</p>&#xA;&#xA;<pre class=""lang-java prettyprint-override""><code>public class Launcher {&#xA;&#xA;    private static final String RESTEASY_SERVLET_NAME = ""resteasy-servlet"";&#xA;&#xA;    public static void main(String[] args) throws Exception {&#xA;        new Launcher().start();&#xA;    }&#xA;&#xA;    void start() throws Exception {&#xA;&#xA;        String port = System.getenv(""PORT"");&#xA;        if (port == null || port.isEmpty()) {&#xA;            port = ""8080"";&#xA;        }&#xA;&#xA;        String contextPath = ""/api"";&#xA;        String appBase = ""."";&#xA;&#xA;        Tomcat tomcat = new Tomcat();&#xA;        tomcat.setPort(Integer.valueOf(port));&#xA;        tomcat.getHost().setAppBase(appBase);&#xA;&#xA;        Context context = tomcat.addContext(contextPath, appBase);&#xA;        context.addApplicationListener(ResteasyBootstrap.class.getName());&#xA;        Tomcat.addServlet(context, RESTEASY_SERVLET_NAME, new HttpServletDispatcher());&#xA;        context.addParameter(""javax.ws.rs.Application"", MyApplication.class.getName());&#xA;        context.addServletMappingDecoded(""/*"", RESTEASY_SERVLET_NAME);&#xA;&#xA;        tomcat.start();&#xA;        tomcat.getServer().await();&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Finally add the Maven Shade plugin to create an executable JAR, where the <code>mainClass</code> attribute references the above create class:</p>&#xA;&#xA;<pre class=""lang-xml prettyprint-override""><code>&lt;build&gt;&#xA;    &lt;plugins&gt;&#xA;        &lt;plugin&gt;&#xA;            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;&#xA;            &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;&#xA;            &lt;version&gt;2.4.3&lt;/version&gt;&#xA;            &lt;configuration&gt;&#xA;                &lt;finalName&gt;tomcat-embedded-example-${project.version}&lt;/finalName&gt;&#xA;            &lt;/configuration&gt;&#xA;            &lt;executions&gt;&#xA;                &lt;execution&gt;&#xA;                    &lt;phase&gt;package&lt;/phase&gt;&#xA;                    &lt;goals&gt;&#xA;                        &lt;goal&gt;shade&lt;/goal&gt;&#xA;                    &lt;/goals&gt;&#xA;                    &lt;configuration&gt;&#xA;                        &lt;transformers&gt;&#xA;                            &lt;transformer implementation=""org.apache.maven.plugins.shade.resource.ManifestResourceTransformer""&gt;&#xA;                                &lt;mainClass&gt;com.example.Launcher&lt;/mainClass&gt;&#xA;                            &lt;/transformer&gt;&#xA;                        &lt;/transformers&gt;&#xA;                    &lt;/configuration&gt;&#xA;                &lt;/execution&gt;&#xA;            &lt;/executions&gt;&#xA;        &lt;/plugin&gt;&#xA;    &lt;/plugins&gt;&#xA;&lt;/build&gt;&#xA;</code></pre>&#xA;&#xA;<p>To compile and run the application, follow these steps:</p>&#xA;&#xA;<ul>&#xA;<li>Open a command line window or terminal.</li>&#xA;<li>Navigate to the root directory of the project, where the <code>pom.xml</code> resides.</li>&#xA;<li>Compile the project: <code>mvn clean compile</code>.</li>&#xA;<li>Package the application: <code>mvn package</code>.</li>&#xA;<li>Look in the target directory. You should see a file with the following or a similar name: <code>tomcat-embedded-sample-1.0-SNAPSHOT.jar</code>.</li>&#xA;<li>Change into the target directory.</li>&#xA;<li>Execute the JAR: <code>java -jar tomcat-embedded-example-1.0-SNAPSHOT.jar</code>.</li>&#xA;<li>The application  is available at <code>http://localhost:8080/api/hello</code>.</li>&#xA;</ul>&#xA;&#xA;<hr>&#xA;&#xA;<p>See more:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://stackoverflow.com/a/39344449/1426227"">Jersey with embedded Tomcat</a></li>&#xA;<li><a href=""https://stackoverflow.com/a/39347148/1426227"">Jersey with embedded Jetty</a></li>&#xA;</ul>&#xA;"
51299100,40737349,8374067,2018-07-12T06:56:58,"<p>As suggested <a href=""https://www.globallogic.com/gl_news/microservice-architecture-api-gateway-considerations/#orchestration"" rel=""nofollow noreferrer"">here</a>, service composition/orchestration isnt suggested to be implemented at gateway layer but a separate hand coded facade layer is preferred.</p>&#xA;&#xA;<p>The reason behind this is scalability. Whenever a service needs to be scaled, API gateway also will have to be scaled unnecessarily. </p>&#xA;&#xA;<p>So the options here are :</p>&#xA;&#xA;<ol>&#xA;<li>Hand coded facade layer</li>&#xA;<li>ESB api for orchestration. Gateway will call ESB api which in turn orchestrate. </li>&#xA;</ol>&#xA;"
40483258,40469391,184883,2016-11-08T09:28:29,"<p>To test if a click handler was added correctly pass a spy into your comment, simulate the click and check that the spy was called. Doing this there is no need to use <code>waitsFor</code>.</p>&#xA;&#xA;<pre><code>import React from 'react';&#xA;import GenericButton from 'components/buttons/GenericButton';&#xA;import { shallow } from 'enzyme';&#xA;import { shallowToJson } from 'enzyme-to-json';&#xA;&#xA;describe('Generic Button', () = &gt; {&#xA;  test('Button action called when clicked', () = &gt; {&#xA;    const action = jest.fn();&#xA;    const component = shallow(&#xA;      &lt;GenericButton action={action}&#xA;            id=""testComponent""/&gt;&#xA;    );&#xA;    component.find('Button').first().simulate('click');&#xA;    expect(action).toHaveBeenCalled();&#xA;  });&#xA;});&#xA;</code></pre>&#xA;"
34598179,34534079,2217011,2016-01-04T19:14:03,<p>Mapping localtime and timezone works perfectly.</p>&#xA;&#xA;<p>Example:</p>&#xA;&#xA;<blockquote>&#xA;  <p>docker run -d -v /var/lib/elasticsearch:/var/lib/elasticsearch -v&#xA;  /etc/localtime:/etc/localtime:ro -v&#xA;  /usr/share/zoneinfo/America/Buenos_Aires:/etc/timezone:ro -p 80:80/tcp&#xA;  -p 9200:9200/tcp -p 514:514/udp petergrace/elk</p>&#xA;</blockquote>&#xA;
51434516,51191291,185907,2018-07-20T03:11:51,"<p>Both technologies provide service mesh communication, and developed with  plugin architecture, the list includes Prometheus,Zipkin etc. </p>&#xA;&#xA;<p>As far as features are concerned, they will be at feature parity if not now, but at some point in the future. Especially with Kubernetes etc, you can always replace one with other without affecting your services much. </p>&#xA;&#xA;<p>I would go with Istio if your services are based on Kubernetes as both are backed by Google, and expect them to work much better compared to Conduit going forward.</p>&#xA;&#xA;<p>Last I read Istio will be supported on Docker Swarm as well, so it all depends where you are running your services and which mesh framework is most suited for it.</p>&#xA;"
51434734,48882829,185907,2018-07-20T03:40:10,"<p>It all depends where your end users are, in case of Netflix, they have differnt types of clients, web/mobile/streaming sticks/bluray players/what not, while web (updated to latest all the time), mobile (updated to latest eventually), bluray player with pre-installed app for example may never get updated. </p>&#xA;&#xA;<p>And you have to version your apis accordingly for each platform and maintain them based on client update cycle for backward compatibility. If you have too many variations in a single api it will be hard to maintain instead it is easier to write an api for each type of client. Unless you have real need for #3 and have enough resources to develop for each type of client I wouldn't jump into it, as you have to maintain many variations of api for the same purpose. </p>&#xA;&#xA;<p>I would start small with #1. </p>&#xA;"
51434938,51408476,185907,2018-07-20T04:09:05,"<p>If you look into ""12 factor app with micro-services"" you would get details about best practices.</p>&#xA;&#xA;<p>If you have your code in a single repo, with CI/CD tools a single checkin any where will deploy all services for backend system and UI on frontend at the same time, this may be ok at first but it will increase deployment times as you add more services, you may have to write some custom code to detect changes specific to UI/Services if you want to avoid this behavior, but you get the advantage of code sharing. </p>&#xA;&#xA;<p>You can code share even with multiple repositories if you keep your JSON structure in a common repository and share it on both client and server side. </p>&#xA;&#xA;<p>This all gets hard to manage and deploy once you add more types of clients for example, iOS/Android and as each team wants to own their own deploy and release cycles. </p>&#xA;&#xA;<p>At the same time you can start with single repository for simplicity, once you start seeing any of these issues, you can move them out into individual repositories.</p>&#xA;"
51502169,51425111,185907,2018-07-24T15:14:35,"<p>There is a known pattern, CQRS (Command Query Responsibility Segregation), you would find lot of examples on how to implement it through events, called EventSourcing. You are not required to use EventSourcing, you can just publish to a Queue and subscribe back to handle the request, you can scale your architecture easily also</p>&#xA;&#xA;<p>In C#, there is a event sourcing framework, <a href=""https://github.com/gautema/CQRSlite"" rel=""nofollow noreferrer"">https://github.com/gautema/CQRSlite</a> and a nice explanation on how to use from Sacha, <a href=""https://www.codeproject.com/articles/991648/cqrs-a-cross-examination-of-how-it-works"" rel=""nofollow noreferrer"">https://www.codeproject.com/articles/991648/cqrs-a-cross-examination-of-how-it-works</a>&#xA; And also you can find more details here, <a href=""https://www.youtube.com/watch?v=t2AI9hODJ2E&amp;t=1247s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=t2AI9hODJ2E&amp;t=1247s</a></p>&#xA;&#xA;<p>Event Sourcing gets little complicated, unless you really have a need for it, Martin Fowler has few good articles about pros and cons of event sourcing, <a href=""https://martinfowler.com/eaaDev/EventSourcing.html"" rel=""nofollow noreferrer"">https://martinfowler.com/eaaDev/EventSourcing.html</a></p>&#xA;"
51502278,51498267,185907,2018-07-24T15:19:23,"<p>From authentication service, you would return a JWT token based on the user role and permissions associated to it. </p>&#xA;&#xA;<p>On the client side, you would parse the JWT Token and based on the permission, you will only show the screens that are applicable to the user, and on the server side, you would read access rights in each REST call, and through unauthorized error if user tries to access a service without right access rights</p>&#xA;"
51775241,49612709,185907,2018-08-09T20:24:09,"<p>It depends on how complex the permission setup is. </p>&#xA;&#xA;<p>If your permissions are super simple, i.e, all services need authentication and have a single a role to work with, you can add at gateway api. </p>&#xA;&#xA;<p>If you need more granular approach you need to do it at the module level. Typically in a micro-service architecture, each service would invoke another service, and having the authorization settings at module level avoids up front knowledge of required permissions needed at gateway api. And you can deploy each of these modules without much thought about other services,  have granular permissions at method level etc.</p>&#xA;"
51861722,51861095,185907,2018-08-15T15:25:46,"<p>You should have asked is ""Kafka better suited for CQRS development"".  </p>&#xA;&#xA;<p>Unlike typical queues, in Kafka you can save messages based on 'time period, number of items' etc. If you store all the events in Kafka from beginning you can use it for CQRS. </p>&#xA;&#xA;<p>Unlike in other queues where messages are deleted as soon as they are consumed kafka can keep them forever instead.</p>&#xA;"
51864173,51827636,185907,2018-08-15T18:11:23,"<p>If you are building microservice, the best way to go through is dockerizing the application and hosting in Kubernetes or something similar which would provide load balanced service url's.</p>&#xA;"
51545224,44704629,185907,2018-07-26T18:16:39,"<blockquote>&#xA;  <p>I know that if I setup the end microservice correctly, the same token&#xA;  used in the proxy api can be used to authorize the request at the end&#xA;  microservice. But how do I pass it? Do I grab the token from the&#xA;  request in the Proxy API and pass it down to the end microservice just&#xA;  like that? is it a good practice to do this?</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes, it is very common to pass the JWT (or any) to pass around, proxy --> service --> proxy --> service. </p>&#xA;&#xA;<p>And each layer can augment the token with additional details like UniqueId (for example when a request hits the first for the first time to track the chain of interactions, circuit breakers etc)</p>&#xA;&#xA;<p>If you are application consists of multiple languages (frameworks), this approach really helps as you don't need to reimplement the authentication in each language and let proxy handle it, this especially useful with container architecture, just make sure that you leave proxy as light weight as possible, you can look into ideas based on Lyft's Envoy proxy. </p>&#xA;"
51545260,39228311,185907,2018-07-26T18:19:20,"<p>Not really sure whats the purpose behind securely storing html files, instead you should serve the pages and validate the REST calls based on User's Authorization settings.</p>&#xA;"
51545361,46920685,185907,2018-07-26T18:26:07,"<p>Not really sure about Netflix Zuul, but we liked the approach presented by Istio (backed by Google, etc) which works really well with Containers (Kubernetes) and you get the support for canary releases <a href=""https://istio.io/blog/2017/0.1-canary/"" rel=""nofollow noreferrer"">https://istio.io/blog/2017/0.1-canary/</a></p>&#xA;"
51545456,46404449,185907,2018-07-26T18:32:03,"<p>Yes, you could develop each module into its own application and release independently, you just need to make sure that you have a way (from menu for example) to go from one application to another application, if you host your all your applications on the same port, you will have a single token that can be shared by all applications, and you can seem less move user from one application to another as in portal experience. </p>&#xA;&#xA;<p>You need to manage your state properly, if you are state baked into Session Storage (Master Data), you can get the experience you are looking for. </p>&#xA;&#xA;<p>If you host your applications, with different URLs, you will not be able to share the token, instead you would have to use an approach how Google authentications works at Gmail, youtube etc. </p>&#xA;"
51733885,51733502,185907,2018-08-07T19:13:07,"<p>For such type of requests, you can handle through Optimistic Concurrency where you would have a column on the database (Timestamp) as a reference to the version number. </p>&#xA;&#xA;<p>Each time when a change is commited it would modify the timestamp value.</p>&#xA;&#xA;<p>If two requests try to commit the change at the same time, only one of them will succeed as the version (Timestamp) column will change by then negating other request from comitting its changes. </p>&#xA;"
50613186,48873507,185907,2018-05-30T20:52:28,"<p>In my opinion, You could always cache data that is static, are expires over a period of time and respond in application agnostic fashion.</p>&#xA;&#xA;<p>Not sure what you meant by caching service descriptions,  You could cache service descriptions as long as they don't change, i.e, if your services are running in Kubernetes, you could just redirect to the internal service, no need to cache any here. </p>&#xA;"
50613356,48370641,185907,2018-05-30T21:06:10,"<p>REST is universally supported, its accepted on all languages, if all your backend services are running in the same location you may not see much different in throughput.</p>&#xA;&#xA;<p>Anyhow its a good idea to invest and learn gRPC or Thrift as mentioned above, you could convert all your internal services to gRPC for example (supported on most languages) and do a conversion between gRPC and REST at your Universal Gateway. </p>&#xA;&#xA;<p>There are two approaches to internal architecture,  CQRS vs Plain REST calls. The eventing/messaging is an example of CQRS, where you can separate read/write side operations and all write side operations can be implements through eventing for better scalability and throughput. </p>&#xA;"
44363962,44360649,1369651,2017-06-05T07:35:20,"<p>As always everything depends on the specifics of the domain but as a general principle cross domain validation should be done via the read model. </p>&#xA;&#xA;<p>In this case, I would maintain a read model within each microservice for use in validation. Of course, that brings with it the question of eventual consistency. &#xA;How you handle that should come from your understanding of the domain. Factors such as the length of the eventual consistency compared to the frequency of updates should be considered. The cost of getting it wrong for the business compared to the cost of development to minimise the problem. In many cases, just recording the fact there has been a problem is more than adequate for the business.</p>&#xA;&#xA;<p>I have a blog post dedicated to validation which you can find here: <a href=""http://danielwhittaker.me/2016/04/20/how-to-validate-commands-in-a-cqrs-application/"" rel=""nofollow noreferrer"">How To Validate Commands in a CQRS Application</a></p>&#xA;"
44152915,44148076,1369651,2017-05-24T08:24:26,"<p>I've written a post on this subject a while ago, you can find it here: <a href=""http://danielwhittaker.me/2015/03/31/how-to-send-emails-the-right-way-in-a-cqrs-system/"" rel=""nofollow noreferrer"">How to Send Emails the Right Way in a CQRS System</a> </p>&#xA;&#xA;<p>The short version is that I would use a process manager. A process manager listens to events and can issue commands as a result of these events. Just make sure you have a mechanism to not re-send emails if you ever re-run your events.</p>&#xA;&#xA;<p>Regarding the UI. I have another post dealing with this question. You can find it here: <a href=""http://danielwhittaker.me/2014/10/27/4-ways-handle-eventual-consistency-ui/"" rel=""nofollow noreferrer"">4 Ways to Handle Eventual Consistency on the UI</a></p>&#xA;&#xA;<p>Here is a short answer. How often do you think once the code is run to send the email, that the email fails to send? Assuming you have a reasonably robust system, I would hope the vast majority of the time it would work. So fake it. And only if there is a problem find some way to notify the user and or admin users. If you want to get fancy you can use things like Signalr or some pub-sub framework for sending messages to the UI.</p>&#xA;&#xA;<p>Anyway - hope that helps.</p>&#xA;"
46034482,46031939,1369651,2017-09-04T09:51:21,"<p>Great question Nick. The concept you are missing is 'Projections'. When an event is persisted you then broadcast the event. You projection code listens for specific events and then do things like update and create a 'read model'. The read model is a version of the end state (usually persisted but can be done in memory). </p>&#xA;&#xA;<p>The nice thing is that you can highly optimise these read models for reading. Say goodbye to complicated and inefficient joins etc. </p>&#xA;&#xA;<p>Becuase the read model is not the source of truth and it is designed specifically for reading, it is ok to have data duplication in it. Just make sure you manage it when appropriate events are received. </p>&#xA;&#xA;<p>For more info check out these articles:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""http://danielwhittaker.me/2014/10/02/cqrs-step-step-guide-flow-typical-application/"" rel=""noreferrer"">Overview of a Typical CQRS and ES Application</a> **</li>&#xA;<li><a href=""http://danielwhittaker.me/2014/10/05/build-master-details-view-using-cqrs-event-sourcing/"" rel=""noreferrer"">How to Build a Master Details View when using CQRS and Event Sourcing</a></li>&#xA;<li><a href=""http://danielwhittaker.me/2014/09/29/handling-concurrency-issues-cqrs-event-sourced-system/"" rel=""noreferrer"">Handling Concurrency Conflicts in a CQRS and Event Sourced system</a></li>&#xA;</ul>&#xA;&#xA;<p>Hope you find these useful.</p>&#xA;&#xA;<p>** The diagram refers to denormalisation where it should be talking about projections.</p>&#xA;"
35758794,35361819,4651827,2016-03-02T21:50:16,"<p>[Update] Microsoft just released the new Service Fabric SDK (1.5.175), with support for Azure Scale Sets.   Ref: <a href=""https://blogs.msdn.microsoft.com/azureservicefabric/2016/02/23/service-fabric-sdk-v1-5-175-and-the-adoption-of-virtual-machine-scale-sets/"" rel=""nofollow"">https://blogs.msdn.microsoft.com/azureservicefabric/2016/02/23/service-fabric-sdk-v1-5-175-and-the-adoption-of-virtual-machine-scale-sets/</a> </p>&#xA;"
48984966,48984289,5635635,2018-02-26T09:16:36,"<p>As you wrote you should use a load balancer placed in front of your services. Now, you should create a docker network without exposing ports. the only container that exposes ports should be the nginx container in order to handle all clients request. The <code>test-api</code>, <code>search</code> and <code>nginx</code> should be part of the same docker network in order to allow nginx to dispatch the request to the right container. Your <code>docker-compose</code> file should look like this:</p>&#xA;&#xA;<pre><code>    version: '3'&#xA;&#xA;    services:&#xA;      loadbalancer:&#xA;        image: nginx&#xA;      ports:&#xA;       - ""80:8080""&#xA;      networks:&#xA;        - my_netowrk&#xA;      test-api:&#xA;        volumes:&#xA;          - ./test-api:/test-api&#xA;        build: test-api&#xA;        networks:&#xA;         - my_netowrk&#xA;&#xA;      redis:&#xA;        image: ""redis:alpine""&#xA;        networks:&#xA;          - my_netowrk&#xA;&#xA;      search:&#xA;        volumes:&#xA;          - ./seach:/search&#xA;        environment:&#xA;          - HTTP_PORT=5000&#xA;          - REDIS_URL=redis://redis:6379/0&#xA;        build: search&#xA;        networks:&#xA;         - my_netowrk&#xA;&#xA;   networks:&#xA;     my_netowrk:&#xA;       driver: &lt;driver&gt;&#xA;</code></pre>&#xA;&#xA;<p>I would advise you to don't use <code>links</code> anymore, they are old and deprecated.&#xA;You can learn more about <code>docker networks</code> from links below:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://docs.docker.com/network/"" rel=""nofollow noreferrer"">https://docs.docker.com/network/</a></li>&#xA;<li><a href=""https://docs.docker.com/compose/networking/"" rel=""nofollow noreferrer"">https://docs.docker.com/compose/networking/</a></li>&#xA;<li><a href=""https://docs.docker.com/compose/compose-file/#network-configuration-reference"" rel=""nofollow noreferrer"">https://docs.docker.com/compose/compose-file/#network-configuration-reference</a></li>&#xA;</ul>&#xA;"
26272901,25812816,3280180,2014-10-09T07:42:07,"<p>In Java:</p>&#xA;&#xA;<pre><code>// In case vm shutdown&#xA;Runtime.getRuntime().addShutdownHook(new Thread() {&#xA;    @Override&#xA;    public void run()&#xA;    {&#xA;        // what should be closed if forced shudown&#xA;        // ....&#xA;&#xA;        LOG.info(String.format(""--- End of ShutDownHook (%s) ---"", ""APPLICATION_NAME""));&#xA;    }&#xA;});&#xA;</code></pre>&#xA;&#xA;<p>Build your own strategy, how to shutdown your app.</p>&#xA;"
30064597,30000824,3945268,2015-05-05T22:55:17,"<p>I'm not sure I entirely understand the question. Are you asking how to use RabbitMQ with Kubernetes? Or how to set up a RabbitMQ cluster: <a href=""https://www.rabbitmq.com/clustering.html"" rel=""nofollow"">https://www.rabbitmq.com/clustering.html</a>? Or how rolling updates interact with RabbitMQ? Or something else?</p>&#xA;&#xA;<p>I think you should be able to create one service and one replication controller per server, and then use the service DNS names in the cluster configuration file. This is the current approach used to run Zookeeper, also. We have a long-standing TODO to make this less verbose (<a href=""https://github.com/GoogleCloudPlatform/kubernetes/issues/260"" rel=""nofollow"">https://github.com/GoogleCloudPlatform/kubernetes/issues/260</a>), but the current approach should be straightforward. You do lose the ability to use a single kubectl rolling-update command to update the cluster, but it's also straightforward to update the instances individually.</p>&#xA;"
33712542,33712470,608820,2015-11-14T19:42:03,"<p>You can pass some query parameters as filters (this is the most common way I've seen). For example</p>&#xA;&#xA;<pre><code>/orders?customerId=123&#xA;</code></pre>&#xA;&#xA;<p>I think that's quite clear, that you want to retrieve all customer orders filtered by customer id. In the same way you can add pagination or other filters.</p>&#xA;&#xA;<p>The important thing to remember is that you want the <code>order</code> resource, so the URL should remain the same. I'm mentioning this, because this has been the most difficult thing for me to change... to think about <strong>resources</strong> rather than remote calls.</p>&#xA;"
44966711,44965110,1463860,2017-07-07T09:01:32,"<p>I think you mix to match await oprators with promises, when you can use only await.</p>&#xA;&#xA;<p>I create simplefied version of your code:</p>&#xA;&#xA;<p>yag-model.js</p>&#xA;&#xA;<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">&#xD;&#xA;<div class=""snippet-code"">&#xD;&#xA;<pre class=""snippet-code-js lang-js prettyprint-override""><code>const request = require('request-promise');&#xD;&#xA;&#xD;&#xA;module.exports.bp = async function getBP () {&#xD;&#xA;&#xD;&#xA;    const options = {&#xD;&#xA;&#xD;&#xA;        uri: `https://api.postcodes.io/random/postcodes`,&#xD;&#xA;        json: true,&#xD;&#xA;        resolveWithFullResponse: true,&#xD;&#xA;        method: 'GET'&#xD;&#xA;    };&#xD;&#xA;&#xD;&#xA;    return request(options).then((response) =&gt; {&#xD;&#xA;&#xD;&#xA;        return response.body&#xD;&#xA;&#xD;&#xA;    }).catch((err) =&gt; {&#xD;&#xA;        console.log(err);&#xD;&#xA;        console.log('errorstatuscode:' + err.statusCode)&#xD;&#xA;    })&#xD;&#xA;};</code></pre>&#xD;&#xA;</div>&#xD;&#xA;</div>&#xD;&#xA;</p>&#xA;&#xA;<p>and usgae in sample <code>bf.js</code></p>&#xA;&#xA;<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">&#xD;&#xA;<div class=""snippet-code"">&#xD;&#xA;<pre class=""snippet-code-js lang-js prettyprint-override""><code>const yagmodel = require('./yag-model');&#xD;&#xA;&#xD;&#xA;async function getAll(){&#xD;&#xA;    const result = await yagmodel.bp();&#xD;&#xA;    console.log(result);&#xD;&#xA;};&#xD;&#xA;&#xD;&#xA;getAll();</code></pre>&#xD;&#xA;</div>&#xD;&#xA;</div>&#xD;&#xA;</p>&#xA;&#xA;<p>And the result is the response on my console.</p>&#xA;&#xA;<pre><code>F:\Projekty\Learn\lear-node&gt;node bf&#xA;{ status: 200,&#xA;result:&#xA; { postcode: 'BH23 5DA',&#xA;   quality: 1,&#xA;   eastings: 420912,&#xA;</code></pre>&#xA;&#xA;<p>I recommend to look on this great resource about asunc functions <a href=""http://exploringjs.com/es2016-es2017/ch_async-functions.html"" rel=""nofollow noreferrer"">http://exploringjs.com/es2016-es2017/ch_async-functions.html</a> from Dr. Axel Rauschmayer</p>&#xA;"
51233220,51189616,116020,2018-07-08T15:00:46,"<p>The first approach is correct, but you should handle empty option on the read side properly</p>&#xA;"
47930838,47915158,181556,2017-12-21T18:36:46,"<p>So far API Management has no specific support for Kubernetes. But it does support calling any HTTP endpoint, so if there is a single HTTP endpoint with proxy/load-balancer that takes nodes and scaling into account before choosing to which node to forward request that would work fine. So despite that this article suggests using NodePort service, I'd give it a shot with LoadBalancer or ExternalName. Those should provide you with a stable endpoint to call and will handle all node specific routing tasks for you.</p>&#xA;"
44667817,44067979,4247860,2017-06-21T05:52:54,"<p>I found solution to my problem, sharing it for a reference.&#xA;Just need to use the addons on Heroku and use the addons config details in any of my config file which can be easily managed by the config server through git.</p>&#xA;&#xA;<p>For an instance, if you are having a addon of Postgres, you would get configuration from Heroku for it(url, username, password etc). Use this information in the configuration file which is being managed by config server.</p>&#xA;"
43495630,43446605,4247860,2017-04-19T12:21:52,"<p>Try either using an API gateway or a proxy. You can use Zuul for a proxy. Please go through <a href=""https://spring.io/guides/gs/routing-and-filtering/"" rel=""nofollow noreferrer"">Zuul starter</a>.</p>&#xA;&#xA;<p>You can even do some more interesting things by having a proxy. Like:</p>&#xA;&#xA;<ol>&#xA;<li>Implementing Security: Implement Validation &amp; Verification as security check over the proxy and can avoid the same over other microservices.</li>&#xA;<li>Response Handling: You can alter a generic response from your microservices in proxy for the client(Web/Mobile Browser/Mobile App)</li>&#xA;</ol>&#xA;&#xA;<p>Hope this helps.</p>&#xA;"
43515759,43426699,4247860,2017-04-20T09:38:48,"<p>Microservices offers <strong>decoupling</strong>. You must <strong>break down your application</strong> into independent domains. Each domain can have a DB. In case other MS needs to access data owned by some other microservices, they have to communicate over the network.</p>&#xA;&#xA;<p>In case you feel that there are too many dependent services and the network calls would be too much, then you can define a domain, clustering the dependent services together.</p>&#xA;&#xA;<p>For instance -- Suppose I have an online Test evaluation Service where a manager of a company can post tests and he can view results of all the employees in his department.</p>&#xA;&#xA;<p>My Microservices for this scenario would be:</p>&#xA;&#xA;<p><strong>Initial Design</strong> </p>&#xA;&#xA;<ol>&#xA;<li>User Service: For login and user information.</li>&#xA;<li>Test Service: Service to evaluate tests.</li>&#xA;<li>Employee: Handles employee details</li>&#xA;<li>Company: Handles organization CRUD</li>&#xA;<li>Department: Handles department CRUD</li>&#xA;</ol>&#xA;&#xA;<p>After breaking it down, seems like employee, Organization and Department service would be making too much network/API calls as they are tightly dependent on each other. So it's better to cluster them.</p>&#xA;&#xA;<p><strong>Updated design</strong></p>&#xA;&#xA;<ol>&#xA;<li>User Service : For login and user information.</li>&#xA;<li>Test Service : Service to evaluate tests</li>&#xA;<li>Organization : Handles Company, Employee and Department related operations.</li>&#xA;</ol>&#xA;&#xA;<p>Each service could have it's own DB and it's independently deployable. User and Test Service can use mongoDB or any NoSql DB and Organization service can use RDBMS.</p>&#xA;&#xA;<p>Hope this helps.</p>&#xA;"
43518981,43378986,4247860,2017-04-20T12:02:26,"<p>Please check JWT. OAuth needs another authentication server to Authenticate &amp; Authorize users. JWT is best suited way for microservices architecture. </p>&#xA;&#xA;<p>You just need to <strong>verify the JWT token's signature</strong>. Check whether it's signed by any of your microservice or not to Authenticate the User(it's simple).</p>&#xA;&#xA;<p>For implementing Authorization, you can add role level information in your JWT token when you are generating the token. </p>&#xA;&#xA;<p>Such that, UI would always access your services with the token and you would always get this information in your JWT token in all the microservices. Make use of Spring security to provide access of certain resources based on the Role.</p>&#xA;&#xA;<p>Please check out this <a href=""https://medium.facilelogin.com/securing-microservices-with-oauth-2-0-jwt-and-xacml-d03770a9a838"" rel=""nofollow noreferrer"">blog</a> for more information on securing your microservices.</p>&#xA;&#xA;<p>Hope this helps.</p>&#xA;"
45077929,45072041,4247860,2017-07-13T10:25:22,"<ol>&#xA;<li><p>You really don't have to forward the request to Auth-server to validate the JWT token. A <strong>JWT token is like a bill note</strong>, once it's signed it can be validated by anyone who is sharing the key.</p>&#xA;&#xA;<p>I would recommend you to <strong>have an edge service</strong> in front of all your API-services. The edge service either shares the key by which JWT token is signed by Auth service or has the public key to verify the signature. </p>&#xA;&#xA;<p>Once the signature is verified, the edge service can extract the required information from the token and add it to request header. Your downstream services can consume this information according to their need. </p></li>&#xA;<li><p>You can use Https to enforce that your request isn't intercepted by anyone over the network. In case, even if someone tries to mess up with the JWT token, the signature won't match and you can detect that. Please go through <a href=""https://stackoverflow.com/questions/44096281/jwt-kong-cannot-create-jwts-with-a-shared-secret/44368412#44368412"">JWT/KONG: Cannot create JWTs with a shared secret</a> to know more about creating-parsing the JWT token with public-private keys.</p></li>&#xA;</ol>&#xA;"
45047143,45047011,4247860,2017-07-12T02:04:21,"<p>This is an interesting problem. The best solution for this could be <strong>Reactive Spring Boot</strong>. You can have your Extract service to be as a Reactive Spring Boot app and instead of sending GBs of data, stream the data to the required service.</p>&#xA;&#xA;<p>Now you might be wondering that while streaming, it might hold on the working thread. The answer is NO. IT works at the OS level. <strong>It doesn't hold up any request thread to stream the results</strong>. That's the beauty of the Reactive Spring Boot.</p>&#xA;&#xA;<p>Go through this and explore<br>&#xA;<a href=""https://spring.io/blog/2016/07/28/reactive-programming-with-spring-5-0-m1"" rel=""nofollow noreferrer"">https://spring.io/blog/2016/07/28/reactive-programming-with-spring-5-0-m1</a></p>&#xA;"
45015799,44988481,4247860,2017-07-10T15:18:01,"<p>It would be way better to <strong>use the same DB</strong> in auth &amp; user service. Auth service just needs to access the credential. </p>&#xA;&#xA;<p>You can even have a security layer at User service to implement the access control over the URI according to the role. </p>&#xA;&#xA;<p>You need something like this:</p>&#xA;&#xA;<ol>&#xA;<li>Define a prefilter in edge service</li>&#xA;<li>Edge service gets authentication request and forwards it to authentication service.</li>&#xA;<li>Auth service uses the same DB as of User service to authenticate the user and generates JWT token.</li>&#xA;<li>All other requests would have the token in the request header.</li>&#xA;<li>Edge service shares the key by which JWT token was created or has a public key to verify the signature.</li>&#xA;<li>Edge service verifies the token in its prefilter and adds user info in the request header in the plain. Redirects request to required service.</li>&#xA;<li>Service takes the request and in its security filter, it uses user information from header to enforce the access control over resources.</li>&#xA;</ol>&#xA;"
45014749,44998425,4247860,2017-07-10T14:29:10,"<p>If you are trying to have 3 different microservices then you can use Zuul proxy.</p>&#xA;&#xA;<p>Have the <strong>Zuul as the API gateway</strong> for your services. It act as a gate keeper for all your services. You can define your custom zuul filters like pre, post etc. </p>&#xA;&#xA;<p>What I would recommend according to your specification to <strong>use zuul pre-filter</strong> and before routing the request to Subscription service, request other two services to get desired inputs. </p>&#xA;&#xA;<p>Here is a link for reference:<br>&#xA;<a href=""https://spring.io/guides/gs/routing-and-filtering/"" rel=""nofollow noreferrer"">https://spring.io/guides/gs/routing-and-filtering/</a></p>&#xA;&#xA;<p>Use this config to route the request:</p>&#xA;&#xA;<pre><code>zuul:&#xA; prefix: /api/v1&#xA; stripPrefix: false&#xA;proxy:&#xA; stripMapping: false&#xA; mapping: /api/v1&#xA; addProxyHeaders: true&#xA;routes:&#xA;Business-API:&#xA; path: /business/**&#xA; url: http://localhost:8213&#xA; stripPrefix: false&#xA;user-service:&#xA;path: /user/**&#xA;url: http://localhost:8212&#xA;stripPrefix: false&#xA;</code></pre>&#xA;&#xA;<p>Regarding the request payload, you can have whatever payload you need to have. Just keep in mind that all the services must be independently deployable. That means if you are deleting your business, you need to decide what you have to do with your subscription. Cascading could lead you into problems. It's better to invest more time and think through over the domain. If your service has too much dependency on each other better to cluster them as one domain. </p>&#xA;&#xA;<p>I have explained about how to define your domain here:<br>&#xA;<a href=""https://stackoverflow.com/questions/43426699/db-design-for-microservice-architecture/43515759#43515759"">DB design for microservice architecture</a></p>&#xA;"
44368412,44096281,4247860,2017-06-05T11:56:02,"<p>You can use <strong>private-public key</strong>  signing method.<br>&#xA;Create your JWT token with a <em>private key</em> and share the <em>public key</em> with all other microservices. Other microservices can verify the signature of the token by using the shared public key.</p>&#xA;&#xA;<p>You can use RSA algorithm for generating the keys &amp; signing the tokens. The private key should be only with the service which is generating the token.</p>&#xA;&#xA;<p>Snippet for generating keys:</p>&#xA;&#xA;<pre><code>    KeyPairGenerator kpg = KeyPairGenerator.getInstance(""RSA"");&#xA;    kpg.initialize(2048);&#xA;    KeyPair kp = kpg.genKeyPair();&#xA;    Key publicKey = kp.getPublic();&#xA;    Key privateKey = kp.getPrivate(); &#xA;</code></pre>&#xA;&#xA;<p>Snippet to generate JWT token. I am using JJwt API for generating the token:</p>&#xA;&#xA;<pre><code>Jwts.builder()&#xA;            .setClaims(payload)&#xA;            .setExpiration(expiryDate)&#xA;            .signWith(SignatureAlgorithm.RS256, privateKey )&#xA;            .compact();&#xA;</code></pre>&#xA;&#xA;<p>Snippet to verify the token with public key:</p>&#xA;&#xA;<pre><code>Jwts.parser() &#xA;       .setSigningKey(publicKey )&#xA;       .parseClaimsJws(jwtToken)&#xA;</code></pre>&#xA;&#xA;<p>Hope this helps.</p>&#xA;"
49347883,49345233,1561140,2018-03-18T12:29:25,"<p>You can keep a bit of data in execution state (data) however it is limited to 32kB (<a href=""https://docs.aws.amazon.com/step-functions/latest/dg/limits.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/step-functions/latest/dg/limits.html</a>).</p>&#xA;"
37937728,37936588,1894196,2016-06-21T07:10:18,"<p>Usually I create all but the concept of micro is opinionated, there are a lot of authentication framework ready in different languages for example this is in PHP</p>&#xA;&#xA;<p><a href=""https://github.com/thephpleague/oauth2-server"" rel=""nofollow"">https://github.com/thephpleague/oauth2-server</a></p>&#xA;&#xA;<p>but there is also something in golang, you can use this libraries to make your work simple.</p>&#xA;&#xA;<p>I tried to search something ready to help you but I have the same feeling there aren't around.</p>&#xA;"
46237295,46185971,8120884,2017-09-15T10:22:01,"<p>Hazelcast allows concurrency across JVMs / applications. A hazelcast data structure and its contents are available across Client application or Nodes within the cluster. Hazelcast manages the consistency. The helps to share cache/data across applications. You can also control data partitioning or affinity leading to fewer networks hops during fetching data, query or executing tasks on the cache server. </p>&#xA;&#xA;<p>Kindly refer to documentation in hazelcast website and guide as below.&#xA;Mastering Hazelcast : <a href=""https://hazelcast.com/resources/mastering-hazelcast/"" rel=""nofollow noreferrer"">https://hazelcast.com/resources/mastering-hazelcast/</a>&#xA;Code Samples : <a href=""https://github.com/hazelcast/hazelcast-code-samples"" rel=""nofollow noreferrer"">https://github.com/hazelcast/hazelcast-code-samples</a></p>&#xA;&#xA;<p>Hope this helps</p>&#xA;"
44221499,36157778,1371188,2017-05-27T20:52:02,"<p>I ran into the same problem while learning about Service Fabric. Turned out that providing just the URI was not enough - I also had to specify the partition key as a magic value of one:</p>&#xA;&#xA;<pre><code>IHelloService service = ServiceProxy.Create&lt;IHelloService&gt;(new Uri(""fabric:/Application1/HelloService""), new ServicePartitionKey(1));&#xA;</code></pre>&#xA;&#xA;<p>Kudos to <a href=""https://disqus.com/home/discussion/thewindowsazureproductsite/service_remoting_in_service_fabric_microsoft_azure/#comment-2476986629"" rel=""nofollow noreferrer"">this thread on disq.us</a>. There is also a deeper explanation provided by a Microsoft engineer Oana Platon as to why the value of 1 works:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Diogo, take a look at this article that explains partitioning:&#xA;  <a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-concepts-partitioning"" rel=""nofollow noreferrer"">link</a>  Specifically, look at ranged&#xA;  partitioning (otherwise known as UniformInt64Partition): ""This is used&#xA;  to specify an integer range (identified by a low key and high key) and&#xA;  a number of partitions (n). It creates n partitions, each responsible&#xA;  for a non-overlapping subrange of the overall partition key range. For&#xA;  example, a ranged partitioning scheme with a low key of 0, a high key&#xA;  of 99, and a count of 4 would create four partitions, as shown below.""&#xA;  Then look at your service manifest and figure out how it is configured&#xA;  - how many partitions and what is the range (low key - high key). If you have one partition, any key in that range goes to the (one)&#xA;  partition, so it doesn't matter which key you specify. If you have&#xA;  more than one partition, you need to figure out with which one your&#xA;  client needs to talk to. Specify a partition key in the range that&#xA;  partition is serving.</p>&#xA;</blockquote>&#xA;&#xA;<p>I must admit that I myself must study partitioning in greater depth to understand this explanation.</p>&#xA;"
50284299,50275156,990517,2018-05-11T02:52:23,"<p>Based on the feedback, I was able to able to throw together a crude solution based on the eShop <a href=""https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/BuildingBlocks/EventBus/EventBusServiceBus/EventBusServiceBus.cs"" rel=""nofollow noreferrer"">Event Bus</a> to create the Azure Subscriptions upon startup:</p>&#xA;&#xA;<pre><code> public EventBusServiceBus(IServiceBusPersisterConnection serviceBusPersisterConnection,&#xA;        ILogger&lt;EventBusServiceBus&gt; logger, IEventBusSubscriptionsManager subsManager, string subscriptionClientName,&#xA;        ILifetimeScope autofac, AzureUserCredentials userCredentials, string subscriptionId, string resourceGroupName, string serviceBusName, string topicName)&#xA;    {&#xA;        _serviceBusPersisterConnection = serviceBusPersisterConnection;&#xA;        _logger = logger;&#xA;        _subsManager = subsManager ?? new InMemoryEventBusSubscriptionsManager();&#xA;&#xA;        _subscriptionClient = new Microsoft.Azure.ServiceBus.SubscriptionClient(serviceBusPersisterConnection.ServiceBusConnectionStringBuilder,&#xA;            subscriptionClientName);&#xA;        _autofac = autofac;&#xA;&#xA;        var credentials = SdkContext.AzureCredentialsFactory.FromServicePrincipal(&#xA;          userCredentials.ClientId, userCredentials.ClientSecret, userCredentials.TenantId, AzureEnvironment.FromName(userCredentials.EnvironmentName));&#xA;&#xA;        var azure = Azure&#xA;                .Configure()&#xA;                .WithLogLevel(HttpLoggingDelegatingHandler.Level.Basic)&#xA;                .Authenticate(credentials)&#xA;                .WithSubscription(subscriptionId);&#xA;&#xA;        var nm = azure.ServiceBusNamespaces.GetByResourceGroup(resourceGroupName, serviceBusName);&#xA;&#xA;        var topic = nm.Topics.GetByName(topicName);&#xA;&#xA;        if (topic == null)&#xA;            throw new ArgumentException($""Topic {topic} does not exist."", nameof(topic));&#xA;&#xA;        Microsoft.Azure.Management.ServiceBus.Fluent.ISubscription subscription = null;&#xA;        try&#xA;        { subscription = topic.Subscriptions.GetByName(subscriptionClientName); }&#xA;        catch { }&#xA;&#xA;        if (subscription == null)&#xA;        {&#xA;            logger.LogInformation($""Creating Azure Subscription '{subscriptionClientName}'"");&#xA;            topic.Subscriptions.Define(subscriptionClientName).WithDeleteOnIdleDurationInMinutes(5).Create();&#xA;        }&#xA;        else&#xA;        {&#xA;            logger.LogInformation($""Azure Subscription '{subscriptionClientName}' already exists. Reusing."");&#xA;        }&#xA;&#xA;        RemoveDefaultRule();&#xA;        RegisterSubscriptionClientMessageHandler();&#xA;    }&#xA;</code></pre>&#xA;"
34619763,33291874,837173,2016-01-05T19:29:36,"<p>Your microservices can be a small application that exposes a few RESTful endpoints, or it can be a background worker that reaps a queue. It can even be an AWS Lambda function that's invoked on some event.</p>&#xA;&#xA;<p>The point is that your application is composed of several smaller applications, thus allowing you a greater amount of agility when it comes to deploying code, programming languages, frameworks, etc. </p>&#xA;"
44990562,44988481,839733,2017-07-08T20:40:29,"<p>Looks to me that you split your services too thin; this happens, and with time, you start to realize that the services need to be more coarse-grained because of maintenance and performance issues. The cost of another HTTP call from auth to user service, and the overhead of maintaining inter-service auth is not trivial.</p>&#xA;&#xA;<p>IMO, user service can exist for other user information, like address etc if those exist, but auth service should be responsible for managing its own data. This is exactly why Spring Security has a <a href=""https://docs.spring.io/spring-security/site/docs/current/apidocs/org/springframework/security/core/userdetails/UserDetailsService.html"" rel=""nofollow noreferrer"">UserDetailsService</a>.</p>&#xA;&#xA;<p>This is really a design choice whether the user credentials and other user information should be in the same table, or even the same database. Different people will give you different answers, but in my opinion and experience, a shared database between a <strong>small</strong> number of <strong>related</strong> services is acceptable, especially because these tables are gonna be related by a foreign key (userId). Distributed transaction is pure evil with microservices so I'm not even going there. When you delete/update a user, go for eventual consistency using events.</p>&#xA;&#xA;<p><strong>Edit</strong>:</p>&#xA;&#xA;<p>After chatting with the OP, I learned that user service is really the OAuth resource server in his design. It's not clear to him, and thus to me, where is the OAuth authorization server. Regardless, I stand by my suggestion to merge the user service and the auth service.</p>&#xA;"
37371477,37289487,839733,2016-05-22T06:59:56,"<p>Answering my own question, I fixed the issue using a <code>BeanDefinitionRegistryPostProcessor</code>. Related JIRA <a href=""https://jira.spring.io/browse/SPR-6428"" rel=""nofollow"">SPR-6428</a> had been filed by another user but was closed.</p>&#xA;&#xA;<pre><code>/**&#xA; * Removes {@link org.springframework.beans.factory.config.PropertyPlaceholderConfigurer} classes that come before&#xA; * {@link PropertySourcesPlaceholderConfigurer} and fail to resolve Spring Cloud properties, thus setting them to default.&#xA; * One such property is {@code spring.application.name} that gets set to 'unknown' thus causing registration with&#xA; * discovery service to fail. This class collects the {@code locations} from these offending&#xA; * {@code PropertyPlaceholderConfigurer} and later adds to the end of property sources available from&#xA; * {@link org.springframework.core.env.Environment}.&#xA; * &lt;p&gt;&#xA; * c.f. https://jira.spring.io/browse/SPR-6428&#xA; *&#xA; * @author Abhijit Sarkar&#xA; */&#xA;@Component&#xA;@Slf4j&#xA;public class PropertyPlaceholderConfigurerPostProcessor implements BeanDefinitionRegistryPostProcessor {&#xA;    private final Set&lt;String&gt; locations = new HashSet&lt;&gt;();&#xA;&#xA;    @Override&#xA;    public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry beanDefinitionRegistry) throws BeansException {&#xA;        String[] beanDefinitionNames = beanDefinitionRegistry.getBeanDefinitionNames();&#xA;&#xA;        List&lt;String&gt; propertyPlaceholderConfigurers = Arrays.stream(beanDefinitionNames)&#xA;                .filter(name -&gt; name.contains(""PropertyPlaceholderConfigurer""))&#xA;                .collect(toList());&#xA;&#xA;        for (String name : propertyPlaceholderConfigurers) {&#xA;            BeanDefinition beanDefinition = beanDefinitionRegistry.getBeanDefinition(name);&#xA;            TypedStringValue location = (TypedStringValue) beanDefinition.getPropertyValues().get(""location"");&#xA;&#xA;            if (location != null) {&#xA;                String value = location.getValue();&#xA;                log.info(""Found location: {}."", location);&#xA;                /* Remove 'classpath:' prefix, if present. It later creates problem with reading the file. */&#xA;                locations.add(removeClasspathPrefixIfPresent(value));&#xA;&#xA;                log.info(""Removing bean definition: {}."", name);&#xA;&#xA;                beanDefinitionRegistry.removeBeanDefinition(name);&#xA;            }&#xA;        }&#xA;    }&#xA;&#xA;    private String removeClasspathPrefixIfPresent(String location) {&#xA;        int classpathPrefixIdx = location.lastIndexOf(':');&#xA;&#xA;        return classpathPrefixIdx &gt; 0 ? location.substring(++classpathPrefixIdx) : location;&#xA;    }&#xA;&#xA;    @Override&#xA;    public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException {&#xA;        PropertySourcesPlaceholderConfigurer configurer =&#xA;                beanFactory.getBean(PropertySourcesPlaceholderConfigurer.class);&#xA;&#xA;        MutablePropertySources propertySources = getPropertySources(configurer);&#xA;&#xA;        locations.stream()&#xA;                .map(locationToPropertySrc)&#xA;                .forEach(propertySources::addLast);&#xA;    }&#xA;&#xA;    private MutablePropertySources getPropertySources(PropertySourcesPlaceholderConfigurer configurer) {&#xA;        /* I don't like this but PropertySourcesPlaceholderConfigurer has no getter for environment. */&#xA;        Field envField = null;&#xA;        try {&#xA;            envField = PropertySourcesPlaceholderConfigurer.class.getDeclaredField(""environment"");&#xA;            envField.setAccessible(true);&#xA;            ConfigurableEnvironment env = (ConfigurableEnvironment) envField.get(configurer);&#xA;&#xA;            return env.getPropertySources();&#xA;        } catch (ReflectiveOperationException e) {&#xA;            throw new ApplicationContextException(""Our little hack didn't work. Failed to read field: environment."", e);&#xA;        }&#xA;    }&#xA;&#xA;    Function&lt;String, PropertySource&gt; locationToPropertySrc = location -&gt; {&#xA;        ClassPathResource resource = new ClassPathResource(location);&#xA;        try {&#xA;            Properties props = PropertiesLoaderUtils.loadProperties(resource);&#xA;            String filename = getFilename(location);&#xA;&#xA;            log.debug(""Adding property source with name: {} and location: {}."", filename, location);&#xA;&#xA;            return new PropertiesPropertySource(filename, props);&#xA;        } catch (IOException e) {&#xA;            throw new ApplicationContextException(&#xA;                    String.format(""Failed to read from location: %s."", location), e);&#xA;        }&#xA;    };&#xA;&#xA;    private String getFilename(String location) {&#xA;        return location.substring(location.lastIndexOf('/') + 1);&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
39723870,39690815,2026293,2016-09-27T11:47:34,<p>I was able to run it successfully on windows by removing the <code>'\'s</code> in your command and running using the spring-cli version 1.4.1  </p>&#xA;&#xA;<p><code>spring init --build maven --groupId com.redhat.examples --version 1.0 --java-version 1.8 --dependencies web -- me hola-springboot hola-springboot Using service at start.spring.io Project extracted to '\Data\hola-springboot'</code></p>&#xA;
50057076,50051429,9302325,2018-04-27T07:03:58,"<p>Instead of waiting for the asynchronous part to complete (which I suppose is an antipattern), I would do the A service to response immediatelly - saying the request was received and the proces behind it was started.</p>&#xA;&#xA;<p>Then, you have two options:</p>&#xA;&#xA;<ol>&#xA;<li>in the front-end periodically check (request) status of the operation,</li>&#xA;<li>implement server to client communication with websocket.</li>&#xA;</ol>&#xA;"
50068828,49579074,9302325,2018-04-27T19:06:04,"<p>You may consider the following,</p>&#xA;&#xA;<p>implement your CustomerService and CartService (as microservices). Then, whenever a Cart Entity is created in the CartService, it sends a request (over message queue) to the CustomerService with information required to identify the Customer Entity. Send back the Customer data to the CartService where it can be used to enrich the Cart Entity.</p>&#xA;&#xA;<p>You may wonder where is the cache? Well, if still required, you may implement it as a Singleton service in the same node the CartService is running (i mean 'close' to the service), and check there, before publishing the request to the CustomerService.</p>&#xA;&#xA;<p>You will end up with multiple caches storing potentially the same Customer information on different machines, but this should not be an issue (or is it in your case?).</p>&#xA;"
50090687,50070895,9302325,2018-04-29T19:54:45,"<p>The process you describe is a good one and brings some 'hidden' benefits you may have not realized yet. I think the main point here is WHAT to communicate to the user.</p>&#xA;&#xA;<p>I think in this scenario you should response immediately saying, the blog post was dispatched to publish. You don't say, it was published, but that the process of publishing it was initiated succesfully. By this you say to the user: your nice blog post is safe (messaging technology will assure this) and we are taking care of making it public. Relax and wait for it being published in a second or two, or a minute.&#xA;The hidden benefit of this approach is, that you may apply additional processing before making the post public, e.g. checking by a moderator?</p>&#xA;&#xA;<p>Everything else is just like you described. It's really just the way how you communicate it to the user. Clear communication of what your system is doing here is the key.</p>&#xA;"
42964124,42958561,7731765,2017-03-22T22:52:31,"<p>Creating RESTful web service from NodeJS which is running on your servers or cloud and let the JAVA Android app consumes the services is possible.</p>&#xA;&#xA;<p>Plus: because you're going to make microservice system, so, actually you don't have to worry about your backend's technology since it is one of benefit we got from adopting microservice.</p>&#xA;&#xA;<p>you can have whatever! that provides APIs to your client to consumes services. For examples, you are going to have 4 services. Two of them may use NodeJS, one of them may use Python Flask and another one may use JAVA Spring. Even data storage can be different since if it's microservice it's the best to have their own database. </p>&#xA;&#xA;<p>Anyway, when we talk about microservice, we will think the another different term, so called Monolithic architecture. As you've done the reading, you'd have seen a lot of mention about this style compare with microservice. </p>&#xA;&#xA;<p>I suggest to read books and papers those talked about microservice. Most of them are mentioned about what are the problems of monolithic and why we need to move to microservice. you can find them in <a href=""https://scholar.google.com/"" rel=""nofollow noreferrer"">https://scholar.google.com/</a> search for microservice</p>&#xA;&#xA;<p>Martin Fowler gave the definition of microservice as follow:</p>&#xA;&#xA;<blockquote>&#xA;  <p>The microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API.</p>&#xA;</blockquote>&#xA;&#xA;<p>and I may refer @rsp's comment: </p>&#xA;&#xA;<blockquote>&#xA;  <p>to have microservices you actually need to have multiple Node applications, each responsible for a fraction of the functionality. Otherwise you have just a service, not microservices.</p>&#xA;</blockquote>&#xA;&#xA;<p>Thus, for me, microservice is much more about designing a system. the following are the key questions:</p>&#xA;&#xA;<ul>&#xA;<li>how do I design my system </li>&#xA;<li>what are good microservice's principles I selected (you can follow the best practices or develop your own as well) </li>&#xA;<li>what is the sets of context I got from investigating my application's function (DDD: bounded-context)</li>&#xA;<li>what are the set of services will be implemented and what context that each service serves</li>&#xA;<li>when I got the design then I have to think: is it too big or too small</li>&#xA;<li>how do services communicate with each others</li>&#xA;<li>etc. (you will also get more questions for yourself while reading materials)</li>&#xA;</ul>&#xA;&#xA;<p>There are more about microservices. I suggest to read </p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://martinfowler.com/articles/microservices.html"" rel=""nofollow noreferrer"">Martin Fowler's website</a> </li>&#xA;<li>""Microservice Architecture"" and ""Building Microservice"" from O'Reilly </li>&#xA;<li>papers from <a href=""https://scholar.google.com/"" rel=""nofollow noreferrer"">scholar.google.com</a></li>&#xA;</ul>&#xA;&#xA;<p>I hope you have fun with your research and building microservice! :D</p>&#xA;"
48178463,48043523,790070,2018-01-10T00:03:01,"<h2>This is possible, but-</h2>&#xA;&#xA;<p>You'll want to think carefully before verifying against a live provider - especially one that you don't control. Anything that changes server state is (very probably) out.</p>&#xA;&#xA;<p>However, there's no technical reason you can't run some provider verification to check that your consumer contract is fulfilled by the currently deployed provider. There are constructors for <a href=""https://github.com/DiUS/pact-jvm/blob/3_5_11/pact-jvm-provider-junit/src/main/java/au/com/dius/pact/provider/junit/target/HttpTarget.java#L38"" rel=""nofollow noreferrer"">host and port</a>:</p>&#xA;&#xA;<pre><code>public final Target target = new HttpTarget(host, port); &#xA;</code></pre>&#xA;&#xA;<p>Some things to be careful of:</p>&#xA;&#xA;<ul>&#xA;<li>It will be very important to write your tests so that they don't depend on the data. This means using <a href=""https://docs.pact.io/documentation/matching.html"" rel=""nofollow noreferrer"">matchers</a> in your consumer tests to ensure that you're validating the shape of the data returned from the provider (rather than validating specific data returned from the provider). This is good practice for writing consumer tests anyway.</li>&#xA;<li>You may run into problems if your contract includes requests that are expected to change server state (it may not be appropriate to make these requests to a live provider, unless you're able to make the requests to a sandbox environment somehow).</li>&#xA;<li>Depending on the size of your contract and/or the normal traffic that the provider gets, it may be impolite to run your own automated tests against it.</li>&#xA;<li>You won't be able to <a href=""https://docs.pact.io/documentation/provider_states.html"" rel=""nofollow noreferrer"">set provider state</a>. Provider states are used to avoid having interdependencies between your contract tests, so if you have to (say) make login requests before doing anything else, you are likely to run into headaches - pact is not designed to have tests that are order dependent or include more than one request.</li>&#xA;<li>Your tests may be brittle if they're going out to a live deployed provider running elsewhere- changes in DNS, server uptime, network timeouts, etc. all could cause your tests to fail unexpectedly.</li>&#xA;</ul>&#xA;&#xA;<h2>A better alternative</h2>&#xA;&#xA;<p>The very best solution is to get whoever controls the provider to do their own verification, using (or including) the pact generated by your consumer. This is a good use case for a pact broker - but depending on your ability to contact the right people, may be a challenge.</p>&#xA;"
40688281,40688160,2494262,2016-11-19T01:26:49,"<p>Namespaces don't usually talk to each other, unless you use full qualified names.</p>&#xA;&#xA;<p>Using the same backend service for Staging and Prod sounds like a very risky bet: anything you screw up in staging will be automatically in Prod.&#xA;Probably not what you want...</p>&#xA;&#xA;<p>but if you want to know if you can, yes you can, if you use FQDN (i.e. service-name.svc..cluster.local ) for reaching the service.</p>&#xA;"
51584465,51530578,606006,2018-07-29T20:52:22,<p>Removing the <code>DefaultRouter</code> made the error go away.</p>&#xA;
28391367,28387907,151350,2015-02-08T06:47:55,<p>Another benefit of the pull based approach is that you don't have to worry about the data getting stale in the queue.</p>&#xA;
31110967,31104540,151350,2015-06-29T08:23:35,"<blockquote>&#xA;  <p>This is super-nice and elegant but in practice it becomes a bit tricky</p>&#xA;</blockquote>&#xA;&#xA;<p>What it means ""in practice"" is that you need to design your microservices in such a way that the necessary business consistency is fulfilled when following the rule:</p>&#xA;&#xA;<blockquote>&#xA;  <p>that services cannot directly connect to a DB ""owned"" by another service.</p>&#xA;</blockquote>&#xA;&#xA;<p>In other words - don't make any assumptions about their responsibilities and change the boundaries as needed until you can find a way to make that work.</p>&#xA;&#xA;<p>Now, to your question:</p>&#xA;&#xA;<blockquote>&#xA;  <p>What are the best patterns to keep things consistent and live a happy life?</p>&#xA;</blockquote>&#xA;&#xA;<p>For things that don't require immediate consistency, and updating loyalty points seems to fall in that category, you could use a reliable pub/sub pattern to dispatch events from one microservice to be processed by others. The reliable bit is that you'd want good retries, rollback, and idempotence (or transactionality) for the event processing stuff.</p>&#xA;&#xA;<p>If you're running on .NET some examples of infrastructure that support this kind of reliability include <a href=""http://particular.net/nservicebus"" rel=""noreferrer"">NServiceBus</a> and <a href=""http://masstransit-project.com/"" rel=""noreferrer"">MassTransit</a>. Full disclosure - I'm the founder of NServiceBus.</p>&#xA;&#xA;<p><strong>Update:</strong> Following comments regarding concerns about the loyalty points: ""if balance updates are processed with delay, a customer may actually be able to order more items than they have points for"". </p>&#xA;&#xA;<p>Many people struggle with these kinds of requirements for strong consistency. The thing is that these kinds of scenarios can usually be dealt with by introducing additional rules, like if a user ends up with negative loyalty points notify them. If T goes by without the loyalty points being sorted out, notify the user that they will be charged M based on some conversion rate. This policy should be visible to customers when they use points to purchase stuff.</p>&#xA;"
43536553,43502357,151350,2017-04-21T07:31:17,"<p>I think the better way to think about this situations that you effectively have two companies - one in the grocery business, with all the corresponding capabilities that make that up, and the other in the photo business. Even if the two ""companies"" happen to share the same incorporation documents, you really shouldn't view this as a single entity.</p>&#xA;"
29692615,29689630,151350,2015-04-17T07:14:08,"<p>Since email is an inherently asynchronous process (fire and forget), and often require the use of somewhat unreliable resources, it is better to integrate with them through a message queue (MSMQ, RabbitMQ, etc) and these tend to have pretty decent .net APIs.</p>&#xA;&#xA;<p>If you go beyond simple things like email, it may be worth your while to look at larger-scale frameworks around queues to give you better support for retries, error management, transaction management, etc. These are often called ""service bus"" technologies and there are a couple good free OSS ones in .net including <a href=""http://masstransit-project.com/"" rel=""nofollow"">MassTransit</a> and <a href=""https://github.com/rebus-org/Rebus"" rel=""nofollow"">Rebus</a>. If/when you go looking for a fully supported .net service bus you'll probably find <a href=""http://particular.net/nservicebus"" rel=""nofollow"">NServiceBus</a> of which, in the interest of full disclosure, I'm the original author.</p>&#xA;"
27296360,26975640,151350,2014-12-04T14:21:14,"<p>The term ESB has gotten overloaded, primarily in the Java world, to mean a big and complex piece of infrastructure that ends up hosting a bunch of poorly implemented logic in a central place.</p>&#xA;&#xA;<p>Lighter-weight technologies like Apache Caml or NServiceBus don't encourage this kind of approach and indeed follow the ""dumb pipes / smart endpoints"" approach that has served as the backbone of the internet from the beginning.</p>&#xA;&#xA;<p><a href=""http://particular.net/nservicebus"" rel=""nofollow"">NServiceBus</a> specifically focuses on providing a higher level framework than most messaging libraries to make it easier to build smart endpoints that are more reliable through its deeper support for once-and-only-once message processing.</p>&#xA;&#xA;<p>Full disclosure - I'm the founder of NServiceBus.</p>&#xA;"
36150093,36137802,151350,2016-03-22T09:05:13,"<p>We handle this with the Outbox approach in NServiceBus:</p>&#xA;&#xA;<p><a href=""http://docs.particular.net/nservicebus/outbox/"" rel=""nofollow"">http://docs.particular.net/nservicebus/outbox/</a></p>&#xA;&#xA;<p>This approach requires that the initial trigger for the whole operation came in as a message on the queue but works very well.</p>&#xA;"
47939981,47938835,2778422,2017-12-22T10:32:09,"<p>As far as I know (and you may also already know) seems you're trying to implement the <a href=""https://martinfowler.com/bliki/CircuitBreaker.html"" rel=""nofollow noreferrer"">Circuit Breaker</a> pattern and whether to implement it as a central service or as part of your business transaction logic.</p>&#xA;&#xA;<p>One parameter to decide if it would be better to have it as a separate service or not is to see if you have only one such transaction or there are more? If there is more than one, then maybe it would be better to pull-out the circuit breaker out of your actual business. It could be a sort of utility component included in different services or an standalone microservice. In case of an standalone service, an option could be to use an off the shelf product/library/framework to do so. I don't know that much about your environment and limitations, but you can even think about using something like Camel or a light BPM engine for this purpose.</p>&#xA;&#xA;<p>In my opinion it would be better anyway to separate this non business logic from your actual transactional business, either as a utility component added as a library or a separate service.</p>&#xA;"
48477726,47555732,619830,2018-01-27T16:12:34,"<p>What about <a href=""https://stackoverflow.com/a/48449321/619830"">this solution</a> based on the combination of a feign <code>ErrorDecoder</code> (extracting what you need from feign exception) and a spring <code>@ExceptionHandler</code> (dumps what you need in a <code>ResponseEntity&lt;Object&gt;</code>)?</p>&#xA;&#xA;<p>I chose to translate the error payload to one that is generic to my API, but actually, you could keep the same.</p>&#xA;"
26637846,26616962,621366,2014-10-29T18:18:47,"<p>I didn’t read the article, so I can only speculate what he can mean exactly, but as he gives ESB as an example against microservices and ZeroMQ as an example for micro services I hope my speculation will be pretty exact:</p>&#xA;&#xA;<p>One of the ideas of Unix (and Linux) is to build small independent applications and connect them via pipes. The probably most common set of two command which I’m using is <code>ps</code> and <code>grep</code>like this: <code>ps aux | grep PROCESS_NAME</code> - here you can see a dumb pipe which only forwards the output of <code>ps</code> to stdin of <code>grep</code>. </p>&#xA;&#xA;<p>Other messaging systems like ZeroMQ work similarly, although they can have a little bit more complexity like round-robin distribution and reliable delivery. Erlang as a language is built on top of small smart endpoints sending messages between each other. The advantages here are obvious and also mentioned in the article, small applications are easier to maintain, decoupling makes it easier to scale.</p>&#xA;&#xA;<p>On the other hand of Microservices are most commonly big enterprise applications, like the mentioned Enterprise Service Bus. I didn’t really work with those enough to give you a specific example, but generally those busses contain a lot of functionality which is either included via scripts or configuration. Such functionality mostly includes a configureable Workflow with advanced routing and  can even transform the messages, so different endpoints can handle them.</p>&#xA;&#xA;<p>An example could be - if you want the perform some advance action in a system, for instance change the requirements in an already running project, this could start a workflow, where the ESB would send out automatically different notifications to different actors around those changed requirements and wait for 1 or more of those actors to confirm before this change would be applied. Which would be basically the opposite - dumb endpoints (which just send/receive the data to/from the bus) and a very smart pipe (the bus, which can be configured or scripted to handle all possible enterprise scenarios).</p>&#xA;&#xA;<p>I’m pretty confident that there exist enterprise service busses which are handling similar scenarios and those are the opposite of simple “dumb” ZeroMQ-like message passing frameworks.</p>&#xA;&#xA;<p>Basically the logic has to be implemented somewhere - either in the big ESB, or in the endpoints. The idea of microservices is to put it into the endpoints rather than into the bus and have a similar philosophy as unix applications.</p>&#xA;"
27349520,27347820,621366,2014-12-08T00:17:10,"<p>The problem you are describing is called service lookup / service registry / resource lookup / .. and it depends. It depends on how large your system is and how dynamic it is. </p>&#xA;&#xA;<p>If you only have few components, it might be feasible enough to store the necessary information in a config file, or pass it as parameter. Generally, many use DNS as a lookup system, but it’s not considered to be a good one, due to the caching and long latency.</p>&#xA;&#xA;<p>I think most distributed systems use <a href=""http://zookeeper.apache.org/"" rel=""nofollow"">Zookeeper</a> to store this information for them. This way, all the services only need to know the IP-addresses of the Zookeeper cluster. If you have replication, you just store multiple addresses inside Zookeeper, and depending on which system you are using, you’ll need to choose an address on your own, or the driver does it (in case you’re connecting to a replicated database for instance).</p>&#xA;&#xA;<p>Another way to do this, is to use a message queue, like ZMQ which will forward the messages to the correct instances. ZMQ can deal with replications and load balancing as well. </p>&#xA;"
51935619,51932126,5084982,2018-08-20T17:15:09,"<p>Ok, after some digging I found out the solution which is to use <code>printSchema</code> from <code>graphql/utilities</code></p>&#xA;&#xA;<pre><code>import { printSchema } from 'graphql/utilities';&#xA;const typeDefs = printSchema(schema);&#xA;</code></pre>&#xA;"
46962046,46786169,1366326,2017-10-26T19:08:56,"<p>If you are using Spray Json, then you might have created a format for your case class, it should look like this:</p>&#xA;&#xA;<p>Assuming: </p>&#xA;&#xA;<pre><code>case class Request(id:Int, name:String, phone:String)&#xA;</code></pre>&#xA;&#xA;<p>You should have a trait like:</p>&#xA;&#xA;<pre><code>import spray.json._&#xA;&#xA;trait RequestJsonSupport extends DefaultJsonProtocol {&#xA;  implicit val requestFormat = jsonFormat3(Request.apply)&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>And then extend it on your route class:</p>&#xA;&#xA;<pre><code>class MyRouteClass(...) extends RequestJsonSupport {...}&#xA;</code></pre>&#xA;&#xA;<p>That way your Akka Http instance knows how to parse a Json input and convert it into your case class. Then you can worry about missing fields and such. Spray will take care of this.</p>&#xA;&#xA;<p>For example, if you sent this:</p>&#xA;&#xA;<pre><code>{&#xA;    ""id"":   ""1205"",&#xA;    ""name"":     ""sekhar"",&#xA;    ""phone"":""1234567890""&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Spray will throw an:</p>&#xA;&#xA;<pre><code>The request content was malformed:&#xA;Expected Int as JsNumber, but got ""1205""&#xA;</code></pre>&#xA;&#xA;<p>Check out Spray Json repo <a href=""https://github.com/spray/spray-json#providing-jsonformats-for-case-classes"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;"
46365028,46364880,4268599,2017-09-22T12:38:11,"<p>Some implementations put the version in the ""Accept-version"" header like: </p>&#xA;&#xA;<pre><code>Accept-version: v1&#xA;Accept-version: v2&#xA;</code></pre>&#xA;&#xA;<p>Other put them in the more standard ""Accept"" header like so:</p>&#xA;&#xA;<pre><code>Accept: application/vnd.example.v1+json&#xA;Accept: application/vnd.example+json;version=1.0&#xA;</code></pre>&#xA;&#xA;<p>Check out: <a href=""https://restfulapi.net/versioning/"" rel=""nofollow noreferrer"">https://restfulapi.net/versioning/</a></p>&#xA;&#xA;<p>Then it's up to your code framework to get them.</p>&#xA;"
47042375,47042162,271415,2017-10-31T18:35:14,"<p>The Lambda service stores your code encrypted in an S3 bucket. AWS doesn't provide any more specific information than that. It's possible that the service has its own S3 bucket(s) and they individually encrypt all objects that they store there.</p>&#xA;&#xA;<p>No, you can't access the Lambda service's S3 bucket(s) directly, but you can download the code you previously uploaded to your Lambda function. Go to the AWS Lambda console, select your Lambda function, then click Actions | Export function, then click Download deployment package. Note: if your code was written in Java (or other compiled language) then the download will contain compiled files, not the original source code.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/WdKae.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WdKae.png"" alt=""enter image description here""></a></p>&#xA;"
37663053,37634349,10039,2016-06-06T17:02:11,"<p>There is an Enterprise Integration Pattern from the book by Hohpe/Wolfe called the <a href=""http://www.enterpriseintegrationpatterns.com/patterns/messaging/StoreInLibrary.html"" rel=""nofollow"">Claim Check Pattern</a> that addresses these concerns.</p>&#xA;&#xA;<p>Essentially the big blob is removed from the message and stored somewhere that both sender and receiver can access, whether that be a common file share, FTP server, an Amazon S3 blob, whatever. It leaves a ""claim check"" behind: some sort of address that describes how to find the blob back.</p>&#xA;&#xA;<p>The tiny message can then be transmitted over Kafka/JMS, or some other message queue system, most of which are fairly bad at dealing with large data blobs.</p>&#xA;&#xA;<p>Of course, a very simple implementation is to leave the files on a file share and only refer to them by file path.</p>&#xA;&#xA;<p>It's more complex when it's preferable to have the blob integrated with the rest of the message, requiring a true Claim Check implementation. This can be handled at an infrastructure level so the message sender and receiver don't need to know any of the details behind how the data is transmitted.</p>&#xA;&#xA;<p>I know that you're in the Java landscape, but in <a href=""http://particular.net/nservicebus"" rel=""nofollow"">NServiceBus</a> (I work for <a href=""http://particular.net/"" rel=""nofollow"">Particular Software</a>, the makers of NServiceBus) this pattern is implemented with the <a href=""http://docs.particular.net/nservicebus/messaging/databus"" rel=""nofollow"">Data Bus feature</a> in a message pipeline step. All the developer needs to do is identify what type of message properties apply to the data bus, and (in the default file share implementation) configure the location where files are stored. Developers are also free to provide their own data bus implementation.</p>&#xA;&#xA;<p>One thing to keep in mind is that with the blobs disconnected from the messages, you have to provide for cleanup. If the messages are one-way, you could clean them up as soon as the message is successfully processed. With Kafka (not terribly familiar) there's a possibility to process messages from a stream multiple times, correct? If so you'd want to wait until it was no longer possible to process that message. Or, if the Publish/Subscribe pattern is use, you would not want to clean up the files until you were sure all subscribers had a chance to be processed. In order to accomplish that, you'd need to set an SLA (a timespan that each message must be processed within) on the message and clean up the blob storage after that timespan had elapsed.</p>&#xA;&#xA;<p>In any case, lots of things to consider, which make it much more useful to implement at an infrastructure level rather than try to roll your own in each instance.</p>&#xA;"
31191807,31190685,1075511,2015-07-02T18:16:33,"<p>I think the service you describe are too small to be practical. You can better solve it with a servicelayer that does the needed stuff.</p>&#xA;&#xA;<p>Following <a href=""http://martinfowler.com/articles/microservices.html"" rel=""nofollow"">http://martinfowler.com/articles/microservices.html</a> microservices should not be too small and not too big. </p>&#xA;&#xA;<blockquote>&#xA;  <p>How big is a microservice?</p>&#xA;  &#xA;  <p>Although “microservice” has become a popular name for this architectural style, its name does lead to an unfortunate focus on the size of service, and arguments about what constitutes “micro”. In our conversations with microservice practitioners, we see a range of sizes of services. The largest sizes reported follow Amazon's notion of the Two Pizza Team (i.e. the whole team can be fed by two pizzas), meaning no more than a dozen people. On the smaller size scale we've seen setups where a team of half-a-dozen would support half-a-dozen services.</p>&#xA;  &#xA;  <p>This leads to the question of whether there are sufficiently large differences within this size range that the service-per-dozen-people and service-per-person sizes shouldn't be lumped under one microservices label. At the moment we think it's better to group them together, but it's certainly possible that we'll change our mind as we explore this style further.</p>&#xA;</blockquote>&#xA;&#xA;<p>In your shopping application I think you can have the following services:</p>&#xA;&#xA;<ol>&#xA;<li>Financial service (creating invoices, handling payments, etc..)</li>&#xA;<li>Logistic service (sending products, creating orders, etc..)</li>&#xA;<li>API connecting the services </li>&#xA;<li>Frontend talking to the API</li>&#xA;</ol>&#xA;"
36083665,36080524,3962935,2016-03-18T11:52:50,"<p>Lets say Service A is sending some events to Kafka queue, you can have a background process running with your Rails app which would lookup into the kafka queue and process those queued messages. For background process you can go for cron-job or sidekiq kind of things.</p>&#xA;"
42633198,31044380,1238839,2017-03-06T18:43:22,"<p>Depending on how you're structuring your micro-services, you could also take advantage of composition.</p>&#xA;&#xA;<p>Since you already have an auth service, you could use it within your Billing service to check the authenticity of the request before processing it.</p>&#xA;&#xA;<p>For example, if you're using a platform like <a href=""https://stdlib.com"" rel=""nofollow noreferrer"">StdLib</a> (we use this in-house):</p>&#xA;&#xA;<pre><code>// Billing&#xA;const lib = require('lib');&#xA;&#xA;module.exports = function(params, callback) {&#xA;  lib.user.isAuthenticated(params, function(err, user) {&#xA;    if (err) return callback(err);&#xA;    // Do stuff with user, process billing&#xA;  });&#xA;};&#xA;</code></pre>&#xA;&#xA;<p>This might not be a great idea for you if you're always using HTTP to communicate between the functions (since this would add probably a good 200-300ms to your request). But StdLib loads up a few services within the same area and you can access them essentially like functions which solves for that problem (at least, so far that we've seen).</p>&#xA;"
42634152,36265833,1238839,2017-03-06T19:38:50,"<p>We definitely use semantic versioning within our infrastructure for micro-services. I highly recommend it.</p>&#xA;&#xA;<p>One thing that we've had to do is lock the usage of some critical services to versions of other services they use.</p>&#xA;&#xA;<p>We use <a href=""https://stdlib.com"" rel=""nofollow noreferrer"">StdLib</a> so versioning has been the easier part. I suggest taking a look at their stuff to see how they handle versioning.</p>&#xA;&#xA;<p>But essentially, in let's say our Article service that handles uploading of images, we've locked using 0.0.2 of the formatImage service.</p>&#xA;&#xA;<p>It looks something like this:</p>&#xA;&#xA;<pre><code>const lib = require('lib')({&#xA;  // AWS_CREDENTIALS...&#xA;});&#xA;&#xA;lib.utils.formatImage['0.0.2']({ key: 'image-key', resize: [200, 300] }, ...)&#xA;</code></pre>&#xA;&#xA;<p>That way, we don't have to worry about that service changing.</p>&#xA;&#xA;<p>Though I do think it would be super cool if <code>lib</code> had the ability to read these dependencies off a file or something like <code>package.json</code> in npm. But one can dream.</p>&#xA;"
42635824,37936588,1238839,2017-03-06T21:23:46,"<p>We use some of the stuff <a href=""https://stdlib.com"" rel=""nofollow noreferrer"">StdLib</a> provides and they also have a <a href=""https://stdlib.com/search"" rel=""nofollow noreferrer"">registry</a> for others to submit their own stuff. You should check it out.</p>&#xA;"
42636114,31046924,1238839,2017-03-06T21:42:00,"<p>We solve this by using composition of services to protect services that need authentication. A lot like the way you would approach it if it was a monolith.</p>&#xA;&#xA;<p>As @odino mentioned:</p>&#xA;&#xA;<ul>&#xA;<li>Use an Auth service to authenticate the user and generate a token (we use an OAuth flow for this)</li>&#xA;<li>Subsequent services would then use the token to verify if the user exists by passing it through the Auth service (""composition"")</li>&#xA;</ul>&#xA;&#xA;<p>Here's an example with <a href=""https://stdlib.com"" rel=""nofollow noreferrer"">StdLib</a> (what we use in-house):</p>&#xA;&#xA;<pre><code>const lib = require('lib');&#xA;&#xA;module.exports = function(params, callback) {&#xA;  lib.user.isAuthenticated(params, (err, user) =&gt; {&#xA;    if (err) return callback(err);&#xA;&#xA;    // We're authenticated – do the rest of the work.&#xA;  });&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>The caveat to this is that if you're using HTTP as the protocol to communicate between your services, you're essentially adding a 200-300ms overhead to your authentication. You can solve for this by loading the services that need authentication within the same container (StdLib does this out of the box iirc)</p>&#xA;"
39697158,39696697,1306679,2016-09-26T07:38:50,"<p>just use a <a href=""https://docs.angularjs.org/api/ng/service/$templateCache"" rel=""nofollow""><code>templateCache</code></a> with the <a href=""https://docs.angularjs.org/api/ng/service/$compile"" rel=""nofollow""><code>compile</code></a> service, this way:</p>&#xA;&#xA;<pre><code>app.controller('masterController', ['$scope', '$compile', '$templateCache', function($scope, $compile, $templateCache){&#xA;   $scope.loadOtherHtmlOnDemandAndAppendItAfterMe = function() {&#xA;      var template = $compile($templateCache.get('yourHTMLFile.html'))($scope);&#xA;&#xA;      angular.element('div').append(template); //append your compiled element wherever you want&#xA;   }&#xA;}]);&#xA;</code></pre>&#xA;&#xA;<p>In short: templateCache is a service that grabs an HTML file/script easily and wrap in a variable, and $compile let all dynamic behaviours works (angular bindings, javascript etc.)</p>&#xA;"
45247086,45243781,8193592,2017-07-21T21:39:52,"<p>No need to rebuild your docker image every time, use <a href=""https://docs.docker.com/engine/tutorials/dockervolumes/"" rel=""nofollow noreferrer"">volumes</a> instead.</p>&#xA;&#xA;<p>For example run:</p>&#xA;&#xA;<pre><code>docker run -v /path/to/the/code/on/host:/path/to/the/code/on/container your image nodemon your_file.js&#xA;</code></pre>&#xA;"
38518517,37864255,4007992,2016-07-22T05:02:05,"<p>This is a networking issue. Simply restrict access to micro service B at a network level. This can be easily done if using Docker for example. You would just not publicly expose the relevant port for micro service B but expose it on a specific network then have micro service join that network.</p>&#xA;&#xA;<p>You could use public/private keys if you wanted to add extra security. Alternatively, it would be simpler to generate a JWT for application A and validate it in micro service B but as you add more micro services this has more management overhead.</p>&#xA;&#xA;<p>Alternatively, you should look into an API Gateway which can handle API access for you </p>&#xA;"
44062101,43290480,2698552,2017-05-19T05:30:00,"<p>For your use case, a simple queue based routing from a single broker should suffice. </p>&#xA;&#xA;<p>Keep only 1 broker running in any one server or on a separate server. </p>&#xA;&#xA;<p>Now while queuing up the tasks, add them to separate queues.</p>&#xA;&#xA;<p>From micro service 1:</p>&#xA;&#xA;<pre><code>In [2]: add.apply_async(args=(12, 1), queue='queue1')&#xA;Out[2]: &lt;AsyncResult: 2fa5ca61-47bc-4c2c-be04-e44cbce7680a&gt;&#xA;</code></pre>&#xA;&#xA;<p>Start a worker to consume only this queue</p>&#xA;&#xA;<pre><code>celery worker -A tasks -l info -Q queue1&#xA;</code></pre>&#xA;&#xA;<p>From micro service 2:</p>&#xA;&#xA;<pre><code>In [2]: sub.apply_async(args=(12, 1), queue='queue2')&#xA;Out[3]: &lt;AsyncResult: 4d42861c-737e-4b73-bfa8-6d1e86241d57&gt;&#xA;</code></pre>&#xA;&#xA;<p>Start a worker to consume only this queue</p>&#xA;&#xA;<pre><code>celery worker -A tasks -l info -Q queue2&#xA;</code></pre>&#xA;&#xA;<p>This will make sure that tasks from a microservice will get executed by worker from that microservice only.</p>&#xA;"
38735567,38734740,1915448,2016-08-03T06:01:22,"<p>From the comments it appears your Logback configuration file is within the classpath in <strong>src/main/resources</strong>, so normally you could use:</p>&#xA;&#xA;<pre><code>logging:&#xA;  config: classpath:logback.xml&#xA;</code></pre>&#xA;"
37406120,37405221,1915448,2016-05-24T06:40:09,"<p>The eureka registry is only used for name translation, you cannot actually access a service by using Eureka.</p>&#xA;&#xA;<p>If you want to resolve the names manually, you can do that. Autowire a <code>LoadBalancerClient</code>, and then you can retrieve the actual destination by using <code>balancerClient.choose(""service-id"")</code>.</p>&#xA;&#xA;<p>For example:</p>&#xA;&#xA;<pre><code>ServiceInstance instance = balancerClient.choose(""my-microservice"");&#xA;&#xA;// Retrieve host name or IP address&#xA;instance.getHost();&#xA;&#xA;// Creates an URI, replacing the service ID with the actual hostname&#xA;balancerClient.reconstructURI(instance, new URI(""http://my-microservice/foo/bar"");&#xA;</code></pre>&#xA;&#xA;<p>But like I said, if your goal is to communicate with your services through Eureka, then that's not possible since that's not what Eureka is made for.</p>&#xA;"
44511731,44503321,31288,2017-06-13T03:08:47,"<p>If an Ingress Controller is overkill for your scenario, you may want to try using a service of type <code>NodePort</code>. You can specify the port, or let the system auto-assign one for you.</p>&#xA;&#xA;<p>A <code>NodePort</code> service exposes your service at the same port on all Nodes in your cluster. If you have network access to your Nodes, you can access your service at the node IP and port specified in the configuration.</p>&#xA;&#xA;<p>Obviously, this does not load balance between nodes. You can add an external service to help you do this if you want to emulate what a real load balancer would do. One simple option is to run something like <a href=""https://github.com/h2non/rocky-cli"" rel=""nofollow noreferrer"">rocky-cli</a>.</p>&#xA;"
28897056,28767707,1604152,2015-03-06T10:41:01,"<p>Let me try to reformulate the problem:</p>&#xA;&#xA;<p><strong>Actors:</strong></p>&#xA;&#xA;<ul>&#xA;<li>X: UserIds (state of account)&#xA;<ul>&#xA;<li>provide service to get ID (based on credentials) and status of account</li>&#xA;</ul></li>&#xA;<li>A: UserProfile &#xA;<ul>&#xA;<li>Using X to check status of a user account. Stores name along with link to account</li>&#xA;<li>provide service to get/edit name based on ID</li>&#xA;</ul></li>&#xA;<li>B: UserBlogs &#xA;<ul>&#xA;<li>Using X in same way. Stores blog post along with link to account when user writes one</li>&#xA;<li>Using A to search blog post based on user name</li>&#xA;<li>provide service get/edit list of blog entries based on ID</li>&#xA;<li>provide service to search for blog post based on name (relies on A)</li>&#xA;</ul></li>&#xA;<li>C: MobileApp&#xA;<ul>&#xA;<li>wraps features of  X, A, B into a mobile app</li>&#xA;<li>provide all services above, relying on well-defined communication contract with all others (following @neleus statement) </li>&#xA;</ul></li>&#xA;</ul>&#xA;&#xA;<p><strong>Requirements:</strong></p>&#xA;&#xA;<ol>&#xA;<li>Work of teams X, A, B, C need to be uncoupled</li>&#xA;<li>Integration environments for X, A, B, C need to be updated with latests features (in order to perform integration tests)</li>&#xA;<li>Integration environments for X, A, B, C need to have 'sufficient' set of data (in order to perform load tests, and to find edge cases)</li>&#xA;</ol>&#xA;&#xA;<p>Following @eugene idea: having mocks for each service provided by every team would allow 1) and 2)</p>&#xA;&#xA;<ul>&#xA;<li>cost is more development from the teams</li>&#xA;<li>also maintenance of the mocks as well as the main feature</li>&#xA;<li>impediment is the fact that you have a monolithic system (you do not have a set of clean well defined/isolated services yet)</li>&#xA;</ul>&#xA;&#xA;<p><strong>Suggested solution:</strong></p>&#xA;&#xA;<p>What about having a shared environment with the set of master data to resolve 3)? Every 'delivered services' (i.e running in production) would be avalailable. Each teams could chose which services they would use from here and which one they would use from their own environment</p>&#xA;&#xA;<p>One immediate drawback I can see is the shared states and consistency of data.</p>&#xA;&#xA;<p>Let's consider automated tests ran against the master data, e.g:</p>&#xA;&#xA;<ul>&#xA;<li>B changes names (owned by A) in order to work on its blog service&#xA;<ul>&#xA;<li>might break A, or C</li>&#xA;</ul></li>&#xA;<li>A changes the status of an account in order to work on some permission scenarios&#xA;<ul>&#xA;<li>might break X, B</li>&#xA;</ul></li>&#xA;<li>C changes all of it on same accounts&#xA;<ul>&#xA;<li>breaks all others</li>&#xA;</ul></li>&#xA;</ul>&#xA;&#xA;<p>The master set of data would quickly become inconsistent and lose its value for requirement 3) above.</p>&#xA;&#xA;<p>We could therefore add a 'conventional' layer on the shared master data: anyone can read from the full set, but can only modify the objects that they have created ?</p>&#xA;"
47885884,47885225,3314232,2017-12-19T11:22:32,"<p>Your architecture must be micro-service and you might use a API Gateway at front of your interfaces, I suggest you to use <a href=""https://getkong.org/"" rel=""nofollow noreferrer"">Kong</a> - Open-Source API Management and Microservice Management, that have many features like OAuth2 &#xA;and many other.</p>&#xA;&#xA;<p>Then you can have one or more client-sides that calls one entry point to your services.</p>&#xA;"
40363496,40233978,2669112,2016-11-01T15:41:17,"<p>A microservice can be a complete application including its own UI, business logic and data. </p>&#xA;"
40363720,29830038,2669112,2016-11-01T15:54:13,"<p>Effectively you can use any language / logging work framework. All you need to do is to introduce one more service, the logging service to your family of services. All your services publish the logs to service bus which sequentially route the logs to the logging service for persistence.  </p>&#xA;"
40384519,40377377,2669112,2016-11-02T16:04:46,"<p>I don't really like direct API calls from service A to service B or vice versa for two reasons. Firstly it create dependency between service A and B. Secondly it could easily create a spaghetti sort of messy relationships as the number of services grows. What I would like to see is a pub / sub pattern, e.g. service A publishes a message to the transportation layer (RabbitMQ is not a bad choice) and move on. The subscription and business logic to interpret the message are encapsulated nicely in service B. By doing that, service B does not need to know anything about service A at all and yet they can talk to each other nicely.</p>&#xA;"
40384789,40383122,2669112,2016-11-02T16:18:11,<p>I think it is really down to how or how intensive your services are used. The beauty of microservices architecture is the services are completely independent to each other so you have the freedom to configure your infrastructure the way you want to maximise throughput. The use of docker seems a little bit irrelevant to your question. </p>&#xA;
40425478,40422613,2669112,2016-11-04T14:43:10,<p>To evaluate the pros and cons I think you should focus on what microservices architecture is aiming to achieve. In my opinion Microservices is architectural style aiming to build loosely couple applications. It is not designed to build high performance application so scarification of performance and data redundancy are something we are ready accept when we decided to build applications in a microservices way. </p>&#xA;&#xA;<p>I don't think you services should share database. Tighter coupling scarify the main objective of the microservices architecture. My suggestion is to create a consolidated data service which pick up the data changes events from all the other services and update the database behind it. You might want to design the database behind the consolidated data service in a way that is optimised for query (like a data warehouse) because that's all this service will be used for. You might want to consider using a NoSQL database to support your consolidated data service. </p>&#xA;
40454364,40447582,2669112,2016-11-06T20:46:33,"<p>If my understanding is correct (see my comment in Maksym's replay), your API gateway is similar to a ESB as in SOA. If that's the case, you can borrow a few common patterns in ESB to help address your concern. In ESB, we frequently put in customized adapters to translate message format between services or between services and open standard. In your case, you will need to introduce an extra layer in your API gateway to translate the public API's data models to the data models MS A and B understand and vice versa. By doing that the public API and MS A and B are still loosely coupled. It's true you will have to make changes to the adapters if data models in either MS A and B or the public API changes. But its impact will be minimum as the adapters are so light weight they are designed to be replaced quickly. </p>&#xA;"
40419161,40413453,2669112,2016-11-04T09:28:32,"<p>So your messagebot will </p>&#xA;&#xA;<ol>&#xA;<li>store notifications</li>&#xA;<li>get data from other services</li>&#xA;<li>compile emails from the data and </li>&#xA;<li>send the compiled emails</li>&#xA;</ol>&#xA;&#xA;<p>In my opinion, your messagebot were given too many tasks. If I were designing the system, I would like to keep the messagebot simpler. The servces should encapsulate the knowledge to compile the email, e.g. manage it's own template and so on. The services will push the compiled emails to a queue so the messagebot can pick up and send. The only logic in the messagebot is to pick up the emails from the queue and send. In this way, it doesn't matter how many more services you are going to have in the future, the messagebot will stay nice and simple. </p>&#xA;"
48210031,48209566,8581432,2018-01-11T15:01:50,"<p>I feel like this depends on if the user is supposed to be able to do anything with the system based on their first action, or they expect it to be reflected somewhere else immediately.</p>&#xA;&#xA;<p>Just as an example since you'd normally manage this info in a session instead of writing to a DB: If the first action is ""add product to cart"" and then the user goes on to try and check out, I think the whole request should fail early instead of save a state that can be seen as ""false"" when it fails later and seemed to be working normally at first.</p>&#xA;&#xA;<p>Another example could be getting some kind of receipt or code they then want to use to redeem something offline, but the people in charge of doing this don't see the code as valid yet and again you have this state of both thinking the other is wrong because they have different data.</p>&#xA;&#xA;<p>Now if you just want to collect some data and process it in a different service, maybe it's better to save it as unsent instead of forcing the user to send the data several times or come back at a later time.</p>&#xA;&#xA;<p>With more details about your requirements or domain you could probably make a more educated guess as to which option is a good solution, since they solve different problems.</p>&#xA;"
42120435,34640611,3017785,2017-02-08T18:03:17,"<p>I recently worked on a solution to this very question and premise, refactoring a large monolith into multiple services in an AWS architecture.</p>&#xA;&#xA;<p>There is no right, wrong or definitive <em>how</em> to this question.<br>&#xA;However, we did implement a solution very similar to the one described in the question above.<br>&#xA;I hope this answer can deliver a good sense of direction for someone who's looking at this for the first time.</p>&#xA;&#xA;<p>This is how we went about it...</p>&#xA;&#xA;<h2>What do we need from an API gateway?</h2>&#xA;&#xA;<ol>&#xA;<li>Highly available</li>&#xA;<li>Secure</li>&#xA;<li>Performant</li>&#xA;<li>Authoritative</li>&#xA;<li>Scalable</li>&#xA;</ol>&#xA;&#xA;<h2>Solution 1: <a href=""https://aws.amazon.com/api-gateway/"" rel=""nofollow noreferrer"">AWS API Gateway</a></h2>&#xA;&#xA;<p><strong>pros</strong></p>&#xA;&#xA;<ol>&#xA;<li>Highly available managed solution.</li>&#xA;<li>Don't need to worry about scalability.</li>&#xA;<li>Supports SSL and custom domains.</li>&#xA;<li>Authoritative through lambda and IAM.</li>&#xA;<li>Plays nice with other AWS services.</li>&#xA;<li>Supports API versioning out of the box.</li>&#xA;<li>Easy monitoring with CloudWatch.</li>&#xA;</ol>&#xA;&#xA;<p><strong>cons</strong></p>&#xA;&#xA;<ol>&#xA;<li><s>Traffic can't be routed directly into an internal network (private VPC segment), meaning an additional gateway would be required.</s><br>&#xA;Edit: &#xA;<a href=""https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-api-gateway-supports-endpoint-integrations-with-private-vpcs/"" rel=""nofollow noreferrer"">Amazon API Gateway Supports Endpoint Integrations with Private VPCs.</a> Thanks @Red for mentioning this.</li>&#xA;<li>Slow, our benchmark showed each request through API Gateway added 100-150 ms latency.</li>&#xA;</ol>&#xA;&#xA;<h2>Solution 2: <a href=""https://getkong.org/"" rel=""nofollow noreferrer"">Kong</a></h2>&#xA;&#xA;<p><strong>pros</strong></p>&#xA;&#xA;<ol>&#xA;<li>Scalable, but needs to implemented and managed on our end.</li>&#xA;<li>Supports SSL and custom domains.</li>&#xA;<li>Authoritative through plugins, with solutions for JWT and OAUTH2 already packaged.</li>&#xA;<li>RESTful API for easy integration with our authentication server.</li>&#xA;<li>Extensible, in case we need some custom logic.</li>&#xA;<li>Fast, our benchmark showed each request through Kong added 20-30 ms latency.</li>&#xA;</ol>&#xA;&#xA;<p><strong>cons</strong></p>&#xA;&#xA;<ol>&#xA;<li>Requires management on our end (upgrades, deployment, maintenance).</li>&#xA;<li>In order to achieve HA, requires an additional endpoint, in the form of a load balancer to route traffic to the actual GW(s).&#xA;<br><br></li>&#xA;</ol>&#xA;&#xA;<h2>Implementation</h2>&#xA;&#xA;<p>We decided to go with Kong.<br>&#xA;The major issue with the hosted solution was the inability to route traffic to our private network, where we also host a private DNS zone.<br>&#xA;Additionally, the extensible nature of Kong allowed us to create <a href=""https://getkong.org/docs/0.9.x/plugin-development/custom-logic/"" rel=""nofollow noreferrer"">custom plugins</a> with logic that is relevant to our solutions.<br>&#xA;We work with an <a href=""https://aws.amazon.com/elasticloadbalancing/applicationloadbalancer/"" rel=""nofollow noreferrer"">ALB</a> to round robin between multiple instances of Kong in different AZs in order to achieve redundancy and high availability. <br>&#xA;The API configuration is saved on a Postgres RDS which is also internal and multi AZ.</p>&#xA;&#xA;<p><strong>Flow</strong></p>&#xA;&#xA;<ol>&#xA;<li>Client authenticates against our authentication server. The authentication server is a micro service behind the Kong GW with a publicly exposed upstream.</li>&#xA;<li>Authentication server creates a <a href=""https://getkong.org/docs/0.9.x/getting-started/adding-consumers/"" rel=""nofollow noreferrer"">consumer</a> with a <a href=""https://getkong.org/plugins/jwt/"" rel=""nofollow noreferrer"">JWT</a> for the individual client.</li>&#xA;<li>Authentication server replies with the JWT.</li>&#xA;<li>Client requests access from an API with the JWT, traffic routed via Kong.</li>&#xA;<li>Kong verifies the JWT and routes the request to the micro service with information about the consumer.</li>&#xA;<li>Micro service responds to the client.</li>&#xA;</ol>&#xA;&#xA;<p><strong>Other</strong></p>&#xA;&#xA;<ol>&#xA;<li>Revoking user access is as easy as deleting the token.</li>&#xA;<li>No sensitive information is stored in the JWT claims.</li>&#xA;<li>All services know about each other through a <a href=""http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-dns.html#vpc-private-hosted-zones"" rel=""nofollow noreferrer"">private DNS zone</a>.&#xA;<br><br></li>&#xA;</ol>&#xA;&#xA;<p><strong>Schema:</strong>&#xA;<br><br></p>&#xA;&#xA;<p><a href=""http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-dns.html#vpc-private-hosted-zones"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ffbsy.png"" alt=""Kong Gateway Schema""></a></p>&#xA;"
41769865,41739975,138553,2017-01-20T18:08:49,"<p>An important thing to mention is that microservices do not have to share the same schema, what is usually referred as <a href=""https://martinfowler.com/bliki/IntegrationDatabase.html"" rel=""nofollow noreferrer"">Integration Database antipattern</a>. But you can actually have different schemas in the same relational database as long as each microservice uses its own schema. What is important here is the potential ability to move some microservice data to a different physical server any time easily. It is simpler to deploy and backup 1 database than, let's say, seven or so, meaning this option is a good choice if you just at the very beginning of your project and don't want to spend too much time in managing bunch of databases. But you and your team have to be <strong>very disciplined</strong> to make sure that microservices are talking to data from their own schemas only.</p>&#xA;&#xA;<p>Another thing is coupling. You can't make microservices completely isolated, meaning that they will still depend on each other to a certain degree. Just take a simple example of Product, Order, Shipment services. Those services have to be aware of each other, there is <strong>no way to make them completely separate</strong>. But you can make them <strong>temporally decoupled</strong> by using <a href=""https://www.tigerteam.dk/2014/microservices-its-not-only-the-size-that-matters-its-also-how-you-use-them-part-3/"" rel=""nofollow noreferrer"">certain design strategies</a>. When services are decoupled in this way, each one of them can be unavailable for some time (e.g. redeployment) with no effect on others. </p>&#xA;&#xA;<blockquote>&#xA;  <p>Due to the more monolithic architecture which goes with shared databases a failure can affect many servers since they are all tied together, even a complete system failure can happen due to the coupling.</p>&#xA;</blockquote>&#xA;&#xA;<p>Temporal coupling will have the same effect even if your microservices are not sharing the same database. The simple rule to follow: don't call one microservice from another synchronously, use asynchronous events and commands instead.</p>&#xA;"
41297592,41285879,138553,2016-12-23T08:11:00,"<p>The recommended way of hosting RabbitMQ subscriber is by writing a windows service using something like <a href=""http://topshelf-project.com/"" rel=""nofollow noreferrer"">topshelf</a> library and subscribe to bus events inside that service on its start. We did that in multiple projects with no issues.</p>&#xA;&#xA;<p>If you are using Azure, the best place to host RabbitMQ subscriber is in a ""Worker Role"".</p>&#xA;"
41436024,41408847,138553,2017-01-03T03:32:01,"<p>Converting in-process FileService component to microservice will definitely have advantages as well as disadvantages. You've listed several of them, but most importantly you have to create a cost/benefit analysis matrix applicable to <b>your business and domain</b> specifically. &#xA;There is no ""best practices"" approach here. </p>&#xA;&#xA;<p>Costs:</p>&#xA;&#xA;<ul>&#xA;<li>is it okay for you to increase response times? Because now you will have to transfer files twice: s3 -> fs microservice -> client microservice</li>&#xA;<li>how more likely situation of losing a connection between nodes becomes? </li>&#xA;<li>how big your files are? The unreliable connection between microservices may become a problem?</li>&#xA;<li>how frequently do you need to access those files? Maybe you will lose the ability to have local cache to speed up the process?</li>&#xA;<li>are you okay with implementing and supporting separate auth microservice or you can just whitelist this service in your firewall</li>&#xA;</ul>&#xA;&#xA;<p>Benefits:</p>&#xA;&#xA;<ul>&#xA;<li>you don't have to redeploy all dependent components every time the logic of storing files or doing retries changes. </li>&#xA;<li>you can move to another cloud provider more easily in the future if necessary, again, without redeploying everyting.</li>&#xA;<li>it is reusable in a heterogeneous environment, where other components may be implemented using different technological stacks</li>&#xA;</ul>&#xA;&#xA;<p>Conclusion: </p>&#xA;&#xA;<p>There is no way to answer those questions without actually talking with business people and discussing risks around such transition. </p>&#xA;"
41352045,41329351,138553,2016-12-27T20:48:32,"<p>From what I understood, you have 3 services: User, Post, Video. &#xA;In Post and Video services you need to implement commands that will have username as an argument, but not userId. And because all data in those services correlated by userId, you thinking about calling user service first to ask for related userId.</p>&#xA;&#xA;<p>The answer is: you should save username-userId association in every service that requires this. </p>&#xA;&#xA;<p>In your specific case, both Post and Video services should subscribe to NewUserRegistered event from User service and maintain their own username-userid map. This allows you to avoid an extra call to User service from other services. </p>&#xA;"
40965842,40947247,138553,2016-12-05T01:30:51,"<blockquote>&#xA;  <p>How database comes along with event sourcing? I have read CQRS pattern, but I can not understand how CQRS pattern is related to event store and event objects ?</p>&#xA;</blockquote>&#xA;&#xA;<p>""Query"" part of CQRS instructs you how to create a projection of events, which is applicable in some ""bounded context"", where the database could be used as a means to persist that projection. ""Command"" part allows you to isolate data transformation logic and decouple it from the ""query"" and ""persistence"" aspects of your app. To simply put it - you just project your event stream into the database in many ways (projection could be relational as well), depending on the task. In this model ""query"" and ""command"" have their own way of projecting and storing events data, optimised for the needs of that specific part of the application. Same data will be stored in events and in projections, this will allow achieving simplicity and loose coupling among subdomains (bounded contexts, microservices).</p>&#xA;&#xA;<blockquote>&#xA;  <p>Can any body provide me a complete picture and set of operations happens with all players to gather: CQRS pattern , Event sourcing (including event storage module) and finally different microservices?</p>&#xA;</blockquote>&#xA;&#xA;<p>Have you seen Greg Young's attempt to provide <a href=""https://github.com/gregoryyoung/m-r"" rel=""noreferrer"">simplest possible implementation</a>? If you still confused, consider creating more specific question about his example.</p>&#xA;&#xA;<blockquote>&#xA;  <p>In a system composed of many microservices, should we have one event storage or each microservice has its own ? or both possible ?</p>&#xA;</blockquote>&#xA;&#xA;<p>It is usually one common event storage, but there definitely could be some exceptions, edge cases where you really will need multiple storages for different microservices here and there. It all depends on the business case. If you not sure - most likely you just need a single storage for now. </p>&#xA;&#xA;<blockquote>&#xA;  <p>same question about CQRS. This pattern is implemented in all microservices or only in one ?</p>&#xA;</blockquote>&#xA;&#xA;<p>It could be implemented in most performance-demanding microservices. It all depends on how complex your implementation becomes when you are introducing CQRS into it. If it gets simpler - why not implement it everywhere? But if people in your team become more and more confused by the need to perform more explicit synchronisation between commands and queries parts - maybe cqrs is too much for you. It all depends on your team, on your domain ... there is no single simple answer, unfortunately. </p>&#xA;&#xA;<blockquote>&#xA;  <p>Finally, in case of using microservice architecture, it is mandatory to have only one DB or each Microservice should have its own ?</p>&#xA;</blockquote>&#xA;&#xA;<p>If same microservices sharing same tables - this is usually considered as an antipattern, as it increases coupling, the system becomes more fragile. You can still share the same database, but there should be no shared tables. Also, tables from one microservice better not have FK's to tables in another microservice. Same reason - to reduce coupling. </p>&#xA;&#xA;<p>PS: consider not to ask coarse-grained questions, as it will be harder to get a response from people. Several smaller, more specific questions will have better chance to be answered.</p>&#xA;"
42377847,42353292,138553,2017-02-21T20:58:37,"<p>Pretty much yes. If one microservice ""knows"" about another microservice - it means that they are highly coupled. It doesn't matter where specifically this knowledge about other service is coming from: hardcoded, config file or maybe some service discovery, it still leads to high coupling.</p>&#xA;&#xA;<p>What's important to understand is that many microservice evangelists are just preaching about how to build monolith apps on top of Web APIs. Some of them probably think that the more buzz words they use - the better ... not quite sure why this happens. It is probably easier to fake a language and become an ""expert"" by using buzzword salad instead of really building fault tolerant and horizontally scalable app. </p>&#xA;&#xA;<p>But there is another way to look at service discovery: when it is used by service clients like SPA application or an <a href=""http://microservices.io/patterns/apigateway.html"" rel=""nofollow noreferrer"">API Gateway</a> it may be very useful. Clients and gateways should know about service APIs, otherwise, the whole thing doesn't make sense. But they can use a registry to make it more flexible/dynamic. </p>&#xA;&#xA;<p>So, to summarize: </p>&#xA;&#xA;<ul>&#xA;<li>if services are using discovery to get more information about each other - this is probably a bad thing and design flaw (pretty sure there are corner cases  where this may be a valid scenario, please post a comment if you know some)</li>&#xA;<li>if discovery is used by other parts of the app, like SPA or API Gateway, this may be useful, but not necessarily it always is.</li>&#xA;</ul>&#xA;&#xA;<p>PS: to avoid high coupling, consider reading <a href=""https://www.tigerteam.dk/2014/microservices-its-not-only-the-size-that-matters-its-also-how-you-use-them-part-3/"" rel=""nofollow noreferrer"">series of articles by Jeppe Cramon</a> that illustrate the problem and possible solutions very nicely.</p>&#xA;"
41652757,41636566,138553,2017-01-14T17:15:18,"<p><a href=""https://stackoverflow.com/users/569662/tom-redfern"">Tom</a> suggested a <a href=""https://stackoverflow.com/questions/30213456/transactions-across-rest-microservices"">pretty good link</a>, where the top-voted answer with its reasoning and solution is the one you can rely on. Your specific problem may be rooted in the fact that Register Service and User Service are separate. Maybe they should not be? </p>&#xA;&#xA;<p>Ideally, Register service should publish ""UserRegistered"" event to a bus and return 200 and nothing more. It should not care (know) at all about any subscribers to that event. </p>&#xA;"
41659946,41655915,138553,2017-01-15T10:08:37,"<blockquote>&#xA;  <p>But what if, for example, a power outage occurred in-between Steps 1 and 2</p>&#xA;</blockquote>&#xA;&#xA;<p>Consider the following approach: </p>&#xA;&#xA;<pre><code>using(var scope = new TransactionScope()) &#xA;{&#xA;    _repository.UpdateUser(data);&#xA;    _eventStore.Publish(new UserUpdated { ... });&#xA;    scope.Complete();&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>This pseudocode assumes that you are using something analogous to <a href=""https://www.asp.net/entity-framework"" rel=""nofollow noreferrer"">Entity Framework</a> and <a href=""https://msdn.microsoft.com/en-us/library/system.transactions.transactionscope.aspx"" rel=""nofollow noreferrer"">TransactionScope</a></p>&#xA;&#xA;<p>So even if your event store is implemented as some external service, your UpdateUser transaction will not be committed until event store signals success. &#xA;There is still <strong>small chance of failure</strong> when you've already got a response from the _eventStore but have not committed ORM transaction scope. In this worst case scenario you will end up with a published event but missing data from DB which always stores the latest snapshot of the state. Essentially, the snapshot becomes invalid for this aggregate. </p>&#xA;&#xA;<p>If your domain can not tolerate such risks, <strong>you should not store state/snapshot in the relational database at all</strong>. Event store will be the only source of truth that you can rely on (this is a recommended approach by many CQRS/ES practitioners). </p>&#xA;&#xA;<blockquote>&#xA;  <p>B: Create an Events table in my main database, using database transactions to insert the Event and commit the relevant changes at the same time. The service would then push the Event to the bus, and then make another commit to the main database to mark the Event as Published.</p>&#xA;</blockquote>&#xA;&#xA;<p>This approach will work as well, however, you will have to reinvent the wheel instead of simply reusing some <a href=""https://geteventstore.com/"" rel=""nofollow noreferrer"">bulletproof implementation</a> of the event store.</p>&#xA;&#xA;<p>Options A and C are too exotic/overengineered to seriously consider as viable. </p>&#xA;"
41652982,41631842,138553,2017-01-14T17:39:05,"<p>Yes, you have to use async communication in order to make sure that services are temporally decoupled. Here is a <a href=""https://www.tigerteam.dk/2014/micro-services-its-not-only-the-size-that-matters-its-also-how-you-use-them-part-1/"" rel=""nofollow noreferrer"">good series of articles</a> that explains reasoning behind that design decision in-depth. </p>&#xA;&#xA;<p>Also, consider reading about <a href=""https://adaptechsolutions.net/cqrs-es-or-is-it/"" rel=""nofollow noreferrer"">CQRS/ES approach</a> to design microservices, it was an eye-opener for me when I first discovered it. </p>&#xA;"
41652401,41640621,138553,2017-01-14T16:42:36,"<p>You can consider changing your sync way of b2b communication to the async one using publish-subscribe pattern. In that situation, services operation will be more independent and you may not need to perform b2b requests all the time. </p>&#xA;&#xA;<p>The way you make it faster in distributed system is through denormalization. If ms2data changes rarely, e.g. you read it more than rewrite, you have to duplicate it across services. By doing this you will reduce latency and temporal coupling. Coupling aspect may be even more important than the speed in many situations.</p>&#xA;&#xA;<p>If ms2data is an information about product, then ms2 should publish ProductCreated event containing ms2data to a bus. Ms1 should be subscribed to this event and store ms2data in its own database. Now, anytime ms1 requires ms2data it will just read it locally without a need to perform requests to ms2. This is what temporal decoupling means. When you following this pattern, your solution becomes more fault tolerant and shutting down ms2 will not influence ms1 in any way.</p>&#xA;&#xA;<p>Consider reading a <a href=""https://www.tigerteam.dk/2014/micro-services-its-not-only-the-size-that-matters-its-also-how-you-use-them-part-2/"" rel=""nofollow noreferrer"">good series of articles</a> that describe the problems behind sync communication in microservices architecture.</p>&#xA;&#xA;<p>Related SO questions <a href=""https://stackoverflow.com/questions/41445442/how-to-sync-the-database-with-the-microservices-and-the-new-one/41475346"">here</a> and <a href=""https://stackoverflow.com/questions/30213456/transactions-across-rest-microservices"">here</a> discussing pretty similar problems, consider taking a look.</p>&#xA;"
41656651,41624628,138553,2017-01-15T00:48:19,"<blockquote>&#xA;  <p>If a microservice depends on retrieving data from another microservice, how can it be run in isolation during development?</p>&#xA;</blockquote>&#xA;&#xA;<p>It should be <strong>always</strong> temporally isolated from other services during development and production as well.</p>&#xA;&#xA;<blockquote>&#xA;  <p>For example, what happens when your order service requests product details, but there is nothing to answer that request?</p>&#xA;</blockquote>&#xA;&#xA;<p>This is a place where design flaw reveals itself: order service <strong>should not</strong> request product details from another service. Product details should be stored in the message (event) that order service will be subscribed to. Order service should be getting this message in an asynchronous manner using publish-subscribe pattern and saving it in its own database. Data about the product will be stored in 2 places as the result of that.</p>&#xA;&#xA;<p>Please consider reading <a href=""https://www.tigerteam.dk/2014/micro-services-its-not-only-the-size-that-matters-its-also-how-you-use-them-part-1/"" rel=""nofollow noreferrer"">this series of articles</a> about microservices for more details. But in a nutshell: your services should be temporally decoupled, so when your product service is down - order service can continue its operations without interruptions. This is the key thing to understand about good distributed systems design in general. </p>&#xA;"
41475346,41445442,138553,2017-01-05T00:14:10,"<blockquote>&#xA;  <p>What if I'm going to create a new service? How do I get the registered user informations and put them in the new service?</p>&#xA;</blockquote>&#xA;&#xA;<p>You have to replay all events to which this new service is subscribed from the beginning of time (you should have an ""event store"" that keeps all events already happened in your application). Also, you can put a bit smarter logic when replaying events by starting from the most recent ones and going back in time. In this way, you will be able to restore most valuable data first. Just be careful to handle interdependent events correctly.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Data source: The events are pushed by the messaging queue(NSQ), If I'm going to copy the data from one of the microservices, how do I make sure the copy source has the latest user informations?</p>&#xA;</blockquote>&#xA;&#xA;<p>You are <strong>not</strong> talking about doing backups, right?</p>&#xA;&#xA;<p>Aside from backups, in the event-driven systems people usually don't copy the data in a classical way, row by row. Instead, they just replaying events from event store from the beginning of time and feeding those events to the event handlers for the new service (or new instance). As a result, new service eventually becomes consistent with the other parts of the system. </p>&#xA;"
42060206,42046797,138553,2017-02-06T04:12:05,"<p>Try splitting your system into services over and over until it makes sense. Use your gut feeling. Read more books, articles, forums where other people describing how they did it. </p>&#xA;&#xA;<p>You've mentioned that there is no point of splitting ProductService into Product and ProductSearch - fair enough, try to implement it like that. If you will end up with a pretty complicated schema for some reason or with performance bottlenecks - it's a good sign to continue splitting further. If not - it is fine like that for your specific domain. </p>&#xA;&#xA;<p>Not all product services made equal. In some situations, you have to be able to create millions or even billions of products per day. In this situation, it is most likely that you should consider separating product catalogue and product search. The reason is performance: to make search perform faster (indexing) you have to slow down inserts. These are two mutually exclusive goals that are hard to reach without separating data into different microservices (which will lead to data duplication as well). </p>&#xA;"
50254151,46522304,4467001,2018-05-09T13:05:15,"<p>After few days searching a solution I found this. You should start to add feign form for spring dependency :</p>&#xA;&#xA;<pre><code>&lt;dependency&gt;&#xA;   &lt;groupId&gt;io.github.openfeign.form&lt;/groupId&gt;&#xA;   &lt;artifactId&gt;feign-form-spring&lt;/artifactId&gt;&#xA;   &lt;version&gt;3.3.0&lt;/version&gt;&#xA;&lt;/dependency&#xA;</code></pre>&#xA;&#xA;<p>Then you feign client need a spring form encoder :</p>&#xA;&#xA;<pre><code>@FeignClient(name=""attachment-service"",  configuration = {AttachmentFeignClient.MultipartSupportConfig.class}&#xA; fallback=AttachmentHystrixFallback.class)&#xA;public interface AttachmentFeignClient {&#xA;&#xA;@RequestMapping(value= {""upload""}, consumes = {""multipart/form-data""})&#xA;void upload(@RequestPart(name=""file"") MultipartFile file, @RequestParam(name=""attachableId"") Long attachableId, &#xA;        @RequestParam(name=""className"") String className, @RequestParam(name=""appName"") String appName);&#xA;&#xA; public class MultipartSupportConfig {&#xA;    @Bean&#xA;    @Primary&#xA;    @Scope(""prototype"")&#xA;    public Encoder feignFormEncoder() {&#xA;        return new SpringFormEncoder();&#xA;    }&#xA;  }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>I hope it will help someone.</p>&#xA;"
46237965,46230643,607033,2017-09-15T10:58:47,"<p>At least add some kind of identifier which is known only by the gateway or check the IP address if it is dedicated to the gateway. I hope you are using an encrypted communication protocol. Can't you do this via VPN, SSH tunnel or something more secure?</p>&#xA;"
37266608,37213471,3455033,2016-05-17T03:30:40,"<p>The rule of thumb is breaking the monolith based on <em>bounded context</em> . The most common way of defining the bounded context is using BU ( Business Unit) . For example the module which does actual payment is mostly a separate BU .</p>&#xA;&#xA;<p>The second thing to consider is the overhead micro-services bring. You should analyse the hardware , monitoring , infra pieces before completely breaking the service. What I have seen is people taking smaller microservices out of monolith instead of going and writing say 10 new service and depreciating the monolith. </p>&#xA;&#xA;<p>My advice will be have an incremental approach . Take the first BU which is being worked upon out of monolith. This will also give a goos learning curve for the whole team.   </p>&#xA;"
44457270,44455242,5946937,2017-06-09T12:00:08,"<p>If a service needs to keep its own state, make it Stateful. If your service uses an external store for state, it's probably fine to make a Stateless service. </p>&#xA;&#xA;<p>So unless you need run a workflow that calls several REST services and need to support retries, progress tracking, store intermediate results, etc. it's probably ok to go with a Stateless service.</p>&#xA;"
41890963,41880229,5946937,2017-01-27T09:45:28,"<p>I'd introduce a third (Stateful) service that holds a queue, 'service 3'.&#xA;Service 1 would enqueue the message. Service 3 would run an infinite loop, trying to deliver the message to service 2.</p>&#xA;&#xA;<p>You could use the <a href=""https://www.nuget.org/packages/ServiceFabric.PubSubActors"" rel=""nofollow noreferrer"">pub/sub</a> package for this. Service 1 is the publisher, Service 2 is the subscriber. </p>&#xA;&#xA;<p>(If you rely on an external queue system like Service Bus, you'll lower the overall availability of the system. Service Bus downtime would lead to messages being undeliverable.)</p>&#xA;"
40969635,40960054,5946937,2016-12-05T08:02:09,"<p>(Edit: interpreted question wrong earlier)</p>&#xA;&#xA;<p>I'd say it's a matter of personal preference which model you choose. </p>&#xA;&#xA;<p>You can have a stateless service running on all nodes, receiving messages and processing them on worker threads.</p>&#xA;&#xA;<p>Actors are less able to singlehandedly process lots of messages because of the Single Entry model (limiting multi-threading options). But Actors can come in numbers. You can have many Actors listening for messages. You'd need to ensure those Actors become &amp; stay alive though.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>original answer:</p>&#xA;&#xA;<p>This nuget package does this: <a href=""https://www.nuget.org/packages/ServiceFabric.ServiceBus.Services"" rel=""nofollow noreferrer"">https://www.nuget.org/packages/ServiceFabric.ServiceBus.Services</a>&#xA;It supports queues, topics, batching and sessions.</p>&#xA;"
38500228,38480000,5946937,2016-07-21T09:26:37,"<p><strong>gateway</strong></p>&#xA;&#xA;<p>I would suggest using a gateway pattern here. (<a href=""https://stackoverflow.com/questions/29996484/api-gateway-proxy-pattern-for-microservices-deployed-using-azure-service-fabric?rq=1"">more info</a>)</p>&#xA;&#xA;<p>By applying this pattern you'll have full control over how incoming http requests are routed (based on versioning, tenant, health, etc).</p>&#xA;&#xA;<p><strong>vmss</strong></p>&#xA;&#xA;<p>Putting services on specific VMS'ses will limit the options SF has to balance load on them. One application may use more memory resources, the other may use more CPU. Those could be combined on one node to optimize resource use.</p>&#xA;&#xA;<p>Use the node sets to optimize for cost, so services demanding less resources can be placed on cheaper nodes. (using <a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-cluster-resource-manager-configure-services/"" rel=""nofollow noreferrer"">placement constraints</a>)&#xA;(Likewise for tentants on lower subscriptions)</p>&#xA;"
45566801,45550556,5946937,2017-08-08T11:12:51,"<p>I'd recommend keeping any knowledge about partitioning away from your service consumers, if possible. This way you can change the service internals without changing anything at the consumer-side. </p>&#xA;&#xA;<p>That leads to option 3, combined with the built-in reverse proxy service.&#xA;In that extra service, you can look up the authenticated user and use its location to determine the service partition.</p>&#xA;&#xA;<p>If you make it a new application, it can become the entrypoint (/proxy) for multiple bounded contexts</p>&#xA;"
49098930,49081095,5946937,2018-03-04T18:36:21,"<ol>&#xA;<li>If one service receives messages from 2 topics, there's little waste of resources. Listening for messages is not a very resource intensive process.</li>&#xA;<li>This depends on your application requirements.</li>&#xA;<li>This depends on whether you are using SBMP / <a href=""https://docs.microsoft.com/en-us/dotnet/api/microsoft.servicebus.messaging.transporttype?view=azure-dotnet"" rel=""nofollow noreferrer"">SOAP</a> (default) or <a href=""https://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-amqp-protocol-guide"" rel=""nofollow noreferrer"">AMQP</a> as the communication protocol. <a href=""http://docs.oasis-open.org/amqp-bindmap/amqp-wsb/v1.0/cs01/amqp-wsb-v1.0-cs01.html#_Toc461801616"" rel=""nofollow noreferrer"">AMQP</a> is connection based. SBMP does (long) polling.</li>&#xA;</ol>&#xA;"
48376996,48347363,5946937,2018-01-22T08:01:52,"<p>If possible, consider adding an interface like <code>IDurationProvider</code>, that has a property of type <code>TimeSpan</code> and register a type that implements it instead. This clarifies the developer &amp; usage intent.</p>&#xA;&#xA;<p>If you decide you really need this, this will work:</p>&#xA;&#xA;<pre><code> services.AddTransient(typeof(TimeSpan), _=&gt; TimeSpan.FromSeconds(1D));&#xA; var serviceProvider = services.BuildServiceProvider();&#xA; var timespan = (TimeSpan)serviceProvider.GetRequiredService(typeof(TimeSpan));&#xA;</code></pre>&#xA;"
48276180,48261542,5946937,2018-01-16T07:40:22,"<p><a href=""https://azure.microsoft.com/en-us/resources/samples/service-fabric-watchdog-service/"" rel=""nofollow noreferrer"">Here's</a> an official example of such a <a href=""https://github.com/Azure-Samples/service-fabric-watchdog-service"" rel=""nofollow noreferrer"">watchdog</a> service.</p>&#xA;"
50857967,50856401,5946937,2018-06-14T12:49:58,"<ul>&#xA;<li><a href=""https://stackoverflow.com/questions/37601834/remote-into-sf-nodes-via-rdp"">RDP into</a> the machine and test the endpoint on localhost. If it doesn't work, it's likely an application error.</li>&#xA;<li><p>Verify the firewall settings, for a rule for incoming traffic on port 8939.</p></li>&#xA;<li><p><a href=""https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-monitor-log"" rel=""nofollow noreferrer"">Enable logging</a> on the Azure Load Balancer to see if the health probe detects the endpoint.</p></li>&#xA;</ul>&#xA;"
40588065,40556757,5946937,2016-11-14T11:49:08,"<p>Yes, you can package Java applications on Windows (<a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-cluster-creation-for-windows-server/"" rel=""nofollow noreferrer"">on prem</a>), as <a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-deploy-existing-app/"" rel=""nofollow noreferrer"">guest executables</a> and run them. They won't be able to use the SDK (actors, services, state).</p>&#xA;"
40671890,40671392,5946937,2016-11-18T07:51:19,"<p><code>Test</code> is an async method that is not awaited from <code>Main</code>. So your program doesn't wait for <code>Test</code> to complete.</p>&#xA;&#xA;<p>Adding the <code>GetAwaiter().GetResult()</code> makes your <code>Test</code> method act like a regular method, that blocks until <code>c.Get()</code> has completed.</p>&#xA;&#xA;<p>So use option 2, and remove 'async'. </p>&#xA;&#xA;<p>(And reserve the use of <a href=""https://msdn.microsoft.com/en-us/magazine/jj991977.aspx"" rel=""nofollow noreferrer"">async void</a> to async eventhandlers.)</p>&#xA;"
48794488,48784692,5946937,2018-02-14T19:05:57,"<p>You can pass command line arguments using <code>Arguments</code>&#xA;and (configuration) files to the executable, by setting <code>WorkingFolder</code> in the service manifest.</p>&#xA;&#xA;<pre><code>&lt;EntryPoint&gt;&#xA;   &lt;ExeHost&gt;&#xA;      &lt;Program&gt;node.exe&lt;/Program&gt;&#xA;      &lt;Arguments&gt;bin/www&lt;/Arguments&gt;&#xA;      &lt;WorkingFolder&gt;CodePackage&lt;/WorkingFolder&gt;&#xA;   &lt;/ExeHost&gt;&#xA;&lt;/EntryPoint&gt;&#xA;</code></pre>&#xA;"
44260764,44253950,5946937,2017-05-30T11:16:00,"<p>I recommend <a href=""https://msdn.microsoft.com/en-us/library/aa738697(v=vs.110).aspx"" rel=""nofollow noreferrer"">converting</a> your ASMX service into a WCF service with a BasicHttpBinding. You can then host your WCF service inside a stateless SF service, like shown <a href=""https://github.com/loekd/ServiceFabric.WcfCalc"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;&#xA;<pre><code>private static ICommunicationListener CreateRestListener(StatelessServiceContext context)&#xA;{&#xA;   string host = context.NodeContext.IPAddressOrFQDN;&#xA;   var endpointConfig = context.CodePackageActivationContext.GetEndpoint(""CalculatorEndpoint"");&#xA;   int port = endpointConfig.Port;&#xA;   string scheme = endpointConfig.Protocol.ToString();&#xA;   string uri = string.Format(CultureInfo.InvariantCulture, ""{0}://{1}:{2}/"", scheme, host, port);&#xA;   var listener = new WcfCommunicationListener&lt;ICalculatorService&gt;(&#xA;                    serviceContext: context,&#xA;                    wcfServiceObject: new WcfCalculatorService(),&#xA;                    listenerBinding: new BasicHttpBinding(BasicHttpSecurityMode.None),&#xA;                    address: new EndpointAddress(uri)&#xA;   );&#xA;   return listener;&#xA;}&#xA;</code></pre>&#xA;"
42397131,42394690,5946937,2017-02-22T16:31:09,"<p>You can only use methods in SF remoting. Change the property into a method <code>GetCorrelationId</code> that returns it as <code>Task</code> of <code>int</code>. And add a method: </p>&#xA;&#xA;<pre><code>Task SetCorrelationId( int id){} &#xA;</code></pre>&#xA;&#xA;<p>Or <strong>preferably</strong>, generate it on the caller and pass it as part of the message argument to 'FooAsync' which is better, because it keeps the service stateless.</p>&#xA;"
43233621,43228611,4454969,2017-04-05T14:08:03,<p>Ideally the transactions should be handled at microservice level but at some point a rollback mechanism or 2-phase commit may be necessary for transactions spanning across various services. </p>&#xA;
29804686,29787063,4454969,2015-04-22T17:19:30,"<p>If you are experimenting with Kafka then I assume you want to use a pub/sub messaging tool. You may want to take a look at  <a href=""https://developer.ibm.com/messaging/mq-light/docs/concepts/"" rel=""nofollow"">MQ Light</a> as another possibility.  It can be used for Pub/Sub messaging between components in a microservices architecture.  It can also be deployed in the Bluemix cloud should you later have a volume increase and need to add more instances. </p>&#xA;"
37397771,37395986,4454969,2016-05-23T18:12:07,"<p>The Bluemix <a href=""https://new-console.ng.bluemix.net/docs/services/MessageHub/index.html"" rel=""nofollow"">MessageHub</a> service is based on Kafka and is designed for microservices architecture. </p>&#xA;"
47295268,47295217,4916026,2017-11-14T21:11:48,"<p>Try using,</p>&#xA;&#xA;<pre><code>mvn spring-boot:run -Drun.jvmArguments='-Dserver.port=8081'&#xA;</code></pre>&#xA;"
45738606,34722107,4922426,2017-08-17T14:59:05,"<p>I think that using Services to expose data from a database is ideal due to the flexibility it provides. Development of a REST service to expose some or all of your data as a service provides flexibility to consume the data directly to the UI via AJAX or by other services which can process the data and generate new information. These consumers do not need to implement a DAO and can be in any language. While a REST Service of your entire database is probably not a Micro-Service, a case could be made for breaking this down as Read only for Students, Professors and Classes for exposing on the School Web site(s), with different services for Create, Update and Delete (CUD) available only to the Registrars office desktop applications.</p>&#xA;&#xA;<p>For example building a Service to exposes a statistical value on data will protect the data from examination by a user/program who only needs a statistical value without the requirement of having the service implement an entire DAO for the components of that statistic. Full function databases like SQL Server or Oracle provide a lot of functionality that application developers can use, including complex queries(using indexes), statistics the application of set operations on data. </p>&#xA;"
35570431,35549233,73274,2016-02-23T06:32:10,"<p>Pods are automatically injected with environment variables containing information about services in their namespace.  As an example, say your pod is in a namespace that also contains a service named 'foo'.  The containers in your pod will be injected with:</p>&#xA;&#xA;<pre><code>FOO_SERVICE_HOST=&lt;host service foo is reachable at&gt;&#xA;FOO_SERVICE_PORT=&lt;port service foo is reachable at&gt;&#xA;</code></pre>&#xA;&#xA;<p>as well as docker-links style environment variables.  Check out <a href=""https://github.com/kubernetes/kubernetes/blob/master/docs/user-guide/services.md#discovering-services"" rel=""nofollow"">the docs here</a>.</p>&#xA;"
48955317,48791411,7417402,2018-02-23T19:53:02,"<p>Looks like you want your messages to be partitioned by resource id.</p>&#xA;&#xA;<p>There is a plugin for that: <a href=""https://github.com/rabbitmq/rabbitmq-sharding"" rel=""nofollow noreferrer"">https://github.com/rabbitmq/rabbitmq-sharding</a>&#xA;It will partition a queue into a number of shards by applying consistent hashing to the message routing key. This is done via <a href=""https://github.com/rabbitmq/rabbitmq-consistent-hash-exchange"" rel=""nofollow noreferrer"">Consistent Hash Exchange</a> as I understand.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/9vStM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9vStM.png"" alt=""sharded queues picture""></a></p>&#xA;&#xA;<p>You can use your resource id as a routing key, so <strong>messages for the same resource will end up in the same shard.</strong> Each consumer will be handling messages from one shard and they should be coming in the publishing order. So you should see messages for the same resource coming in order.</p>&#xA;"
44540825,44540545,608058,2017-06-14T09:32:36,"<p>End to end process of registering services to a central place and reaching out to target service using service registry is service discovery. </p>&#xA;&#xA;<p>This is pretty much like using DNS for finding IP address of a site and then reaching that site using the IP address. </p>&#xA;&#xA;<p>I am a user of Kubernetes and it also talks on similar lines: &#xA;<a href=""https://kubernetes.io/docs/concepts/services-networking/service/#discovering-services"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/concepts/services-networking/service/#discovering-services</a></p>&#xA;&#xA;<p>in short, service discovery is not a module with specific role but the steps involved in connecting from serviceA to serviceB end-to-end. </p>&#xA;"
44557017,44495957,608058,2017-06-15T01:14:59,"<p>See kubectl get logs for any obvious errors. In this case, as suspected, it looks like it is insufficient resources problem (or a service that has resource leaks). &#xA;If possible, try increasing resources to see if it helps. </p>&#xA;"
52062758,52062284,88122,2018-08-28T16:42:36,"<p>As you mention ""authenticated"" I will assume you are speaking about OpenID Connect as <a href=""https://ldapwiki.com/wiki/OAuth%202.0%20NOT%20an%20Authentication%20protocol"" rel=""nofollow noreferrer"">OAuth 2.0 NOT an Authentication protocol</a>.</p>&#xA;&#xA;<p>The only true indication for the user id is the ""sub"" value which is dependent on the OpenID Connect Service Provider and which is a <strong>locally unique</strong> and never re-assigned identifier within the <strong>Issuer</strong> for the Authenticated Entity, which is intended to be consumed by the OAuth Client. Two Subject Identifier types are defined by OpenID Connect:</p>&#xA;&#xA;<ul>&#xA;<li>public - provides the same sub (subject) value to all OAuth Clients. It is the default if the provider has no subject_types_supported element in its discovery document. </li>&#xA;<li>pairwise - provides a different Sub value to each OAuth Client, so as not to enable OAuth Client to correlate the End-User's activities without permission.</li>&#xA;</ul>&#xA;&#xA;<p>The OpenID Connect Provider's OpenID Connect Discovery document <em>SHOULD</em> list its supported Subject Identifier types in the subject_types_supported element. &#xA;If there is more than one type listed in the array, the OAuth Client MAY elect to provide its preferred identifier type using the subject_type parameter during Registration.</p>&#xA;&#xA;<p>Sub MUST NOT exceed 255 ASCII characters in length.</p>&#xA;&#xA;<p>Sub value is a Case-sensitive string.</p>&#xA;&#xA;<p>So the sub value is UNIQUE only within the context of a Issuer value.</p>&#xA;"
49509924,49502594,88122,2018-03-27T10:02:20,"<p>In a word no. This would be a <strong>violation</strong> of the user's Trust.&#xA;And in case you have not heard, that is a bad thing.</p>&#xA;&#xA;<p>There is an Internet Draft RFC <a href=""https://tools.ietf.org/html/draft-wdenniss-oauth-incremental-auth-01"" rel=""nofollow noreferrer"">OAuth 2.0 Incremental Authorization</a> (put forth by Google)&#xA;There is some talk about it <a href=""http://ldapwiki.com/wiki/OAuth%202.0%20Incremental%20Authorization"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;"
43239296,42981194,1335354,2017-04-05T18:48:11,"<p>I agree with #5. The two services will have many dependencies since accessing UserService will always relate to AuthService, and they probably access related data.&#xA;If you separate them you may need to separate their shared logic like generating the passport. It can be in a service that has an operation that receives a userID either from AuthService (after validating credentials) or from UserService (after registration).</p>&#xA;&#xA;<p>Also handling the absence of AuthService will raise questions about other scenarios, like if the user already registered and cannot login, how do you handle it?.. If you're showing an error message, why not when they register?..  just a question for more thoughts about your requirements.</p>&#xA;&#xA;<p>Cheers</p>&#xA;"
48841528,48836908,2855738,2018-02-17T12:57:16,"<p>Yes, Logic Apps are primarily Http based services.  Whether or not it's 'micro' really doesn't matter because 'micro' is too abstract to have any real meaning.  It was a useful marketing term at one point but it's tour on the tech fashion runway has ended.  So, don't even think about that. ;)</p>&#xA;&#xA;<p>What the authors are trying to express is that you should <strong>avoid chaining dependencies</strong> in an app's architecture.  A waits for B which waits for C which waits for D which waits for E, etc...  That's the first line in the graphic.</p>&#xA;&#xA;<p>Instead, Basket can check Catalog on it's own, then call Ordering, while Inventory is checked in the background.  You only one level deep instead of 4.</p>&#xA;"
29387411,29387250,368186,2015-04-01T09:39:37,"<p>I'd recommend looking into service discovery. <a href=""https://github.com/Netflix/curator/wiki/Service-Discovery"" rel=""nofollow"">Netflix</a> descibes this as:</p>&#xA;&#xA;<blockquote>&#xA;  <p>A Service Discovery system provides a mechanism for:</p>&#xA;  &#xA;  <ul>&#xA;  <li>Services to register their availability </li>&#xA;  <li>Locating a single instance of a particular service</li>&#xA;  <li>Notifying when the instances of a service change</li>&#xA;  </ul>&#xA;</blockquote>&#xA;&#xA;<p>Packages such as Netflix's Eureka could be of help. (EDIT - actually this looks like it might be AWS specific)</p>&#xA;&#xA;<p>This should work well with continuous delivery as the services can make themselves unavailable, be updated and then register availability again.</p>&#xA;"
42639769,31046924,5153594,2017-03-07T03:41:31,"<p>We are using a slightly different strategy. We have skipped the signing or encryption of JWT which makes our JWT just a Base64 encoded string of our token object and a JWT once created is valid for all our microservices. We have made sure that our micro-services are not accessible directly but only through an Api Gateway. API gateway does the authentication(Http Basic), generates the token, caches the token and validate it for every requests and then pass it as a header while delegating to the micro-service. This micro-service can access another service by just passing the JWT it received from Api Gateway and we are using JWT as just a mechanism to know who the logged in user is.</p>&#xA;&#xA;<p>Each micro-service upon receiving the request checks if JWT is present in header. If present, fetch privileges that are available for the user specified in the token and use them for authorization. Authorization(privileges) make sure that right person is trying to access the resource.</p>&#xA;"
42558000,42549749,5153594,2017-03-02T14:24:16,"<p>A Microservice architecture involves a number of services that may have hierarchical dependencies. For example, Service A depends on Service B and Service B in turn depends on Service C and so on. Again, there can be multiple instances of each services and the location at which they are deployed may change dynamically. For example, Service C is getting too many hits and you may dynamically scale up/down this service.</p>&#xA;&#xA;<p>Usually, an end user cannot access any of these micro-services directly. It can be accessed only through a edge-service whose location is fixed, say www.microservice.com/apis, and will be responsible for doing authentication, rate limiting etc. So, all service calls will go through the edge-service and load balancing at this stage is primarily achieved using a hardware load balancer OR Amazon ELBs.</p>&#xA;&#xA;<p>Now, suppose that a user wants to access Service A. He has to go through edge-service which will delegate the request to Service A. Service A has to call Service B to get some information. In this case, Service A is a client of Service B and Service A need to find all the available instances of B and then make a call to one of these instances. This is where a Microservice Registry like <a href=""https://www.consul.io/"" rel=""nofollow noreferrer"">Consul</a>,<a href=""https://github.com/Netflix/eureka"" rel=""nofollow noreferrer"">Eureka</a> etc comes into picture. Each service instance will be continuously updating its status (IP, Port, Up/Down etc) to the registry. The edge-service will also be using this registry to find the instance location of Service A. </p>&#xA;&#xA;<p>Service A should also make sure that all calls to Service B are load balanced. It is not feasible to use ELBs for load-balancing here because it means you need to have 50 ELBs if you have 50 services. There are software load-balancers that can do this in a more effective manner. For eg: <a href=""https://github.com/Netflix/ribbon"" rel=""nofollow noreferrer"">Ribbon</a> is a good Http client library with load-balancer support written in Java. </p>&#xA;"
50730703,50725904,8236858,2018-06-06T22:51:51,"<p>If you want to have DNS based service discovery to communicate with the server follow the below steps: </p>&#xA;&#xA;<ol>&#xA;<li>Enable kube-dns addon via <code>minikube addons enable kube-dns</code> command. This will enable the service discovery in your kubernetes cluster.</li>&#xA;<li>Make sure that kube-dns addon is enable using <code>minikube addons list</code> command.</li>&#xA;<li>In your client application code change the server URL endpoint to the following : <code>http://lucky-server:8888</code>. ""lucky-server"" is the <strong>metadata.name</strong> property of your Kubernetes server service yaml definition.</li>&#xA;<li>Or else instead of <code>lucky-server</code> you can use fully qualified name <code>lucky-server.default.svc.cluster.local</code> in the server URL since you are deploying your service in default namespace. </li>&#xA;</ol>&#xA;"
50698665,50697247,8236858,2018-06-05T11:12:07,<p>As @suren stated you should specify the target port in the service definition.</p>&#xA;&#xA;<p>And you should change the endpoint URL of the server that client calls to reflect minikube_host_ip. There are couple of ways to achieve that. The naive method would be as follows. </p>&#xA;&#xA;<p>Change your Kubernetes service for the server to have a static Nodeport as follows: </p>&#xA;&#xA;<pre><code>kind: Service&#xA;apiVersion: v1&#xA;metadata:&#xA;  name: lucky-server&#xA;spec:&#xA;  selector:&#xA;    app: lucky-server&#xA;  ports:&#xA;  - protocol: TCP&#xA;    port: 8080&#xA;    nodePort: 32002&#xA;  type: NodePort&#xA;</code></pre>&#xA;&#xA;<p>And in your client code just change the endpoint of the server as follows: &#xA;http://{minikube_host_ip}:32002  Replace your {minikube_host_ip} with the ip address of the minikube host here.</p>&#xA;&#xA;<p>But if you don't want to hard code the minikube ip you can inject it as an environment variable in your Kuberenetes deployment script. And that environment variable should be captured in your docker file.  </p>&#xA;
43143487,43142821,7327034,2017-03-31T14:43:32,"<p>right image, each in own pod.  multi containers in a pod should really only be used when they are highly coupled or needed for support of the main container such as a data loader.</p>&#xA;&#xA;<p>With separate pods, it allows for each service to be updated and deployed independently.  It also allows for more efficient scaling. in the future, you may need 2 or 3 content pods but still only one authorization. if they are all together you scale them all since you don't have a choice with them all together in the same pod.  </p>&#xA;"
42683753,34158847,3279163,2017-03-08T23:14:50,"<p>I know this question is 1 year old, but I would like to add my answer to the first point.</p>&#xA;&#xA;<p>One option would be to use some form of CQRS and store on the OrderDB also some of the user details when creating an order. This way when you have to show the list of orders you already have all the details you need. Also, the order document would represent a photograph of the user state at the moment of the order creation.</p>&#xA;&#xA;<p>Of course, in case you don't have the user details when storing the order, you just need to make a GET call to the User Service, but that would be 1 call, not 100.</p>&#xA;"
51784069,51783877,2172731,2018-08-10T10:06:21,"<p>Spring Cloud Ribbon talks to discovery client for getting information about running instances of a given service. Discovery client keeps an in-memory cache of eureka registrations, to make lookup faster.</p>&#xA;&#xA;<p>You can have a look at the Spring Cloud Netflix documentation for more information:</p>&#xA;&#xA;<p><a href=""https://cloud.spring.io/spring-cloud-netflix/multi/multi_spring-cloud-eureka-server.html"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-netflix/multi/multi_spring-cloud-eureka-server.html</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>The Eureka server does not have a back end store, but the service instances in the registry all have to send heartbeats to keep their registrations up to date (so this can be done in memory). Clients also have an in-memory cache of Eureka registrations (so they do not have to go to the registry for every request to a service).</p>&#xA;</blockquote>&#xA;&#xA;<p>Also, for Ribbon with Eureka:&#xA;<a href=""https://cloud.spring.io/spring-cloud-netflix/multi/multi_spring-cloud-ribbon.html#_using_ribbon_with_eureka"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-netflix/multi/multi_spring-cloud-ribbon.html#_using_ribbon_with_eureka</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>When Eureka is used in conjunction with Ribbon (that is, both are on the classpath), the ribbonServerList is overridden with an extension of DiscoveryEnabledNIWSServerList, which populates the list of servers from Eureka. </p>&#xA;</blockquote>&#xA;"
41278185,40238733,736571,2016-12-22T07:50:42,"<p>Use an external address in your <a href=""https://www.consul.io/docs/agent/services.html"" rel=""nofollow noreferrer"">service definition</a>.  For example, suppose you had an external LDAP server that you wanted to run a TCP check against:</p>&#xA;&#xA;<pre><code>{&#xA;  ""service"": {&#xA;    ""name"": ""ldap"",&#xA;    ""port"": 4432,&#xA;    ""checks"": [&#xA;      {&#xA;        ""tcp"": ""my-ldap.example.org:4432"",&#xA;        ""interval"": ""10s""&#xA;      }&#xA;    ]&#xA;  }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>You could then query consul at <code>ldap.service.consul</code>.  Be sure to look into prepared queries and the <code>nearest</code> attribute.</p>&#xA;"
41278269,40010594,736571,2016-12-22T07:56:23,"<p>Along with what janisz said, you can also run your services in Nomad and Nomad will automatically register and deregister your services for you.  See the <a href=""https://www.nomadproject.io/docs/service-discovery/index.html"" rel=""nofollow noreferrer"">Nomad Service Discovery</a> docs for additional details.</p>&#xA;"
41277617,41247817,736571,2016-12-22T07:13:39,"<p>Wait!  Please don't do this!</p>&#xA;&#xA;<p>DO. NOT. RUN. <code>CONSUL</code>. AS. ROOT.</p>&#xA;&#xA;<p>Please.  You can, but don't.  Instead do the following:</p>&#xA;&#xA;<ol>&#xA;<li>Run a caching or forwarding DNS server on your VMs.  I'm bias toward <code>dnsmasq</code> because of its simplicity and stability in the common case.</li>&#xA;<li>Configure <code>dnsmasq</code> to forward the TLD <code>.consul</code> to the <code>consul</code> agent listening on <code>127.0.0.1:8600</code> (the default).</li>&#xA;<li>Update your <code>/etc/resolv.conf</code> file to point to <code>127.0.0.1</code> as your nameserver.</li>&#xA;</ol>&#xA;&#xA;<p>There are a few ways of doing this, and the official docs have a write up that is worth looking into:</p>&#xA;&#xA;<p><a href=""https://www.consul.io/docs/guides/forwarding.html"" rel=""nofollow noreferrer"">https://www.consul.io/docs/guides/forwarding.html</a></p>&#xA;&#xA;<p>That should get you started.</p>&#xA;"
52058449,52057828,3561275,2018-08-28T12:53:46,"<p>Another really good resources aside from what others have mentioned is: <a href=""https://12factor.net/"" rel=""nofollow noreferrer"">https://12factor.net/</a></p>&#xA;&#xA;<p>It doesn't talk about any implementations (so you wont find references to .NET Core in there) but it does talk about how to design your application so it behaves more like a microservice - including dealing with scaling and processes that are stateless.</p>&#xA;&#xA;<p>And to clarify a point of confusion: there's no special application type for a microservice. If your application behaves like a microservice, it is a microservice!</p>&#xA;"
49198499,49198200,103739,2018-03-09T16:53:01,"<p>I think the best solution for you is to distance yourself from the managing the uploads, especially if scale is a concern of yours.</p>&#xA;&#xA;<p>In my opinion, I think using S3 is your best option, specifically levering it's POST object callbacks, <a href=""https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingHTTPPOST.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingHTTPPOST.html</a></p>&#xA;&#xA;<p>By doing this you can have users upload documents directly to S3 rather than having your server be the middleman, then you can get the file and manipulate it after your server is notified of the objects presence in the S3 bucket.</p>&#xA;&#xA;<p>When it comes to downloading, you should also just serve the files directly from S3. You can use signed requests along with CloudFront to prevent the user from poking around your S3 buckets. You will still have to check a DB for permissions for the file in question, there is no way around this unless you somehow embed the permissions in the users session or something else.</p>&#xA;"
51303355,51300620,7188539,2018-07-12T10:31:55,"<p>You have the private repository I guess.&#xA;If it is Private repository then pull requires the docker login credentials.&#xA;Follow this link: <a href=""https://ropenscilabs.github.io/r-docker-tutorial/04-Dockerhub.html"" rel=""nofollow noreferrer"">https://ropenscilabs.github.io/r-docker-tutorial/04-Dockerhub.html</a></p>&#xA;&#xA;<p>With this you are able to pull the Docker Image.</p>&#xA;"
50310125,49875284,2143547,2018-05-12T19:48:04,"<p>That is possible. The configuration would go to the application.yml file (see <a href=""https://springframework.guru/using-yaml-in-spring-boot-to-configure-logback"" rel=""nofollow noreferrer"">https://springframework.guru/using-yaml-in-spring-boot-to-configure-logback</a>). Having many services to configure this may not be a solution for you as the configuration gets mangled with each services configuration. </p>&#xA;&#xA;<p>To keep things separated you could use groovy (<a href=""https://springframework.guru/logback-configuration-using-groovy"" rel=""nofollow noreferrer"">https://springframework.guru/logback-configuration-using-groovy</a>), if you have it, or try to incorporate a logback.yml via <code>external configuration files</code> (<a href=""https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-external-config.html"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-external-config.html</a>) which I have not used yet.</p>&#xA;"
50081486,50041559,187195,2018-04-28T22:07:31,"<p>The general approach is to replicate this data. Each microservice has a copy of the data it needs to it's job well. If you're thinking this makes your solution more complicated, you're right, that is the tradeoff to get the benefits of independently releasable, isolated services.</p>&#xA;&#xA;<p>Something else to consider is that if you have N microservices that need more than references to the same bits of data, it's likely you'd be best served keeping those use cases together in a single service, rather than breaking it up. Fred George has a good talk about watching out for simply trying to split your microservices into 'entity services':</p>&#xA;&#xA;<p><a href=""https://www.youtube.com/watch?v=vs_XiP5Lkgg"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=vs_XiP5Lkgg</a></p>&#xA;&#xA;<p>I would suggest not allowing more than one independently deployed service to read and write the same data, because you will end up with a coupling preventing e.g. schema migrations of that data without potential risks.</p>&#xA;"
31912466,31912285,2090555,2015-08-10T05:55:29,"<p>Why not create build chains and do a right configuration of upstream and downstream projects? Ideally all these should be built independently. A and B are not dependent, so should be the first one to be built. Then C and in the last D.</p>&#xA;&#xA;<p>So, A and B are upstream projects and C is configured as the downstream project and A and B's config.</p>&#xA;&#xA;<p>Similarily for D-  A,B and C should be configured as upstream projects.</p>&#xA;&#xA;<p>Regarding query, </p>&#xA;&#xA;<blockquote>&#xA;  <p>Is this OKAY i.e. C being built with v1 (version 1) of A and B &amp; D gets built with v1 of A, v2 of B and v1 of C ??</p>&#xA;</blockquote>&#xA;&#xA;<p>Well as far as I understand maven will keep the later versions of B (out of v1 and v2) when creating the war of D. If war contains multiple versions of B, then its a bigger issue related to classpaths and you need to have just one and right version of B.</p>&#xA;&#xA;<p>Rest all processes are standard dev practices, no deviations experienced till date.</p>&#xA;&#xA;<p>Just one thing to consider, lets say if you want to release C, then you should be firstly releasing A and B.</p>&#xA;"
51207008,51206924,4187707,2018-07-06T09:17:29,<p>You can see the status and when it was created by typing <code>docker ps</code></p>&#xA;
45553062,45551966,5040443,2017-08-07T18:03:10,"<blockquote>&#xA;  <p>But one service can not interact with another service since they are not on the same machine and tcp://localhost:61001 will obviously not work.</p>&#xA;</blockquote>&#xA;&#xA;<p>Actually, they can. You are right that <code>tcp://localhost:61001</code> will not work, because using <code>localhost</code> within a container would be referring to the container itself, similar to how <code>localhost</code> works on any system by default. This means that your services cannot share the same host. If you want them to, you can use one container for both services, although this really isn't the best design since it defeats one of the main purposes of Docker Compose.</p>&#xA;&#xA;<p>The ideal way to do it is with docker-compose links, the guide you referenced shows how to define them, but to actually <em>use</em> them you need to use the linked container's name in URLs as if the linked container's name had an IP mapping defined in the original container's <code>/etc/hosts</code> (not that it actually does, but just so you get the idea). If you want to change it to be something different from the name of the linked container, you can use a link alias, which are explained in the same guide you referenced.</p>&#xA;&#xA;<p>For example, with a <code>docker-compose.yml</code> file like this:</p>&#xA;&#xA;<pre><code>a:&#xA;  expose:&#xA;    - ""9999""&#xA;b:&#xA;  links:&#xA;    - a&#xA;</code></pre>&#xA;&#xA;<p>With <code>a</code> listening on <code>0.0.0.0:9999</code>, <code>b</code> can interact with <code>a</code> by making requests from within <code>b</code> to <code>tcp://a:9999</code>. It would also be possible to shell into <code>b</code> and run</p>&#xA;&#xA;<pre><code>ping a&#xA;</code></pre>&#xA;&#xA;<p>which would send ping requests to the <code>a</code> container from the <code>b</code> container.</p>&#xA;&#xA;<p>So in conclusion, try replacing <code>localhost</code> in the request URL with the literal name of the linked container (or the link alias, if the link is defined with an alias). That means that </p>&#xA;&#xA;<pre><code>tcp://&lt;container_name&gt;:61001&#xA;</code></pre>&#xA;&#xA;<p>should work instead of</p>&#xA;&#xA;<pre><code>tcp://localhost:61001&#xA;</code></pre>&#xA;&#xA;<p>Just make sure you define the link in <code>docker-compose.yml</code>.</p>&#xA;&#xA;<p>Hope this helps</p>&#xA;"
41453584,41431506,5043003,2017-01-03T22:49:01,<p>Try this config and let me know if this works out for you. I think you will have to define a global <code>strip-prefix:true</code> like below. Actually it should also work without strip prefix since by default it will strip both the prefix.</p>&#xA;&#xA;<pre><code>zuul:&#xA; prefix: /gateway&#xA; strip-prefix: true&#xA;   routes:&#xA;     microservice1:&#xA;        path: /microservice1/**&#xA;        serviceId: microservice1&#xA;        strip-prefix: true&#xA;     microservice2:&#xA;        path: /microservice2/**&#xA;        serviceId: microservice2&#xA;        strip-prefix: true&#xA;</code></pre>&#xA;
35589256,35585519,5043003,2016-02-23T22:09:39,"<p>The fix seems to be using the ANGEL.SR6 spring cloud version instead of BRIXTON.  But In case you want to use Brixton this is what i wrote to override the CORS filter in Zuul.</p>&#xA;&#xA;<pre><code> @Bean&#xA; public CorsFilter corsFilter() {&#xA;     final UrlBasedCorsConfigurationSource urlBasedCorsConfigurationSource= new UrlBasedCorsConfigurationSource();&#xA;     final CorsConfiguration corsConfig = new CorsConfiguration();&#xA;     corsConfig.setAllowCredentials(true);&#xA;     corsConfig.addAllowedOrigin(""*"");&#xA;     corsConfig.addAllowedHeader(""*"");&#xA;     corsConfig.addAllowedMethod(""OPTIONS"");&#xA;     corsConfig.addAllowedMethod(""HEAD"");&#xA;     corsConfig.addAllowedMethod(""GET"");&#xA;     corsConfig.addAllowedMethod(""PUT"");&#xA;     corsConfig.addAllowedMethod(""POST"");&#xA;     corsConfig.addAllowedMethod(""DELETE"");&#xA;     corsConfig.addAllowedMethod(""PATCH"");&#xA;     urlBasedCorsConfigurationSource.registerCorsConfiguration(""/**"", corsConfig);&#xA;     return new CorsFilter(urlBasedCorsConfigurationSource);&#xA; }&#xA;</code></pre>&#xA;"
42319492,42311050,5043003,2017-02-18T19:08:49,"<p>If you are using springBoot application you will need the annotaion @SpringBootApplication thats why that annotation is there on the project you are seeing. @EnableConfigServer is when you are using the spring-cloud config server it is used to externalize the configuration properties but since you have the application.yml inside the project so you donot need that either.</p>&#xA;&#xA;<p>I am thinking you have a spring boot application for both Microservices and the Eureka server. You need to annotate the eureka main class with </p>&#xA;&#xA;<pre><code>@SpringBootApplication&#xA;@EnableEurekaServer&#xA;@EnableDiscoveryClient&#xA;&#xA;public class EurekaServerApplication {&#xA;&#xA;    public static void main(String[] args) {&#xA;        SpringApplication.run(EurekaServerApplication.class, args);&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Additionally you need annotate you microservice's main class with..</p>&#xA;&#xA;<pre><code>@SpringBootApplication&#xA;@EnableDiscoveryClient&#xA;public class MicroApplication {&#xA;&#xA;    public static void main(String[] args) {&#xA;        SpringApplication.run(MicroApplication.class, args);&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Since you donot have you application.yml file in the question here is what you need.</p>&#xA;&#xA;<p>You need the below configuration in application.yml of the microservices.</p>&#xA;&#xA;<pre><code>eureka:&#xA;  client:&#xA;    serviceUrl:&#xA;      defaultZone: ${eurekaurl:http://localhost:8761/eureka/} &#xA;</code></pre>&#xA;&#xA;<p>In the Eureka Server application.yml file I have this in mine. you might need to tweak it based on what you want.</p>&#xA;&#xA;<pre><code>info:&#xA;  component: Registry Server&#xA;&#xA;server: &#xA;  port: ${port:8761}&#xA;&#xA;&#xA;eureka:&#xA;  client:&#xA;    registerWithEureka: false&#xA;    fetchRegistry: false&#xA;  server:&#xA;    enable-self-preservation: false&#xA;    waitTimeInMsWhenSyncEmpty: 0&#xA;  instance:&#xA;    hostname: localhost&#xA;    lease-expiration-duration-in-seconds: 15&#xA;    lease-renewal-interval-in-seconds: 5&#xA;</code></pre>&#xA;"
38834352,38764797,5043003,2016-08-08T16:34:11,"<p>To read the authorization header you will need to create a filter in ZUUL my thought is you will need a pre filter you can change it based on your need. Here is what you will need.</p>&#xA;&#xA;<pre><code>public class TestFilter extends ZuulFilter {&#xA;&#xA;@Override&#xA;public boolean shouldFilter() {&#xA;&#xA;    return true;&#xA;}&#xA;&#xA;@Override&#xA;public Object run() {&#xA;&#xA;    final RequestContext ctx = RequestContext.getCurrentContext();&#xA;    final HttpServletRequest request = ctx.getRequest();&#xA; //Here is the authorization header being read.&#xA;    final String xAuth = request.getHeader(""Authorization"");&#xA; //Use the below method to add anything to the request header to read downstream. if needed.&#xA;    ctx.addZuulRequestHeader(""abc"", ""abc""); &#xA;&#xA;    return null;&#xA;}&#xA;&#xA;@Override&#xA;public String filterType() {&#xA;&#xA;    return ""pre"";&#xA;}&#xA;&#xA;@Override&#xA;public int filterOrder() {&#xA;&#xA;    return 1;&#xA;}&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>You will need a <code>@Bean</code> declaration for Filter in the class where you have <code>@EnableZuulProxy</code> </p>&#xA;&#xA;<pre><code>@Bean&#xA;public TestFilter testFilter() {&#xA;    return new TestFilter();&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Hope this helps.!!!</p>&#xA;"
39841785,36461493,5043003,2016-10-03T23:08:21,"<p>We finally got this working [Coded by one of my colleague]:-</p>&#xA;&#xA;<pre><code>public class CustomErrorFilter extends ZuulFilter {&#xA;&#xA;    private static final Logger LOG = LoggerFactory.getLogger(CustomErrorFilter.class);&#xA;    @Override&#xA;    public String filterType() {&#xA;        return ""post"";&#xA;    }&#xA;&#xA;    @Override&#xA;    public int filterOrder() {&#xA;        return -1; // Needs to run before SendErrorFilter which has filterOrder == 0&#xA;    }&#xA;&#xA;    @Override&#xA;    public boolean shouldFilter() {&#xA;        // only forward to errorPath if it hasn't been forwarded to already&#xA;        return RequestContext.getCurrentContext().containsKey(""error.status_code"");&#xA;    }&#xA;&#xA;    @Override&#xA;    public Object run() {&#xA;        try {&#xA;            RequestContext ctx = RequestContext.getCurrentContext();&#xA;            Object e = ctx.get(""error.exception"");&#xA;&#xA;            if (e != null &amp;&amp; e instanceof ZuulException) {&#xA;                ZuulException zuulException = (ZuulException)e;&#xA;                LOG.error(""Zuul failure detected: "" + zuulException.getMessage(), zuulException);&#xA;&#xA;                // Remove error code to prevent further error handling in follow up filters&#xA;                ctx.remove(""error.status_code"");&#xA;&#xA;                // Populate context with new response values&#xA;                ctx.setResponseBody(“Overriding Zuul Exception Body”);&#xA;                ctx.getResponse().setContentType(""application/json"");&#xA;                ctx.setResponseStatusCode(500); //Can set any error code as excepted&#xA;            }&#xA;        }&#xA;        catch (Exception ex) {&#xA;            LOG.error(""Exception filtering in custom error filter"", ex);&#xA;            ReflectionUtils.rethrowRuntimeException(ex);&#xA;        }&#xA;        return null;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
43727481,43088918,5043003,2017-05-01T22:35:39,"<p>A couple of places for you to take a look at:-</p>&#xA;&#xA;<p><a href=""https://start.spring.io/"" rel=""nofollow noreferrer"">https://start.spring.io/</a> -This will help you generate the project quick and fast.</p>&#xA;&#xA;<p><a href=""https://github.com/spring-cloud-samples"" rel=""nofollow noreferrer"">https://github.com/spring-cloud-samples</a> - Here you could find the samples to look at.</p>&#xA;&#xA;<p>Also, learn a little bit on spring-boot since you want to use spring-cloud. Here is the link <a href=""https://projects.spring.io/spring-boot/"" rel=""nofollow noreferrer"">https://projects.spring.io/spring-boot/</a></p>&#xA;&#xA;<p>For the question on your application.yml vs application.properties it's up to you what you want to use. The properties/yml file go in the resource folder in case you get confused. In case cloud-config is being used then it is externalised though the bootstrap.yml stays in the resource folder. The samples link should give you enough examples to understand the structure or else create a project using the first link which comes with the structure then you can play with it.</p>&#xA;&#xA;<p>I hope this helps.</p>&#xA;"
41120469,41036545,7215932,2016-12-13T11:51:52,"<p>Add @JsonView to your POJO, like this:</p>&#xA;&#xA;<pre><code>public class Task {&#xA;    @JsonView({ServiceView.class})&#xA;    public double operand1;&#xA;    @JsonView({ServiceView.class})&#xA;    public double operand2;&#xA;    @JsonView({ServiceView.class})&#xA;    public double multiplicationResult;&#xA;&#xA;    public Object otherField;&#xA;&#xA;   public interface ServiceView{}&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>and then send it to your service using <code>com.fasterxml.jackson.databind.ObjectMapper</code>, like this:</p>&#xA;&#xA;<pre><code>    myService.send(new ObjectMapper().writerWithView(ServiceView.class)&#xA;                                     .writeValueAsString(task));&#xA;</code></pre>&#xA;"
44202052,44167052,2176572,2017-05-26T12:47:46,"<p>JVM naturally pools HTTP connections for the <code>HttpURLConnection</code> (via <a href=""http://docs.oracle.com/javase/8/docs/technotes/guides/net/http-keepalive.html"" rel=""nofollow noreferrer"">http://docs.oracle.com/javase/8/docs/technotes/guides/net/http-keepalive.html</a>). So, it should be happening for JAX-WS and JAX-RS out of the box. Usually, other non-<code>HttpURLConnection</code> based frameworks (like netty) support http connection pooling as well. So it's very likely you don't need to worry about this by yourself in your code. You need to calculate how many connections you would need to pool though, but it's a configuration sort of thing.</p>&#xA;&#xA;<p>You could check that TCP connections are not closed after getting an HTTP response by sniffing traffic from you application by <code>tcpdump</code> or <code>Wireshark</code> and checking if there is no TCP FIN happening after you get the result.</p>&#xA;"
26377936,25812816,4144700,2014-10-15T08:30:08,"<p>Inspired by praveenag's answer, I dug into Jetty's code.</p>&#xA;&#xA;<p>If you start DropWizard providing:</p>&#xA;&#xA;<pre><code>-DSTOP.PORT=xxxx -DSTOP.KEY=my_secret_key&#xA;</code></pre>&#xA;&#xA;<p>as Java options,</p>&#xA;&#xA;<p>It tells Jetty to listen on that port for a stop request.</p>&#xA;&#xA;<p>You can then write to that socket to tell Jetty to shutdown. I did this from R like this:</p>&#xA;&#xA;<pre><code>socket = make.socket(""localhost"", 8082)&#xA;write.socket(socket, ""my_secret_key\r\n"")&#xA;write.socket(socket, ""stop\r\n"")&#xA;close.socket(socket)&#xA;</code></pre>&#xA;&#xA;<p>I guess you can do the same from any other language.</p>&#xA;"
42212607,42199241,5281340,2017-02-13T20:05:55,"<p>There are few options:</p>&#xA;&#xA;<ul>&#xA;<li><p>if you have not too many microservices, they are beefy, with alot of ACL/scoping logic in inter-service communication, nothing is better than good old REST via HTTP/1.1. It is default most basic language of the web, and any language have loads of good REST-connectors, which makes development of the communication breeze.</p></li>&#xA;<li><p>if there are many services, message persistence required, and distributed transactions of any sort are unavoidable then the choice is powerful u-boat message-bus like RabbitMQ (AMQP). Any other messaging protocols/queues could be used (ZeroMQ/via Redis) - and the choice depends what traits are mandatory for your system ISC.  </p></li>&#xA;<li><p>IPC (via TCP or UDP), as mentioned above - the prefference over this option depends on the language in which your services written, as doing this in some languages is easer than in others , and this require most of the code written (and if there few languages in your system - this will need to be written in each of them)</p></li>&#xA;<li><p>Sockets, as mentioned above.</p></li>&#xA;</ul>&#xA;&#xA;<p>My personal favorites are top 2 - one offers simplicity and familiarity, other speed and robustness and control.</p>&#xA;"
42264997,36291878,5281340,2017-02-16T04:30:20,"<p>Normally, API Gateway is integral part of any MS system. &#xA;All the services encapsulated and should be not accessible without API Gateway.</p>&#xA;&#xA;<p>Such encapsulation allows direct communication between the services, without providing the requester payload, which should be required if the request comes straight from API Gateway. </p>&#xA;&#xA;<p>In that case the request threated as something different, and follows different logic/middleware pipeline. No additional special users needed.</p>&#xA;"
42314386,42281634,5281340,2017-02-18T11:11:58,"<p>Applying any model/domain logic on your API-gateway is bad decision, and considered as bad practice. API Gateway should only do your systems's CAS (with relying onto Auth service which holds the logic), And convert incoming external requests into inner system requests (different headers/ requester payload data) and proxy formatted requests to services for any other work, recieves them, cares about encapsulating errors, and presents every response in proper external form. </p>&#xA;&#xA;<p>Another point, if there is alot of joins between two models required for application core flow (validation/scoping etc) then perhaps you should reconsider to which Business Domain your models/services are bound. If it's same BD perhaps they should be together. Priciples of Domain-Driven-Design helped me to understand where real boundaries between micro-services are.</p>&#xA;&#xA;<p>If you work with loopback (like we are and face same problem we faced - that loopback have no proper join implementation) you can have separate Report/Combined data service, which is only one who can access to all the service databases and does it only for READ purposes - i.e. queries. Provide it with separately set-up read-only wide access to the db - instead of having only one datasource being set up (single database) it should be able to read from all the databases which are in scope of this query-join db user.</p>&#xA;&#xA;<p>Such service should able to generate proper joins with expected output schema from configuration json - like loopback models (thats what I did in same case). Once abstraction is done it's pretty simple to build/add any equery with any complex joins. It's clean, and it's easy to reason about. Also, it's DBA friendly. For me such approach worked well so far. </p>&#xA;"
46163153,46153057,5281340,2017-09-11T19:32:16,"<p>There is simple approach - To have UI gateway. Gateway that instead of API calls will route and proxy asset request calls (static files serving). </p>&#xA;&#xA;<p>If entities belong to the same bounded context (<code>pay.acme.com frontend &lt;-&gt; pay.acme.com backend</code>) they should definitely exist as single backend microservice that serves one of:</p>&#xA;&#xA;<ul>&#xA;<li>isomorphic js </li>&#xA;<li>thin client apps with real-time reload (then UI gateway will need to proxy <code>ws</code> connection)</li>&#xA;<li>thick client app (SPA) </li>&#xA;</ul>&#xA;&#xA;<p>Such microservice is regular microservice which API (if exists) should be accessible through API gateway, and UI should be proxied via UI proxy/gateway.</p>&#xA;&#xA;<p>Hope that helps. </p>&#xA;"
42722392,42595302,5281340,2017-03-10T15:47:03,"<p>The first approach is not microservice way, by definition.</p>&#xA;&#xA;<p>And yes, idea is to split - each service for Bounded Context - One for Users, one for Inventory, Todo things etc etc.</p>&#xA;&#xA;<p>The idea of microservices, at very simple, assumes:</p>&#xA;&#xA;<ol>&#xA;<li>You want to pay extra dev-ops work for modularity, and complete/as much as possible removal of dependencies between different bounded contexts (see dev/product/pjm teams).</li>&#xA;<li>It's idea lies around ownership, modularity, allowing separate teams develop their own piece of code, without requirement from them to know the rest of the system . As long as there is Umbiqutious Language (common set of conventions/communication protocols/terminology/documentation) they can work in completeley isolated, autotonmous fashion.</li>&#xA;<li>Maintaining, managing, testing, and develpoing become much faster - in cost of initial dev-ops and sophisticated architecture engeneering investment.</li>&#xA;<li>Sharing code should be minimal, and if required, could be done to represent the Umbiqutious Language (common communication interface/set of conventions). Sharing well-documented code, which acts as integration/infrastructure mini-framework, and have special dev/dev-ops/team attached to it ccould be easy business, as long as it, as i said, well-documented, and threated as separate architecture-related sub-project.</li>&#xA;</ol>&#xA;&#xA;<p>Properly engeneered Microservice architecture could lessen maintenance and development times by huge margin, but it requires quite serious reason to use it (there lot of reasons, and lots of articles on that, I wont start it here) and quite serious engeneering investment at start.</p>&#xA;&#xA;<p>It brings modularity, concept of ownership, de-coupling of different contexts of your app. </p>&#xA;&#xA;<p>My personal advise check if you really need MS architecture. If you can not invest engenerring though and dev-ops effort at start and do not have proper reasons for such system - why bother? </p>&#xA;&#xA;<p>If you do need MS, i would really advise against the first method. You will develop wrong thing's, will miss the true challenges of MS, and could end with huge refactor, which could take more work than engeneering MS system from start properly. It's like to make square to make it fit into round bucket later. </p>&#xA;&#xA;<p><strong>Now answering your question title: granularity.</strong> (your question body bit different from your post title).</p>&#xA;&#xA;<p>Attach it to Domain Model / Bounded Context. You can make meaty services at start, in order to avoid complex distributed transactions. </p>&#xA;&#xA;<p>First just answer question if you need them in your design/architecture? &#xA;If not, probably you did a good design.&#xA;Passing reference ids between models from different microservices should suffice, and if not, try to rethink if more of complex transactions could be avoided.</p>&#xA;&#xA;<p>If your system have unavoidable amount of distributed trasnactions, perhaps look towards using/making some CQRS mini-framework as your ""shared code infrastructure component"" / communication protocol. </p>&#xA;"
42740529,40448015,5281340,2017-03-11T20:54:56,"<p>Your design is OK. </p>&#xA;&#xA;<ol>&#xA;<li><p>If your API gateway needs to implement (and thats probably the case) CAS/ some kind of Auth (via one of the services - i. e. some kind of User Service) and also should track all requests and modify the headers to bear the requester metadata (for internal ACL/scoping usage) - Your API Gateway should be done in Node, but should be under Haproxy which will care about load-balancing/HTTPS</p></li>&#xA;<li><p>Discovery is in correct position - if you seek one that fits your design look nowhere but <a href=""https://www.consul.io/"" rel=""nofollow noreferrer"">Consul</a>. </p></li>&#xA;<li><p>You can use consul-template or use own micro-discovery-framework for the services and API-Gateway, so they share end-point data on boot.</p></li>&#xA;<li><p>ACL/Authorization should be implemented per service, and first request from API Gateway should be subject to all authorization middleware.</p></li>&#xA;<li><p>It's smart to track the requests with API Gateway providing request ID to each request so it lifecycle could be tracked within the ""inner"" system.</p></li>&#xA;<li><p>I would add Redis for messaging/workers/queues/fast in-memory stuff like cache/cache invalidation (you can't handle all MS architecture without one) - or  take RabbitMQ if you have much more distributed transaction and alot of messaging</p></li>&#xA;<li><p>Spin all this  on containers (Docker) so it will be easier to maintain and assemble. </p></li>&#xA;<li><p>As for BI why you would need a service for that? You could have external ELK Elastisearch, Logstash, Kibana) and have dashboards, log aggregation, and huge big data warehouse at once.</p></li>&#xA;</ol>&#xA;"
42383460,42375333,5281340,2017-02-22T05:34:47,"<p>There are few requirements to make it work:</p>&#xA;&#xA;<ol>&#xA;<li><p>As mentioned above, X1 and X2 would use internal service A for credential verification. </p></li>&#xA;<li><p>there will be salts in user/api client records bound to the A's datasource.</p></li>&#xA;<li><p>X1 and X2 encryption keys should be shared, and they should use same structure of JWT's <code>sub</code> object. <code>iss</code> object should also be checked (normally bound to the api client to which user requesting authentication is bound). The logic of verefication should be shared between X1 and X2.</p></li>&#xA;</ol>&#xA;&#xA;<p>The token have data forged into it - and if it provided by same service A, and then hashed, and structured and then checked by X1 and X2 in same way.</p>&#xA;"
42399523,36144330,5281340,2017-02-22T18:31:01,"<p>Metrics could be collected via <a href=""https://docs.docker.com/engine/api/"" rel=""nofollow noreferrer"">Docker API</a> ( and <a href=""https://www.datadoghq.com/blog/how-to-collect-docker-metrics/"" rel=""nofollow noreferrer"">cool blog post</a> ) and it's often used for that. &#xA;Tinkering with DAPI and docker stack tools (compose/swarm/machine) could provide alot of tools to scale microservice architecture efficiently. </p>&#xA;&#xA;<p>I could advise in favor of <a href=""https://www.consul.io/"" rel=""nofollow noreferrer"">Consul</a> to manage discovery in such resource-aware system.</p>&#xA;"
48664199,48663545,1561148,2018-02-07T12:45:49,"<blockquote>&#xA;  <p>how can I combine multiple <code>docker-compose.yml</code> files from each service so they work together?</p>&#xA;</blockquote>&#xA;&#xA;<p>If you choose this path, make sure that your containers attach to the <strong>same network</strong>. You can find information about this case here: <a href=""https://docs.docker.com/compose/networking/#use-a-pre-existing-network"" rel=""nofollow noreferrer"">Networking in Compose: Use a pre-existing network</a>.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Or should I use one big <code>docker-compose.yml</code> file to define all the services under one network?</p>&#xA;</blockquote>&#xA;&#xA;<p>It is again up to you to decide if this should be your approach. In this case, you are correct, all the containers can attach to the <strong>default</strong> network that is going to be created. From the same docs:</p>&#xA;&#xA;<p><em>Note: Your app’s network is given a name based on the “project name”, which is based on the name of the directory it lives in. You can override the project name with either the <code>--project-name</code> flag or the <code>COMPOSE_PROJECT_NAME</code> environment variable.</em></p>&#xA;&#xA;<blockquote>&#xA;  <p>Or should I base each of my service only on one container (ubuntu as base and install php, mysql, nginx through Dockerfile)</p>&#xA;</blockquote>&#xA;&#xA;<p>I am not quite sure, what you mean here exactly...</p>&#xA;&#xA;<ul>&#xA;<li>if you talk about having all these installed to one container, don't do it. This way you get back to a ""monolith"" approach.</li>&#xA;<li>if you talk about starting with ""ubuntu"" as a base and continue by building separate containers for php, mysql, nginx etc, you can do it. But I would suggest you to better use the official images provided by <a href=""https://hub.docker.com"" rel=""nofollow noreferrer"">https://hub.docker.com</a></li>&#xA;</ul>&#xA;"
45735523,45559979,1303612,2017-08-17T12:46:01,"<p>You should use client tokens using the <a href=""https://tools.ietf.org/html/rfc6749#section-4.4"" rel=""nofollow noreferrer"">client_credentials</a> flow for interservice communication. This flow is exposed by default on the /oauth/token endpoint in spring security oauth. </p>&#xA;&#xA;<p>In addition to this, you could use private apis that are not exposed to the internet and are secured with a role that can only be given to oauth clients. This way, you can expose privileged apis that are maybe less restrictive and have less validation since you control the data passed to it. </p>&#xA;&#xA;<p>In your example, you could expose an public endpoint <code>GET account-service/account/current</code> (no harm in getting information about yourself) and a private api <code>GET account-service/internal/account/user-id/{userId}</code> that could be used exclusively by oauth clients to query any existing user.</p>&#xA;"
41150359,38786207,1303612,2016-12-14T19:12:07,"<p>Shameless plug for a little library I did that uses reflection to dynamically rethrow checked exceptions (and unchecked if they are on the Feign interface) based on an error code returned in the body of the response. </p>&#xA;&#xA;<p>More information on the readme : &#xA;<a href=""https://github.com/coveo/feign-error-decoder"" rel=""nofollow noreferrer"">https://github.com/coveo/feign-error-decoder</a></p>&#xA;"
44075028,31525237,979772,2017-05-19T16:39:06,"<p>Here are the answers to your questions.</p>&#xA;&#xA;<blockquote>&#xA;  <ol>&#xA;  <li>how can I have the id retured with the token returned?</li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p>You need a <a href=""http://docs.spring.io/spring-security/oauth/apidocs/org/springframework/security/oauth2/provider/token/TokenEnhancer.html"" rel=""nofollow noreferrer"">TokenEnhancer</a> to this job. Here is the relevant stackoverflow questio for an example - <a href=""https://stackoverflow.com/questions/28492116/can-i-include-user-information-while-issuing-an-access-token"">can I include user information while issuing an access token?</a></p>&#xA;&#xA;<blockquote>&#xA;  <ol start=""2"">&#xA;  <li>How does the PhotoService checks with AuthServer for validity and required scopes?</li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p>It doesn't. That is the whole beauty of this architecture. The scopes and the validity are part of the token itself. The Photo Service receives the token and decodes the information from it. Unless otherwise, you may very well be using Jwt tokens. If you go to the website, you will see an example of the token and its decoded value side by side. Here is the screenshot for quick reference.&#xA;<a href=""https://i.stack.imgur.com/ka8hi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ka8hi.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<blockquote>&#xA;  <ol start=""3"">&#xA;  <li>Why is <code>-d code=jYWioI</code> passed?</li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p>I am not sure. <code>-d</code> parameter of <code>curl</code>(<a href=""https://curl.haxx.se/docs/manpage.html#-d"" rel=""nofollow noreferrer"">man page here</a>) is used to send data to server. I am quite sure there is a mistake with that parameter and it is additional to this request and hence, the server just ignore it. You should be fine, it you removed it.</p>&#xA;"
48766988,48763985,3998013,2018-02-13T12:36:14,"<p>You can do it. I have had Docker on standard EC2 instances running without problem. By ""my microservice"" you mean a single microservice, right?</p>&#xA;&#xA;<p>You don't need service discovery or routing rules?</p>&#xA;"
50262458,50070987,4723005,2018-05-09T21:39:51,"<p>Worked with AWS support to get to the bottom of this one. In its current form (May 2018), if you want separate SAM templates for each lambda for deployment reasons, but have a shared API Gateway resource for those lambdas, then you must have a separate dedicated API Gateway SAM template. Additionally, the Events parameter in AWS::Serverless::Function must be excluded in favor of paths definitions in AWS::Serverless::Api.</p>&#xA;&#xA;<p>Lastly, the AWS::Serverless::Function(s) must be exported under Output and then imported by the AWS::Serverless::Api resource.</p>&#xA;"
42379452,42379365,596285,2017-02-21T22:46:12,"<p>There are currently two options for 1.13:</p>&#xA;&#xA;<ol>&#xA;<li>connect to each node and run a <code>docker logs</code> on the individual containers.</li>&#xA;<li>enable experimental options on your docker daemon which will let you use <a href=""https://docs.docker.com/engine/reference/commandline/service_logs/"" rel=""nofollow noreferrer""><code>docker service logs</code></a></li>&#xA;</ol>&#xA;"
38872672,38872460,596285,2016-08-10T11:51:48,"<p>I'm not an expert in node.js, but you're attempting to run index.js but don't have an index.js inside your lib folder. If you wanted to run the disco.js, that would be:</p>&#xA;&#xA;<pre><code>docker run -v $PWD/lib:/usr/src/app -p 3010:3000 -i \&#xA;  davesrepo/dynamo node disco.js&#xA;</code></pre>&#xA;"
50841607,50820762,5985853,2018-06-13T16:00:23,"<p>First, not everything in Vert.x is a lambda expression. That's just quite a weird tutorial you've found. As you can see, it uses <code>java.util.ServiceLoader</code> which is not a Vert.x class. Nor I'm familiar with anyone else recommending to use this class with Vert.x applications.<br>&#xA;What it tries to do is to load your classes dynamically. What you probably miss is putting the correct file in <code>META-INF</code> directory, as described here: <a href=""https://docs.oracle.com/javase/tutorial/ext/basics/spi.html#register-service-providers"" rel=""nofollow noreferrer"">https://docs.oracle.com/javase/tutorial/ext/basics/spi.html#register-service-providers</a></p>&#xA;&#xA;<p>Anyway, that's not the way I would recommend to use VertX. Instead, go with the regular VertX tutorial, which is excellent: <a href=""https://vertx.io/docs/vertx-web/java/#_handling_requests_and_calling_the_next_handler"" rel=""nofollow noreferrer"">https://vertx.io/docs/vertx-web/java/#_handling_requests_and_calling_the_next_handler</a></p>&#xA;"
36198194,36197572,3568957,2016-03-24T10:38:25,"<p>You can use <a href=""http://cloud.spring.io/spring-cloud-netflix/spring-cloud-netflix.html"" rel=""nofollow"">Spring Cloud Netflix and Eureka</a> as Service Discovery and <a href=""https://spring.io/blog/2015/01/20/microservice-registration-and-discovery-with-spring-cloud-and-netflix-s-eureka#client-side-load-balancing-with-ribbon"" rel=""nofollow"">Client-Side Load Balancing with Ribbon</a>.</p>&#xA;&#xA;<p>With the help of those you can communicate between microservice by 'service names' instead of service locations.</p>&#xA;&#xA;<p>Check out <a href=""https://bitbucket.org/ikryvorotenko/microservises-demo/"" rel=""nofollow"">this demo</a>. It should be VERY USEFULL for undestanding microservices communications.</p>&#xA;&#xA;<p>Here we have 2 simple microservices and Discovery Service for communications between them.</p>&#xA;"
50275948,36680157,5150013,2018-05-10T15:02:23,"<p>For the benefit of others running into this problem, apart from using <code>HTTP/2</code>, <code>SSL/TLS Offloading</code>, <code>Co-location</code>, consider using <code>Caching</code> where you can. This not only improves performance, but reduces dependency on downstream services. Also, consider data formats that are perform well.</p>&#xA;"
45010782,44758955,5545789,2017-07-10T11:24:29,"<p>I can confirm that the release 2017.7.0 fixed the problem and everything is now starting up correctly.</p>&#xA;&#xA;<p>Thank you, Ken, for your quick response and support.</p>&#xA;"
46644982,46644740,2799677,2017-10-09T11:06:15,"<p>That's true, integration by database between microservices is one of example point of failure, so the best option is separate storage for each microservices. It does not mean that you can not duplicate data between databases, on the contrary you can. In this situation you can store base information about customers in the dabase for Finance microservice.</p>&#xA;&#xA;<p>If you do not want duplicate data, you can create method getCustmersSimpeDetail with parameter as list of customers id. Which option is the best? It depends on concrete business scenarios.</p>&#xA;"
48842777,48838101,324157,2018-02-17T15:17:38,"<p>You most definitely would want to ensure up-front that you can catch as many invalid business cases as possible.  There are a couple ways to deal with this.  It is the same situation as one would have when booking a seat on an airline.  Although they do over-booking which we'll ignore for now :)</p>&#xA;&#xA;<p>Option 1: You could reserve an inventory item as part of the order.  This is more of a pessimistic approach but your item would be reserved while you wait for the to be confirmed.</p>&#xA;&#xA;<p>Option 2: You could accept the order only if there is an inventory item available but <em>not</em> reserve it and hope it is available later.</p>&#xA;&#xA;<p>You could also create a back-order if the inventory item isn't available and you want to support back-orders.</p>&#xA;&#xA;<p>If you go with option 1 you could miss out on a customer if an item has been reserved for customer A and customer B comes along and cannot order.  If customer A decides not to complete the order the inventory item becomes available again but customer B has now gone off somewhere else to try and source the item.</p>&#xA;&#xA;<p>As part of the fulfillment of your order you have to inform the inventory bounded context that you are now taking the item.  However, you may now find that both customer A and B have accepted their quote and created an order for the last item.  One is going to lose out.  At this point the one not able to be fulfilled will send a mail to the customer and inform them of the unfortunate situation and perhaps create a back-order; or ask the customer to try again in X-number of days.</p>&#xA;&#xA;<p>Your domain experts should make the call as to how to handle the scenarios and it all depends on item popularity, etc.</p>&#xA;"
47321989,47311911,324157,2017-11-16T05:03:53,"<p>I'm not familiar with your technical infrastructure but the way I implement projections is as follows:</p>&#xA;&#xA;<p>Each domain event has a <em>global</em> sequence number that spans all aggregate roots.  A projection is a read model that has an arbitrary name and the last processed position represented by that global sequence number.  I can add a new projection at any time, along with its event handlers, and it will start at position 0.  I can clear a projection at any time and set back the position to 0.  I can also use a combination of adding a new projection that will replace an existing one, have that build even if it takes days, and then remove the old one.</p>&#xA;&#xA;<p>There is a service that monitors the projections and uses the event store almost like a queue.  The projection service checks for events with global ids <em>after</em> the current position and hands those off to handlers and then updates the position.  This is where your projection may even filter on event types to improve performance.</p>&#xA;&#xA;<p>That is the basic idea.  Your projections are then what you query.  Once a projection has caught up to the ""head"" of the event store the events from the event store are going to be trickle fed into the projection.</p>&#xA;&#xA;<p>How that will translate into your technical space I'm not quite sure.  I have a bit of an experiment called <a href=""https://github.com/Shuttle/Shuttle.Recall"" rel=""nofollow noreferrer"">Shuttle.Recall</a> going on C# if you'd like to have a look to get some ideas.</p>&#xA;"
38255609,37254914,1320510,2016-07-07T21:30:13,"<p>You should bump to version 4.0.   The idea is not the name of the service, but a hint to its history and lineage.  In this example, though the method of calling the executable has changed, the linage of the database is intact and you want to keep the history that the previous version existed.</p>&#xA;&#xA;<p>Bumping the major version already signals to all users that incompatible changes have occurred, so no one will pick this new invocation version up by accident.</p>&#xA;&#xA;<p>Semantic versioning is deliberately underspecified so that the core tenets will continue to be followed even if others disagree with an argument.  That is, semantic versioning will only specify what you <em>must</em> do, not what you <em>should</em> do.</p>&#xA;"
28578640,28572202,542270,2015-02-18T07:56:37,"<p>When <code>com.github.johnrengelman.shadow</code> plugin is used you need to execute <code>shadowJar</code> task and run artifact created under <code>&lt;project_root&gt;/build/libs</code> with name <code>&lt;name&gt;-all.jar</code>.</p>&#xA;&#xA;<p>BTW: I know that you provided only parts of the code but I found some errors here (e.g. <code>SimpleCOntroller</code> and <code>SimpleController</code>). <a href=""https://dropwizard.github.io/dropwizard/getting-started.html"" rel=""nofollow"">Here</a> a great tutorial on getting started with dropwizard can be found.</p>&#xA;"
33717950,33712470,542270,2015-11-15T08:54:38,<p>In general you should beware of using endpoint that are more or less similar to the one you suggested:</p>&#xA;&#xA;<pre><code>/ordersByCustomers/{customerid}&#xA;</code></pre>&#xA;&#xA;<p>Why? Because this is not RESTful in general (even in microservices environment) and make the API difficult to understand and you by the consumers. What if you need <code>orderByWhatever</code>? Will you be introducing new endpoint every single time you need a new set of data? Try to avoid so opinionated endpoints.</p>&#xA;&#xA;<p>What @Augutsto suggested is fully correct. If you're afraid of having a complicated logic in <code>GET</code> request this is the situation where you can break REST rules. I mean introducing:</p>&#xA;&#xA;<pre><code>POST /orders/filter/&#xA;</code></pre>&#xA;&#xA;<p>Where all the filtering logic will be passed in requests body - so it's easier to carry complicated logic as well.</p>&#xA;
50266263,50265973,945214,2018-05-10T05:46:11,"<p>This is a typical gotcha in the world of microservices. Even in micro service oriented architectures, the focus should be on the solving very specific business problems and then have supporting database tables. </p>&#xA;&#xA;<p>It seems that a 'StudentRegistration' service might help in this case, which can get hold of the joins in question. Having a third service like 'StudentRegistration' might introduce more than obvious network calls within the services, if caching is used properly, these microservices could respond very fast.</p>&#xA;&#xA;<p>Infact, then you could evaluate that whether you need all three services at the end, i.e. Student, Course and StudentRegistration OR some combination(s) can be merged, as per your business-processes or use-cases. That can be done if a combination of some of them are not able to answer most of the business questions without knowing about each other. </p>&#xA;&#xA;<p>Finally, it might not be completely wrong to actually implement microservices over a shared DB. See: <a href=""http://microservices.io/patterns/data/shared-database.html"" rel=""nofollow noreferrer"">http://microservices.io/patterns/data/shared-database.html</a></p>&#xA;&#xA;<p>Please note that having totally separate databases for such closely related microservices and keeping those tables in sync all the time, through eventing model is not straightforward, so be careful there. It is not impossible, however. There are documented design patterns to cater to business transactions spanning multiple microservices, namely: CQRS pattern (<a href=""https://martinfowler.com/bliki/CQRS.html"" rel=""nofollow noreferrer"">https://martinfowler.com/bliki/CQRS.html</a>), SAGA pattern(<a href=""http://microservices.io/patterns/data/saga.html"" rel=""nofollow noreferrer"">http://microservices.io/patterns/data/saga.html</a>) and Event sourcing: <a href=""http://microservices.io/patterns/data/event-sourcing.html"" rel=""nofollow noreferrer"">http://microservices.io/patterns/data/event-sourcing.html</a>.</p>&#xA;"
49139108,45115860,3902270,2018-03-06T20:04:59,"<p>What you are looking for is an API-gateway. An API-gateway is a form of reverse-proxy too. I've looked into complete solutions like Tyk, Traefik and Zuul which all are great solutions. Of course one can use Nginx, but then you probably have a lot you have to incorporate using Lua scripting, such as client for OpenID-Connect/OAuth for authorization and such. Another things to take into consideration is security, logging, metrics, rate-limiting, (dynamic routing) and more. I really wouldn't base my API-gateway on Nginx. Nginx-Plus includes an API-gateway, but requires payment.</p>&#xA;&#xA;<p>I'm currently working on a very large project and have found that it requires a lot of customization work in form of plugins for the solutions mentioned above. So much in fact, that it will be a lot easier for me to write my API-gateway from scratch (using Golang and libraries).</p>&#xA;&#xA;<p>Since you mention you have tried go-kit and micro, I would suggest you try Traefik as it is very simple to set up and includes a Web UI for convenience.</p>&#xA;"
48872252,45776238,3902270,2018-02-19T18:35:20,"<p>I'm structuring it like this; mono-repo per. project approach. Taking into account that these services are closely related:</p>&#xA;&#xA;<pre><code>github.com/user/some_project/&#xA;├── pkg/ (common own-created packages for all services)&#xA;|   ├── errors/&#xA;|   ├── log/&#xA;|   ├── metrics/&#xA;|   ├── sd/&#xA;|   |   ├── consul/&#xA;|   |   └── kubernetes/&#xA;|   └── tracing/&#xA;├── services/&#xA;|   ├── account/&#xA;|   |   ├── pb/&#xA;|   |   |   ├── account.proto&#xA;|   |   |   └── account.pb.go&#xA;|   |   ├── handler.go&#xA;|   |   ├── main.go&#xA;|   |   ├── main_test.go&#xA;|   |   ├── Dockerfile&#xA;|   |   └── README.md&#xA;|   ├── auth/&#xA;|   ├── frontend/&#xA;|   └── user/&#xA;├── vendor/ (common vendor-packages for all services)&#xA;├── Gopkg.lock&#xA;├── Gopkg.toml&#xA;├── Makefile&#xA;├── README.md&#xA;└── docker-compose.yml&#xA;</code></pre>&#xA;&#xA;<p>Later, when you start on your second microservices project, many of the packages in the /pkg directory would be required there too. What to do? Copy/paste? No! Instead, extract these packages from the project, i.e. log, metric... into a new separate repository, so that all projects imports these from that one repo.</p>&#xA;&#xA;<p>Congrats, you are now also a great step on your way to create your own microservices-framework! Or at least your own kit :)</p>&#xA;"
35913794,35913253,569662,2016-03-10T10:27:44,"<p>There is a good <a href=""http://martinfowler.com/bliki/MonolithFirst.html"" rel=""nofollow"">article by Fowler</a>, in which he poses that many projects which start off with a microservice-based design run into problems very quickly. Conversely, many successful microservice-based applications are the ones which began life as a monolith. </p>&#xA;&#xA;<p>To sum up this principal:</p>&#xA;&#xA;<blockquote>&#xA;  <p>...you shouldn't start a new project with microservices, even if&#xA;  you're sure your application will be big enough to make it worthwhile.</p>&#xA;</blockquote>&#xA;&#xA;<p>While Fowler does not express his personal opinion on this, he does speculate on the benefits of what he calls <em>Monlith-First</em>.</p>&#xA;&#xA;<blockquote>&#xA;  <p>...a monolith-first strategy, where you should build a new application&#xA;  as a monolith initially, even if you think it's likely that it will&#xA;  benefit from a microservices architecture later on...It may be hard to scale a &#xA;  poorly designed but successful software system, but that's still a&#xA;  better place to be than its inverse...you need to prioritize speed...</p>&#xA;</blockquote>&#xA;&#xA;<p>So, the benefits of the Monlith-First approach is that you can build a monolith quickly because generally the requirements are well known and relatively few. Additionally, you get something to market fast so then you get to understand your application and how it behaves in the real world. </p>&#xA;&#xA;<p>What stikes me as the main benefit is that you will more clearly understand where the boundaries between the business capabilities you are supporting naturally fall within your application, than if you had tried to define these boundaries up front (a necessary and very important design step in microservice-based designs). </p>&#xA;&#xA;<p>He goes onto expand on how you can effectively plan a Monolith-first design, which basically involves keeping your code nice and modular with an eye on the future breaking-out of modules as required.</p>&#xA;&#xA;<blockquote>&#xA;  <p>...design a monolith carefully, paying attention to modularity within&#xA;  the software, both at the API boundaries and how the data is stored.</p>&#xA;</blockquote>&#xA;&#xA;<p>In my experience a services-based approach works well with a lot of up-front analysis time spent with business domain experts, and a mature delivery team who are comfortable working with soa and service-based in general. </p>&#xA;"
36886961,36871886,569662,2016-04-27T10:18:41,"<blockquote>&#xA;  <p>Should this be handled inside the user service with kind of an ACL</p>&#xA;</blockquote>&#xA;&#xA;<p>I think this is the best option. You could delegate the actual authorization to a separate service which the User service can call with the identity of the caller and the ""claim"" the caller is making (eg ""I am allowed to see Email Address for User""). The claims can be evaluated on a per call basis. </p>&#xA;&#xA;<p>However, is is arguable whether you actually need to query the user service at all. It would mean a change to your design but imagine for a minute that the Notifications service already knew about the user, for example the user ID and email address, then the notifications service would not need to query anything to be able to do it's job. </p>&#xA;&#xA;<p>In order for the notifications service to have the user data already, it is necessary for that data to have been sent to the notifications service at some point in the past. A good time to do this would be when the user is first created, or any time a user details are changed. The best way to distribute this kind of information is in the form of an event message, although you could have the distribution based on an http POST to the notifications service. </p>&#xA;"
36847592,36844842,569662,2016-04-25T17:48:28,"<p>As @dbugger says in the comments, you can have the subscribers send an ack or something back the publisher. </p>&#xA;&#xA;<p>However, it is the publisher's responsibility to ensure the events were received by the subscribers?</p>&#xA;&#xA;<p>Kind of the point of publishing events is that the publisher doesn't (and shouldn't) need to know about the state of consumers subscribing to the event, whether the subscribers ignored the message, or even if there are no subscribers.</p>&#xA;&#xA;<p>If the publisher does need to know, then rather than an event, the publisher should be sending a command direct to the consumer in a request-response pattern, rather than an event. </p>&#xA;&#xA;<p>This is because a command message assumes knowledge of the recipient whereas an event message assumes no knowledge. </p>&#xA;&#xA;<p>In terms of knowing from a top-down perspective when an event has arrived: well, you should be using a durable message transport which can guarantee delivery of your event, but even with durability you can still drop messages. </p>&#xA;&#xA;<p>The only real way of doing it is to implement some kind of instrumentation which allows you to track the ""conversations"" which are encoded in the events being published from place to place. There are tools available for this (I have only used one, for NServicebus called <a href=""http://particular.net/servicepulse"" rel=""nofollow"">ServicePulse</a>).</p>&#xA;"
38506383,38504315,569662,2016-07-21T14:02:09,"<blockquote>&#xA;  <p>Or is there another way I've not thought of?</p>&#xA;</blockquote>&#xA;&#xA;<p>3) Add the organisation to Contact domain model. </p>&#xA;&#xA;<p>To me, both contacts and organisations naturally fall under the same ""Contact"" or ""CRM"" business domain. </p>&#xA;&#xA;<p>Put another way, if you were to remove the Contacts Service, would the Organisation Service be able to do any useful work? If not, then this may be a signal that you should simply make Organisation a part of the Contact business domain.</p>&#xA;&#xA;<p>This appears especially true as one of the options you outline the organisation service is just serving as a facade onto the contacts service. If there is no material difference between the Contact model in the Contact service and the representation of that model in the Organisation service, then they are effectively interchangeable and so should not be separated.</p>&#xA;"
38614493,38614201,569662,2016-07-27T13:33:19,"<blockquote>&#xA;  <p>As I understand right now, it's just separated self-contained services&#xA;  that can be invoked by each other or by the client</p>&#xA;</blockquote>&#xA;&#xA;<p>You understand well! That's a pretty decent definition right there: </p>&#xA;&#xA;<ul>&#xA;<li><strong>separated</strong> - designed, built, deployed, and managed separately</li>&#xA;<li><strong>self-contained</strong> - fully autonomous</li>&#xA;<li><strong>can be invoked by each other</strong> - communicate over well-known protocols, sync or async.</li>&#xA;<li><strong>or by the client</strong> - each service exposes it's own composable UI component</li>&#xA;</ul>&#xA;&#xA;<blockquote>&#xA;  <p>If our system allows people to comment on product as well as user,&#xA;  should we store the comments of both systems in a comments table</p>&#xA;</blockquote>&#xA;&#xA;<p>You would only do this if you had a separate Comments service. From your own definition, services should be self-contained. That means not having shared dependencies, like data models or database tables. </p>&#xA;&#xA;<blockquote>&#xA;  <p>Should each service use its own database or store them in one place?</p>&#xA;</blockquote>&#xA;&#xA;<p>Again, quoting from your definition, services should be separated (or at least separatable), so as long as they do not use the same database instance they <em>could</em> share a server instance.</p>&#xA;"
38501827,38492388,569662,2016-07-21T10:31:47,"<blockquote>&#xA;  <p>...all logic for microservices should be self-contained and&#xA;  duplication of logic is ok</p>&#xA;</blockquote>&#xA;&#xA;<p>I think this is the core of the issue you are struggling with. Is this statement actually true?</p>&#xA;&#xA;<p>A quick google search later:&#xA;<a href=""http://www.simplicityitself.io/our%20team/2015/01/12/sharing-code-between-microservices.html"" rel=""nofollow"">http://www.simplicityitself.io/our%20team/2015/01/12/sharing-code-between-microservices.html</a></p>&#xA;&#xA;<p>This article talks about this exact question, which we can now frame as <em>What is the appropriate level of re-use in microservice architecture?</em></p>&#xA;&#xA;<p>The author provides a list of reasons why developers feel the need to share code, ordered from lowest to highest in terms of coupling and loss of isolation:</p>&#xA;&#xA;<blockquote>&#xA;  <ul>&#xA;  <li>Leverage existing technical functionality</li>&#xA;  <li>Sharing data schemas, using a class, for example, as an enforcement of a shared schema.</li>&#xA;  <li>Sharing data sources, use of the same database by multiple services.</li>&#xA;  </ul>&#xA;</blockquote>&#xA;&#xA;<p>Though this list covers most of the reasons, I would add another important reason to share code, which is to do with a common framework for rapid standing up of microservices, commonly called the <a href=""http://microservices.io/patterns/microservice-chassis.html"" rel=""nofollow"">Microservice Chassis</a> pattern.</p>&#xA;&#xA;<p>The author goes onto say:</p>&#xA;&#xA;<blockquote>&#xA;  <p>It is of utmost importance to pin down your motivation for wanting to&#xA;  share code, as unfortunately there is no right answer to this&#xA;  question. Like everything else, it’s contextual.</p>&#xA;</blockquote>&#xA;&#xA;<p>So, all that said, should you centralize your connectors or not?</p>&#xA;&#xA;<p>Well, where do these dependencies fit in on our list? And what degree of coupling can you endure before you're no longer doing microservices but building a monolith instead? </p>&#xA;&#xA;<p>These are not easy questions to answer, but hopefully this will help guide you to the correct conclusion. </p>&#xA;"
38508707,38508387,569662,2016-07-21T15:45:13,<p><strong>SOA</strong></p>&#xA;&#xA;<p>Building software out of widgets.</p>&#xA;&#xA;<p><strong>Microservices</strong></p>&#xA;&#xA;<p>Building software out of small widgets.</p>&#xA;&#xA;<p><strong>Web Services</strong></p>&#xA;&#xA;<p>Makes widgets look like the internet</p>&#xA;&#xA;<p><strong>REST</strong></p>&#xA;&#xA;<p>Makes widgets look a lot like the internet</p>&#xA;
38632200,38629237,569662,2016-07-28T09:23:38,"<blockquote>&#xA;  <p>Currently I'm using TransactionScope for 'wrap' the transaction, but&#xA;  it didn't work</p>&#xA;</blockquote>&#xA;&#xA;<p>Wrapping calls to physically external resources in a transaction scope only works under very precisely configured (some would say <em>contrived</em>) conditions. </p>&#xA;&#xA;<p>In your example (assuming service and database are on different physical hosts to each other and the caller), you are going from the caller host, to the service host, across the service boundary, to the database host, into the database, back out across to the service host, back across the service boundary, and back onto the caller host.</p>&#xA;&#xA;<p>In order to propagate a distributed transaction from your client to the database, each intermediary at each step in the call chain must enlist in the transaction. </p>&#xA;&#xA;<p>In order to do this: </p>&#xA;&#xA;<ul>&#xA;<li>MSDTC must be enabled and configured properly on each participating host, </li>&#xA;<li>Service calls must be made using a binding which supports WS-AtomicTransaction (like wsHttpBinding), and</li>&#xA;<li>Services must have been specifically built to support transactional behaviour.</li>&#xA;</ul>&#xA;&#xA;<p>So unless all this has been done, the fact you are making two services calls in a row makes no difference in this case. A single service call would fail to propagate the transaction down to the DB and back again.</p>&#xA;&#xA;<p>Even if you are already aware of the above and have done all these steps in order to support distributed  transactional support across multiple service calls, I would still not recommend you do this. Transactions are costly and encourage shared environmental dependencies to a solution. This is especially so as you are planning a microservices style approach. </p>&#xA;&#xA;<p>Must simpler would be to have each service expose a recovery or rollback operation, so the caller can take the appropriate action and rollback any calls made previously to a failed call. Such an approach is known as a <a href=""http://soapatterns.org/design_patterns/compensating_service_transaction"" rel=""nofollow"">compensation pattern</a>.</p>&#xA;"
45435492,45420766,569662,2017-08-01T10:35:07,"<p>I would definitely do as you are suggesting. More important than the technological concerns (scalability, fault tolerance etc), is setting up a beneficial dynamic between yourself and your supplier. </p>&#xA;&#xA;<p>If you exposed a queued endpoint directly, then you effectively hand control of contract to the message producer. As the producer of the message, they are effectively able to inject any message they want onto the queue. </p>&#xA;&#xA;<p>Queued data formats are notoriously difficult to nail down from a contract perspective. As well as payload, you have additional concerns such as addressing, headers, encoding, retry, and grouping semantics. With http, in contrast, the entire contract can be completely nailed down by using a definition convention such as swagger.</p>&#xA;&#xA;<p>Importantly, by exposing a synchronous service you retain control over the contract. As well as controlling the contract (and therefore the format of the conversation) you will also be able to implement throttling, strict validation, etc with meaningful and helpful responses, all of which would be much harder in an asynchronous scenario. </p>&#xA;&#xA;<p>This is an important distinction because the balance of power in a producer-consumer relationship is very different to the balance of power in a service-consumer relationship. You will find that this simplifies your dealings with said supplier going forward.</p>&#xA;"
38454598,38453830,569662,2016-07-19T09:31:33,"<p>Strictly, a 200 only means that the request was successfully processed. This is regardless of the actual outcome of the call from a business perspective. </p>&#xA;&#xA;<p>You are in effect relying on a convention of ""we will throw an exception or otherwise fail the call if the user is not authenticated"", to authenticate users.</p>&#xA;&#xA;<p>Depending on the convention, you could conceivably have a scenario where a user was unauthenticated but the call was still successfully processed. </p>&#xA;&#xA;<p>From this perspective, it may be worth having the service2 return a response which could then be interrogated to close this circle. </p>&#xA;&#xA;<p>Alternatively, you could have the client call the authentication service directly, retrieve a token, and then present this token with any other request. This would mean that service1 is no longer responsible for having to know that a caller is authenticated. </p>&#xA;&#xA;<blockquote>&#xA;  <p>The question is whether or not to then test in Service 1 each time the&#xA;  response is received</p>&#xA;</blockquote>&#xA;&#xA;<p>Sorry, I would appear to have misunderstood the spirit of the question somewhat. </p>&#xA;&#xA;<p>I am slightly confused - are you asking that, if the system under test is service1 then should any response from service2 also be part of that test? </p>&#xA;&#xA;<p>I would say you would have to have some test which could prove that the sercvice2 response interrogation logic is correct, but this could be done at the unit test level. I don't think you'd need to do this for tests running against a deployed instance of the service, which is by nature more about the service behaviour at the boundary rather than internally. </p>&#xA;"
40650918,40645778,569662,2016-11-17T09:19:34,"<p>There are many ways to address this problem, but perhaps the simplest one is this:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Ensure service A already has the data it requires from service C at&#xA;  the time it needs it. That way it will not have to make the call in&#xA;  the first place.</p>&#xA;</blockquote>&#xA;&#xA;<p>This would require that at some time in the past, service C would have to event out changes to its own state so that other services can consume these changes. Such an arrangement promotes service autonomy, which is a way of saying that services do not have a dependency on the current state of other services. </p>&#xA;&#xA;<p>The usual way of achieving this would be to have service C publish an event message whenever it's internal state changed. </p>&#xA;"
39086533,39071178,569662,2016-08-22T18:38:50,"<p>Claiming that using NuGet to reuse code is the same thing as ""doing"" SOA is a bit like saying that using a food processor is the same thing as cooking a three-course meal. The food processor may be related to the outcome, and useful during execution, even becoming a prerequisite occasionally, but that doesn't mean it can be directly compared in this way. </p>&#xA;&#xA;<p>One way to counter this view is to argue that SOA is perfectly possible without any reuse of code, hence no requirement for package management. If you want to read about what it means to do SOA, see another post of mine <a href=""https://stackoverflow.com/a/38052562/569662"">here</a>.</p>&#xA;&#xA;<p>However, I doubt that this is a view you can defeat with logical reasoning. I suspect that what your management is really saying is ""SJDoodle keeps going on about SOA? Sounds a bit expensive.""</p>&#xA;&#xA;<p>The good news is that you can practice SOA without letting anyone know you're doing it. Even if you're forced to build you application as a monolith, there is nothing to stop you architecting it with microservices in mind. Certain design patterns can be useful to you here, including the <a href=""http://alistair.cockburn.us/Hexagonal+architecture"" rel=""nofollow noreferrer"">Ports and Adapters</a> pattern and <a href=""http://codebetter.com/iancooper/2011/04/27/why-use-the-command-processor-pattern-in-the-service-layer/"" rel=""nofollow noreferrer"">Command Processor</a> pattern. </p>&#xA;&#xA;<p>Martin Fowler has <a href=""http://martinfowler.com/bliki/MonolithFirst.html"" rel=""nofollow noreferrer"">written</a> about this before, and in some cases, it's better to start this way than trying to execute a services-based approach from the start.</p>&#xA;"
37838485,37439039,569662,2016-06-15T14:35:52,"<blockquote>&#xA;  <p>I am trying to get my head around micro services messaging instead of&#xA;  pure REST</p>&#xA;</blockquote>&#xA;&#xA;<p>One of the key misunderstandings when considering the differences between calling a service via synchronous and asynchronous means, is that the nature of the calls are directly transferrable and equivalent to each other. </p>&#xA;&#xA;<p>This can, actually, be true. For example, you could define an operation such as:</p>&#xA;&#xA;<pre><code>HTTP 1.1 /api/Thingys/1234 GET&#xA;</code></pre>&#xA;&#xA;<p>which would be equivalent to </p>&#xA;&#xA;<pre><code>channel.Send(new GetThingyQuery{ ThingyId=1234; });&#xA;void HandleResponse(Thingy thingy);&#xA;</code></pre>&#xA;&#xA;<p>as an exchange of messages, from one service to another. </p>&#xA;&#xA;<p>There is another way to think about this, however. In the above example, imagine service A needs a <em>Thingy</em> from service B in order to do some process. There are two ways we can implement this:</p>&#xA;&#xA;<ol>&#xA;<li>Service A calls the GetMyThingy() operation on service B.</li>&#xA;<li>Service A doesn't call GetMyThingy, because service A <em>already has a copy of the Thingy</em>.</li>&#xA;</ol>&#xA;&#xA;<p>So how does service A already have the Thingy? </p>&#xA;&#xA;<p>Service B already sent it to Service A in the form of an <a href=""http://www.enterpriseintegrationpatterns.com/patterns/messaging/EventMessage.html"" rel=""nofollow"">Event</a>. In fact, Service B didn't have to actually send the thingy explicitly to service A. Service B didn't even need to know about service A, because service B <a href=""http://www.enterpriseintegrationpatterns.com/patterns/messaging/PublishSubscribeChannel.html"" rel=""nofollow"">published</a> the thingy.</p>&#xA;&#xA;<p>So in this case, the operation moves from service B to service A and becomes:</p>&#xA;&#xA;<pre><code>void HandleThingyEvent(ThingyEvent event);&#xA;</code></pre>&#xA;&#xA;<p>So what caused the event to be published by service B? Well, this could be as simple as a new thingy being added, or an update to an existing thingy.</p>&#xA;&#xA;<blockquote>&#xA;  <p>...how you would cancel a request.</p>&#xA;</blockquote>&#xA;&#xA;<p>This brings us onto how we can answer your main question, which as we can now see is ""Remove the need for a request entirely"". We do this by using events and publishing, which, while they can be implemented synchronously, are much better suited to one-way messaging. </p>&#xA;"
37924190,37889067,569662,2016-06-20T13:45:41,"<p>If anyone else is wondering wtf a <em>Reactive Microservice</em> is, then see below. </p>&#xA;&#xA;<p>From the book ""Reactive Microservices Architecture"" by Jonas Bonér, O'Reilly press:</p>&#xA;&#xA;<blockquote>&#xA;  <p><strong>What Is a Reactive Microservice?</strong></p>&#xA;  &#xA;  <p>One of the key principles in employing a Microservices-based&#xA;  Architecture is Divide and Conquer: the decomposition of the system&#xA;  into discrete and isolated subsystems communicating over well defined&#xA;  protocols.</p>&#xA;  &#xA;  <p>Isolation is a prerequisite for resilience and elasticity and requires&#xA;  asynchronous communication boundaries between services to decouple&#xA;  them in:</p>&#xA;  &#xA;  <ul>&#xA;  <li>Time - Allowing concurrency</li>&#xA;  <li>Space - Allowing distribution and mobility—the ability to move serv‐ ices around</li>&#xA;  </ul>&#xA;  &#xA;  <p>When adopting Microservices, it is also essential to eliminate shared&#xA;  mutable state<sup>1</sup> and thereby minimize coordination,&#xA;  contention and coherency cost, as defined in the Universal Scalability&#xA;  Law<sup>2</sup> by embracing a <a href=""https://en.wikipedia.org/wiki/Shared_nothing_architecture"" rel=""nofollow"">Share-Nothing Architecture</a>.</p>&#xA;  &#xA;  <p><sup>1</sup> For an insightful discussion on the problems caused by a&#xA;  mutable state, see John Backus’ classic Turing Award Lecture “<a href=""http://worrydream.com/refs/Backus-CanProgrammingBeLiberated.pdf"" rel=""nofollow"">Can&#xA;  Programming Be Liberated from the von Neumann Style?</a>”</p>&#xA;  &#xA;  <p><sup>2</sup> Neil Gunter’s <a href=""http://www.perfdynamics.com/Manifesto/USLscalability.html"" rel=""nofollow"">Universal Scalability Law</a> is an&#xA;  essential tool in understanding the effects of contention and&#xA;  coordination in concurrent and distributed systems.</p>&#xA;</blockquote>&#xA;&#xA;<p>Using this definition, Thing would be a system composed of reactive microservices, and Subthing1.. etc would be the individual reactive microservices.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Can You give me a real time live working practical example of Thing1,&#xA;  SubThing1, SubThing2, and SubThing3.</p>&#xA;</blockquote>&#xA;&#xA;<p>How about: </p>&#xA;&#xA;<ul>&#xA;<li>Thing = Amazon website</li>&#xA;<li>SubThing1 - Pricing service</li>&#xA;<li>SubThing2 - Product service</li>&#xA;<li>SubThing3 - Stock service</li>&#xA;<li>etc...</li>&#xA;</ul>&#xA;"
37833504,37830008,569662,2016-06-15T10:58:28,"<p>You can use the <a href=""http://www.enterpriseintegrationpatterns.com/patterns/messaging/Aggregator.html"" rel=""noreferrer"">Aggregator</a> pattern, also called <em>Parallel Convoy</em>. </p>&#xA;&#xA;<p>Essentially you need to have some way of identifying messages which need to be aggregated, and when the aggregated set as a whole has been recieved, so that processing can start. </p>&#xA;&#xA;<p>Without going out and <a href=""http://rads.stackoverflow.com/amzn/click/0321200683"" rel=""noreferrer"">buying the book</a>*, the Apache Camel integration platform website has some <a href=""http://camel.apache.org/aggregator.html"" rel=""noreferrer"">nice resource</a> on implementing the aggregator pattern. While this is obviously specific to Camel, you can see what kind of things are involved.</p>&#xA;&#xA;<p><sup>* disclaimer, I am not affiliated in any way with Adison Wesley, or any of the authors of the book...</sup></p>&#xA;"
37991553,37977210,569662,2016-06-23T12:24:17,"<p>If you found yourself in a position where you are often required to return large, report-style, lists (for want of a better term), then perhaps the answer is in providing a ""Search"" microservice. </p>&#xA;&#xA;<p>If the two entities definitely do not belong together (you could argue from your example, that Address is simply a property of Customer and so what are they doing in two different places), then maintaining some kind of denormalised, or ViewModel-friendly, representation of both entities together would seem reasonable. </p>&#xA;"
38009473,38006427,569662,2016-06-24T09:03:35,"<blockquote>&#xA;  <p>I....want to implement it in micro services</p>&#xA;</blockquote>&#xA;&#xA;<p>I would suggest that the decision to use microservices for a new application is often made without fully considering the premium you will pay in terms of up-front complexity.</p>&#xA;&#xA;<p>This premium has two main components:</p>&#xA;&#xA;<ol>&#xA;<li>Cost associated with building a <a href=""http://microservices.io/patterns/microservice-chassis.html"" rel=""nofollow noreferrer"">microservice chassis</a> </li>&#xA;<li>Cost for greater up-front business domain analysis, to identify the natural <a href=""http://martinfowler.com/bliki/BoundedContext.html"" rel=""nofollow noreferrer"">context boundaries</a> of your application.</li>&#xA;</ol>&#xA;&#xA;<p>The point being, even though your application will benefit from a microservice approach overall, it is rarely a good idea to start off with this approach. </p>&#xA;&#xA;<p>See my previous post <a href=""https://stackoverflow.com/a/35913794/569662"">here</a> for more information. </p>&#xA;"
38122005,38106036,569662,2016-06-30T11:37:56,"<blockquote>&#xA;  <p>...there don't seem to be any tools to automate the schema&#xA;  generation...</p>&#xA;</blockquote>&#xA;&#xA;<p>Agreed that tooling is thin on the ground. However, there is this: <a href=""https://github.com/NJsonSchema/NJsonSchema"" rel=""nofollow"">https://github.com/NJsonSchema/NJsonSchema</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>How do you guys do it?</p>&#xA;</blockquote>&#xA;&#xA;<p>Interestingly I was having this exact discussion with a colleague not 10 minutes ago :p</p>&#xA;&#xA;<p>We require a build (or acceptance test) failure if a model deviates from a pre-defined schema. </p>&#xA;&#xA;<p>So you could have a step in your build which uses the NJsonSchema package to generate the schema from your model. Then you have a comparison step which compares the outputted schema against the API docs schema. </p>&#xA;&#xA;<p>Conversely, you could generate code from your schema and then compare the output to the models in your build output. </p>&#xA;&#xA;<blockquote>&#xA;  <p>...question our decision to use a messaging architecture based on&#xA;  message queues..</p>&#xA;</blockquote>&#xA;&#xA;<p>Stay the course, friend.</p>&#xA;"
38184619,38178535,569662,2016-07-04T12:19:09,"<blockquote>&#xA;  <p>We are trying to re-use the code of existing API as far as possible</p>&#xA;</blockquote>&#xA;&#xA;<p>Why do you want to do this? Are you looking at it as a potential time-saving? Don't, it won't save you any time. Are you worried about the code being in two places at the same time? Don't be - services should be autonomous, which means we must embrace a certain amount of redundancy.</p>&#xA;&#xA;<blockquote>&#xA;  <p>We are finding it tough to share the Controllers between monolithic&#xA;  API and micro services</p>&#xA;</blockquote>&#xA;&#xA;<p>That is probably a signal that you should not be doing this. </p>&#xA;&#xA;<blockquote>&#xA;  <p>Any other ways to share the controllers?</p>&#xA;</blockquote>&#xA;&#xA;<p>Well, I think we can all agree that the controllers themselves shouldn't really be shared. However, the controller can (and <a href=""https://www.google.co.uk/webhp?q=skinny%20controller"" rel=""nofollow"">should</a>) simply delegate call handling to another dependency, so in theory you could just nuGet the dependency and share it between services like that. </p>&#xA;"
39356828,39348995,569662,2016-09-06T19:54:22,"<p>Yes, in the asynchronous world it's called the <a href=""http://www.enterpriseintegrationpatterns.com/patterns/messaging/DocumentMessage.html"" rel=""nofollow"">Document Message pattern</a>. This can be easily adapted to synchronous communication.</p>&#xA;&#xA;<p>In your case the document is passed from one service to another and then sent back completed. More details <a href=""http://www.informit.com/articles/article.aspx?p=1398616&amp;seqNum=2"" rel=""nofollow"">here</a>.</p>&#xA;"
39368793,39358842,569662,2016-09-07T11:33:23,"<blockquote>&#xA;  <p>I wanted to know is okay to expose Entity structure to external&#xA;  clients?</p>&#xA;</blockquote>&#xA;&#xA;<p>By exposing your entity structure directly across the service boundary you are breaking pretty much every <a href=""http://architectopia.blogspot.co.uk/2008/01/four-tenets-of-soa.html"" rel=""nofollow"">tenet of SOA</a> in some way.</p>&#xA;&#xA;<p>These tenets are there for a reason: to protect you from pain. The pain you could suffer from such an approach includes but is not limited to:</p>&#xA;&#xA;<ol>&#xA;<li>Can't modify your data model without breaking consumers</li>&#xA;<li>Leak datastore implementation-specific concepts into consumers</li>&#xA;<li>Service layer is basically anonymous and lacks any business definition </li>&#xA;<li>New consumers have no way of knowing how to consume the service</li>&#xA;</ol>&#xA;&#xA;<blockquote>&#xA;  <p>Approach 1: Convert json string to dynamic and the build the required&#xA;  entity by mapping properties.</p>&#xA;</blockquote>&#xA;&#xA;<p>Depending on your exact requirements, there may be a reason you want use this approach. For example, if all you're doing is exposing what is basically a database driver over http, then this approach is probably fine. </p>&#xA;&#xA;<p>However, this does not actually address any of the pain points and so is similar to your original approach. </p>&#xA;&#xA;<blockquote>&#xA;  <p>Approach 2: Create strongly type DTO</p>&#xA;</blockquote>&#xA;&#xA;<p>This is a better approach, but only marginally - it addresses pain points one and two above, but does nothing for 3 and 4. I would suggest a tweak to this approach which will address these:</p>&#xA;&#xA;<p>Approach 3: Create strongly type DTO and expose it directly.</p>&#xA;&#xA;<pre><code>[HttpPost]&#xA;public void DoSomethingWithDocument([FromBody] DocumentDTO docDto)&#xA;{&#xA;    // map from docDto to docEntity&#xA;    ...&#xA;&#xA;    _dbContext.Documents.Add(docEntity);&#xA;    _dbContext.SaveChanges();&#xA;}&#xA;</code></pre>&#xA;&#xA;<blockquote>&#xA;  <p>Since all the services are decoupled from each other there is no&#xA;  strongly typed data contract.</p>&#xA;</blockquote>&#xA;&#xA;<p>Trying to avoid coupling by refusing to share contract does not make sense. A well-defined contract is one of the best ways to decouple services.</p>&#xA;&#xA;<blockquote>&#xA;  <p>I am using Newtonsoft.json for serialization/de-serialization since it&#xA;  has better performance than Microsoft's native&#xA;  DataContractJsonSerializer</p>&#xA;</blockquote>&#xA;&#xA;<p>I would be careful if you find yourself radically deviating from a well-trodden path in order to incorporate a specific tool or technology for no other reason than performance. Could be a <a href=""http://c2.com/cgi/wiki?PrematureOptimization"" rel=""nofollow"">premature optimisation</a>.</p>&#xA;"
44326822,44313956,569662,2017-06-02T10:34:13,"<p>It's a trade off. I would personally not store the user details alongside the user identifier in the dependent services. But neither would I query the users service to get this information. What you probably need is some kind of read-model for the system as a whole, which can store this data in a way which is  optimized for your particular needs (reporting, displaying together on a webpage etc).</p>&#xA;&#xA;<p>The read-model is a pattern which is popular in the event-driven architecture space. There is a really good article that talks about these kinds of questions (in two parts):</p>&#xA;&#xA;<p><a href=""https://www.infoq.com/articles/microservices-aggregates-events-cqrs-part-1-richardson"" rel=""nofollow noreferrer"">https://www.infoq.com/articles/microservices-aggregates-events-cqrs-part-1-richardson</a>&#xA;<a href=""https://www.infoq.com/articles/microservices-aggregates-events-cqrs-part-2-richardson"" rel=""nofollow noreferrer"">https://www.infoq.com/articles/microservices-aggregates-events-cqrs-part-2-richardson</a></p>&#xA;&#xA;<p>Many common questions about microservices seem to be largely around the decomposition of a domain model, and how to overcome situations where requirements such as querying resist that decomposition. This article spells the options out clearly. Definitely worth the time to read.</p>&#xA;&#xA;<p>In your specific case, it would mean that the File and Friends services would only need to store the primary key for the user. However, all services should publish state changes which can then be aggregated into a read-model. </p>&#xA;"
40221269,40208887,569662,2016-10-24T14:43:16,"<blockquote>&#xA;  <p>...if I were to implement a microservices architecture, would the&#xA;  centralized read/write database become a bottleneck?</p>&#xA;</blockquote>&#xA;&#xA;<p>There is a assumption built-in to your question. The assumption is that the microservices must share the same master tables in the database. </p>&#xA;&#xA;<p>In fact, further down your question you give voice to this concept directly:</p>&#xA;&#xA;<blockquote>&#xA;  <p>...but they all rely on each other in the database, so exclusive,&#xA;  parallel databases wouldn't be appropriate.</p>&#xA;</blockquote>&#xA;&#xA;<p>If the microservices are sharing database tables then all you have effectively done is built one single ""service"" with multiple components which all happen to consume each other over some out-of-band transport rather than in-memory by direct binary reference. </p>&#xA;&#xA;<p>One of the most important concepts behind service orientation is <a href=""https://en.wikipedia.org/wiki/Service_autonomy_principle"" rel=""nofollow"">autonomy</a>, which basically means each service should own its own data. </p>&#xA;&#xA;<p>Extending your example, the <em>users service</em> will know about teams. How will it know? Well, the <em>teams service</em> will push team data to users service. Similarly, the <em>team_members service</em> will receive data from both services. Now, all services have all the data they need to do their jobs.</p>&#xA;&#xA;<p>So by implementing your services as autonomous the potential for contention on the same set of base tables dissapears.</p>&#xA;"
42490900,42457900,569662,2017-02-27T16:33:26,"<blockquote>&#xA;  <p>I assume that I could copy the accounts table over to the image and&#xA;  video databases and only store the accounts data that is needed but&#xA;  this seems like a nightmare in terms of maintaining consistency, etc.&#xA;  as account information is updated frequently, not to mention if other&#xA;  microservices need this same dependency</p>&#xA;</blockquote>&#xA;&#xA;<p>I think that would certainly be overkill. Actually, the extent of this dependence appears to rest on both video and image services needing to know the account with which they should associate their own entities. In both cases, this can be achieved by simply storing an account ID against the entity.</p>&#xA;&#xA;<p>This reduces any consistency concerns down to account IDs changing, which can be mitigated by making accounts immutable. What that means is that once an account is created it can never be removed. That way, it's impossible for a video or image to be associated with an account which no longer exists. </p>&#xA;"
42550705,42549749,569662,2017-03-02T08:52:14,"<blockquote>&#xA;  <p>the client is responsible for determining the network locations of&#xA;  available service instances and load balancing requests across them.</p>&#xA;</blockquote>&#xA;&#xA;<p>I'm not sure where you are reading that, but the whole concept of a service consumer being forced to implement load balancing is ridiculous. </p>&#xA;&#xA;<p>The client-side discovery pattern is actually just another way of saying defer call routing (and sometimes channel construction) until run-time by way of a service registry. As Richardson <a href=""http://microservices.io/patterns/client-side-discovery.html"" rel=""nofollow noreferrer"">says</a> in his patterns pages:</p>&#xA;&#xA;<blockquote>&#xA;  <p>When making a request to a service, the client obtains the location of&#xA;  a service instance by querying a Service Registry, which knows the&#xA;  locations of all service instances.</p>&#xA;</blockquote>&#xA;&#xA;<p>Load balancing is a completely different concern, and would always be delegated, ideally to an actual load balancer, though depending on the services platform you are using you may have access to some kind of in-process distributor.</p>&#xA;"
42529389,42528718,569662,2017-03-01T10:35:04,"<blockquote>&#xA;  <p>The recipient of an invoice must be a valid contact.</p>&#xA;</blockquote>&#xA;&#xA;<p>This is a business rule. The question should be asked, what does this business rule mean for my application? Who should take responsibility for implementing this rule, or can the responsibility be shared? </p>&#xA;&#xA;<p>One possibility is that, yes, the business rule is about invoices so it should be the responsibility of the Invoices Service to implement it. </p>&#xA;&#xA;<p>However, the business rule is really about the creation of invoices. And the owner of invoice creation in your architecture is, strangely, not the Invoices Service. The reason for this is that the name of the command is <code>CreateInvoiceCommand</code>.</p>&#xA;&#xA;<p>Let's think about this - the Invoices Service will never just create an invoice on its own. It just provides the capability. In this architecture, the actual owner of invoice creation is the sender of the command. </p>&#xA;&#xA;<p>Using this line of reasoning, if the business rule is saying that invoice creation cannot happen against an invalid recipient, then it becomes the responsibility of the command sender to ensure this business rule is implemented. </p>&#xA;&#xA;<p>This would be a very different scenario if, rather than receiving a command, the Invoices Service subscribed to events. As an example, an event called <code>WidgetSold</code>. In this scenario, the owner of invoice creation clearly would be the Invoicing service, and so the business rule would be implemented there instead.</p>&#xA;&#xA;<blockquote>&#xA;  <p>If the user clicks the create invoice for contact 42 button, it's the&#xA;  user's responsibility to take care that contact 42 exists</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes, that is correct. The user's intention is to create an invoice. The business rules around invoice creation should, therefore, be enforced at this point. How this happens (or whether this happens at all) is a different question.</p>&#xA;&#xA;<blockquote>&#xA;  <p>But what if the user doesn't care? Then it would create an invoice&#xA;  with an invalid recipient id.</p>&#xA;</blockquote>&#xA;&#xA;<p>Also correct. As you say, there are side-effects to this approach, one of which is that you can end up with inconsistent data across your system. That is one of the realities of SOA. </p>&#xA;&#xA;<blockquote>&#xA;  <p>Isn't this somehow similar to this: The Invoice has a currencyCode&#xA;  property, it's a String.</p>&#xA;</blockquote>&#xA;&#xA;<p>I don't know if I agree or not. Is asking <em>is this a valid ISO currency?</em> different to asking <em>is entity 42 valid according to another system?</em>. I would think so. </p>&#xA;&#xA;<blockquote>&#xA;  <p>Isn't it kinda the same as <em>given recipient is not null and is valid&#xA;  according to my Contacts Database?</em></p>&#xA;</blockquote>&#xA;&#xA;<p>I agree that in reality, you could implement this validation in the service. I am just saying that I don't think it's the right place for it. If you wanted to do this, you would have to either call out the another service or store all contacts locally, as you framed your question originally. I think it's simpler to just do it outside of the service.</p>&#xA;"
23056484,22513893,569662,2014-04-14T09:25:18,"<p>I have posted about this <a href=""https://stackoverflow.com/a/10161240/569662"">before</a> - but generally speaking, synchronous operations (for example, a user clicking a button and expecting some data back) are synchronous for a reason. </p>&#xA;&#xA;<p>That is to say, synchronous - not because of the technology used to process their call - but because of a built-in and generally inflexible expectation on the part of the user that things should happen in real time and not ""offline"" (even if there is no material difference most of the time).  </p>&#xA;&#xA;<p>So, it's generally unwise to put any kind of offline or asynchronous technology stack in between a user and their expected response. </p>&#xA;&#xA;<p>As with all things, exceptions abound (and could spawn a whole new conversation), but certain types of user calls can and should be handled ""off-line"" depending on the situation. </p>&#xA;&#xA;<p>However, I do feel that the focus of your assertion:</p>&#xA;&#xA;<blockquote>&#xA;  <p>I like the idea of using messaging to decouple the clients from the&#xA;  services</p>&#xA;</blockquote>&#xA;&#xA;<p>misses the point somewhat. We don't actually want to decouple clients (or consumers) and services. </p>&#xA;&#xA;<p>We would expect a client for, say, the Accounts Payable business capability to be highly coupled to the accounts payable microservice. </p>&#xA;&#xA;<p>Similarly we would expect a high degree of coupling between a service endpoint signature <code>bool ProcessTransaction(Transaction transaction)</code> and the consumer of such an operation.</p>&#xA;&#xA;<p>Where decoupling becomes really important is decoupling between services which support different business capabilities. </p>&#xA;&#xA;<p>And it's here that the benefits of messaging really make a difference. Let me know what you think and if this has helped you at all. </p>&#xA;"
44072144,44065208,569662,2017-05-19T14:05:10,"<blockquote>&#xA;  <p>My question is how can I ensure data integrity between databases, that Orders table won't point to non-existent Customers?</p>&#xA;</blockquote>&#xA;&#xA;<p>It's a great question. There is an important dimension missing from it though, which is that of the <em>span of time</em> over which you wish to establish the referential integrity. </p>&#xA;&#xA;<p>If you ask, ""How can I ensure that all my data is 100% consistent at all times?"" - well the answer is you can't. If you want that you will need to enforce it, either via foreign key constraints (which are unavailable across databases), or by making sure you never write to one database and not the other database outside of some distributed transaction (which is absurd and would defeat the purpose of using service orientation).</p>&#xA;&#xA;<p>If you ask, ""How can I ensure that all my data is 100% consistent after a reasonable span of time?"", then there are things you can do. A common approach is to implement durable, asynchronous <a href=""http://www.enterpriseintegrationpatterns.com/patterns/messaging/EventMessage.html"" rel=""nofollow noreferrer"">eventing</a> between your services. This ensures that changes can be written locally and then dispatched remotely in a reliable, but offline manner. A further thing you can do is have a scheduled caretaker process which periodically remediates inconsistencies in your data. </p>&#xA;&#xA;<p>However it has to be said that outside of a transaction, even over a reasonable span of time, consistency is impossible to guarantee absolutely. If absolute consistency is a requirement for your application then service orientation may not be the approach for you. </p>&#xA;"
44133284,44131588,569662,2017-05-23T11:17:38,"<blockquote>&#xA;  <p>Is it a good practice to share Response/Request models between ... a&#xA;  REST microservice and a Client library?</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes, it's a good practice. Request/response models are part of the service definition, so are one of the few things which should be shared between a service and its consumers. Vertical coupling between services and consumers is unavoidable, so should be embraced. Care should, however, be taken when sharing models between services.</p>&#xA;&#xA;<blockquote>&#xA;  <p>now I am confusing about the following: if another microservice will&#xA;  use that Client library</p>&#xA;</blockquote>&#xA;&#xA;<p>I'm not sure what you mean - the service doesn't use the client library. </p>&#xA;&#xA;<p>Ideally, there should be no sharing of models between services unless they are models which represent cross-cutting concerns such as security, monitoring, etc.</p>&#xA;"
36719332,36716838,569662,2016-04-19T13:05:09,"<blockquote>&#xA;  <p>webapp -> orchestration -> <strong>service</strong> -> persistance </p>&#xA;  &#xA;  <p>api -> api gw -> orchestration -> <strong>service</strong> -> persistance (emphasis mine)</p>&#xA;</blockquote>&#xA;&#xA;<p>Can we back up a bit? I'd like to question the terminology used here. </p>&#xA;&#xA;<p>To me, the above stacks dont make much sense within the context of SOA. </p>&#xA;&#xA;<p>You have the <em>service</em> sandwiched between things called <em>orchestration</em> and <em>persistence</em>. However, in an SOA design, all of the above elements are necessary to create a single service. </p>&#xA;&#xA;<p>But what <em>is</em> persistence in your example? Whatever it is, it would appear to be outside of the service. So how can the service persist data? The web app/API also appear to be outside the service. So how can the service display it's data on the screen? </p>&#xA;&#xA;<p>If you look at the tenets of SOA, specifically the second tenet:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Services are <a href=""http://serviceorientation.com/serviceorientation/service_autonomy"" rel=""nofollow"">autonomous</a></p>&#xA;</blockquote>&#xA;&#xA;<p>If services are supposed to be autonomous, then the service needs to be responsible for <a href=""http://microservices.io/patterns/data/database-per-service.html"" rel=""nofollow"">persisting it's own data</a>. The service also needs to be take responsiblity for displaying it's internal state via the UI. </p>&#xA;&#xA;<blockquote>&#xA;  <p>What I'm having serious thoughts about are an orchestration layer</p>&#xA;</blockquote>&#xA;&#xA;<p>It should follow on from this that services should also take responsibility for how internal state is communicated to the outside world, including between itself and other services. </p>&#xA;&#xA;<p>If the service needs to consume data from another service, it becomes the responsibility of the consumer service to get that data. If the service's state changes, it is the responsibility of that service to allow the outside world to know about the state change.</p>&#xA;&#xA;<p>Orchestration is a <a href=""https://en.wikipedia.org/wiki/Concern_(computer_science)"" rel=""nofollow"">concern</a>, just like persistence, instrumentation, etc, and so in the context of SOA is best implemented in a distributed, autonomous manner, rather than as a centalised one.</p>&#xA;&#xA;<p>So, in answering your questions:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Where does billing belong?</p>&#xA;</blockquote>&#xA;&#xA;<p>Billing belongs to it's own vertical service stack, comprising UI views, persistence, orchestration, communications, deployment and management.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Which solution do you prefer? Pros/cons.</p>&#xA;</blockquote>&#xA;&#xA;<p>As stated, I don't think the choice should be made at the level the question is posed. I think that whichever services require orchestration should be take responsibility for it.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Other suggestions?</p>&#xA;</blockquote>&#xA;&#xA;<p>Have a look at <a href=""https://vimeo.com/5022174"" rel=""nofollow"">this</a> if you haven't already seen it.</p>&#xA;"
36689667,36660279,569662,2016-04-18T09:05:48,"<p>The second tennet of SOA states:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Services are <a href=""http://serviceorientation.com/serviceorientation/service_autonomy"" rel=""nofollow"">autonomous</a></p>&#xA;</blockquote>&#xA;&#xA;<p>What this means is that as much as possible, there should be no horizontal coupling between services. Taking a specific example from your question, the Department service should not be coupled to the Projects service. </p>&#xA;&#xA;<p>However, by modelling both department and projects data within a single database, you are already introducing coupling between them. </p>&#xA;&#xA;<p>To decouple your services then, you should give each service it's own database. Services would then have to communicate via some other mechanism to exchange data with each other, rather than via the database. </p>&#xA;&#xA;<p>However, this approach introduces complexity into your solution. This is known as the <em><a href=""http://martinfowler.com/bliki/MicroservicePremium.html"" rel=""nofollow"">microservice premium</a></em>. You should ask yourself if this cost is worth the benefit of a microservice-based approach, at least to begin with. </p>&#xA;"
36689269,36680157,569662,2016-04-18T08:46:48,"<p>If latency is really an issue to you then you should probably not be using service calls between your components. Rather you should minimize the number of times control passes to an out-of-band resource and be making the calls in-process, which is much faster.</p>&#xA;&#xA;<p>However, in most cases, the overheads incurred by the service ""wrappers"" (channel construction, serialisation, marshalling, etc), are negligible enough and still well within <em>adequate</em> latency tolerances for the business process being supported.</p>&#xA;&#xA;<p>So you should ask yourself:</p>&#xA;&#xA;<ol>&#xA;<li>Is latency really an issue for you, in respect to the business process? In my experience only engineers care about latency. Your business customers do not.</li>&#xA;<li>If latency is an issue, then can the latency definitively be attributed to the cost of making the service calls? Could there be another reason the calls are taking a long time?</li>&#xA;<li>If it is the services, then you should look at consuming the service code as an assembly, rather than out-of-band.  </li>&#xA;</ol>&#xA;"
36645418,36642718,569662,2016-04-15T11:03:25,"<p>I am going to recommend an approach which to some extent would depend on the answer to the following question:</p>&#xA;&#xA;<p>What is the maximum acceptable time delay from the time when a message is committed to the database to the time the message becomes available for a customer to see it?</p>&#xA;&#xA;<p>If the answer to this question is > 10ms then you could consider using <a href=""http://udidahan.com/2009/12/09/clarified-cqrs/"" rel=""nofollow"">read-write separation</a> - yours would appear to be a good use-case. </p>&#xA;&#xA;<p>Although this would arguably introduce more complexity into your solution, the benefits of this approach would include:</p>&#xA;&#xA;<ul>&#xA;<li>No contention between database writes and reads</li>&#xA;<li>Writes can be scaled independently.</li>&#xA;<li>Your written data can be stored in relational format</li>&#xA;<li>Customer data can be read in the manner which most simplifies retreval and display concerns (eg denormalised, aligned with viewmodel)</li>&#xA;</ul>&#xA;&#xA;<p>Applying this to your architecture, even without the use of some kind of durable queuing transport it is still possible but more difficult to implement read-write separation. Rather than capitalise on events you would have to make the entire thing command driven. </p>&#xA;&#xA;<p>The main difference here is that you would need to enforce ""transactionability"" across your main database write and the subsequent call to the service responsible for updating the read model. </p>&#xA;"
35747024,28319278,569662,2016-03-02T12:17:18,"<blockquote>&#xA;  <p>...the above mentioned endpoints should be restricted ONLY to other&#xA;  microservices within the same application...</p>&#xA;</blockquote>&#xA;&#xA;<p>What you are talking about in a broad sense is authorisation. </p>&#xA;&#xA;<p>Authorisation is the granting or denying of ""powers"" or ""abilities"" within your application to authentic users.</p>&#xA;&#xA;<p>Therefore the job of any authorisation mechanism is to validate the ""claim"" implicit in any inbound API request - that the user is allowed to do the thing encoded in the request.</p>&#xA;&#xA;<p>As an example, imagine I turned up at your API with a PUT request for Widget 1234:</p>&#xA;&#xA;<pre><code>PUT /widgetservice/widget/1234 HTTP/1.1&#xA;</code></pre>&#xA;&#xA;<p>This could be interpreted as me (Bob Smith, a known user) making a claim that I am allowed to make changes to a widget in your system with id 1234.</p>&#xA;&#xA;<p>Whatever you do to validate this claim, I hope you can see this needs to be done at the application level, rather than at the API level. In fact, authorisation is an application-level concern, rather than an API-level concern (unlike authentication, which <em>is</em> very much an API level concern).</p>&#xA;&#xA;<p>To demonstrate, in our example above, it's theoritically possible I'm allowed to create a new widget, but not to update an existing widget:</p>&#xA;&#xA;<pre><code>POST /widgetservice/widget/1234 HTTP/1.1&#xA;</code></pre>&#xA;&#xA;<p>Or even I'm allowed to update only widget 1234 and requests to change other widgets should not be allowed</p>&#xA;&#xA;<pre><code>PUT /widgetservice/widget/5678 HTTP/1.1&#xA;</code></pre>&#xA;&#xA;<blockquote>&#xA;  <p>How to achieve that restricted api access to other microservices&#xA;  within the same application?</p>&#xA;</blockquote>&#xA;&#xA;<p>So this becomes a question about how can you build authorisation into your application so that you can validate individual requests coming from known users (in your case your other services in your ecosystem are just another kind of known user). </p>&#xA;&#xA;<p>Well, and apologies but I'm going to be prescriptive here, you could use a claims-based authorisation service, which stores valid claims based on user identity or  membership of roles. </p>&#xA;&#xA;<p>It depends largely on how you are handling authentication, and whether or not you are supporting roles as part of that process. You could store claims against individual users but this becomes arduous as the number of users increases. <a href=""http://oauth.net/2/"" rel=""nofollow"">OAuth</a>, despite being pretty heavy to implement, is a leading platform for this.</p>&#xA;&#xA;<blockquote>&#xA;  <p>I am building huge application using microservices architecture</p>&#xA;</blockquote>&#xA;&#xA;<p>The only thing I will say here is read <a href=""http://martinfowler.com/bliki/MonolithFirst.html"" rel=""nofollow"">this</a> first.</p>&#xA;"
45177810,45170527,569662,2017-07-18T21:58:25,"<p>The ideal solution is to never be in a position where you are deploying into an environment where you don't already know the versions of your dependencies. That way madness lies.</p>&#xA;&#xA;<p>Avoiding this is a governance concern and so should be central to any service oriented approach to building software. </p>&#xA;&#xA;<p>For instance, let's say you are developing version 2.0 of your service A. In your target environment, you have service B version 1.0 and service C version 1.0. </p>&#xA;&#xA;<p>So the first step on the path to stress-free releases, as part of your development build, you should be running a set of nearest neighbour automated tests, which stub out B v1.0 and C v1.0 based on the service contracts (more on this later). This can be facilitated using test double tools such as <a href=""http://www.mbtest.org/"" rel=""nofollow noreferrer"">mountebank</a>.</p>&#xA;&#xA;<p>Then, just as you have created your v2.0 release branch, you learn that another team is about to release v1.1 of service C. It should always be possible to work out whether v1.0 to v1.1 constitutes a breaking change to the v1.0 contract for service C (more on this later).</p>&#xA;&#xA;<p>If v1.1 is a breaking change, no problem, you update your tests with v1.1 of the service C contract and fix any failures. You are then good to create a new v2.0.1 patch branch and release. If for whatever reason you are forced to release before service C you can still release from the v2.0 branch. </p>&#xA;&#xA;<p>If v1.1 is not a breaking change, no problem, just release off your existing branch.</p>&#xA;&#xA;<p>There are various strategies for coping with the overhead produced by a centralised release management protocol such as described above.</p>&#xA;&#xA;<p>As stated earlier, contracts for all dependent services should be used when testing your service. (Note: it's very important for the nearest neighbour tests to be driven from contracts, rather than using existing code models, such as DTOs defined in the service's unit tests). Contracts for all the services should be based on a standard (such as <a href=""http://editor.swagger.io/#/"" rel=""nofollow noreferrer"">swagger</a>) which supplies a complete service description, and be very easy to find - the use of a <a href=""https://www.infoq.com/articles/SimpleServiceRepository"" rel=""nofollow noreferrer"">service repository</a> can simplify this.</p>&#xA;&#xA;<p>Also stated earlier, it should always be possible to know if new versions of dependent services have the potential to break your service. One strategy is to agree on a versioning convention which bestows some kind of meaning when incrementing the version. For instance, you could use a major.minor.patch (eg v1.0.0) where a change the major version number constitutes a change to the service contract and therefore has the potential to break things. In our previous example, service C went from v1.0 to v1.1. With a convention such as the one described above, we could be sure that the change would not break us, as the major version number was unchanged.  </p>&#xA;&#xA;<p>While it can be cumbersome to set up and maintain a centralised release management protocol, the benefit is that you always have full confidence that by deploying your service, nothing will break. What's more, this avoids having any complex (and to my mind, contrived) runtime dependency resolution, such as you are proposing in your original question.</p>&#xA;"
32392765,32364506,964926,2015-09-04T07:46:29,"<p>What you are missing here is that microservices is not suitable for any software system in the world! Think about the drivers for migrating your current monolithic system to microservices before doing anything. Are you seeking for high availability and scalability? Do you want to have freedom for writing each thread in different programming languages? Is your system that complicated that could not be comprehended in a monolithic style? and finally, are you ready for paying the expenses of having a microservices style? </p>&#xA;&#xA;<p>Microservices brings in many complexities to the system and may cause performance penalties in favor of higher scalability due to chattiness of services. If performance is an important concern, the system is not that large, and your answer to most of the above questions is ""NO"", I strongly suggest that you do not go for microservices style. Instead, try to modularize your current code base and refactor the code for better quality and comprehensibility.</p>&#xA;&#xA;<p>Regarding Docker, you can use it even with the monolithic style in order to remove some of the deployment barriers and inconsistency in the development and the deployment environments. If the mentioned issues around deployment do not bother you, do not go for docker either since it will be just a layer of computational overhead.</p>&#xA;"
35819395,35817536,4190526,2016-03-05T20:20:37,"<p>If <code>service1-frontend</code> needs to access <code>service1-backend</code> via <code>name:port</code>, you need:</p>&#xA;&#xA;<ol>&#xA;<li><code>service1-backend</code> needs to have a port expose specified in its Dockerfile</li>&#xA;<li><code>service1-frontend</code> needs to have a link to <code>service1-backend</code> in the compose file.</li>&#xA;</ol>&#xA;&#xA;<p>Then in the frontend, you can access backend simply use <code>service1-backend:port</code>.</p>&#xA;"
49525322,49525246,2160958,2018-03-28T02:52:08,"<h1>Do not do it</h1>&#xA;&#xA;<p>The idea of splitting your application into microservices is to keep it independent. Therefore centralised configuration breaks this idea plus doing so (for example with some kind of proxy microservice) you would have to probably run them on the same machine.</p>&#xA;&#xA;<h1>Is is for local development ?</h1>&#xA;&#xA;<p>If it is, simply create <a href=""https://docs.docker.com/compose/gettingstarted/"" rel=""nofollow noreferrer"">docker-compose</a> containers to allow developers easy setup of development environment. Still this will require multiple configurations for each container/service</p>&#xA;&#xA;<h1>Do not do microservices</h1>&#xA;&#xA;<p>Maybe what you want to active is not microservice architecture. Take a look <a href=""https://martinfowler.com/bliki/BoundedContext.html"" rel=""nofollow noreferrer"">here</a>. Might be what you wanted instead and services should be easy to port into bounded context.</p>&#xA;&#xA;<p>Also keep in mind that <strong>bounded contexts are not microservices</strong></p>&#xA;"
44874171,44873885,3874623,2017-07-02T18:54:17,"<p>I'm not sure this question has one good answer. But here are some thoughts:</p>&#xA;&#xA;<ol>&#xA;<li><p>Are you sure you need individual auth and user services and can't build it as one unit? This would be more efficient — in general people try to avoid calling lambda functions from lambda functions because it's slower and can be hard to test/debug.</p></li>&#xA;<li><p>If you need to communicate between lambdas you can invoke one directly rather than going through the api gateway using the SDK of the language you're using. For example javascript: <a href=""http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Lambda.html#invoke-property"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Lambda.html#invoke-property</a> This way you can control access with roles and policies.</p></li>&#xA;<li><p>Don't reinvent the wheel. Have you looked at AWS Cognito? <a href=""http://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p></li>&#xA;</ol>&#xA;"
45565616,45486658,683327,2017-08-08T10:16:09,"<p>Based on my understanding ,i will suggest point 1</p>&#xA;&#xA;<p><strong>Addition of new Microservices</strong>  it will be easier to add new microservices and that will still be able to work without much changes.</p>&#xA;&#xA;<p><strong>Business Logic</strong> should be part of microservice it self and not of messaging queue</p>&#xA;"
45541784,45538292,683327,2017-08-07T07:46:48,"<p>This are the changes you need to perform to achieve the same with Rabbit MQ</p>&#xA;&#xA;<ul>&#xA;<li>Create 2 seperate queue, one for each B and C service</li>&#xA;<li>Change your logic to read message from queue such that only one&#xA;instance will read the message from queue ,using blocking&#xA;connection thing of rabbitmq.</li>&#xA;</ul>&#xA;&#xA;<p>this way, when multiple instance of B and C are running both will get the message and still be scalable.</p>&#xA;"
43039582,42981194,683327,2017-03-27T06:47:14,"<p>Here is my thought </p>&#xA;&#xA;<p>1.<strong>User Service</strong>  - should be responsible for</p>&#xA;&#xA;<p>Creation of user which includes user name ,password (hashed) , email and any other profile data</p>&#xA;&#xA;<p>validation of  input data against validation rules</p>&#xA;&#xA;<p>validation of user using his password</p>&#xA;&#xA;<p><strong>pros</strong></p>&#xA;&#xA;<p>further adition of profile data is easy</p>&#xA;&#xA;<p>finding and validatng user in single request</p>&#xA;&#xA;<p>user related login in single place</p>&#xA;&#xA;<p>2.<strong>Authentication Service</strong> should be responsible only to generate tokens based upon successful user validation via user service</p>&#xA;&#xA;<p>these token should be used for further processing by all other services in ecosystem and will make sure proper authorisation</p>&#xA;&#xA;<p><strong>pros</strong> </p>&#xA;&#xA;<p>future addition of services that will require user authentication and authorisation can work independently and will require only security token.</p>&#xA;&#xA;<p>Generation of token based on previous token can be easy ,user will not need to enter his user name and password each time his token is about to expire.</p>&#xA;"
45710209,45667346,683327,2017-08-16T09:44:35,"<p>Create a seperate queue for each microservice (if not already created)</p>&#xA;&#xA;<p>In Rabbit MQ ,use <strong>Fanout Exchange</strong> hence each queue will reacieve all the messages.</p>&#xA;&#xA;<p>Now if multiple instance of a single microservice is bind with same queue then only one instance will be able to pick a message from that queue.Due to round robin nature of queue.</p>&#xA;"
45710460,45655728,683327,2017-08-16T09:56:30,"<p><strong>Microservice</strong>  is well defined when you are following <strong>SOC - seperation of Concern</strong> on the entity/domain level ,where each entity / domain are independent of any other service.</p>&#xA;&#xA;<p><strong>for example</strong> user service will only be responsible for storing, updating and deleting user related informations.</p>&#xA;&#xA;<p><strong>Microservice backend and frontend</strong> microservice can further be splitted in 2 parts</p>&#xA;&#xA;<ol>&#xA;<li>frontend microservice which exposes rest endpoint just like <strong>Web API</strong> </li>&#xA;<li>backend microservice which actually perform all the operations.</li>&#xA;</ol>&#xA;&#xA;<p><strong>Rest API</strong> is more of endpoints exposed to outer world and can be used with microservices as well, as explained above.</p>&#xA;"
45713804,45642575,683327,2017-08-16T12:40:28,"<p><strong>MicroServices</strong> simple independent units communicate with message broker ,read a requirement message from its own queue and send the response to the message broker again ,which broadcast the same resposne to all the other queues.</p>&#xA;&#xA;<p><strong>API Gateway</strong> provide rest API and post a requirement message to message broker , that message will consist of details required by microservice to process a request, also has its own unique queue which is binded with message broker and once a message arrive which has a solution for specific request ,it will respond back to the client.</p>&#xA;"
45730402,29644916,683327,2017-08-17T08:42:48,"<p>you can use <a href=""http://docs.identityserver.io/en/release/#"" rel=""nofollow noreferrer"">idenitty server 4</a> for authentication and authorisation purpose</p>&#xA;&#xA;<p>you must use <strong>Firewall Architecture</strong> hence you have more control over <strong>secutiry , robustness ,scalability and ease of use</strong></p>&#xA;"
45730794,43850359,683327,2017-08-17T09:02:22,"<p>Microservice is best when you need scalability and flexibility ,</p>&#xA;&#xA;<p>With your current architecture diagram ,i can understand that you are making synchronous call to recieve data hence sale service has to wait for data to come from all the service to get the full result hence more delay in the output.</p>&#xA;&#xA;<p>Another observation is scalability of microservices which seems not being acheved right now ?</p>&#xA;&#xA;<p>If you can make asyncronous call to microservices then you can imrpove performance and club the data once every service return the results desired.</p>&#xA;"
45731062,39485459,683327,2017-08-17T09:14:48,<p>you can mix and match of different stretagies</p>&#xA;&#xA;<p>For Major changes you can use <strong>Routing based versioning</strong> </p>&#xA;&#xA;<p>For other you can use <strong>Adapter Based Versioning</strong></p>&#xA;
45812798,42873668,683327,2017-08-22T08:39:08,"<p><strong>Microservice</strong> must be created to have a one and only one resposiblity , in your case Notofication service should be <strong>seperate microservice</strong> ,which perform the action depending on the events. </p>&#xA;&#xA;<p>hence you will able to <strong>scale up or down</strong> you notification service when number of nitifcation increases or decreases</p>&#xA;"
45812961,45431599,683327,2017-08-22T08:47:12,"<p>Try to have as much microservice as you can based up single responsiblity system.</p>&#xA;&#xA;<p>Create  a API service and allow it to generate <strong>events</strong> which will be then <strong>consumed</strong> by other microservices and will <strong>provide results</strong> based on required parameters.</p>&#xA;&#xA;<p>Now <strong>API can club the data</strong> and can <strong>respond back</strong> with the required format.</p>&#xA;&#xA;<p>having <strong>multiple microservices</strong> will help you in <strong>scaling up and down</strong> , if you had only 2 microservices it is more or less like a monolithic services only.</p>&#xA;&#xA;<p>Take the decision, considering future in mind.</p>&#xA;"
45840705,45771237,683327,2017-08-23T13:10:07,"<p>My answer is based on assumption that each service is independent and don't interact with each others and <strong>can possibly scaled up or down</strong>.</p>&#xA;&#xA;<p>Use <strong>Redis data cache service</strong>, introduce a variable there. Each service will be able to refer that variable and will update when ever they make a API call, write <strong>some conditions</strong> so no service is allow to make calls if its reach to 1000 for that specific second .</p>&#xA;&#xA;<p>Hence they will not be able to make more than 1000 call per seconds.</p>&#xA;"
48915328,44692442,3567935,2018-02-21T20:58:02,"<ol>&#xA;<li><p>For consuming https service, you have to trust the certificate provided by the service if they are using self-signed certificate. For that you have to configure trust-store, not key-store</p></li>&#xA;<li><p>If the self-signed certificate DN and domain name of the service are not matching, you have to configure zuul to disable that validation - but please note that this is not recommended in production&#xA;Configuration is given below:</p>&#xA;&#xA;<pre><code>zuul.sslHostnameValidationEnabled=false&#xA;</code></pre></li>&#xA;</ol>&#xA;"
32414168,32392202,1014079,2015-09-05T14:26:48,"<p>There are a number of things to bear in mind when separating out a functionality into communicating micro-services:</p>&#xA;&#xA;<h2>Tying of scala versions between server and client</h2>&#xA;&#xA;<p>If your server requires specific versions of scala (because, for example, you use a library that only exists for version 2.10), this should not impact your choice of scala version in the client. This points towards the idea of having the classes representing your communication path, as being in a separate project which can be cross-compiled separately.</p>&#xA;&#xA;<h2>Tying of libraries between server and client</h2>&#xA;&#xA;<p>The less requirements your shared library places on your client code, the better. Even forcing a particular choice of Play server enforces a level of rigidity and coupling between client and server that is best avoided.</p>&#xA;&#xA;<p>The best option is that this library causes a dependency on zero other libraries.</p>&#xA;&#xA;<h2>Supporting protocol changes over time</h2>&#xA;&#xA;<p>One of the advantages of having separate services is that they can be upgraded and improved at separate points in time. You should always try and have the server support the previous version of your communications protocol, whenever it changes. This allows you to roll back an update easily, and also update the client at a different point in time.</p>&#xA;&#xA;<p>Not allowing backwards compatibility means you need to update both services in lock-step. This not only reduces a lot of the advantages of using micro-services, it also makes it a huge pain to deal with rollbacks, if that becomes necessary.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>The universal story here is to enforce as little as possible in the way of choice (scala version, library version, time period when protocol changes must happen) on the client, through what choices are made on the server.</p>&#xA;&#xA;<p>If you can follow this approach, I don't see a problem with using code to enhance the accessibility of talking to a service.</p>&#xA;"
45867817,37938389,895295,2017-08-24T17:37:28,"<p>I know this is a slightly old post, but just in case, Amazon have the Cloud HSM solution that enables integration directly using JCE or PKCS#11 which means there should not be any changes to your app (it will be as if you were accessing a proxy to the keystore).  Here's a link with more details:</p>&#xA;&#xA;<p><a href=""https://aws.amazon.com/cloudhsm/faqs/"" rel=""nofollow noreferrer"">https://aws.amazon.com/cloudhsm/faqs/</a></p>&#xA;"
44000525,44000273,2691904,2017-05-16T11:42:29,"<p>There's not a lot to go on, but from what you've said it sounds like a job for <a href=""http://www.enterpriseintegrationpatterns.com/patterns/messaging/PipesAndFilters.html"" rel=""nofollow noreferrer"">""pipes and filters""</a>! </p>&#xA;&#xA;<p>To get a more precise answer, you might want to ask yourself some more detailed questions: </p>&#xA;&#xA;<p>If you need to do any validation or transformation of the incoming message?  Will you want to handle all requests the same way, or are there different types?  Are the external services likely to change, and if so, will they do this frequently?  What do you want to do if the final web service call fails (should you rollback the database record?)? How do you want to report failures/responses - do you need to report these back?  Do you need a mechanism to track the progress of a particular request?</p>&#xA;"
49291648,48804675,2890048,2018-03-15T04:32:25,"<p>I went through this document : <a href=""https://try.servicefabric.azure.com"" rel=""nofollow noreferrer"">Party Cluster</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>Party Cluster runs for an hour ( Around 1 hour ), Then it's&#xA;  automatically taken down. At that point, all our applications will be&#xA;  removed and We will have to connect to a new cluster to keep partying.&#xA;  We can only connect to one party cluster at a time.</p>&#xA;</blockquote>&#xA;"
44147868,43440170,7875136,2017-05-24T02:18:28,"<pre><code>ReflectionTestUtils.setField(producerConfiguration, ""messageProducer"", realProducer);&#xA;Object target = AopTestUtils.getUltimateTargetObject(fxRatesEventPublisher);&#xA;int recoveryAge = -1;&#xA;ReflectionTestUtils.setField(target, ""maxRecoveryAge"", recoveryAge);&#xA;</code></pre>&#xA;"
51798424,51726683,1135424,2018-08-11T09:45:44,"<p>There are multiple ways and approaches for doing this besides being bound to your current setup and infrastructure without excluding the flexibility to implement/modify the existing code base.</p>&#xA;&#xA;<p>When trying to communicate between services behind the API Gateway is something that needs to be carefully implemented to avoid loops, exposing your data or even worst, blocking your self, see the ""generic"" image to get a better understanding: <a href=""https://i.stack.imgur.com/15l6C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/15l6C.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>While using <strong>HTTP</strong> for communicating between the services it is often common to see traffic going out the current infrastructure and then going back through the same API Gateway, something that could be avoided by just going directly the other service in place instead. </p>&#xA;&#xA;<p>In the previous image for example, when <code>service B</code> needs to communicate with <code>service A</code> it is advisable to do it via the internal (ELB) endpoint instead of going out and going back through the API gateway.</p>&#xA;&#xA;<p>Another approach is to use ""only""  <strong>HTTP</strong> in the API Gateway and use other protocols to communicate within your services, for example, <a href=""https://grpc.io/"" rel=""nofollow noreferrer"">gRPC</a>. (not the best alternative in some cases since depends on your architecture and flexibility to modify/adapt existing code)</p>&#xA;&#xA;<p>There are cases in where your infrastructure is more complex and you may not communicate on demand within your containers or the endpoints are just unreachable, in this cases, you could try to implement an <a href=""https://read.acloud.guru/event-driven-architecture-with-sqs-and-aws-lambda-cf2ebd529ae3"" rel=""nofollow noreferrer"">event-driven architecture</a> (SQS and AWS Lambda)</p>&#xA;&#xA;<p>I like going <strong>asynchronous</strong> by using events/queues when possible, from my perspective ""scales"" better and must of the services become just consumers/workers besides no need  to listen for incoming request (no HTTP needed), here is an article,  explaining how to use <a href=""https://codeburst.io/using-rabbitmq-for-microservices-communication-on-docker-a43840401819"" rel=""nofollow noreferrer"">rabbitmq for this purpose communicating microservices within docker</a></p>&#xA;&#xA;<p>These are just some ideas that hope could help you to find your own ""best"" way since is something that varies too much and every scenario is unique.</p>&#xA;"
46103700,26706240,1135424,2017-09-07T19:22:52,"<p>I would add to the list of supervisors, <a href=""https://immortal.run/post/run.yml/"" rel=""nofollow noreferrer"">immortal</a> A *nix cross-platform (OS agnostic) supervisor that simplifies the 12 factors practices.</p>&#xA;&#xA;<blockquote>&#xA;  <p>timestamp</p>&#xA;  &#xA;  <p>By default the option timestamp within the log is set to false, this is in favor of applications that follow the 12 factor and in cases where log is structured “JSON”, it could be parsed easily.</p>&#xA;</blockquote>&#xA;"
40516283,40495213,6317376,2016-11-09T21:25:46,"<p>With our projects we follow a pattern similar to this, but not that fine grained. Each SF Application is contained in it's own repo, but we'll have multiple specific microservices in an application. We separate our applications into specific pieces of functionality in respect to the end application (Data Tier, Middle Tier, Presentation, Analytics, etc). When we upgrade we'll upgrade specific applications at a time, not necessarily specific services. Upgrading specific services is a huge pita ops wise. We still have a shared interfaces project and we use SF remoting to communicate between the different applications and we are able to do that because we manage containers and interfaces in its own repo that we then distribute via a private nuget server. This makes things difficult workflow wise but in the end it's nice because it makes us remain aware of interface compatibility between applications. We also have some core microservices that every application will have which we distribute using <a href=""https://www.nuget.org/packages/SFNuGet/"" rel=""nofollow noreferrer"">SF Nuget</a>. It's still young and has some sharp edges, but it's awesome. </p>&#xA;"
45553269,45551966,1364288,2017-08-07T18:14:46,"<p>If changing the hard-coded addresses is not an option for now, perhaps you could modify the startup scripts of your containers to forward forward ports in each local container to the required services in other machines.</p>&#xA;&#xA;<p>This would create some complications though, because you would have to setup ssh in each of your containers, and manage the corresponding keys.</p>&#xA;&#xA;<p>Come to think of it, if encryption is not an issue, ssh is not necessary. Using <a href=""http://www.dest-unreach.org/socat/doc/socat.html"" rel=""nofollow noreferrer"">socat</a> or <a href=""https://linux.die.net/man/1/redir"" rel=""nofollow noreferrer"">redir</a> would <a href=""https://superuser.com/questions/50331/port-forwarding-on-linux-without-root-or-ssh"">probably be enough</a>.</p>&#xA;&#xA;<pre><code>socat TCP4-LISTEN:61001,fork TCP4:othercontainer:61001&#xA;</code></pre>&#xA;"
47146608,47143597,1771073,2017-11-06T22:04:38,<p>Make sure the context component scan base package is correct. If not specified specify it using regex.</p>&#xA;
45203376,45203055,70465,2017-07-20T00:52:04,"<p>I think if you are able to adapt your nomenclature/approach a bit, you will find that you can use Apache NiFi to perform almost all of the tasks in your system. While NiFi is designed to handle streaming data rather than batching/jobs, you can use CRON timing to schedule task execution (the initial REST API invocation and data retrieval) and then have the flowfiles sent to follow-on processors. With this model, you can ingest data, treat the data as discrete records and convert easily between various formats like XML, JSON, CSV, Avro, etc., filter, route, transform, and query the data, and then route it to the destination SQL database. All of these operations will have live monitoring, provenance records, auditing, replayability, logging, fine-grain user access controls, etc. This will also have the added benefit that you do not need to write any code to handle these activities -- you simply drag and drop the components on the flow. You can make modifications to any part of the flow without influencing unrelated operations, so there is no interruption to the data flow. </p>&#xA;"
40841133,40840470,5318465,2016-11-28T09:39:18,"<p>It's possible to change the increment step &amp; offset of auto increment to ensure uniqueness over a cloud of MySQL hosts. All instances can have the same increment size, but a different initial offset which will ensure no primary keys clash.</p>&#xA;&#xA;<p>This approach also has the minor advantage that you can tell immediately from the primary column in row of data which database instance created it.</p>&#xA;&#xA;<p>In my.cnf, add:</p>&#xA;&#xA;<pre><code>auto-increment-increment = 100&#xA;auto-increment-offset = N&#xA;</code></pre>&#xA;&#xA;<p>Where N is between 0 and 99 and unique to each database instance. If you plan for more or less than 100 database instances then you can change the value of 'auto-increment-increment' as well.</p>&#xA;&#xA;<p>With this configuration, a instance with N=1 will generate auto increment values of </p>&#xA;&#xA;<pre><code>1,101,201,301,401,501... and so on&#xA;</code></pre>&#xA;&#xA;<p>N=2 will generate</p>&#xA;&#xA;<pre><code>2,102,202,302,402,502... and so on&#xA;</code></pre>&#xA;&#xA;<p>Also ensure that that primary key column size also takes into account that increments will now be in steps of 100 and will overflow sooner.</p>&#xA;"
50484673,50484505,1326913,2018-05-23T09:27:57,"<p>There are all kinds of ways to make them asynchronous. I think the question is too broad, however...</p>&#xA;&#xA;<p>The first thing is to stop returning a boolean, otherwise the caller blocks while the method runs. In asynchronous code, the caller does not block, but is ""called back"" when the operation has completed.</p>&#xA;&#xA;<p>You could return a <a href=""https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html"" rel=""nofollow noreferrer""><code>CompletableFuture</code></a> which provides lots of ways to chain asynchronous actions.</p>&#xA;&#xA;<pre><code>public CompletableFuture addReview(Review review) {&#xA;   // TODO&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Or make the caller provide a callback function which you will call when the result is ready.</p>&#xA;&#xA;<pre><code>public void addReview(Review review, Callback callback) {&#xA;   // TODO: Add the review and when finished&#xA;   callback.onSuccess();&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Where <code>Callback</code> is an interface that would look something like this:</p>&#xA;&#xA;<pre><code>interface Callback {&#xA;&#xA;    void onSuccess();&#xA;&#xA;    void onFailure(Throwable cause);&#xA;}&#xA;</code></pre>&#xA;"
51630837,51579626,6050113,2018-08-01T10:18:18,<p>I have done it. The issue was related with plugin of RabbitMQ that is <strong>rabbitmq_delayed_message_exchange</strong>. I installed and enabled it. After that <em>x-delay</em> feature worked</p>&#xA;
50545511,45655728,9851969,2018-05-26T17:46:05,"<p>The majority of the answers is based on the old-school understanding of API as a programmatic interface. Nowadays, this meaning is melted and start confusing people becuase some developers started (for simplicit or by mistake) interpred the API of an application as the application per se. In such case, it is impossible to distinguish between the modern API and Microservices. Nonetheless, we can say that an API-application can comprise many Microservices, the most of which interact within the application via Microservice's APIs while others may expose their APIs as Applications's APIs. Also, a Microservice (as a service) may not include other Microservices (services), but may orchestrate a composition of Microservices via API-bases invocations. Applications may contain Microservices but, in the best practices, may not contain other Applications. </p>&#xA;"
43639611,43282528,3965249,2017-04-26T16:18:05,"<p>An event store can be nothing more than a journal of events that is replayed, in full, to regenerate an service's original state. If you use a compacted topic in Kafka, you can minimise the restore time (a compacted topic just drops old events for the same key). This is fine for runtime state. </p>&#xA;&#xA;<p>There are a number of options for facilitating queries. If you don't mind getting into the whole KStreams thing, the simplest is to materialise a queryable view in a KTable or State Store. This is a database (it's using RocksDB behind the scenes) constructed inside your service. It acts as a disk-backed cache over the data in the backing log. This has the useful property that the backing stream can be shared by many services, but the materialized view is owned entirely by each service. </p>&#xA;&#xA;<p>More generally, a good approach is to do the simplest thing that will work, then evolve it. Try to keep services stateless and event driven. Pull in KTables or states stores if your requirements necessitate stateful elements. If your data requirements grow, look to branch out into an independent database. If you started with a kafka-backed store you can typically migrate the data relatively easily with the Connect api (although your logic may be affected).</p>&#xA;&#xA;<p>One trick worth noting for this type of implementation is to avoid synthesising request-response channels between services. Instead follow and Event Driven Architecture where you build up a shared narrative of events. Martin Fowler has <a href=""https://martinfowler.com/eaaDev/EventCollaboration.html"" rel=""nofollow noreferrer"">a good write up</a> on this from a while back. He calls it Event Collaboration. </p>&#xA;"
48062058,47915099,3965249,2018-01-02T13:39:15,"<p>There is an example below which does something similar to this. The code provides an interface for submitting new Orders, having them validated by a set of three services, then querying the result. </p>&#xA;&#xA;<p>Orders are validated by three different microservices. Each one emits a pass or fail as to whether the validation passes. Back in the Orders Service a Kafka Streams job collects the validation results and aggregates an overall pass or fail. </p>&#xA;&#xA;<p><a href=""https://github.com/confluentinc/kafka-streams-examples/tree/4.0.0-post/src/main/java/io/confluent/examples/streams/microservices"" rel=""nofollow noreferrer"">https://github.com/confluentinc/kafka-streams-examples/tree/4.0.0-post/src/main/java/io/confluent/examples/streams/microservices</a></p>&#xA;&#xA;<p><a href=""https://www.confluent.io/blog/building-a-microservices-ecosystem-with-kafka-streams-and-ksql/"" rel=""nofollow noreferrer"">https://www.confluent.io/blog/building-a-microservices-ecosystem-with-kafka-streams-and-ksql/</a></p>&#xA;&#xA;<p>This model should work for your use case too. </p>&#xA;"
46728455,46725906,1163329,2017-10-13T11:04:38,"<p>Usally you use a JWT Token. Have a look here:</p>&#xA;&#xA;<p><a href=""https://www.toptal.com/java/rest-security-with-jwt-spring-security-and-java"" rel=""nofollow noreferrer"">https://www.toptal.com/java/rest-security-with-jwt-spring-security-and-java</a></p>&#xA;"
44346532,44343697,837953,2017-06-03T17:07:15,"<p>I will run two of them on two separated services. Using Docker for WCF and Owin for Web API. &#xA;If the problem you have is trying to share code then just have a shared library outside of the two services.  </p>&#xA;&#xA;<p>An sample on WCF on Docker here <a href=""https://blogs.msdn.microsoft.com/webdev/2017/02/20/lets-try-wcf-self-hosted-services-in-a-container/"" rel=""nofollow noreferrer"">https://blogs.msdn.microsoft.com/webdev/2017/02/20/lets-try-wcf-self-hosted-services-in-a-container/</a></p>&#xA;"
47943273,47916614,837953,2017-12-22T14:41:45,<p>In simple term yes your example is correct given the machine capacity is greater than n+m*x. </p>&#xA;&#xA;<p>Using Service Fabric allow you to have high density of services and share cluster resource between services depend on demand. </p>&#xA;&#xA;<p>Another important feature of Service Fabric or any orchestration  is it offer reliable services where consumer can consume services even one node is down.</p>&#xA;
47017944,47017657,6413377,2017-10-30T14:37:51,"<p>You do not have Lomboks <code>@NonNull</code> annotation on those fields. I noticed it just now.</p>&#xA;&#xA;<p>You have only <code>@javax.validation.constraints.NotNull</code> annotation on those fieldṣ and <code>@RequiredArsgConstructor</code> does not apply on that.</p>&#xA;&#xA;<p>Add the <code>@NonNull</code> in addition to the <code>@NotNull</code> annotation. It might be that you do not need the <code>@NotNull</code> anymore so try to remove it  also.</p>&#xA;&#xA;<pre><code>@NonNull &#xA;@NotNull&#xA;@Size(min = 1, max = 20)&#xA;private String firstName;&#xA;&#xA;@NonNull &#xA;@NotNull&#xA;@Size(min = 1, max = 50)&#xA;private String lastName;&#xA;</code></pre>&#xA;"
45093029,44392569,64833,2017-07-14T00:59:41,"<p>The <a href=""https://github.com/istio/istio"" rel=""nofollow noreferrer"">project's github site</a> does state the following explicitly:  </p>&#xA;&#xA;<blockquote>&#xA;  <p>Istio currently only supports the Kubernetes platform, although we&#xA;  plan support for additional platforms such as Cloud Foundry, and Mesos&#xA;  in the near future.</p>&#xA;</blockquote>&#xA;&#xA;<p>I don't know about the plans for Docker Swarm however I believe it probably would figure in the plans.</p>&#xA;&#xA;<p>The roadmap at <a href=""https://istio.io/docs/reference/release-roadmap.html"" rel=""nofollow noreferrer"">https://istio.io/docs/reference/release-roadmap.html</a> shows that VM support is planned for 0.2</p>&#xA;&#xA;<p>You can see that work is happening in the Cloud Foundry world when you see issues such as <a href=""https://github.com/istio/pilot/issues/333"" rel=""nofollow noreferrer"">this</a>. </p>&#xA;"
35348758,35348080,525378,2016-02-11T19:51:13,"<p>I will definitely go with number 1 if you want to keep the micro services architecture solid; in the case where you see the ""User/User State"" as a totally separate product that can function by itself. </p>&#xA;&#xA;<p>I think that an extra call to prevent redundant implementations/data is the way to go, if Redis is behind it, and the API in front of is performing then you should not have a problem with the extra call. and you get a win by keeping your system as decoupled as possible.</p>&#xA;&#xA;<p>If that is not the case, then 2 will be a good solution. you will be faced by challenges regarding data integrity and replication problems, but it is one of the methods.</p>&#xA;"
50266272,43350278,988738,2018-05-10T05:46:47,"<p>I know this is a late response but I thought I would share it.</p>&#xA;&#xA;<p>We have a number of microservices and each has its own semver. We're using docker container to deploy and even to release to the client (we release the docker image). There is a manifest for the docker to specify what version of what microservice is included... this is a man-job because we need to pick the compatible versions. Our devops are planning to use <a href=""https://kubernetes.io/"" rel=""nofollow noreferrer"" title=""kubernetes"">kubernetes</a> and I think it has a good dependency management.</p>&#xA;&#xA;<p>The version of the product (marketing version) is completely different. The development doesn't care about the marketing version... they bump up their version depending on overall major/minor changes. This is actually the version of the docker image we release. </p>&#xA;&#xA;<p>Back to microsevices, we are using a automatic versioning... so the developers have to add a changelog to a change-log file by specifying the type of their change and based on that the build process bumps up the version (whether it's minor, major or patch). And once the changes are merged into main branch, the branch automatically gets tagged with that version. So basically we release continuously. </p>&#xA;&#xA;<p>In terms of shared libraries, we are also following semver. We are taking the advantage of maven range versions to fetch latest <strong>compatible</strong> version and if there is a major version, that needs a developer involved to upgrade the dependent applications .</p>&#xA;&#xA;<p>There are also these two articles I found:</p>&#xA;&#xA;<p><a href=""http://www.byconvention.com/blog/2016/1/15/semantic-versioning-vs-the-marketing-department"" rel=""nofollow noreferrer"">using a core to plug microsevices in</a> and &#xA;<a href=""https://auth0.com/blog/introduction-to-microservices-part-4-dependencies/"" rel=""nofollow noreferrer"">having multiple concurrent instances</a></p>&#xA;"
35166533,35065875,2444610,2016-02-03T00:42:23,"<p>Unfortunately, there isn't any good answer for you yet.</p>&#xA;&#xA;<p>Supporting streaming RPCs from the browser fully requires HTTP2 trailers to be supported by the browsers, and at the time of the writing of this answer, they aren't.</p>&#xA;&#xA;<p>See <a href=""https://github.com/grpc/grpc/issues/2786"">this issue</a> for the discussion on the topic.</p>&#xA;&#xA;<p>Otherwise, yes, you'd require a full translation system between WebSockets and gRPC. Maybe getting inspiration from <a href=""https://github.com/gengo/grpc-gateway"">grpc-gateway</a> could be the start of such a project, but that's still a very long shot.</p>&#xA;"
36064890,36049030,3044418,2016-03-17T15:24:19,"<p>You can put app/ in $GOPATH/src. Then whenever you're ready to build, you specify the path of your source files, relative to where they are in GOPATH. </p>&#xA;&#xA;<p>For example:</p>&#xA;&#xA;<p>if your app source is in <code>$GOPATH/src/app/</code> and your .go files are in <code>$GOPATH/src/app/posts/</code> then you can build a source (lets say posts.go in app/posts/) with <code>go build $GOPATH/src/app/posts/posts.go</code> or better <code>go build posts/posts.go</code> with <code>app/</code> as your current working directory. </p>&#xA;"
37766848,30621628,6454082,2016-06-11T17:51:12,"<p>I believe the ability to achieve functional testing on a service is a good marker for quality. Integration testing is expensive, slow, and painful. Integration testing is not the place to state if your behaviour is correct, its historical purpose is to state if components correctly interact.</p>&#xA;"
29834133,29825744,364098,2015-04-23T20:54:43,"<p>Eranda covered some of this in his answer, but I think three of the key drivers are:</p>&#xA;&#xA;<ol>&#xA;<li>Are you modeling a Request-Response type interaction?</li>&#xA;<li>Can your interaction be asynchronous?</li>&#xA;<li>How much knowledge does the sender of the information need to have about the recipients?</li>&#xA;</ol>&#xA;&#xA;<p>It is possible to do Request-Response type interactions with an asynchronous messaging infrastructure but it adds significantly to the complexity, so generally Request-Response type interactions (i.e. does the sender need some data returned from the recipient) are more easily modeled as RPC/REST interactions.</p>&#xA;&#xA;<p>If your interaction can be asynchronous then it is possible to implement this using a REST interaction but it may scale better if you use a fire and forget messaging type interaction.</p>&#xA;&#xA;<p>An asynchronous messaging interaction will also be much more appropriate if the provider of the information doesn't care who is consuming the information.  An information provider could be publishing information and new consumers of that information could be added to the system later without having to change the provider.</p>&#xA;"
48686941,48313647,129346,2018-02-08T13:43:20,"<p>1) Yes. Event sourcing is only an architectural pattern and not mandatory for microservices.</p>&#xA;&#xA;<p>2), 3) There is <a href=""https://moleculer.services"" rel=""nofollow noreferrer"">full microservices framework for NodeJS</a>. It contains a lot of microservices features. You don't need to worry about how transporting messages &amp; events, handle retries or implement circuit breaker logic. You can focus only on business logic.</p>&#xA;&#xA;<p>4) Yes, if you store models in memory, all instances will receive model events in order to update local projections/aggregations.</p>&#xA;"
47062115,47061556,1834562,2017-11-01T19:16:41,"<p>You don't need access to <code>Dockerfile</code> of your other UI projects.&#xA;For all your UI projects, build their image locally in their own project.&#xA;Then reference those images to your docker-compose file like below.</p>&#xA;&#xA;<p>While referencing, you can mount the respective codebase inside your docker.&#xA;This way, even if you change your UI code, you never have to rebuild your docker image again.</p>&#xA;&#xA;<pre><code>version: ""2""&#xA;services:&#xA;  UIProjectA:&#xA;    image: ui_project_A_local_build:0.0.1&#xA;    container_name: ui_project_A_local_build&#xA;    ports:&#xA;      - ""8081:8080""&#xA;    volumes:&#xA;      - /path/to/local/folder/in/project/A:/path/to/docker/folder&#xA;  UIProjectB:&#xA;    image: ui_project_B_local_build:0.0.1&#xA;    container_name: ui_project_B_local_build&#xA;    ports:&#xA;      - ""8082:8080""&#xA;    volumes:&#xA;      - /path/to/local/folder/in/project/B:/path/to/docker/folder&#xA;</code></pre>&#xA;"
51426495,51362482,4931,2018-07-19T15:25:27,"<p>No, it won't work like you hope it will. Assuming you do db write, API call, db commit then what happens if the API succeeds and then the db aborts? Notice that this is exactly the same as if the entire db transaction is after the API call.</p>&#xA;&#xA;<p>The only way to do this like you want is to have commit and rollback operations on the API and then drive the cross-system transaction with 2PC/3PC/Consensus.</p>&#xA;&#xA;<p>Or you can simplify and turn the transaction into a workflow/fsm: there is a state in the database row for each system being updated (+1 for COMPLETE). After the driving system updates each external system it moves the fsm to the next state, that of the system it will update next. If the remote call fails, the driver will retry calling the system until it succeeds</p>&#xA;"
45831710,45830528,5446211,2017-08-23T05:59:36,"<p>although I didn't gone to such length as implementing an Auth0, I created a separate user management (wp users) by leveraging on wordpress rest api and its native js client(backbone js). It's by no means completed, but the functionality is there.</p>&#xA;&#xA;<p>Below is the screenshot: </p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/Aze2W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Aze2W.png"" alt=""User Management""></a></p>&#xA;"
29138354,29128437,966211,2015-03-19T06:21:22,"<p>Emperor's purpose is managing processes (vassals), the 'routing' concept is the job of 'corerouters' (fastrouter, httprouter, sslrouter, rawrouter). For your case the subscription system could be the best solution:</p>&#xA;&#xA;<p><a href=""http://uwsgi-docs.readthedocs.org/en/latest/SubscriptionServer.html"" rel=""nofollow"">http://uwsgi-docs.readthedocs.org/en/latest/SubscriptionServer.html</a></p>&#xA;&#xA;<p>in 2.1 (current github master) there is the possibility to subscribe 'by mountpoint'</p>&#xA;"
45490985,34790550,1299267,2017-08-03T17:30:25,"<p>It depends on what you do with the incoming data. </p>&#xA;&#xA;<p>If you need to process the data (cpu time > io time), then you need to match the number of physical cores to the number of data processing threads.</p>&#xA;&#xA;<p>If most of the time is spent in IO (retrieving/storing the data) then you can start with cores * 2 and set the max to something that you must determine through testing the cpu usage and the throughput. I personally like the powers of 4 per core (4, 16, 64, 256). This will quickly narrow you down down onto the order of magnitude.</p>&#xA;&#xA;<p><a href=""https://javaee.github.io/grizzly/coreconfig.html#/Thread_Pool_Configuration"" rel=""nofollow noreferrer"">https://javaee.github.io/grizzly/coreconfig.html#/Thread_Pool_Configuration</a></p>&#xA;"
43545167,43545080,3872068,2017-04-21T14:24:07,"<p>Check ""accept"" header  of your request</p>&#xA;&#xA;<p>Accept: application/json&#xA;Content-Type: application/json</p>&#xA;"
40949247,34604106,6625860,2016-12-03T15:14:45,"<p>Yes, you should add author_id column on blog database. But you shouldn't add foreign key.</p>&#xA;&#xA;<p>If you want to get list of posts with related user information, you should use API Gateway pattern. API Gateway is a linking element for all microservices. This element make requests to microservices and then merge responses programmatically.</p>&#xA;&#xA;<p>For example, you can use this function for merge responses:</p>&#xA;&#xA;<pre><code>function(a, b, select) {&#xA;  var m = a.length, n = b.length, c = [];&#xA;  for (var i = 0; i &lt; m; i++) {&#xA;    var x = a[i];&#xA;    for (var j = 0; j &lt; n; j++) { // all combinations&#xA;      var y = select(x, b[j]);  // filter out the rows and columns you want&#xA;      if (y) c.push(y);         // if a row is returned add it to the table&#xA;    }&#xA;  }&#xA;&#xA;  return c;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Moreover, you can find turnkey solutions for API Gateway.</p>&#xA;"
48811217,48809275,2696260,2018-02-15T15:52:00,<p>Add the following to your <code>application.properties</code></p>&#xA;&#xA;<pre><code>server.servlet.context-parameters.auth.token.url=&lt;your-value&gt;&#xA;server.servlet.context-parameters.auth.system.username=&lt;your-value&gt;&#xA;server.servlet.context-parameters.auth.system.password=&lt;your-value&gt;&#xA;</code></pre>&#xA;&#xA;<p>This will expose the values as context parameters.</p>&#xA;
38275308,38178208,1189444,2016-07-08T20:58:10,"<p>CakePHP is based on MVC patter and this allow you a lot of things, its true Cake is not the most popular PHP framework at the moment but I think this framework could be used to be part of the a micro service ecosystem.</p>&#xA;&#xA;<p>You can use like an API it to serve resources encoding your results on JSON or you can use it to manage authentication sessions of your users and interact with other kind of micro services</p>&#xA;&#xA;<p>As personal comment I dont like how Cake handle the View layer of the MVC but you can server your content as JSON and handle the data with a JS client or even an mobile app</p>&#xA;"
49910492,49910421,342852,2018-04-18T23:28:10,"<p>First of all: those aren't exact yes/no questions. I'll give you my opinion, but others will disagree.</p>&#xA;&#xA;<p>You have created what most people would agree qualifies as a Microservice. But a Microservice does not make a Microservice architecture, in the same way that a tree doesn't make a forest.</p>&#xA;&#xA;<p>A Microservice architecture is defined by creating a greater application that consists of several distributed components. What you have done is created a monolith (which is absolutely fine in most cases).</p>&#xA;&#xA;<p>Almost every talk about Microservices that I have attended has featured this advice: start with a monolith, evolve to microservices once you need it.</p>&#xA;&#xA;<p>Regarding the last question: your application is horizontally scalable if it is stateless. If you keep any session state, it can still be horizontally scalable, but you'll need a smart LB managing sticky sessions or distributed sessions. That's when things get interesting, and when you can start thinking about a Microservice architecture.</p>&#xA;&#xA;<p>Common problems are: how can I still show my customers my website, if the order database, cart service, payment provider etc. are down. Service discovery, autoscaling, retry strategies, evolving Rest apis, those are common problems in a Microservice architecture. The more of them you use and need, the more you can claim to have a Microservice architecture.</p>&#xA;"
35044157,35040975,1723204,2016-01-27T17:30:35,"<p>Generally i would advice against doing that much work on legacy applications unless all involved parties understand you are doing a complete rebuild.</p>&#xA;&#xA;<p>The thing is what is the problem you are trying to solve. Maintainability of the reporting tool? increased deployment speed? implement a interface with an other system? solve some performance issue?</p>&#xA;&#xA;<p>Once you identified what problem you are trying to solve then cut it into the smalles piece that makes sense (for microservices) and then  you can start defining your domain model (ddd). For example make a separate reporting service to generate some weekly report. Then try to decide if that is really solving your problem. add 2 months to all your estimates and check if the business still wants it.</p>&#xA;&#xA;<p>If that's the case go ahead and build it by just replacing pieces 1 by 1. Especially if you don't know where to start don't overcomplicate things. Try to solve 1 problem that the business has and make the smallest possible prototype to show that feature can be delivered. If that's possible you got yourself some goodwill for other changes that need to be doing. But don't decide to use ddd or microservices or nservicebus as your tools to solve a problem. Those should be a result after doing the analysis of the problem you are trying to solve.</p>&#xA;&#xA;<p><strong>Update2</strong>&#xA;DDD is great when communication is a big problem. When there is a complex business domain and or when developers often (slightly) misinterpret what the business wants.</p>&#xA;&#xA;<p>Microservices is a great tool when you need to be able to scale. It also helps when you want to try out new things often. Maintaining and debugging your events can be a real pain though. And be carefull when you need to stack/aggregate events (I need X to happen if event A &amp; B are both raised in a certain flow)</p>&#xA;&#xA;<p>Servicebusses are great when a large part of your application can happen asynchronously. An email that needs to be send sometime in the near future but not necessarily this microsecond. Document generation, generating monthly invoices, or processing incomming requests (async). It will be a pain if you ever need to wait for the response message of an event.</p>&#xA;&#xA;<p><strong>UPDATE</strong> and solve a real problem. don't add something too simple and use it to introduce a service bus (or another cool technology x). If you need scaling then solve a problem that actually requires the scaling.</p>&#xA;"
51018399,51017164,1266756,2018-06-25T07:39:22,"<p>First: Read ""How to Write Go Code"" and stick to it and only to it.</p>&#xA;&#xA;<ol>&#xA;<li><p>There are no ""local packages"" for the go tool. Forget that distinction of local, non-local, remote, whatever. For the go tool there are just packages and these packages have an import path. This import path determines where the go tool will look for the code in the filesystem. If your import path is <code>import ""int.microsoft/secret/new/foo""</code> then your code for package foo must be located on your machine under `$GOPATH/src/int.microsoft/secret/new/foo"".</p>&#xA;&#xA;<p>For <code>go build</code> and (go install, go test, ...) there is <em>no</em> meaning in the import path at all: ""int.microsoft/secret/new/foo"" is no other string than ""example1/foo"" or ""github.com/someone/tool7/bar"". These are just strings and the packages are searched in these paths. None of these packages is a ""local"" package, they all are just packages.</p>&#xA;&#xA;<p>So if you put your code for package ""whatever"" under $GOPATH/src/some/strange/path/whatever you have to import it like <code>import ""some/strange/path/whatever""</code>.</p></li>&#xA;<li><p>If you want to share your code with someone else, keep a remote backup or want to use a codehosting site like github to simplify synchronizing your three workstations you can upload your code to e.g. github. This is unrelated to how you import your packages.</p>&#xA;&#xA;<p>The <em>only</em> subcommand in the go tool where import path do have an additional meaning other than being a path is for <code>go get</code> which knows about a few code hosting sites like github and interpretes an import path of ""github.com/someone/prjct"" and can clone this git repo for your, conveniently placing it in the right folder under your $GOPATH.</p>&#xA;&#xA;<p>So: No. There is absolutely no need to upload each and every bit of code. (E.g. I have a lot of stuff just on my machine under $GOPATH/src/tmp).</p>&#xA;&#xA;<p>Splitting code into a) different packages and b different files is unrelated to all this. Just do as you find convenient and logical.</p></li>&#xA;<li><p>Well, no; simpliy because you cannot keep several files in one file. Did you mean ""several files in one folder""? Yes that is fine. Split into files and packages if the split makes sense. Take a look at the standard library or any other open source project.</p></li>&#xA;</ol>&#xA;"
51277800,51277418,2448964,2018-07-11T05:25:26,"<p>You can also use <a href=""https://github.com/GoogleContainerTools/jib"" rel=""nofollow noreferrer"">Jib</a> and set</p>&#xA;&#xA;<pre><code>&lt;configuration&gt;&#xA;    &lt;container&gt;&#xA;        &lt;ports&gt;&#xA;            &lt;port&gt;xxxx&lt;/port&gt;&#xA;        &lt;/ports&gt;&#xA;    &lt;/container&gt;&#xA;&lt;/configuration&gt;&#xA;</code></pre>&#xA;"
49211140,49210733,2779972,2018-03-10T16:02:45,"<p>When you're using Docker Swarm Mode, you get service discovery with load balancing for free.<br>&#xA;DNSRR is in the key concept: <a href=""https://docs.docker.com/engine/swarm/key-concepts/#load-balancing"" rel=""nofollow noreferrer"">https://docs.docker.com/engine/swarm/key-concepts/#load-balancing</a></p>&#xA;&#xA;<p>Say you deploy <code>OCR-app</code>.  </p>&#xA;&#xA;<p><code>docker service create --network dev --name ORC-app --replicas 5 OCR-app:latest</code></p>&#xA;&#xA;<p>The docker manager will deploy <code>OCR-app</code> in this case five times on nodes of your swarm network. Every other service which is part of the same docker network <code>dev</code> can request the <code>OCR-app</code> by it's name. E.g. <code>GET http://OCR-app:4000/do/something</code>.<br>&#xA;Internally docker swarm uses round robin for forward the request automatically to one of the five services.</p>&#xA;"
51043264,51040814,5604676,2018-06-26T12:44:41,"<p>Of course there is DRY rule in programming. But, as Sam Newman said in his book ""Building Microservice"": Don't Repeat Yourself inside one microservice.</p>&#xA;&#xA;<p><strong>Common entities:</strong>&#xA;Let's look at your example with <strong><code>ResponseC</code></strong>. Imagine that something in <strong><code>ServiceB</code></strong> has changed so now one of the field from the response have changed - now you have to update each service that uses this shared library, even if the service don't need this changed field. If you had the <strong>ResponseA</strong>, <strong>ResponseB</strong> and <strong>ResponseC</strong> for each service, you didn't had to update each service with the new dependency.</p>&#xA;&#xA;<p><strong>Common logic:</strong>&#xA;Basically the same rules are applied here. However it's common to use some third party libraries for common microservices issues like time-outs and retries. What else I can suggest is to look at <code>service mesh</code> implementation like <code>istio</code> and <code>linkerd</code>. Service mesh will give the possibility to these issue to the infrastructure layer so you can focus on writing business logic.</p>&#xA;"
47972107,47971020,4230468,2017-12-25T21:22:33,<p>Sending unary almost always faster.&#xA;Use streaming to send big files.</p>&#xA;
44633959,44619634,8170564,2017-06-19T15:01:26,"<p>You can setup multi <strong>nodejs</strong> services and all of them using <strong>socket.io-redis</strong> lib connect to a Redis server to share socketID of all <strong>nodejs</strong> services. That mean you storage all <strong>socket info</strong> on Redis server. When you emit an event they will automatically emit to all <strong>nodejs</strong> services.</p>&#xA;&#xA;<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">&#xD;&#xA;<div class=""snippet-code"">&#xD;&#xA;<pre class=""snippet-code-js lang-js prettyprint-override""><code>const server = require('http').createServer();&#xD;&#xA;const io = require('socket.io')(server);&#xD;&#xA;const redis = require('socket.io-redis');&#xD;&#xA;io.adapter(redis({ host: 'localhost', port: 6379 }));&#xD;&#xA;&#xD;&#xA;io.on('connection', socket =&gt;{&#xD;&#xA;  // put socket of user to a room name by user_id&#xD;&#xA;  socket.join(user_id);&#xD;&#xA;});&#xD;&#xA;&#xD;&#xA;// emit an event to a specify user, all nodejs keep that user connection will be emit&#xD;&#xA;io.to(user_id).emit('hello', {key: 'ok'})</code></pre>&#xD;&#xA;</div>&#xD;&#xA;</div>&#xD;&#xA;</p>&#xA;"
38624600,38507565,1098564,2016-07-27T23:07:11,"<p>I created a youtube screencast to show how to create jhipster (3.5.1) microservices with UAA. I believe the problems you are facing have to do with the order of startup of services or that old services were not regenerated with 3.5.0+ code. As you can see in the screencast and from the sourcecode on github, it works as is.</p>&#xA;&#xA;<p><a href=""https://simplestepsolutions.wordpress.com/2016/07/27/jhipster-3-5-1-microservices-with-uaa/"" rel=""nofollow"">screencast</a></p>&#xA;"
36427278,36422243,1098564,2016-04-05T13:07:35,"<p>The UI for login does NOT need to be on the Authorization Server (see <a href=""https://tools.ietf.org/html/rfc6749#section-4.3"" rel=""nofollow"">https://tools.ietf.org/html/rfc6749#section-4.3</a> Resource Owner Password Credentials Grant). This leaves it up to the client to collect the credentials and make a direct token request to the Authorization Server.</p>&#xA;&#xA;<p>However, I don't generally recommend bypassing the AS UI as it means that each client application will have to implement a login logic and you won't get single sign-on between those client applications.</p>&#xA;"
46671668,46668418,7530308,2017-10-10T16:34:04,"<p>Here is an example with ExecutorService:</p>&#xA;&#xA;<pre><code>import java.util.concurrent.ExecutorService;&#xA;import java.util.concurrent.Executors;&#xA;&#xA;import javax.annotation.PreDestroy;&#xA;import javax.servlet.http.HttpServletRequest;&#xA;&#xA;@RestController&#xA;public class MyController {&#xA;&#xA;    // Instantiate an executor service&#xA;    private ExecutorService executor = Executors.newSingleThreadExecutor();&#xA;&#xA;    @PreDestroy&#xA;    public void shutdonw() {&#xA;        // needed to avoid resource leak&#xA;        executor.shutdown(); &#xA;    }&#xA;&#xA;    @GetMapping&#xA;    public Object gerUrl(HttpServletRequest request) {&#xA;        // execute the async action, you can use a Runnable or Callable instances&#xA;        executor.submit(() -&gt; doStuff());    &#xA;        return ""ok"";&#xA;    }&#xA;&#xA;    private void doStuff(){}&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>You can use the Executors factory class to build a ExecutorService. Those methods might help you:</p>&#xA;&#xA;<pre><code>java.util.concurrent.Executors&#xA;Executors.newSingleThreadExecutor() // jobs are queued and executed by a single thread&#xA;Executors.newCachedThreadPool() // new threads are instantiated as needed and cached&#xA;Executors.newFixedThreadPool(int nThreads) // user defined number of threads&#xA;</code></pre>&#xA;&#xA;<p>.</p>&#xA;&#xA;<pre><code>@EnableAsync&#xA;@SpringBootApplication&#xA;public class MyApplication extends SpringBootServletInitializer {&#xA;&#xA;    @Override&#xA;    protected SpringApplicationBuilder configure(SpringApplicationBuilder application) {&#xA;        return application.sources(MyApplication.class);&#xA;    }&#xA;&#xA;    public static void main(String[] args) throws Exception {&#xA;        SpringApplication.run(MyApplication.class, args);&#xA;    }&#xA;&#xA;}&#xA;&#xA;&#xA;import javax.annotation.PreDestroy;&#xA;&#xA;import org.springframework.context.annotation.Configuration;&#xA;import org.springframework.scheduling.annotation.AsyncConfigurerSupport;&#xA;import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;&#xA;&#xA;@Configuration&#xA;public class AsyncConfiguration extends AsyncConfigurerSupport {&#xA;&#xA;    private ThreadPoolTaskExecutor executor;&#xA;&#xA;    @Override&#xA;    public Executor getAsyncExecutor() {&#xA;        executor = new ThreadPoolTaskExecutor();&#xA;        executor.setCorePoolSize(20);&#xA;        executor.setMaxPoolSize(50);&#xA;        executor.setQueueCapacity(1000);&#xA;        executor.initialize();&#xA;        return executor;&#xA;    }&#xA;&#xA;    @PreDestroy&#xA;    public void shutdownExecutors() {&#xA;        executor.shutdown();&#xA;    }&#xA;&#xA;}&#xA;&#xA;&#xA;@Service&#xA;public class MyService {&#xA;&#xA;    @Async&#xA;    public void doStuff(){&#xA;        // Async method&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Both techniques are quite good, but the first one with ExecutorService give you more control.</p>&#xA;"
35329264,35329128,4812613,2016-02-11T01:32:34,"<p>Too many ways, too many approaches, too many best practices.  It really all depends on the analysis of your application, trying to figure out where the natural breaks are.</p>&#xA;&#xA;<p>One place I start is looking at the data model.  Lots of people advocate each microservice having its own database.  Well, that's fine and dandy, but that can really be difficult to achieve without breaking things all over the place.  But if you get lucky and there's a place where the data segregates nicely, than see what services would go with it and try breaking it out.</p>&#xA;&#xA;<p>If you do not adhere to the separate database mentality, then I start with the low-hanging fruit, often times nothing more than simple CRUD operations with just a little business logic mixed in, providing some of the basic support for other larger-grained services to come.  Of course, this becomes more iterative, not sure your organization will like it.</p>&#xA;&#xA;<p>Which brings me to methodology.  Organizations who've created monolithic applications often have methodologies that support them, whereas microservices require a much different approach to application development.  Is your organization ready for that?</p>&#xA;&#xA;<p>Needless to say, there's no right answer.  I've gone to many conferences where these concepts are high on the interest list and the fact is there's no silver bullet, everyone has different ideas of what is right, and there's exceptions galore.  You're just going to have to bite the bullet and cross your fingers, unfortunately.</p>&#xA;"
49388385,49175047,438853,2018-03-20T15:36:17,"<p>I have built an example to help me understand the capabilities in using Rx in .NET.</p>&#xA;&#xA;<p>Here is the code. </p>&#xA;&#xA;<pre><code>class Program&#xA;{&#xA;    static void Main(string[] args)&#xA;    {&#xA;        new Program().Run().Wait();&#xA;    }&#xA;&#xA;    public async Task Run()&#xA;    {&#xA;&#xA;&#xA;        var resources =&#xA;            new List&lt;string&gt;&#xA;                {&#xA;                    ""http://www.google.com""&#xA;                    ,""http://www.yahoo.com""&#xA;                    ,""http://cnn.com""&#xA;                    ,""http://microsoft.com""&#xA;                }&#xA;                .ToObservable()&#xA;                .Select(s =&gt; Observable.FromAsync(() =&gt; DownloadAsync(s)))&#xA;                .Concat()&#xA;                .Where(w =&gt; w.IsSuccessStatusCode)&#xA;                .Select(s =&gt; Observable.FromAsync(() =&gt; GetResources(s)))&#xA;                .Concat()&#xA;                .Where(w =&gt; w.Any())&#xA;                .SelectMany(s =&gt; s)&#xA;                .ToEnumerable()&#xA;                .OrderBy(o =&gt; o.Name);&#xA;&#xA;        foreach (var re in resources)&#xA;        {&#xA;            Console.WriteLine(re.Name);&#xA;        }&#xA;&#xA;        Console.ReadLine();&#xA;    }&#xA;&#xA;    public async Task&lt;HttpResponseMessage&gt; DownloadAsync(string url)&#xA;    {&#xA;        var request = new HttpRequestMessage(HttpMethod.Get, url);&#xA;&#xA;        var client = new HttpClient();&#xA;&#xA;        return await client.SendAsync(request);&#xA;    }&#xA;&#xA;    public async Task&lt;IEnumerable&lt;Resource&gt;&gt; GetResources(HttpResponseMessage message)&#xA;    {&#xA;        var ignoreContent = await message.Content.ReadAsStringAsync();&#xA;        return new List&lt;Resource&gt; { new Resource { Name = message.Headers.Date.ToString() } };&#xA;    }&#xA;}&#xA;public class Resource&#xA;{&#xA;    public string Name { get; set; }&#xA;}&#xA;</code></pre>&#xA;"
44348404,40216362,3717445,2017-06-03T20:46:32,<p>I suggest that you use POJO + JSON + <strong>GSON</strong> like this:</p>&#xA;&#xA;<p>POJO(DTO) -> Serialize to JSON (Using GSON) -> Transfer it over Message Broker (RabbitMQ) -> Deserialize it (Using GSON) -> Getting POJO(DTO) again and Consuming it.</p>&#xA;&#xA;<p>This Approach is common when you are using <strong>Event sourcing + CQRS</strong> and when you want adhere to <strong>CAP</strong> theorem and Using <strong>BASE</strong> instead of ACID (ACID is not possible to be implemented in distributing systems)</p>&#xA;
44348659,44301997,3717445,2017-06-03T21:21:45,"<p>I suggest this architecture:</p>&#xA;&#xA;<ol>&#xA;<li>Netflix Eureka : for Service discovery</li>&#xA;<li>Consul or Config Server : for saving configurations in environment base on 12 factors</li>&#xA;<li>Zuul : for Intelligent and programmable routing</li>&#xA;<li>Netflix Ribbon : for Client-Side Load Balancing</li>&#xA;<li>Zipkin : for tracing</li>&#xA;<li>Turbine : for metrics aggregation </li>&#xA;<li>Netflix Feign : for Declarative REST API implementation</li>&#xA;<li>Hysterix : for circuit breaker (one of the EIP patterns)</li>&#xA;<li>RabbitMQ (Spring-AMQP) or Kafka (Spring-Kafka and Kafka Stream) for having asynchronous communication style</li>&#xA;<li>Grafana + Prometheus + Prometheus-jmx-exporter for monitoring system</li>&#xA;<li>Docker : for virtualization and container base architecure</li>&#xA;<li>Docker Swarm or Kubernetes : for scalability, automation and Container Management</li>&#xA;</ol>&#xA;&#xA;<p>note : Prometheus is a time-series database (including monitoring features) you can also use InfluxDb or Graphite  instead of it.</p>&#xA;"
44348736,44265295,3717445,2017-06-03T21:31:04,<p>I think you have many options:</p>&#xA;&#xA;<ol>&#xA;<li>Redis</li>&#xA;<li>Redis Sentinel (support clustering)</li>&#xA;<li>ETCD (from CoreOS) : Support Clustering </li>&#xA;<li>Apache Geode</li>&#xA;<li>Aerospike : Support clustering even between data centers</li>&#xA;</ol>&#xA;&#xA;<p>and many commercial products like SAP HANA.</p>&#xA;&#xA;<p>note : Aerospike is free but for clustering between data centers I think you should pay.</p>&#xA;&#xA;<p>I myself have used <strong>ETCD</strong> for same scenarios (thanks to Brandon Philips).</p>&#xA;
44348518,44313956,3717445,2017-06-03T21:01:12,"<p>If you are worry about a high volume of messages and high TPS for example 100,000 TPS for producing and consuming events I suggest that Instead of using RabbitMQ use apache <strong>Kafka</strong> or <strong>NATS</strong> (Go version because NATS has Rubby version also) in order to support a high volume of messages per second.</p>&#xA;&#xA;<p>Also Regarding Database design you should design each micro-service base business capabilities and bounded-context according to domain driven design (DDD). so because unlike SOA it is suggested that each micro-service should has its own database then you should not be worried about normalization because you may have to repeat many structures, fields, tables and features for each microservice in order to keep them Decoupled from each other and letting them work independently to raise <strong>Availability</strong> and having <strong>scalability</strong>.</p>&#xA;&#xA;<p>Also you can use <strong>Event sourcing + CQRS</strong> technique or <strong>Transaction Log Tailing</strong> to circumvent <strong>2PC</strong> (2 Phase Commitment) - <em>which is not recommended when implementing microservices</em> - in order to exchange events between your microservices and manipulating states to have Eventual Consistency according to <strong>CAP</strong> theorem. </p>&#xA;"
44186630,44181863,3717445,2017-05-25T17:41:17,"<p>You can not compare Micro-services with load balancer... you should compare it with monolithic or SOA architecture.</p>&#xA;&#xA;<p>In monolithic approach you mainly have only one database for the whole system and a monolithic application as a single project for your business.</p>&#xA;&#xA;<p>monolithic is single unit But SOA is a coarse-grain approach and Microservice is fine-grain approach. In microservice architecture instead of designing a monolithic system you design different micro-services around your business capabilities and base on your domain and bounded-context.</p>&#xA;&#xA;<p>each micro-services may have their own database. for e.g. order micro-service may have mysql database, recommendation micro-service may have Cassandra database and user-search micro service may have Elasticsearch or SOLR database.</p>&#xA;&#xA;<p>In microservices each micro-service can talk to another base on two different communication style: </p>&#xA;&#xA;<ol>&#xA;<li>Sync (Rest is suggested)</li>&#xA;<li>Async (via message brokers like Kafka, RabbitMQ, ActiveMQ or NATS&#xA;and etc.)</li>&#xA;</ol>&#xA;&#xA;<p>Scaling up-down in micro-services architecture is much easier than monolithic systems and you can even change a part of system and redeploy it independently without affecting the whole system.</p>&#xA;&#xA;<p>Also micro-services adhere to let-it-crash paradigm and with using EIP patterns like Circuit-Breaker you can let user think system is always up and working and Base on CAP theorem you can have high-available system by compensating for consistency and having Eventual Consistency according to <strong>BASE</strong> instead of <strong>ACID</strong></p>&#xA;&#xA;<p>For load balancing Client-side Load Balancing with <strong>Ribbon</strong> devised by Netflix is very viable approach.</p>&#xA;&#xA;<p>Also with using NginX, Docker Swarm and kubernetes you can implement load balancing.</p>&#xA;&#xA;<p>In a nutshell there is nothing to do about comparing Microservices with Load balancer.</p>&#xA;"
44186865,44169046,3717445,2017-05-25T17:53:16,<p>You can use Circuit Breaker pattern for e.g. hystrix circuit breaker from netflix.</p>&#xA;&#xA;<p>It is possible to open circuit-breaker base on a timeout or when service call fails or inaccessible. </p>&#xA;
44117645,44114755,3717445,2017-05-22T16:16:43,"<p>you can not implement traditional transaction system in micro-services in a distributed environment.</p>&#xA;&#xA;<p>You should you <strong>Event Sourcing + CQRS</strong>  technique and because they are atomic you will gain something like implementing transactions or 2PC in a monolithic system.</p>&#xA;&#xA;<p>Other possible way is <strong>transaction-log-mining</strong> that I think linked-in is using this way but it has its own cons and pros. for e.g. binary log of different databases are different and event in same kind of database there are differences between different versions.</p>&#xA;&#xA;<p>I suggest that you use Event Sourcing + CQRS and string events in an event-store  then try reaching <strong>eventual consistency</strong> base on <strong>CAP theorem</strong> after transferring multiple  events between micro-service A and B and updating domain <strong>states</strong> in each step.</p>&#xA;&#xA;<p>It is suggested that you use a message broker like <strong>ActiveMQ</strong>, <strong>RabbitMQ</strong> or <strong>Kafka</strong> for sending event-sourced events between different microservices and string them in an event store like mysql or other systems.</p>&#xA;&#xA;<p>Another benefit of this way beside mimicking transactions is that you will have a complete <strong>audit log</strong>.  </p>&#xA;"
42843710,42822348,640325,2017-03-16T19:59:01,"<p>You're approach is good. NpgsqlException usually means a network/IO error, although you can examine the inner exception and check for IOException to be sure. </p>&#xA;&#xA;<p>PostgresException is thrown when PostgreSQL reports an error, which in most cases is a problem with the query. However, there may be some transient server-side issues (e.g. too many connections), you can examine the SQL error code for that - see <a href=""https://www.postgresql.org/docs/9.6/static/errcodes-appendix.html"" rel=""nofollow noreferrer"">the PG docs</a>.</p>&#xA;&#xA;<p>It may be a good idea to add an <code>IsTransient</code> property to these exceptions, encoding these checks inside PostgreSQL itself - you're welcome to open an issue for that on the Npgsql repo.</p>&#xA;"
39605743,39604499,455493,2016-09-21T00:33:32,"<p>Startup.cs is quite ASP.NET Core specific, which by itself is a web stack and comes hosted with WebListener or Kestrel behind IIS. </p>&#xA;&#xA;<p>In an console application you wouldn't use a traditional Startup.cs, though you could to have a consistent base, but it would look a bit differently though, because you are in control of creating the IoC container (rather than ASP.NET Core doing it for you in an web application). Normally the Startup.cs is processed by the <code>WebHostBuilder</code> in <code>Program.cs</code>, to allow it to inject it's own stuff before passing the <code>IServiceCollection</code> to the <code>ConfigureServices</code> method. </p>&#xA;&#xA;<p>You're right that you need an console application and there you'd do all the stuff yourself. </p>&#xA;&#xA;<pre><code>public class Program&#xA;{&#xA;    public IConfigurationRoot Configuration { get; private set; }&#xA;    public IServiceProvider Provider { get; private set; }&#xA;&#xA;    public static void Main()&#xA;    {&#xA;        var programm = new Programm();&#xA;        program.Run();&#xA;    }&#xA;&#xA;    private void Run() &#xA;    {&#xA;        var builder = new ConfigurationBuilder()&#xA;            .AddJsonFile(""appsettings.json"", optional: true, reloadOnChange: true)&#xA;            .AddEnvironmentVariables();&#xA;&#xA;        Configuration = builder.Build();&#xA;&#xA;        var services = new ServiceCollection();&#xA;        ConfigureServices(services);&#xA;        this.Provider = services.BuildServiceProvider();&#xA;&#xA;        var host = this.Provider.GetRequiredService&lt;MyHost&gt;();&#xA;        // blocking operation, the host should be something that keeps the service running&#xA;        // once it stops, the application stops too as there are no additional &#xA;        host.Run();&#xA;    }&#xA;&#xA;    private void ConfigureServices(IServiceCollection services) &#xA;    {&#xA;        services.AddTransient&lt;IMyService, MyService&gt;();&#xA;        services.AddSingleton&lt;MyHost&gt;();&#xA;        ...&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>What you want to use as host, is up to you. It could be i.e. an Azure Web Task/Host:</p>&#xA;&#xA;<pre><code>using(var jobHost = new JobHost(new JobHostConfiguration(Configuration.GetConnectionString(""AzureWebJobsDashboard"")))) &#xA;{&#xA;    jobHost.RunAndBlock();&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>which would get started and keep the application running and receiving Azure messages and events registered within it. Or you use some of the other background tasks frameworks like Hangfire.</p>&#xA;&#xA;<p>In ASP.NET Core all of this is handled by the framework, but ASP.NET Core is made for web stuff and hence, depends on an endpoint. Most of the stuff inside <code>Configure</code> method of <code>Startup</code> class used in ASP.NET Core application is about registering middlewares, which work on the <code>HttpContext</code>. You don't have these in a microservice w/o an endpoint. No endpoint, no <code>HttpContext</code>, no need for middlewares. </p>&#xA;&#xA;<p>Of course you also don't have an <code>IApplicationBuilder</code> in an console application, just an <code>IServiceProvider</code> to resolve the services you registered earlier. </p>&#xA;"
51590632,51566509,455493,2018-07-30T09:16:16,"<p>You should treat each microservice as a service boundary. Means no aggregates ever leave the bounded context. The reason for this is, if you do that your leak your domain knowledge into other bounded contexts and have a hard dependency on it. Any change on the aggregate will break any depending service. </p>&#xA;&#xA;<p>If you need to retrieve data from other bounded context you should do that through an anti-corruption layer. In terms of Microservices that can be translated into DTOs. This way when you add or remove a property you don't necessary break the contracts with the outside contexts.</p>&#xA;&#xA;<p>From the provided information its hard to tell if <code>Player</code> and <code>Classification</code> belong to the same context or not. Basically there's nothing wrong to compose your UI data from multiple microservices (i.e. displaying orders and delivery notes on the same UI form, whereas they come from different bounding contexts, namely order and logistics). </p>&#xA;&#xA;<p>However if your <code>Player</code> model/aggregates directly depend on each other and neither of them can be used independently of the other one, then its very likely its part of the same bounded context. </p>&#xA;"
42766620,41783704,2105414,2017-03-13T14:51:44,"<blockquote>&#xA;  <p>The second senario is we manually want to trigger the loading of a specific version of rules on every pod running the execution application preferably via a rest call.</p>&#xA;</blockquote>&#xA;&#xA;<p>What about using <a href=""https://docs.openshift.com/container-platform/latest/dev_guide/deployments/deployment_strategies.html#when-to-use-a-rolling-deployment"" rel=""nofollow noreferrer"">Rolling Updates</a>? When you want to change the version of rules to be fetched within all execution pods, tell OpenShift to do rolling update which kills/starts all your pods one by one until all pods are on the new version, thus, they fetch the specific version of rules at the startup. The trigger of Rolling Updates and the way you define the version resolution is up to you. For instance: Have an ENV var within a pod that defines the version of rules that are going to be fetched from db, then change the ENV var to a new value and perform Rollling Updates. At the end, you should end up with new set of pods, all of them fetching the version rules based on the new value of the ENV var you set.</p>&#xA;"
45533818,45533748,997958,2017-08-06T15:54:16,"<p>Every version of UserService needs to be <em>backwards</em> and <em>forwards</em> compatible. This way clients can talk to any version of the service and not crash.</p>&#xA;&#xA;<p>Of course, the details of how this is achieved depends on your architecture.</p>&#xA;"
33179601,33179127,3686982,2015-10-16T21:11:02,"<p>You could use JWT (JSON Web Tokens). </p>&#xA;&#xA;<p>Then the resource sever will self verify the token. All that you need is to share a same salt.</p>&#xA;&#xA;<p>Basicly a JWT is seprated in 3 parts and encrypted</p>&#xA;&#xA;<ul>&#xA;<li>Header (hashing algo info)</li>&#xA;<li>Body (data)</li>&#xA;<li>Signature (for verification)</li>&#xA;</ul>&#xA;&#xA;<p>After the token verification you could extract the needed data (for example user id) from the token body and do your work.</p>&#xA;&#xA;<p>I will not get into details on how to build a JWT they are much resources on that, but the right track is to use an already build library.</p>&#xA;"
38431852,38254720,5919046,2016-07-18T08:24:14,"<p>There is definitely no limitations whether you deploy your microservices on local, physical servers or in the cloud. Both approaches are valid, but they impose different advantages and disadvantages.</p>&#xA;&#xA;<p>With local/physical servers, you will have:</p>&#xA;&#xA;<ul>&#xA;<li>bigger operations overhead (it is better you have good DevOps in your team)</li>&#xA;<li>manual scaling (when you experience bigger traffic, you need to manually fire up new instances, or use some management tool for this)</li>&#xA;<li>manual fault detection - if a server goes down (this depends on your/company's server enviorenment) someone will need to fix this ""manually""</li>&#xA;<li>it is cheaper (a friend is buying old server instances on Amazon and running their semi-microservice architecture on them, he calculated they achieve quite big savings this way)</li>&#xA;</ul>&#xA;&#xA;<p>With cloud infrastructure, you get some of the below advantages (in contrary to above disadvantages):</p>&#xA;&#xA;<ul>&#xA;<li>less operations overhead (the cloud will take care of most of operations)</li>&#xA;<li>flexible scaling (when your traffic goes up, cloud can automatically fire up new instances, when it goes down, it will shutdown instances)</li>&#xA;<li>error/fault handling - if there occurs a problem in the cloud, you do not need to worry</li>&#xA;</ul>&#xA;&#xA;<p>I did not mention all the advantages and disadvantages of given approaches, as it also depends on the project (will it receive different traffic on different times of day, does it need to keep data locally or can it be in a foreign country in a cloud, ...).</p>&#xA;"
38708545,38697565,5919046,2016-08-01T21:38:49,"<blockquote>&#xA;  <p>So far, my developing application is planned to be available on web and mobile so I decided to do it as microservice.</p>&#xA;</blockquote>&#xA;&#xA;<p>Choosing microservice (MS) approach based on the fact that your application will support more than one platform is not advisable. If you have never before dealt with microservice architecture, it is perhaps better to build a modular monolith with strong context boundaries. This way it will be easier to focus on programming and implementing application, and later on you can gradually break down monolith into microservices, one at a time (i. e. you start with modules that have low traffic, like sign up service). Also, going straight to microservices, except if you are doing a project for your own experience or fun, suits into category of premature optimization. For more info about Monolith first approach, Martin Fowler wrote a <a href=""http://martinfowler.com/bliki/MonolithFirst.html"" rel=""nofollow"">great article</a> about it.</p>&#xA;&#xA;<blockquote>&#xA;  <p>As actions above, how many services should I break into?</p>&#xA;</blockquote>&#xA;&#xA;<p>[I am answering in context of splitting backend system and calling MS from frontend via API.]&#xA;Sign up and sign in should be a different service, as generally sign up is used less frequently (once a user signs up, he just signs in from now on) than sign in. Facebook sign in would go with sign in MS. &#xA;Viewing articles is also more frequent than posting, editing or deleting (either you have a blog platform or Facebook), so one MS would provide data to view articles, and one MS would be used to post, edit or delete articles.</p>&#xA;&#xA;<blockquote>&#xA;  <p>when I split services separately how can I get articles data with the authors to render on my sites?</p>&#xA;</blockquote>&#xA;&#xA;<p>Most adviced approach with MS artchitecture is polyglot persistence, where every MS would have its own database, containing the tables it actually edits or updates. Then you can take approach of accessing data of other MS through calling its APIs (view article service has, in its Article table, also ID of author, then you call user service/sign up service with ID of author to get its full info), though this provides more tightly coupled architecture.</p>&#xA;&#xA;<p>Another option is to store data in form of events, where new user registering is an event, that gets stored in user service MS database and sent to some queue or topic, and other services can register to receive events from it and store the events in their database. This way you will duplicate data, but will have more loosely coupled (and asynchronous) architecture. Google event driven data management for more on this approach.</p>&#xA;"
29593474,29591967,4412230,2015-04-12T19:08:32,"<p>Previous answers are great. &#xA;Microservices architecture is <em>just</em> a functional decomposition design.</p>&#xA;&#xA;<p>I suggest you to read this blog post : <a href=""http://blog.arungupta.me/microservice-design-patterns/"" rel=""nofollow"">Microservice Design Patterns</a></p>&#xA;&#xA;<p>From a technical point of view, there is a a lot of tools like <a href=""https://docs.docker.com/userguide/dockerizing/"" rel=""nofollow"">Docker</a> (to run each microservice as a linux container) and Kubernetes to orchestrate them <strong>as a service</strong> (here is a <a href=""https://github.com/GoogleCloudPlatform/kubernetes/blob/master/examples/guestbook/README.md"" rel=""nofollow"">Kubernetes sample</a>). </p>&#xA;"
51911851,51911797,3182406,2018-08-18T19:19:25,"<p>you can start with the following:</p>&#xA;&#xA;<p>Monitoring : </p>&#xA;&#xA;<pre><code>Start with spring-boot-admin and prometheus.&#xA;https://github.com/codecentric/spring-boot-admin&#xA;</code></pre>&#xA;&#xA;<p>Deployment:</p>&#xA;&#xA;<pre><code>Start with docker and docker-compose and move to kubernetes.&#xA;</code></pre>&#xA;&#xA;<p>Few examples for docker compose:</p>&#xA;&#xA;<ol>&#xA;<li><a href=""https://github.com/jinternals/spring-cloud-stream"" rel=""nofollow noreferrer"">https://github.com/jinternals/spring-cloud-stream</a></li>&#xA;<li><a href=""https://github.com/jinternals/spring-micrometer-demo"" rel=""nofollow noreferrer"">https://github.com/jinternals/spring-micrometer-demo</a></li>&#xA;</ol>&#xA;"
28388185,28387907,825158,2015-02-07T22:25:44,"<p>I think you already pretty much explained that the second (pull-based) approach is better.</p>&#xA;&#xA;<ol>&#xA;<li>If a user's request should anyway be processed asynchronously, why wait for the data to be gathered and then return a response. You need just to queue a work item and return HTTP response.</li>&#xA;<li>Passing data via queue is not a good option. If you get the data upstream, you will have to pass it somehow other than via queue to the worker (usually BLOB storage). That is additional work that is not really needed in your case.  </li>&#xA;</ol>&#xA;"
51734423,51670858,243782,2018-08-07T19:49:37,"<p>To handle more users, or more throughput in the same second you need to ensure multiple things: </p>&#xA;&#xA;<ul>&#xA;<li>have more than 1 worker, usually scale up the size of the worker to 10 from start</li>&#xA;<li>this way you have 10 parallel workers</li>&#xA;<li>put 10 different messages into the queue so each worker pickup a job to tackle</li>&#xA;<li>monitor queue and if more jobs keep accumulating add more workers</li>&#xA;<li>monitor machine CPU and Ram state and if starts to throttle around 80% of CPU you should consider adding another machine that consumes jobs from the same queue</li>&#xA;<li>you could have different machines for different needs (SSD for fast IO, high end CPU for quick jobs, lower machines for transactional states etc..)</li>&#xA;</ul>&#xA;"
44571980,44571838,6412102,2017-06-15T15:55:35,"<p>This is the approach I usually take: for each message type, a class is responsible for both encoding the message (generating JSON/XML/whatnot), validating it and parsing it. Classes go into a library that is then used on both the service which is sending the message and on the service that is receiving it.</p>&#xA;"
50599096,50598938,5108806,2018-05-30T07:47:25,"<p>For connect you should using </p>&#xA;&#xA;<blockquote>&#xA;  <p>depends_on</p>&#xA;</blockquote>&#xA;&#xA;<pre><code>services:&#xA;    some_service:&#xA;        image: link&#xA;        depends_on:&#xA;           -api&#xA;    api:&#xA;      image: link&#xA;</code></pre>&#xA;&#xA;<p>More info at docs: <a href=""https://docs.docker.com/compose/compose-file/#update_config"" rel=""nofollow noreferrer"">docker docs</a> </p>&#xA;"
37987198,37985551,3018055,2016-06-23T09:12:50,"<p>If your micro services have a REST API available, you can use an embeded javascript code instead of iframe. Your embed code would look something like:</p>&#xA;&#xA;<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">&#xD;&#xA;<div class=""snippet-code"">&#xD;&#xA;<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;div id=""embed_id""&gt;&lt;/div&gt;&#xD;&#xA;&lt;script type=""text/javascript""&gt;&#xD;&#xA;  &#xD;&#xA;(function(){&#xD;&#xA;  var embededJavascriptElement = document.createElement(""script""); &#xD;&#xA;  embededJavascriptElement.type = ""text/javascript"";&#xD;&#xA;  embededJavascriptElement.charset= ""utf-8"";&#xD;&#xA;  embededJavascriptElement.id = ""embed_script"";&#xD;&#xA;  embededJavascriptElement.src = ""&lt;path to your script&gt;"";&#xD;&#xA;  (document.getElementsByTagName(""head"")[0] || document.getElementsByTagName(""body"")  [0]).appendChild(embededJavascriptElement);&#xD;&#xA; })();&#xD;&#xA;&lt;/script&gt;</code></pre>&#xD;&#xA;</div>&#xD;&#xA;</div>&#xD;&#xA;</p>&#xA;&#xA;<p>The script above will insert another script tag into your DOM. That script will be interpreted, will search for the element with id =""embed_id"" and will render the embeded code into that div (you need to implement this behavior). </p>&#xA;"
33034122,33033834,465223,2015-10-09T09:02:23,"<p>There are some frameworks out there to reduce such a boilerplate code. I use Spring Boot in a current project (though not for microservices). If you already have Spring based projects, then it really simplifies the development of microservices (or any other not-Microservice-application based on Spring). Checkout some of the examples: <a href=""https://github.com/spring-projects/spring-boot/tree/master/spring-boot-samples"" rel=""nofollow"">https://github.com/spring-projects/spring-boot/tree/master/spring-boot-samples</a></p>&#xA;"
49589816,49588249,465223,2018-03-31T16:10:14,"<p>The Eureka server itself has a REST api which allows you to manage it. You can find the documentation here: <a href=""https://github.com/Netflix/eureka/wiki/Eureka-REST-operations"" rel=""nofollow noreferrer"">https://github.com/Netflix/eureka/wiki/Eureka-REST-operations</a>&#xA;If you use a framework or language which doesn't provide direct integration with Eureka, like Spring Boot, you have to do it manually.</p>&#xA;"
45213098,45212821,465223,2017-07-20T11:14:55,"<p>Even if this question is way too broad, i'll try to answer your question as good as i can:</p>&#xA;&#xA;<ol>&#xA;<li><p>A multimodule project is not the only way (and i would even say, not a recommended way for different services). Usually you have completely separated Maven projects for each service.</p></li>&#xA;<li><p>Every service has to have its own data model and entity classes. Services should never share any entities, and should not access the same databases/schemas. They can use the same database server with different schemas.</p></li>&#xA;<li><p>In every service, which uses Hibernate.</p></li>&#xA;</ol>&#xA;"
49473285,35065875,337735,2018-03-25T06:54:34,"<p>An official grpc-web (beta) implementation was released on 3/23/2018. You can find it at</p>&#xA;&#xA;<blockquote>&#xA;  <blockquote>&#xA;    <p><a href=""https://github.com/grpc/grpc-web"" rel=""noreferrer"">https://github.com/grpc/grpc-web</a></p>&#xA;  </blockquote>&#xA;</blockquote>&#xA;&#xA;<p>The following instructions are taken from the README:</p>&#xA;&#xA;<h2>Define your gRPC service:</h2>&#xA;&#xA;<pre><code>service EchoService {&#xA;  rpc Echo(EchoRequest) returns (EchoResponse);&#xA;&#xA;  rpc ServerStreamingEcho(ServerStreamingEchoRequest)&#xA;      returns (stream ServerStreamingEchoResponse);&#xA;}&#xA;</code></pre>&#xA;&#xA;<h2>Build the server in whatever language you want.</h2>&#xA;&#xA;<h2>Create your JS client to make calls from the browser:</h2>&#xA;&#xA;<pre><code>var echoService = new proto.grpc.gateway.testing.EchoServiceClient(&#xA;  'http://localhost:8080');&#xA;</code></pre>&#xA;&#xA;<h3>Make a unary RPC call</h3>&#xA;&#xA;<pre><code>var unaryRequest = new proto.grpc.gateway.testing.EchoRequest();&#xA;unaryRequest.setMessage(msg);&#xA;echoService.echo(unaryRequest, {},&#xA;  function(err, response) {&#xA;    console.log(response.getMessage());&#xA;  });&#xA;</code></pre>&#xA;&#xA;<h3>Streams from the server to the browser are supported:</h3>&#xA;&#xA;<pre><code>var stream = echoService.serverStreamingEcho(streamRequest, {});&#xA;stream.on('data', function(response) {&#xA;  console.log(response.getMessage());&#xA;});&#xA;</code></pre>&#xA;&#xA;<h3>Bidirectional streams are NOT supported:</h3>&#xA;&#xA;<p>This is a work in progress and on the <a href=""https://github.com/grpc/grpc-web/blob/master/ROADMAP.md#bidi-support"" rel=""noreferrer"">grpc-web roadmap</a>. While there is an <a href=""https://github.com/grpc/grpc-web/blob/master/net/grpc/gateway/examples/echo/echo.proto#L96"" rel=""noreferrer"">example protobuf</a> showing bidi streaming, <a href=""https://github.com/grpc/grpc-web/issues/24#issuecomment-303285538"" rel=""noreferrer"">this comment</a> make it clear that this example  doesn't actually work yet.</p>&#xA;&#xA;<p>Hopefully this will change soon. :)</p>&#xA;"
47383486,47380189,14663,2017-11-20T00:17:07,"<p>We are also undergoing a similar change.</p>&#xA;&#xA;<p>The question about whether you enforce contracts is independent of if you connect through the service bus or directly.  You could build your microservices to use SOAP and WSDL.  The industry as a whole seems to be moving away from this.  We are using REST.</p>&#xA;&#xA;<p>The team responsible for deploying the microservice needs to treat all external parties like customers.  This means when changes occur, they need to keep backwards compatibility and then undergo a change management process in other teams to have them upgrade before decommissioning the old version.  We avoid breaking changes as much as possible, and use semantic versioning otherwise.  Automated tests help keep all this possible.</p>&#xA;&#xA;<p>In terms of governance, I would set ground-rules around the following:</p>&#xA;&#xA;<ul>&#xA;<li>what is (and is not) considered a breaking change, and how this is to be handled in the versioning system</li>&#xA;<li>how/where documentation for services is to be published and updated</li>&#xA;<li>how clients are to authenticate themselves</li>&#xA;<li>security recommendations such as TLS and authentication mechanisms</li>&#xA;</ul>&#xA;&#xA;<p>While you may not have a canonical data model covering all services, it could be wise to introduce some smaller conventions suitable to your domain.  In my domain, this means always using 3-character ISO standard currency codes alongside monetary amounts.  We never assume the currency or use a different representation.</p>&#xA;"
49651838,49650746,7636295,2018-04-04T13:05:30,"<p>The solution I used is to build a SocketListener and encode the file content in UTF-8 and write them to the http response. It's easier than your option two, personally speaking.</p>&#xA;"
25600920,25600580,2067527,2014-09-01T07:36:16,"<p><strong>From my project experience</strong></p>&#xA;&#xA;<p>Share a WSDL when using SOAP (not the service model code, since they should be generated from the WSDL). When using REST, have distinct models (copy yes but not share) for client and server. As soon as the second or third consumer comes into play, you'll get into trouble. Keep them decoupled. The operating and usage of a service changed in my past more often than the data structures. Another client wants to use your service or a second version has to be operated at the same time.</p>&#xA;&#xA;<p><strong>Some additional thoughts</strong></p>&#xA;&#xA;<p>Sharing is partial contradictive to scalability. Share-nothing and share-some/share-all have both pros and cons. Sharing nothing gives you full flexibility at any time. Microservices are independent components providing particular domain services.</p>&#xA;&#xA;<p>Sharing business domain data models is a common pattern (<a href=""http://www.ivarjacobson.com/resources/resources/books/#object%20oriented%20software"" rel=""nofollow"">http://www.ivarjacobson.com/resources/resources/books/#object%20oriented%20software</a>) which prevents duplications of the same. Since microservices divide and conquer the business parts it might get hard to share something of the business domain data model.</p>&#xA;&#xA;<p>Microservices communicate with each other so I understand the need of sharing these communication data models (mostly HTTP based). Sharing these data models might be OK in case you have a one to one mapping between service provider and consumer. As soon as you have multiple consumers for one service needing different models/fields within the model it gets tough.</p>&#xA;"
29161298,29117570,2067527,2015-03-20T07:13:52,"<p>So you're having two services:</p>&#xA;&#xA;<ol>&#xA;<li>Invoice micro service</li>&#xA;<li>Shipment micro service</li>&#xA;</ol>&#xA;&#xA;<p>In real life, you would have something where you hold the order state. Let's call it order service. Next you have order processing use cases, which know what to do when the order transitions from one state to another. All these services contain a certain set of data, and now you need something else, that does all the coordination. This might be:</p>&#xA;&#xA;<ul>&#xA;<li>A simple GUI knowing all your services and implementing the use cases (""I'm done"" calls the shipment service)</li>&#xA;<li>A business process engine, which waits for an ""I'm done"" event. This engine implements the use cases and the flow. </li>&#xA;<li>An orchestration micro service, let's say the order processing service itself that knows the flow/use cases of your domain</li>&#xA;<li>Anything else I did not think about yet</li>&#xA;</ul>&#xA;&#xA;<p>The main point with this is that the control is external. This is because all your application components are individual building blocks, loosely coupled. If your use cases change, you have to alter one component in one place, which is the orchestration component. If you add a different order flow, you can easily add another orchestrator that does not interfere with the first one. The micro service thinking is not only about scalability and doing fancy REST API's but also about a clear structure, reduced dependencies between components and reuse of common data and functionality that are shared throughout your business.</p>&#xA;&#xA;<p>HTH, Mark</p>&#xA;"
29675451,28319278,1415751,2015-04-16T12:59:16,"<p>RestAPI is not only way to do it, one of the some ideas that i have seeming is about the usage of <strong><em>Service Registry</em></strong> link Eureka (Netflix), Zookeeper (Apache) and others.</p>&#xA;&#xA;<p>Here is an example: </p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://github.com/tiarebalbi/qcon2015-sao-paolo-microservices-workshop"" rel=""nofollow"">https://github.com/tiarebalbi/qcon2015-sao-paolo-microservices-workshop</a></li>&#xA;</ul>&#xA;"
29675246,29644916,1415751,2015-04-16T12:50:12,"<p>Based on what I understand, a good way to resolve it is by using the OAuth 2 protocol <em>(you can find a little more information about it on <a href=""http://oauth.net/2/"" rel=""noreferrer"">http://oauth.net/2/</a>)</em></p>&#xA;&#xA;<p>When your user logs into your application they will get a token and with this token they will be able to send to other services to identify them in the request.</p>&#xA;&#xA;<p><img src=""https://i.stack.imgur.com/JPB8c.png"" alt=""OAuth 2 Model""></p>&#xA;&#xA;<p>Example of Chained Microservice Design&#xA;<img src=""https://i.stack.imgur.com/cFWFh.png"" alt=""Architecture Model""></p>&#xA;&#xA;<p>Resources:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""http://presos.dsyer.com/decks/microservice-security.html"" rel=""noreferrer"">http://presos.dsyer.com/decks/microservice-security.html</a></li>&#xA;<li><a href=""https://github.com/intridea/oauth2"" rel=""noreferrer"">https://github.com/intridea/oauth2</a></li>&#xA;<li><a href=""https://spring.io/guides/tutorials/spring-security-and-angular-js/"" rel=""noreferrer"">https://spring.io/guides/tutorials/spring-security-and-angular-js/</a></li>&#xA;</ul>&#xA;"
34207595,34163944,4237127,2015-12-10T17:00:43,<p>The issue is with namespaces.If we use the default namespace it should be able to pick up the new properties addedd by default.</p>&#xA;
34594944,34593341,5683783,2016-01-04T15:55:34,"<p>It's a tricky one to trade off the different options against each other as they're not necessarily mutually exclusive but here's a bit of start on some of the points you mention, also I know this isn't necessarily tackling your Service Fabric question but I'm hoping it is still useful.</p>&#xA;&#xA;<p>A micro services environment is great from a code perspective, it makes the developers lives easy.  By centralising functionality to one service it means they can focus on the problem without thinking of unnecessary dependencies.  For example if you were to group all your methods in one project and you had to change the interface you may decide to create a new end point.  In this example you may have to consider the consumers of methods that aren't changing, do we want to force them to uplift or are we happy for them to point to the previous version but of course then you're supporting multiple versions.</p>&#xA;&#xA;<p>The big problem is microservices breed isolated knowledge and by taking the approach that the developer of one service doesn't need to know who or what is consuming it means you're unable to answer what can often be a very important question of who or what is consuming this service and do we need to force them to uplift to a new end point.  In practical terms this can leave your web servers in a bit of a mess with old services knocking about that don't need to be.</p>&#xA;&#xA;<p>Independent of what your solution is I'd always try and group your logic and place similar methods in the same project.  I think you have to accept that you make decisions without full knowledge of how the solution will develop in the future and therefore you may have to shuffle things about.  </p>&#xA;&#xA;<p>Another consideration would be management of requirements and parallel development.  With micro services you're probably able to manage business requirements much more easily than with a single service.  Assume you get given 10 requirements and are requested to deliver 5 in the next month and the other 5 the month after.  Say for example the first 5 requirements are focused in business logic a and the next lot in business logic b, if you have two developers you can get them to work on this in parallel without having to branch and merge.  This is obviously a nice example to illustrate the point however it's a relatively easy conversation to have if they're split over different services because it's easy to say to the business peeps that your business logic A changes need to go together.</p>&#xA;&#xA;<p>Stateful webservices can be tricky to quickly test, admittedly you can get round this but I always like being able to throw a request a service and get a response, if I have to prep things it slows me down and this can be frustrating when diagnosing live issues. </p>&#xA;"
45805326,45630225,905031,2017-08-21T20:51:05,"<p>Without knowing your data volumes, specifically how much new or diff data you have per day and how you are doing paging with your REST APIs - here is my guidance...</p>&#xA;&#xA;<p>If you go down the path of a using Spring Batch you are more than likely going to have to come up with your own sharding mechanism: how will you divide up REST calls to instantiate your Spring services? You will also be in the Kub management space and will have to handle retries with the streaming API to BQ.</p>&#xA;&#xA;<p>If you go down the Dataflow route you will have to write a some transform code to call your REST API and peform the paging to populate your PCollection destined for BQ.  With the recent addition of Dataflow templates you could: create a pipeline that is triggered every N hours and parameterize your REST call(s) to just pull data ?since=latestCall. From there you could execute BigQuery writes.  I recommend doing this in batch mode as 1) it will scale better if you have millions of rows 2) be less cumbersome to manage (during non-active times).</p>&#xA;&#xA;<p>Since Cloud Dataflow has built in re-try logic for BiqQuery and provides consistency across all input and output collections -- my vote is for Dataflow in this case.</p>&#xA;&#xA;<p>How big are your REST call results in record count?</p>&#xA;"
45566738,45554704,3986759,2017-08-08T11:10:00,"<p>Take a look at the OAuth2 implementation for Loopback: <a href=""https://github.com/strongloop/loopback-component-oauth2"" rel=""nofollow noreferrer"">https://github.com/strongloop/loopback-component-oauth2</a></p>&#xA;"
51012770,51012725,1383240,2018-06-24T18:24:31,"<p>The correct way is to map those responses to classes:</p>&#xA;&#xA;<pre><code>public class Product{&#xA;    private Long id;&#xA;    private String name;&#xA;    private Double price;&#xA;&#xA;    //constructors, getters, setters&#xA;}&#xA;&#xA;public class Person{&#xA;    private Long id;&#xA;    private String name;&#xA;    private String mail;&#xA;&#xA;    //constructors, getters, setters&#xA;}&#xA;&#xA;public class Entity{&#xA;    private Person person;&#xA;    private List &lt;Product&gt; products;&#xA;&#xA;    //constructors, getters, setters&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>In this way you have three different <code>POJO</code>s that you can use according to the needs (the type of API you are calling). </p>&#xA;"
41227788,41171413,1387080,2016-12-19T17:19:06,<p>I figured out that docker in docker has still some weird behaviors. I fixed my issue by adding a new gitlab ci runner that is a shell runner. Therefore docker-compose is run on my host and everything works flawless.</p>&#xA;&#xA;<p>I can reuse the same runner for starting docker images in production as I do for integration testing. So the easy fix has another benefit for me.</p>&#xA;&#xA;<p>The result is a best practice to avoid pitfalls:</p>&#xA;&#xA;<p>Only use docker in docker when there is a real need. </p>&#xA;&#xA;<p>For example to make sure fast io communication between your host docker image and your docker image of interest.</p>&#xA;&#xA;<p>Have fun using docker (in docker (in docker))  :]</p>&#xA;
42439743,42439626,2584392,2017-02-24T13:23:50,"<p>Rather than using auth, you can use the verb <code>users</code>. Hence the routes would change to</p>&#xA;&#xA;<pre><code>POST /users # Signup&#xA;POST /users/token # Login&#xA;PUT /users # Update profile&#xA;GET /users/me # Profile of logged in user&#xA;POST /users/reset&#xA;POST /users/forgot&#xA;DELETE /users/:id # deactivate account&#xA;</code></pre>&#xA;&#xA;<p>Now this is more of a personal preference but the endpoints are more or less compatible with <a href=""http://nodewebapps.com/2017/02/13/six-best-practices-when-building-rest-apis/"" rel=""nofollow noreferrer"">best practices</a>.</p>&#xA;"
47805785,47736658,3784008,2017-12-14T04:12:35,"<p><a href=""https://github.com/ruby-grape/grape#authentication"" rel=""nofollow noreferrer"">Grape has built-in Basic and Digest authentication</a> (the given <code>block</code> is executed in the context of the current Endpoint). Authentication applies to the current namespace and any children, but not parents.</p>&#xA;&#xA;<pre><code># Basic authentication example&#xA;http_basic do |username, password|&#xA;  # verify user's password here&#xA;  { 'test' =&gt; 'password1' }[username] == password&#xA;end&#xA;&#xA;# Digest authentication example:&#xA;http_digest({ realm: 'Test Api', opaque: 'app secret' }) do |username|&#xA;  # lookup the user's password here&#xA;  { 'user1' =&gt; 'password1' }[username]&#xA;end&#xA;</code></pre>&#xA;&#xA;<p>More information on the differences between these two is <a href=""https://stackoverflow.com/questions/9534602/what-is-the-difference-between-digest-and-basic-authentication"">available in other answers on StackOverflow</a>.</p>&#xA;&#xA;<p>More advanced authentication implementations are possible with Grape, for example <a href=""https://github.com/ruby-grape/grape#register-custom-middleware-for-authentication"" rel=""nofollow noreferrer"">OAUTH2</a>, by utilizing additional gems.</p>&#xA;"
37756358,37749087,248392,2016-06-10T20:05:10,"<p>Good question! So there are basically three forces at play here:</p>&#xA;&#xA;<ol>&#xA;<li>if a service goes down, any of the events it may have missed need to be replayed to keep it consistent</li>&#xA;<li>the events, as they happen in ""time"", have a ""this happened before that"" ordering to them</li>&#xA;<li>there may be (but doesn't have to be) another party interested in overseeing a cloud of events to make sure a certain state is achieved.</li>&#xA;</ol>&#xA;&#xA;<p>For both #1 and #2 you want some sort of persistent log of events. A traditional message queue/topic may provide this though you have to consider the cases when messages may be processed out of order wrt to transactions/exception/fault behaviors. A more simple log like Apache Bookkeeper, Apache Kafka, AWS Kinesis etc can store/persist these types of events in sequence and leave it to the consumers to process in order/filter out duplicates/partition streams etc. </p>&#xA;&#xA;<p>number 3 to me is a state machine. however you implement the state machine is really up to you. Basically this state machine keeps track of what events have happened and transitions to allowed states (and potentially participates in emitting events/commands) based on the events in the other systems.</p>&#xA;&#xA;<p>For example, a real-world use case might look like an ""escrow"" when you're trying to close on a house. The escrow company not just handles the financial transaction, but usually they work with the real-estate agent to coordinate getting papers in order, papers signed, money transferred, etc. After each event, the escrow changes state from ""waiting for buyer signature"" to ""waiting for seller signature"" to ""waiting for funds"" to ""closed success"" ... they even have deadlines for these events to happen, etc and can transition to another state if money doesn't get transferred like ""transaction closed, not finished"" or something.</p>&#xA;&#xA;<p>This state machine in your example would listen on the pub/sub channels and captures this state, runs timers, emits other events to further the systems involved, etc. It doesn't necessarily ""orchestrate"" them per se, but it does track the progress and enforce timeouts and compensations where needed. This could be implemented as a stream processor, as a process engine, or (imho best place to start) just a simple ""escrow"" service. </p>&#xA;&#xA;<p>There's actually more to keep track of like what happens if a ""escrow"" service goes down/fails, how does it handle duplicates, how does it handle unexpected events given it state, how does it contribute to duplicate events, etc... but hopefully enough to get started.</p>&#xA;"
51411688,36026454,9361992,2018-07-18T22:09:33,"<p>As I've not able figure this one out using input streams, absolute vs relative pathings, here is an alternative:</p>&#xA;&#xA;<p>Instead of using put, use rename</p>&#xA;&#xA;<pre><code>    try&#xA;    {&#xA;        channelSftp.cd(destinationDirectory);&#xA;        channelSftp.rename(pathToFile1, pathToFile2);&#xA;        channelSftp.cd(currentDirectory);&#xA;    } catch (SftpException e)&#xA;    {&#xA;        e.printStackTrace();&#xA;    }&#xA;</code></pre>&#xA;&#xA;<p>Basically it renames a file from one directory to another. Move to the destination directory, use rename to create that file there, and change back to the directory you were in.</p>&#xA;&#xA;<p>(I would've rather used the .put method, but ran into the same issue.)</p>&#xA;"
36913941,36858626,596041,2016-04-28T11:56:51,"<p>A message queue can, like any messaging system, work in two fundamental modes: at-least once delivery, or at most once delivery. Both strive for exactly once delivery, but we're talking about edge cases here. You're going to have to choose one of them. If all your communication between the producers and consumers (including producer-producer and consumer-consumer) goes through the message queue, then only the message queue needs a zookeeper cluster. Centralizing all messaging through a single system is the preferred way of doing this.</p>&#xA;&#xA;<p>Are you aiming for exactly once delivery because it's wasteful to do the same work twice, or because everything burns to the ground if you do the same thing twice? </p>&#xA;&#xA;<p>If the former, build something simple. Either the message queue itself can keep track of this, since it will stop asking new consumers to consume the task once one of them replies back with the result, or, if the storage has to be a bit more permanent, use redis or couchbase or cassandra or some simple key/value store to kinda store what things have successfully completed. Keep note of the requests you have sent out, but haven't received an answer from, in memory. Store the ""this action is compeleted"" note in the database.</p>&#xA;&#xA;<p>If the latter, you're going to have a harder time designing this system. You need to be able to tell if a process crashed, or if it just took longer than usual. You also need to continue where it left of, possibly by doing the work all over again. If the update is something like increment ten different counters, then doing the update again might increment some of the counters twice. </p>&#xA;"
48937358,48824086,9052489,2018-02-22T21:19:22,"<p>I have the same problem. </p>&#xA;&#xA;<p>Change the keycloak client to bearer-only did not solve the problem.</p>&#xA;&#xA;<p>The problem in there is the public endpoints. </p>&#xA;&#xA;<pre><code>@Override&#xA;protected void configure( HttpSecurity http ) throws Exception {&#xA;    super.configure( http );&#xA;    http&#xA;        .csrf().disable()&#xA;        .sessionManagement()&#xA;        .sessionCreationPolicy( SessionCreationPolicy.STATELESS )&#xA;        .and()&#xA;        .requestMatchers()&#xA;        .and()&#xA;        .authorizeRequests()&#xA;        .antMatchers( ""/tenants**"" ).permitAll() // &lt;- public endpoint&#xA;        .anyRequest()&#xA;        .authenticated();&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>For these endpoints, spring security creates an anonymous authentication with the dummy user ""anonymousUser."" This authentication is an instance of AnonymousAuthenticationToken that cannot be casted to KeycloakAuthenticationToken.</p>&#xA;&#xA;<p>If you try to make a request from an authenticated client, like an angular app, it will work because it can resolve the authentication through the token sent on the authorization header. This is the reason for the bearer-only approach works in same cases.</p>&#xA;&#xA;<p>I want to make a request to the backend to get the correct realm to authenticate on angular side. In that moment I don't have any authentication token to send, so the spring security will create the anonymous authentication.</p>&#xA;&#xA;<p>The exception is only thrown when the keycloak.cors=true.</p>&#xA;&#xA;<p>The problem happens when the method getSecurityContext() of SimpleHttpFacade.class is called on the class AuthenticatedActionsHandler.class by the method corsRequest().</p>&#xA;&#xA;<p>Does anyone have a working solution?</p>&#xA;&#xA;<p>Best Regards</p>&#xA;"
41758119,41747165,1062217,2017-01-20T07:22:20,"<p>In order to setup a Service Fabric cluster you need at <strong>least 3 machines</strong> (or you cannot reach quorum in your cluster). If you run it on Azure then you also choose the Reliability and durability tier for your nodetypes <a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-cluster-capacity#the-reliability-characteristics-of-the-cluster"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-cluster-capacity#the-reliability-characteristics-of-the-cluster</a>. A higher reliability tier (Silver, Gold or Platinum) means that you need further nodes (machines) in cluster.</p>&#xA;&#xA;<p>You can run multiple instances of the same application and of different deployment versions in the same cluster. You need to consider how your services are assigned ports (for those that expose HTTP endpoints), otherwise these will conflict if you have multiple instances of the same application type in the same cluster. There is currently no way to provision new instances through Visual Studio, you need to use PowerShell, the API or the Service Fabric Explorer.</p>&#xA;"
41902798,41880229,1062217,2017-01-27T20:49:32,"<p>​&#xA;I think that there is never completely any solution that is 100% sure to <em>never</em> loose a message between two parties. Even if you had a service bus for instance in between two services, there is always the chance (possibly very small, but never null) that the service bus goes down, or that the communication to the service bus goes down. With that being said, there are of course models that are less likely to very seldom loose a message, but you can't completely get around the fact that you still have to handle errors in the client.</p>&#xA;&#xA;<p>In fact, Service Fabric fault handling is mainly designed around clients retrying communication, rather than having the service or an intermediary do that. There are many reasons for this (I guess) but one is the nature of distributed, replicated, reliable services. If a service primary goes down, a replica picks up the responsibility, but it won't know what the primary was doing right at the moment it died (unless it replicated over it's state, but it might have died even before that). The only one that really knows what it wants to do in this scenario is the client. The client knows what it is doing and can react to different fault scenarios in te service. In Fabric Transport, most know exceptions that could ""naturally"" occur, such as the service dying or the network cable being cut of by the janitor are actuallt retried automatically. This includes re-resolving the address just in case the service primary was replaced with a secondary.</p>&#xA;&#xA;<p>The same actually goes for a scenario where you introduce a third service or a service bus. What if the network goes down before the message has completely reached the service? In this case only the client knows that something went wrong and what it intended to send. What if it goes down after it reached the service but before the response was sent? In this case the client has to assume the message never reached and try to resend it. This is also why service methods are recommended to be <strong>idempotent</strong> - the same call can be made a number of times by the same client.</p>&#xA;&#xA;<p>Even if you were to introduce a secondary part, like the service bus, there is still the same risk that the service bus goes down, or more likely, the network connecting to the service bus goes down. So, client needs to retry, and when it has retried a number of times, all it can do is put the message in a queue of failed messages or simply just log it, or throw an exception back to the original caller (in your scenario, the browser).</p>&#xA;&#xA;<p>Ok, that's was me being pessimistic. But it could happen. All of the things above, its just that some are not very likely to happen. But they might happen.&#xA;On to your questions:</p>&#xA;&#xA;<p><strong>1)</strong> the problem with making a stateless service stateful is that you now have to handle partitions in your caller. You can put up Http listeners for stateful services, but you have to include the partition and replica information in the Uri, and that won't work with the load balancer, so in this case the browser has to select partition when calling the API. Not an ideal solution.</p>&#xA;&#xA;<p><strong>2)</strong> yes, you could do this, i.e. introduce <em>something</em> else in between that queues messages for you. There is nothing that says that a Service Bus or a Database is more reliable than a Stateful service with a reliable queue there, it's just up to you to go for what you are most comfortable with. I would go for a Stateful service, just so I can easily keep everything within my SF application. But again, this is not 100% protection from disgruntled janitor with scissors, for that you still need clients that can handle faults.</p>&#xA;&#xA;<p><strong>3)</strong> make sure you have a way of handling the errors (retry) and logging or storing the messages that fail (after retries) with the client (Service 1). </p>&#xA;&#xA;<p><strong>3.a)</strong> One way would be to have it store it localy on the node it is running and periodically (RunAsync for instance) try to re-run those failed messages. This might be dangerous in the scenario where the node it is running on is completely nuked and looses it data though, that data won't be replicated. </p>&#xA;&#xA;<p><strong>3.b)</strong> Another would be to use semantic logging with ETW and include enough data in the events to be able to re-create the message from the logged and build some feature, a manual UI perhaps, where you can re-run it from the logged information. Much like you would retry a failed message on an error queue in a service bus.</p>&#xA;&#xA;<p><strong>3.c)</strong> Store the failed messages to anything else (database, service bus, queue) that doesn't fail for the same reasons your communication with Service 2.</p>&#xA;&#xA;<p>My main point here is (and I could maybe have started with that) is that there are plenty of scenarios where only the client knows enough to handle the situation. So, make sure you have a strategy for handling faults in your clients.</p>&#xA;"
41323452,40495213,1062217,2016-12-25T18:12:14,"<p><strong>tl;dr</strong>: Figure out what best works for your development team(s) in terms of managing code and releases of individual services. Use <a href=""https://stackoverflow.com/questions/34873891/differential-packaging"">diff packages</a> to upgrade only changes in your Service Fabric applications. Smallest repo size should be one Service Fabric Application contained in one Visual Studio Solution.</p>&#xA;&#xA;<p><strong>Longer version</strong>:&#xA;It is fully possible to split your Service Fabric Application into multiple applications, the smallest being one Service Fabric Application for each microservice you have. If this is a good idea or not completely depends on the type of application you are trying to build. Are there any dependecies between the services? How do you partition services and could there be any scenario when you want to do that in a coordinated maner? How are you planning to monitor your services? If you wan't to do that in a coordinated maner then again it might make sense to have more services in the same application.</p>&#xA;&#xA;<p>Splitting the code into repos that are smaller than your Visual Studio solution would likely only lead to trouble for you. You could technically work with Git submodules or subtrees to some effect, but the way Visual Studio handles project references inside solutions would likely make you end up in merge-hell very soon.</p>&#xA;&#xA;<p>When it comes to upgrading your Service Fabric Application there is actually a way for you to upgrade only the changed services in your application based on the version numbers in the service manifest. This is called a <em>diff package</em> and can be used to deploy an application to a cluster where that application has been deployed at least once (i.e. it is an upgrade, not install). This could greatly affect the upgrade time of your deployment if you have only upgrade a minority of the services in the application.&#xA;<a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-application-upgrade-advanced"" rel=""nofollow noreferrer"">The full documentation for this can be found here</a>. There is also a <a href=""https://stackoverflow.com/questions/34873891/differential-packaging"">SO answer</a> that describes it.</p>&#xA;&#xA;<p><em>I would say that your choice is, as much in development, a trade-off between different gains.</em></p>&#xA;&#xA;<p>Splitting the services into more fine grained application containing fewer service could make upgrades easier (but this effect could to some extent technically also be achieved by using diff packages). The downside of this approach is that you would have to manage dependencies as strict interfaces between your services. One approach for that would be to publish your service/actor interfaces to a private NuGet-feed. This in turn introduces some additional complexity in your development pipeline.</p>&#xA;&#xA;<p>Keeping everything in the same repo, same Visual Studio solution, same Service Fabric Application could work for smaller solutions but will likely be hard to work with in the long run if your solution grows in terms of merges, versioning and releases.</p>&#xA;"
42244969,42240919,1062217,2017-02-15T09:09:24,"<p>First of all, why does the customer want to use Service Fabric and a microservices archtecture when it at the same time sounds like there are <em>other</em> parts of the solution (webjobs etc) that will not be a part of thar architecture but rather live in it's own ecosystem (yet share logic)? I think it would be good for you to first understand the underlying requirements that should guide the architecture. What is most imortant?</p>&#xA;&#xA;<ul>&#xA;<li>Scalability? Flexibility?</li>&#xA;<li>Development and deployment? Maintinability?</li>&#xA;<li>Modularity in ability to compose new solutions based on autonomous microservices?</li>&#xA;</ul>&#xA;&#xA;<p>The list could go on. Until you figure this out there is really no point in designing further as you don't know <em>what you are designing for</em>...</p>&#xA;&#xA;<p>As for sharing business logic with webjobs, there is nothing preventing you from sharing code packages containing the same BL, it doesn't have to be a shared service and it doesn't mean that it has to be packaged the same way in relation to its interface or persistance. Another thing to consider is, why do you wan't to run webjobs when you can build similar functionality in SF services?</p>&#xA;"
43520452,43515150,1062217,2017-04-20T13:06:41,"<p>Yes, you can control this very explicitly. </p>&#xA;&#xA;<p>In your ApplicationManifest you get a number of DefaultServices added, these are autogenerated from the Services added to you Application. When you deploy from Visual Studio (or VSTS) you (implicitly) use the <code>Deploy-FabricApplication.ps1</code> script that is automatically added to your Application project. This script does many things at the same time. Namely:</p>&#xA;&#xA;<ol>&#xA;<li><p>Reads and parses publisher profile files to get the parameter values for the selected target (e.g. Cloud, Local1Node, Local2Node, etc.)</p></li>&#xA;<li><p>Upload a new image version to the cluster image store for the <em>ApplicationType</em> with a new version number.</p></li>&#xA;<li><p>Create or upgrade an application instance based on the new image and version and create or upgrade an instance for each DefaultService in the ApplicationManifest</p></li>&#xA;</ol>&#xA;&#xA;<p>Now, you can do all of these steps yourself, and specifically, you can choose which services to spin up instances for.</p>&#xA;&#xA;<p>You can use either the <a href=""https://docs.microsoft.com/en-us/powershell/module/servicefabric/?view=azureservicefabricps"" rel=""nofollow noreferrer"">PowerShell (or CLI)  cmdlets</a> or use <a href=""https://docs.microsoft.com/en-us/dotnet/api/system.fabric.fabricclient?view=servicefabric-5.5.216"" rel=""nofollow noreferrer"">FabricClient</a> through .NET code.</p>&#xA;&#xA;<p>What you need to do then is (for PowerShell but similar for FabricClient) is:</p>&#xA;&#xA;<ul>&#xA;<li>Connect to the cluster <a href=""https://docs.microsoft.com/en-us/powershell/module/servicefabric/Connect-ServiceFabricCluster?view=azureservicefabricps"" rel=""nofollow noreferrer"">Connect-​Service​Fabric​Cluster</a></li>&#xA;<li>Copy the application package to the cluster with <a href=""https://docs.microsoft.com/en-us/powershell/module/servicefabric/copy-servicefabricapplicationpackage?view=azureservicefabricps"" rel=""nofollow noreferrer"">Copy-ServiceFabricApplicationPackage</a></li>&#xA;<li>If it is a new - register the application type with <a href=""https://docs.microsoft.com/en-us/powershell/module/servicefabric/register-servicefabricapplicationtype?view=azureservicefabricps"" rel=""nofollow noreferrer"">Register-​Service​Fabric​Application​Type</a></li>&#xA;<li>-or- upgrade an existing with <a href=""https://docs.microsoft.com/en-us/powershell/module/servicefabric/start-servicefabricapplicationupgrade?view=azureservicefabricps"" rel=""nofollow noreferrer"">Start-​Service​Fabric​Application​Upgrade</a></li>&#xA;<li>For each service you want to create an instance for (yes, you can create multiple instances of the same service inside the same application, and this is not the same as <em>instances</em> for a StatelessService) using <a href=""https://docs.microsoft.com/en-us/powershell/module/servicefabric/New-ServiceFabricService?view=azureservicefabricps"" rel=""nofollow noreferrer"">New-​Service​Fabric​Service&#xA;</a></li>&#xA;</ul>&#xA;&#xA;<p>All though this is (a lot) more work than just using the supplied Deploy-FabricApplication.ps1, it gives you complete control over how and when services are deployed. It is also necessary to do this, if you for instance, want to run multiple tenants for the same application on the same cluster, or run multiple versions/environments of the same application on the same cluster (e.g. running Development and Testing on the same cluster)</p>&#xA;"
43704153,43689879,1062217,2017-04-30T07:46:06,"<p>You can set it up by hooking in to the underlying communication in Service Fabric. Have a look at <a href=""https://stackoverflow.com/q/41629755/1062217"">this question</a> for an explanation of the fabric transport message dispatching and how to hook into it to log and audit communication and usage of your services.</p>&#xA;"
42705169,42332737,7489353,2017-03-09T20:51:32,"<p>I thought I would follow up with some details on my implementation here, in case it was useful for anybody else looking to secure an API Gateway using Reference Tokens, with downstream services that require a JWT.</p>&#xA;&#xA;<p>I have implemented an ASP.Net Core middleware that runs in the API Gateway.  The middleware runs in the pipeline after the Identity Server Authentication middleware and before the MVC middleware.  The purpose of the middleware is to take the inbound reference token and exchange this with Identity Server (using Extension Grants) for a JWT that can be used in the Authorization header when making calls to the downstream microservices.</p>&#xA;&#xA;<p>The middleware is available on <a href=""https://github.com/clawrenceks/ReferenceTokenExchange"" rel=""nofollow noreferrer"">GitHub</a> and also as a <a href=""https://www.nuget.org/packages/Clawrenceks.Middleware.ReferenceTokenExchange/"" rel=""nofollow noreferrer"">NuGet package</a>.  Further detail on the use case of the middleware can be seen in the GitHub ReadMe and there is also full detail on installing and configuring the middleware available in the <a href=""https://github.com/clawrenceks/ReferenceTokenExchange/wiki"" rel=""nofollow noreferrer"">GitHub Wiki</a>.</p>&#xA;&#xA;<p>In addition to performing the exchange of a Reference Token for an Access Token the middleware also has built in support for caching, reducing the number of round trips to the Identity Server to perform token exchange.</p>&#xA;&#xA;<p>If anybody wants to use the middleware and has questions regarding the installation and configuration that are not covered by the documentation, please add an issue in the GitHub repository tagged as Question.</p>&#xA;"
52065586,52058409,218441,2018-08-28T20:15:13,<p>I don't think it is a good idea. It is important that every micro service implements (and documents) its own internal error codes - they are an important part of the service definition. I think it is a bad idea to re-use the same internal error code(s) for different services. Make the documentation part of the service definition (swagger is useful).</p>&#xA;&#xA;<p>It should be easy to understand a service from a client perspective. It should be clean what error codes can be returned for a given service and route. If a client only needs to consume service A - then why force them to use a module that defines error codes for all services?</p>&#xA;
50320476,50314370,218441,2018-05-13T20:42:20,"<p>Sounds like you are a bit confused, I would really recommend you picking up a book on the subject.</p>&#xA;&#xA;<p>Monolithic vs micoservice application is more about how you package and deploy your application and in some sense how coupled modules/subsystems are. The extreme example: you always deploy the entire monolithic applications for the smallest possible change, and in the microservice example you just need to make the same change to one service.</p>&#xA;&#xA;<p>Rest API and SOAP are protocols for how (http) messages are passed between client &lt;-> server and has nothing to do with monolithic vs micro.</p>&#xA;&#xA;<p>Monolithic application can of course have public http API's, and it might not be possible for a user (sending request) of that API to tell the architecture style of that application. And why would she care about that?</p>&#xA;&#xA;<p>I think this is a nice start: <a href=""https://martinfowler.com/articles/microservices.html"" rel=""nofollow noreferrer"">https://martinfowler.com/articles/microservices.html</a></p>&#xA;"
50344648,50338845,218441,2018-05-15T07:31:13,"<p>I say it depends on the use case. If service A needs to know what service B knows, then it is perfectly sane for service A to make a REST call to service B. But if the combined knowledge for A and B is only needed in your gateway then the gateway can combine the results.</p>&#xA;"
48005205,48004725,5746249,2017-12-28T09:50:22,"<p>Just give you the simplest answer:&#xA;In general containers can communicate among each others with any protocols (http,ftp,tcp,udp) not limit to only rest(http/s) </p>&#xA;&#xA;<ol>&#xA;<li>using the internal/ external IPs and ports</li>&#xA;<li>using the internal/ external names (dns):&#xA;&#xA;<ul>&#xA;<li>in your Micro-service is in the same cluster on multi-host -> you should be able to write the program in your Springboot to call http://{{container service name}} , It's the built-in feature of containers  </li>&#xA;<li>if you have more microservices in different cluster or hosts or the internet , you can use APIM (API management) or reverse-proxy(NGINX,HAProxy) to manages the service name eg. &#xA;microservice1.yourdomain.com —> container1  or service1(cluster)&#xA;microservice2.yourdomain.com —> container2 or service 2(cluster)&#xA;yourdomain.com/microservice1—> container2 or service 2(cluster)&#xA;yourdomain.com/microservice2—> container1  or service1(cluster)</li>&#xA;</ul></li>&#xA;</ol>&#xA;&#xA;<p>PS . there are more sophisticated techniques out there but it fundamentally come down above approaches. </p>&#xA;"
32661253,32093067,3653449,2015-09-18T21:23:55,"<p>I don't know much about Akka, but from reading quickly it seems that it is an app framework.  Kubernetes is at a bit of a lower-level.  Kubernetes runs your containers and manages them for you.  We don't have a concept of queues or mailboxes.</p>&#xA;&#xA;<p>Kubernetes will soon have L7 load balancing so you can do URL maps.</p>&#xA;&#xA;<p>As for fault tolerance - kubernetes ensures that your stated intentions are true - run N copies of this container.  That container might be an Akka app or might be mysql - dopesn't matter.</p>&#xA;&#xA;<p>There are a bunch of guides on Docker + Akka.  Kubernetes makes managing docker containers easier, but the app is still yours :)</p>&#xA;"
27284232,25595492,4316489,2014-12-04T00:29:54,"<p>While implementing a microservice architecture at my previous job we decided the best approach was in alignment with #1, Add identity service and authorize service access through it. In our case this was done with tokens. If a request came with an authorization token then we could verify that token with the identity service if it was the first call in the user's session with the service. Once the token had been validated then it was saved in the session so subsequent calls in the user's session did not have to make the additional call. You can also create a scheduled job if tokens need to be refreshed in that session.</p>&#xA;&#xA;<p>In this situation we were authenticating with an OAuth 2.0 endpoint and the token was added to the HTTP header for calls to our domain. All of the services were routed from that domain so we could get the token from the HTTP header. Since we were all part of the same application ecosystem, the initial OAuth 2.0 authorization would list the application services that the user would be giving permission to for their account.</p>&#xA;&#xA;<p>An addition to this approach was that the identity service would provide the proxy client library which would be added to the HTTP request filter chain and handle the authorization process to the service. The service would be configured to consume the proxy client library from the identity service. Since we were using Dropwizard this proxy would become a Dropwizard Module bootstrapping the filter into the running service process. This allowed for updates to the identity service that also had a complimentary client side update to be easily consumed by dependent services as long as the interface did not change significantly.</p>&#xA;&#xA;<p>Our deployment architecture was spread across AWS Virtual Private Cloud (VPC) and our own company's data centers. The OAuth 2.0 authentication service was located in the company's data center while all of our application services were deployed to AWS VPC.</p>&#xA;&#xA;<p>I hope the approach we took is helpful to your decision. Let me know if you have any other questions.</p>&#xA;"
46758538,45794390,2613577,2017-10-15T18:28:47,"<p>You cannot run your angular tests without a browser as it is part of a front end application that only works when run on a browser. I see that you are trying to run it on PhantomJs. PhantomJs is a javascript project that is meant to emulate a browser for angular application testing. </p>&#xA;&#xA;<p>From your perspective, all that you want is to ensure that the test runs and you dont really care about a browser opening. This is precisely what a 'headless' browser is. A browser with no GUI. PhantomJs, an example of a headless browser, has become popular in the past as it can run tests in pipeline without the need for a GUI. Today Chrome also offers a headless mode for test runners.</p>&#xA;&#xA;<p>If you have set up your Karma configuration file properly with PhantomJs then you will find that it works correctly. Here is a link of person doing what you want in docker environment for a gitlab CI pipeline.</p>&#xA;&#xA;<p><a href=""https://medium.com/letsboot/angular-4-and-testing-angular-cli-gitlab-ci-ng-test-phantomjs-tdd-afc20f50b928"" rel=""nofollow noreferrer"">https://medium.com/letsboot/angular-4-and-testing-angular-cli-gitlab-ci-ng-test-phantomjs-tdd-afc20f50b928</a></p>&#xA;"
40702659,40702179,3477322,2016-11-20T09:51:24,"<p><strong>In short</strong>, you could use the serverless architecture i.e (with AWS's APIGateway and Lambda services) to build robust micro service based web applications. &#xA;Since you said that you were new to micro services architecture, I am listing down the best approaches.</p>&#xA;&#xA;<p><strong>Frontend/client</strong></p>&#xA;&#xA;<p>Single page applications(SPA) work well in the front-end and as they are a static site, they could be easily deployed to S3. This is the most cost efficient approach for SPAs. Here is a video deploying <a href=""https://egghead.io/lessons/misc-hosting-a-static-website-on-amazon-s3"" rel=""nofollow noreferrer"">SPA on S3</a>. This video will guide you through step by step instructions for deploying your SPA.</p>&#xA;&#xA;<p>In case, you use react and redux in the front end, check out these <a href=""https://stackoverflow.com/questions/39783099/deploying-react-redux-app-in-aws-s3"">steps</a> for deploying react app to S3.</p>&#xA;&#xA;<p><strong>Backend</strong></p>&#xA;&#xA;<p>AWS EC2 is a good option. But there are many more alternatives available. As you said, you were new to backend, setting up EC2, VPC's and Elastic-ip is a little difficult process.</p>&#xA;&#xA;<p>Nowadays, SPA's cover a lot of business logic, routing, etc., We need our <strong>backend</strong> only as API's for performing CRUD operations with database. I would like to suggest a bleeding edge technology called <a href=""https://serverless.com/"" rel=""nofollow noreferrer"">serverless</a>. Here is the <a href=""https://github.com/serverless/serverless/blob/master/docs/01-guide/README.md"" rel=""nofollow noreferrer"">tutorial</a> for launching your backend within 5 minutes. AWS lambda is a service that is called as <strong>function as service</strong>. You can build your backend using <strong>AWS lambda + API gateway + DynamoDB</strong>. </p>&#xA;&#xA;<p><strong>For eg</strong>: say you want to register some details in backend, you will <strong>POST</strong> all the data from client to your backend with url and proper path. In AWS lambda, you write your logic for <strong>POST</strong> as a function, which contains the logic to parse the data from request and send to dynamoDB. Now, this function can be exposed to world by connecting this function with API gateway( an another service in AWS). At the end we get an API, which can be used in your angular 2 APP. SO, on invoking the POST, <strong>angular 2 -> API gateway -> Lambda(extract request and send to DB) -> dynamoDB</strong>.</p>&#xA;&#xA;<p>Benefits of using serverless compared to EC2.</p>&#xA;&#xA;<ol>&#xA;<li>You don't need to manage your server(EC2) from updating the new security patch to auto-scaling, everything is taken care by lambda. Serverless is a fully managed service.</li>&#xA;<li>You only pay when your lambda functions are invoked. On the contrast, even though your web app doesn't receive traffic for a given day, you have to pay the day-tariff for the given day.</li>&#xA;</ol>&#xA;&#xA;<p>Here is my <a href=""https://github.com/Lakshman-LD/LetsMeetUp"" rel=""nofollow noreferrer"">github repo</a> which could be a boiler plate for reactJS + Serverless + graphQL web app.</p>&#xA;&#xA;<p>Having said, try serverless when compared to traditional backend approach. Any questions on this would be welcomed.</p>&#xA;"
48943093,48934158,6375498,2018-02-23T07:26:26,"<p>If you use an EC2 instance then it's configuration is no different to what you do on your local machine/server. It's just a virtual machine. No need to dockerize or anything like that. And if you're new to AWS, I'd rather suggest to to just that. Once you get your head around, you can explore other options.</p>&#xA;&#xA;<p>For example, AWS Beanstalk seems like a popular option. It provides a very secure and reliable configuration out of the box with no effort on your part. And yes, it does use docker under the hood, but you won't need to deal with it directly unless you choose to. Well, at least in most common cases. It supports few different ways of deployment which amazon calls ""Application Environments"". See <a href=""https://aws.amazon.com/elasticbeanstalk/getting-started/"" rel=""nofollow noreferrer"">here</a> for details. Just choose the one you like and follow instructions. I'd like to warn you though that whilst Beanstalk is usually easier then EC2 to setup and use when dealing with a typical web application, your mileage might vary depending on your application's actual needs.</p>&#xA;"
36646101,36645517,1804618,2016-04-15T11:36:33,"<p>If you design a Draws service, you url should be:</p>&#xA;&#xA;<blockquote>&#xA;  <p><a href=""http://url:port/draws/"" rel=""nofollow"">http://url:port/draws/</a>{gameNo}</p>&#xA;</blockquote>&#xA;&#xA;<p>If you plan to extend the service with other endpoints, probably you are designing something more generic. If this is the case, name it accordingly.</p>&#xA;&#xA;<p>In any way you shouldn't go with draws/draws.</p>&#xA;&#xA;<p>Also, I would recommend to use lower case and https (if you can).</p>&#xA;&#xA;<p>Recently I posted a summary of the most often questions that arise when designing RESTful API - <a href=""http://goinbigdata.com/how-to-design-practical-restful-api/"" rel=""nofollow"">How To Design Practical RESTful API</a> , which could be helpful.</p>&#xA;"
44522088,44521929,1747018,2017-06-13T12:52:17,"<p>It is called serverless as you dont manage and maintain the underlying server and the runtime.&#xA;Basically you write your code in one of the supported languages, say node.js, and then configure events that will trigger your code.&#xA;Example in case of AWS, the events can be a API GW call, a SQS message, a SNS notification etc.</p>&#xA;&#xA;<p>So it can be better depending on what you are planning on doing.&#xA;Do note that there are certain limits that AWS imposes by default on accounts for AWS Lambda.&#xA;Also there can be slight startup penalty for a Lambda.&#xA;A plus point of Lambda vs Hosting your code in EC2 is that with Lambda you dont get charged if your code is not used/triggered.&#xA;However, do note that for functions that have heavy usage it might be better to &#xA;host your own EC2.</p>&#xA;&#xA;<p>Most important a Lambda has to be stateless. </p>&#xA;&#xA;<p>Considering all the above factors you can take a call on whether AWS Lambda and Serverless Architecture fits your needs.</p>&#xA;"
43846465,43396744,2600522,2017-05-08T11:27:02,"<p><strong>Problem</strong></p>&#xA;&#xA;<p>How to aggregate ReactJS UI components during server side rendering on the Web API gateway. </p>&#xA;&#xA;<p><strong>Solution</strong></p>&#xA;&#xA;<p>Use a templating framework like <a href=""http://mustache.github.io/"" rel=""nofollow noreferrer"">Mustache</a> to inject the rendered HTML output of each ReactJS component server side, then serve this HTML back to the client. </p>&#xA;&#xA;<p>Github repo <a href=""https://github.com/damorton/dropwizardheroku-webgateway"" rel=""nofollow noreferrer"">https://github.com/damorton/dropwizardheroku-webgateway</a></p>&#xA;&#xA;<p><strong>Server Side</strong></p>&#xA;&#xA;<p>The solution I implemented on the Web API gateway first requested the JSON data from the microservice, then rendered the ReactJS component while injecting the JSON data from the microservice as <code>Props</code>. Once I had the fully rendered ReactJS component with data as a HTML string I used Mustache templating to inject the fully rendered ReactJS component HTML into the Mustache template, then returned this to the client. </p>&#xA;&#xA;<p><strong>WebGatewayResource.java</strong></p>&#xA;&#xA;<pre><code>@GET&#xA;@Produces(MediaType.TEXT_HTML)&#xA;public IndexView index() throws IOException {&#xA;&#xA;    // Get events json data from Events microservice&#xA;    ApiResponse events = getEventsJsonData();&#xA;&#xA;    // Render the Events component and pass in props&#xA;    @SuppressWarnings(""unchecked"")&#xA;    List&lt;Object&gt; eventsProps = (List&lt;Object&gt;) events.getList();&#xA;    String eventsComponent = this.nashornController.renderReactJsComponent(kEventsUiComponentRenderServerFunction, eventsProps);&#xA;&#xA;    IndexView index = new IndexView(eventsComponent);&#xA;    return index;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p><em>Note:</em> Dropwizard performs alot of magic around Mustache templating so all that was needed was to create an <code>index.mustache</code> file and reference this when constructing the <code>IndexView</code> class. Returning this <code>View</code> to the client tells Dropwizard to render the view and return the HTML. </p>&#xA;&#xA;<p><strong>index.mustache</strong></p>&#xA;&#xA;<pre><code>&lt;!DOCTYPE html&gt;&#xA;&lt;html&gt;&#xA;&lt;head&gt;&#xA;&lt;meta charset=""UTF-8"" /&gt;&#xA;&lt;title&gt;Dropwizard Heroku Event Service&lt;/title&gt;&#xA;&lt;script type=""text/javascript"" src=""https://cdnjs.cloudflare.com/ajax/libs/react/15.5.4/react.min.js""&gt;&lt;/script&gt;&#xA;&lt;script type=""text/javascript"" src=""https://cdnjs.cloudflare.com/ajax/libs/react/15.5.4/react-dom.min.js""&gt;&lt;/script&gt;&#xA;&lt;script type=""text/javascript"" src=""https://cdnjs.cloudflare.com/ajax/libs/react/15.5.4/react-dom-server.min.js""&gt;&lt;/script&gt;&#xA;&lt;script type=""text/javascript"" src=""https://cdnjs.cloudflare.com/ajax/libs/babel-standalone/6.24.0/babel.min.js""&gt;&lt;/script&gt;&#xA;&lt;script type=""text/javascript"" src=""https://cdnjs.cloudflare.com/ajax/libs/axios/0.16.1/axios.min.js""&gt;&lt;/script&gt;&#xA;&lt;/head&gt;&#xA;&lt;body&gt;&#xA;  &lt;h1&gt;Events&lt;/h1&gt;&#xA;  &lt;div id=""events""&gt;{{{eventsComponent}}}&lt;/div&gt;&#xA;  &lt;script type=""text/javascript"" src=""assets/js/bundle.js""&gt;&lt;/script&gt;&#xA;  &lt;script type=""text/javascript"" src=""assets/js/client.js""&gt;&lt;/script&gt;&#xA;&lt;/body&gt;&#xA;&lt;/html&gt;&#xA;</code></pre>&#xA;&#xA;<p><strong>Client side</strong></p>&#xA;&#xA;<p>On the client side, to fix an issue with the client and server side rendered HTML being different due to the props for the ReactJS components not being available when the component was mounted initially, a javascript function is called when the page loads to request the JSON data from the gateway. </p>&#xA;&#xA;<p><strong>client.js</strong></p>&#xA;&#xA;<pre><code>var loadEventsFromServer = function(eventsUrl) {&#xA;    axios.get(eventsUrl).then(function(res) {&#xA;        var data = res.data.list;       &#xA;        renderClientEvents(data);&#xA;    });&#xA;};&#xA;&#xA;loadEventsFromServer('https://domain.webapigateway.com/events');&#xA;</code></pre>&#xA;&#xA;<p><strong>ReactJS</strong></p>&#xA;&#xA;<p>The client side HTML is not re-rendered when the component is mounted, React is aware of the already existing HTML from the server side render and only adds the event listeners for each component when it mounts. This allows React to update its components individually, and also takes advantage of server side rendering. </p>&#xA;"
48625734,48624302,637514,2018-02-05T15:20:57,"<p>I think it may sense to separate into different repositories and have different jenkins jobs for each one. &#xA;Then you split your microservices architecture in different repositories and you can even test each microservices running the docker images for each microservice.</p>&#xA;&#xA;<p>This is the way we are doing in my company. And for me, it looks better at the end, but it can be a big effort at the beginning.</p>&#xA;"
35634098,35632607,1428554,2016-02-25T17:23:29,"<p>PURGE is not a standard HTTP method. It is just something configured in Varnish VCL - usually in this fashion or similar: </p>&#xA;&#xA;<pre><code>if (req.method == ""PURGE"") {&#xA;        if (!client.ip ~ purge) {&#xA;                return(synth(405,""Not allowed.""));&#xA;        }&#xA;        return (purge);&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>(See: <a href=""https://www.varnish-cache.org/docs/trunk/users-guide/purging.html"" rel=""nofollow"">https://www.varnish-cache.org/docs/trunk/users-guide/purging.html</a>)</p>&#xA;&#xA;<p>When you call PURGE on a resource (URL) it will be removed from the cache (Varnish) so for the next GET request on the same resource Varnish will call the backend and cache its response. If you then call PURGE again on this resource it will be evicted from the cache again. </p>&#xA;"
32532010,28500066,2149706,2015-09-11T20:57:53,"<p>I find very useful to first copy the artifacts to a specified area on the server to keep track of the deployed artifacts and not to start the app from the jenkins job folder. Then create a server log file there and start to listening to it on the jenkins window until the server started.</p>&#xA;&#xA;<p>To do that I developed a small shell script that you can find <a href=""https://github.com/rcoli/spring-boot-jenkins/blob/master/README.md"" rel=""nofollow noreferrer"">here</a></p>&#xA;&#xA;<p>You will also find a small article explaining how to configure the project on jenkins.</p>&#xA;&#xA;<p>Please let me know if worked for you. Thnaks</p>&#xA;"
48210953,48209566,9181003,2018-01-11T15:48:05,"<p>Yes, it depends on whether your operation is atomic (so the user wants immediate feedback), or just a background processing job, as Nicolas pointed out.&#xA;I just want to add that you might consider implementing a circuit breaker pattern to retry it a few times before reporting failure to the user, or persisting the message as NOT_SENT, depending on your requirements.</p>&#xA;"
45340970,45325062,665163,2017-07-27T03:40:19,"<p>Short answer: no, contract testing isn't <em>necessary</em> in any situation, the same as unit testing.</p>&#xA;&#xA;<p>Long answer: Not having testing greatly reduces your confidence as a developer to deploy without breaking anything.  Unit testing is good for testing an individual function, while contract testing is good at figuring if your changes will affect any consumers of the data you provide.  The consumers of your data could be anyone, it could be someone across the room from you, a client external of the company or even yourself.  The whole point is to try to segment and simplify the development process so that problems are caught earlier on.  It also has the added benefit that you don't need to run the data producer locally just to have the consumer working while developing, which is definitely a great bonus when the consumer doesn't (or can't) have access provider code, like an external client.</p>&#xA;&#xA;<p>These tools are meant to make your life as a developer simpler and easier to manage, Pact strives to accomplish this in your workflow and to prevent issues from happening in production and giving the developer a quicker feedback loop of potential issues.</p>&#xA;"
51676847,51676756,6171211,2018-08-03T16:37:00,"<p>In spring configuration, you have declared the base package of your jpa repositories in ""com.techprimers.stock.dbservice.repository""<br>&#xA;<code>@EnableJpaRepositories(basePackages = ""com.techprimers.stock.dbservice.repository"")</code></p>&#xA;&#xA;<p>But from the repository code, actually it is inside package ""com.example.dbservice.repository""</p>&#xA;&#xA;<p>So change the base package in configuration like:</p>&#xA;&#xA;<pre><code>@EnableJpaRepositories(basePackages = ""com.example.dbservice.repository"")&#xA;@SpringBootApplication&#xA;public class DbServiceApplication {&#xA;&#xA;    public static void main(String[] args) {&#xA;        SpringApplication.run(DbServiceApplication.class, args);&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Update:&#xA;Now the error is for jaxb components missing, java.xml.bind was part of java till jdk 8 and is deprecated from 9. Add following in your pom.xml</p>&#xA;&#xA;<pre><code>&lt;dependency&gt;&#xA;    &lt;groupId&gt;javax.xml.bind&lt;/groupId&gt;&#xA;    &lt;artifactId&gt;jaxb-api&lt;/artifactId&gt;&#xA;    &lt;version&gt;2.3.0&lt;/version&gt;&#xA;&lt;/dependency&gt;&#xA;</code></pre>&#xA;"
47543088,47529688,2308683,2017-11-29T00:20:21,"<p>Singular relational databases work fine, but they scale poorly. Especially for large scale web services. </p>&#xA;&#xA;<p>You need to measure your data ingestion volume/size to determine if you need Hadoop (more specifically HDFS) for batch analytics on top of Elasticsearch. But likely not. You can use a Standalone Apache Spark cluster to run things against Elasticseach directly. </p>&#xA;&#xA;<p>However, you could also <a href=""https://www.confluent.io/blog/simplest-useful-kafka-connect-data-pipeline-world-thereabouts-part-1/"" rel=""nofollow noreferrer"">use Kafka as a message bus between your JDBC compatible database as well as loading an Elasticsearch index</a>. And Spark Streaming works great with Kafka. </p>&#xA;&#xA;<p>And if you want to add Hadoop into the mix, you can just pull the same data from Kafka to fill in an HDFS directory. </p>&#xA;&#xA;<p>There are many blogs talking about microservices communication via Kafka</p>&#xA;"
48160611,48143008,2308683,2018-01-09T02:19:19,"<blockquote>&#xA;  <p>will it reprocess from where it had stopped?</p>&#xA;</blockquote>&#xA;&#xA;<p>It should, if your consumer has not committed the consumed offsets. </p>&#xA;&#xA;<p>If your application restarts between an auto-commit interval then you will experience duplicate messages, though, starting at the most recent committed offset for the consumer group. </p>&#xA;&#xA;<p>You're able to manually control the commit, if this is not wanted behavior </p>&#xA;"
49443648,49443206,949067,2018-03-23T06:27:54,"<p>I don't think you're comparing apples to apples here. <code>OSGI</code> is an <em>application</em> architecture, while <code>microservices</code> is a <em>distributed systems</em> concept.</p>&#xA;&#xA;<p>In my experience, microservices offer a number of benefits:</p>&#xA;&#xA;<ol>&#xA;<li>Individual microservices are easy to deploy, test, and maintain.</li>&#xA;<li>Microservices are language agnostic. That means you could write one microservice in python, another in javascript, a third in go, and a yet another in java.</li>&#xA;<li>Microservices are easy to scale <em>individually</em>. That means that if one type of request is made more often than others, you could scale the microservice you need to, without scaling anything else in the system.</li>&#xA;<li>Each microservice in your system owns its own data. This ensures clear boundaries and separation of concerns.</li>&#xA;</ol>&#xA;&#xA;<p>However, they also have some drawbacks:</p>&#xA;&#xA;<ol>&#xA;<li>There are more infrastructure concerns when deploying.</li>&#xA;<li>It's difficult to keep messaging between microservices clean and efficient.</li>&#xA;<li>It's harder to do end-to-end testing on a system with many moving parts.</li>&#xA;<li>There's more overhead in messaging. Instead of a call to another service being a direct method call, it needs to use HTTP or some other form of network communication.</li>&#xA;</ol>&#xA;&#xA;<p>There's a good article describing some of the differences <a href=""http://paulonjava.blogspot.ca/2014/04/micro-services-vs-osgi-services.html"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;"
41747040,41746212,1150795,2017-01-19T16:36:46,"<p>You shouldn't take project decisions basing on what's ""hot"" right now. What are you gonna do next year, when something else will be ""hot""? Perform refactoring again? The microservice architecture will be fine only for several types of projects, but not for all of them. It is possible to achieve good architecture and clean code without microservices. First of all, you should perform a deeper analysis of your project and it's domain. You should think if such type of architecture will really help you and why. In microservice architecture, you can also have a mess but on a different level of abstraction. It can be a subject of long article or publication. I suggest you, to perform a deeper research before starting any concrete work.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Is there such a 'silver bullet'?</p>&#xA;</blockquote>&#xA;&#xA;<p>Of course not.</p>&#xA;"
50439755,50438873,629323,2018-05-20T21:56:42,"<p>Microservices aren't a platform or technology so if you can make small independently deployable services then they are microservices. Sure - some tech helps but it depends on your situation.</p>&#xA;&#xA;<p>If you only need a few services you probably don't need anything complex. Make sure services are well modeled, own their own data and ideally have a good monitoring and deployment pipeline setup. Design for service failure where possible.</p>&#xA;&#xA;<p>Do you need to scale each part independently? Ideally, you should be able to but do services have very different requirements? You could have many small App service plans but that comes at cost of unused resources so split when you need to.</p>&#xA;"
50412312,50410038,3256139,2018-05-18T13:22:00,"<p>The authentication form will be on your auth microservice. Think for example as you use google login. You'll see google login page even if you have your own microservice. It's because authentication provider is google and you redirect user to login to google.</p>&#xA;&#xA;<p>I implemented all grant types a while back. May be <a href=""https://github.com/anealkeshi/springboot-microservices"" rel=""nofollow noreferrer"">this</a> will help you.</p>&#xA;"
49839429,29591967,1697099,2018-04-15T07:19:56,"<p>A Microservice is basically a self contained process that provides a unique and single business capability. We don't create web Microservice, business logic Microservice, or datebase Microservice.</p>&#xA;&#xA;<p><strong>Why Microservice?</strong>   </p>&#xA;&#xA;<ul>&#xA;<li>Microservice make our system loosely coupled, i.e. if we need to update, repair, or replace a Microservice, we don't need to rebuild our entire application, just swap out the part that needs it.</li>&#xA;<li>To built each Microservice can use different languages and tools. Microservices communicate with well defined interface </li>&#xA;<li>The communication should be stateless for scalability(copies of Microservice) and reliability(one copy fail other copy can serve), the most common methods for communication between Microservices are HTTP and messaging. </li>&#xA;<li>Each Microservice should have it's own datastore.</li>&#xA;<li>Small team capable to work on design, web development, coding, database admin and operations.  </li>&#xA;</ul>&#xA;&#xA;<p><a href=""https://www.youtube.com/watch?v=PY9xSykods4"" rel=""nofollow noreferrer"">source</a></p>&#xA;"
34904942,34903605,1393484,2016-01-20T16:10:12,"<h1>Pros</h1>&#xA;&#xA;<hr>&#xA;&#xA;<p>Sam Newman in <a href=""http://rads.stackoverflow.com/amzn/click/1491950358"">Building Microservices</a>, enumerates the key benefits of Microservices as following:</p>&#xA;&#xA;<h3>Technology Heterogeneity</h3>&#xA;&#xA;<p>With    a   system  composed    of  multiple,   collaborating   services,   we  can decide  to  use different&#xA;technologies    inside  each    one.    This    allows  us  to  pick    the right   tool    for each    job,    rather  than&#xA;having  to  select  a   more    standardized,   one-size-fits-all   approach    that    often   ends    up  being&#xA;the lowest  common  denominator.</p>&#xA;&#xA;<h3>Resilience</h3>&#xA;&#xA;<p>A   key concept in  resilience  engineering is  the <em>bulkhead</em>. If  one component   of  a   system&#xA;fails,  but that    failure doesn’t cascade,    you can isolate the problem and the rest    of  the&#xA;system  can carry   on  working.    Service boundaries  become  your    obvious bulkheads.  In  a&#xA;monolithic  service,    if  the service fails,  everything  stops   working.    With    a   monolithic&#xA;system, we  can run on  multiple    machines    to  reduce  our chance  of  failure,    but with&#xA;microservices,  we  can build   systems that    handle  the total   failure of  services    and degrade&#xA;functionality   accordingly.</p>&#xA;&#xA;<h3>Scaling</h3>&#xA;&#xA;<p>With    a   large,  monolithic  service,    we  have    to  scale   everything  together.   One small   part    of&#xA;our overall system  is  constrained in  performance,    but if  that    behavior    is  locked  up  in  a&#xA;giant   monolithic  application,    we  have    to  handle  scaling everything  as  a   piece.  With&#xA;smaller services,   we  can just    scale   those   services    that    need    scaling,    allowing    us  to  run&#xA;other   parts   of  the system  on  smaller,    less    powerful    hardware.</p>&#xA;&#xA;<h3>Ease of  Deployment</h3>&#xA;&#xA;<p>A   one-line    change  to  a   million-line-long   monolithic  application requires    the whole&#xA;application to  be  deployed    in  order   to  release the change. That    could   be  a   large-impact,&#xA;high-risk   deployment. In  practice,   large-impact,   high-risk   deployments end up  happening&#xA;infrequently    due to  understandable  fear.</p>&#xA;&#xA;<p>With    microservices,  we  can make    a   change  to  a   single  service and deploy  it  independently&#xA;of  the rest    of  the system. This    allows  us  to  get our code    deployed    faster. If  a   problem does&#xA;occur,  it  can be  isolated    quickly to  an  individual  service,    making  fast    rollback    easy    to&#xA;achieve.</p>&#xA;&#xA;<h3>Organizational   Alignment</h3>&#xA;&#xA;<p>Microservices   allow   us  to  better  align   our architecture    to  our organization,   helping us&#xA;minimize    the number  of  people  working on  any one codebase    to  hit the sweet   spot    of  team&#xA;size    and productivity.   We  can also    shift   ownership   of  services    between teams   to  try to  keep&#xA;people  working on  one service colocated.</p>&#xA;&#xA;<h3>Composability</h3>&#xA;&#xA;<p>One of  the key promises    of  distributed systems and service-oriented    architectures   is  that&#xA;we  open    up  opportunities   for reuse   of  functionality.  With    microservices,  we  allow   for our&#xA;functionality   to  be  consumed    in  different   ways    for different   purposes.   This    can be&#xA;especially  important   when    we  think   about   how our consumers   use our software.</p>&#xA;&#xA;<h3>Optimizing   for Replaceability</h3>&#xA;&#xA;<p>If  you work    at  a   medium-size or  bigger  organization,   chances are you are aware   of  some&#xA;big,    nasty   legacy  system  sitting in  the corner. The one no  one wants   to  touch.  The one that&#xA;is  vital   to  how your    company runs,   but that    happens to  be  written in  some    odd Fortran&#xA;variant and runs    only    on  hardware    that    reached end of  life    25  years   ago.    Why hasn’t  it&#xA;been    replaced?   You know    why:    it’s    too big and risky   a   job.</p>&#xA;&#xA;<p>With    our individual  services    being   small   in  size,   the cost    to  replace them    with    a   better&#xA;implementation, or  even    delete  them    altogether, is  much    easier  to  manage.</p>&#xA;&#xA;<h1>Cons</h1>&#xA;&#xA;<hr>&#xA;&#xA;<p>The most important disadvantage of Microservices is that they   have    all the associated  complexities    of&#xA;distributed systems,    and while   we  have    learned a   lot about   how to  manage  distributed&#xA;systems well    it  is  still   hard.   If  you’re  coming&#xA;from    a   monolithic  system  point   of  view,   you’ll  have    to  get much    better  at  handling&#xA;deployment, testing,    and monitoring  to  unlock  the benefits.   You’ll&#xA;also    need    to  think   differently about   how you scale   your    systems and ensure  that    they    are&#xA;resilient.  Don’t   also    be  surprised   if  things  like    distributed transactions    or  <em>CAP    theorem</em>&#xA;start   giving  you headaches,  either!</p>&#xA;&#xA;<h1>Closing Remarks</h1>&#xA;&#xA;<hr>&#xA;&#xA;<p>Just quoting from <a href=""http://martinfowler.com/articles/microservices.html"">Martin Fowler</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p>One reasonable argument we've heard is that you shouldn't start with a&#xA;  microservices architecture. Instead begin with a monolith, keep it&#xA;  modular, and split it into microservices once the monolith becomes a&#xA;  problem.</p>&#xA;</blockquote>&#xA;"
46007266,44946517,8549708,2017-09-01T20:01:28,"<p>You must add the microservice path to the webpack dev server proxy context.&#xA;If your microservice name is <code>my-microservice</code> then add a row</p>&#xA;&#xA;<pre><code>'/my-microservice',&#xA;</code></pre>&#xA;&#xA;<p>to the file <code>webpack.dev.js</code> into section <code>devServer.proxy.context</code>.&#xA;Note: in gateway, if you are generating the front-end for the entity located in the microservice then the JHipster entity generator already adds this line.</p>&#xA;"
50791132,46131196,8420428,2018-06-11T05:46:31,"<p>I find I have to add these two applications to the appllication.properties,it can work.Only one is not enough.</p>&#xA;&#xA;<pre><code>eureka.client.register-with-eureka=false&#xA;&#xA;eureka.client.fetch-registry=false&#xA;</code></pre>&#xA;"
43871245,43870576,4445516,2017-05-09T13:29:15,"<p>Just to make it more clear, a Microservice itself, fulfilles its business responsibility without depending any other service(s).  In your case 'ServiceA' is not a microservice but it's just a service because it dependent to another some service to complete its job. </p>&#xA;&#xA;<p>So do you able to change the contracts of the service? For example modify ServiceB input &amp; outputs to return/maintain 100 instances per call rather than 1. It will provide making more less request and taking less time. It makes sense that if you develop a batch-behaviour application/service, your dependent-services which manages collective data will help you to saving network and total runtime cost. But in this case 'serviceA' will not be a microservice.</p>&#xA;&#xA;<p>So if you really determined to make it a real ""microservice"" just combine service A and service B together. Microservice concept is not about making a service small as ""micro"". A Microservice just responsible for doing its job that related to its business domain without depending another domain/service/module etc.</p>&#xA;&#xA;<p>Combining 2 services which are in same domain to 1 and process some bulk data in this service does not violate microservice concept.</p>&#xA;"
50174356,47071729,321356,2018-05-04T11:56:24,"<p>We had the same problem.</p>&#xA;&#xA;<p>In our setup, an AWS Application ELB has a target group of 4 EC2 instances. On each of the EC2 instances, there is an Apache2 which forwards to a Tomcat.</p>&#xA;&#xA;<p>The ELB has a default connection KeepAlive of 60 seconds. Apache2 has a default connection KeepAlive of 5 seconds. If the 5 seconds are over, the Apache2 closes its connection and resets the connection with the ELB. However, if a request comes in at precisely the right time, the ELB will accept it, decide which host to forward it to, and in that moment, the Apache closes the connection. This will result in said 502 error code.</p>&#xA;&#xA;<p>The solution is: When you have cascading proxies/LBs, either align their KeepAlive timeouts, or - preferrably - even make them a little longer the further down the line you get.</p>&#xA;&#xA;<p>We set the ELB timeout to 60 seconds and the Apache2 timeout to 120 seconds. Problem gone.</p>&#xA;"
48392086,48363883,6878028,2018-01-22T23:51:55,"<p>If I got your query correctly maybe below might assist.</p>&#xA;&#xA;<p>Justification is mainly based on the scale you expect the app you are developing and also your organisation/architecture policies (you might want to enforce).</p>&#xA;&#xA;<p>If tokens are used for all API calls and making it consistence will make it easy when a wider team of developers working on the project down the track. Also make it easy for troubleshooting, as the scope of each Lambda function (in/outs) logic is clear and consistent.</p>&#xA;&#xA;<p>Also another thought is around cost, API calls does cost you at scale. So that should be taken in to consideration as well during architecture. But (in my opinion) consistency in the app call flow, for the savings it might give for traffic not hitting API gateway, where the call is made directly from Lambda to internal resources might be negligible.</p>&#xA;&#xA;<p>Anyways some thoughts to consider.</p>&#xA;"
46583882,46583498,4519500,2017-10-05T10:51:04,<p>Inside the directory where you've got the Dockerfile you will need to do a build like:</p>&#xA;&#xA;<pre><code>docker build -t microservicehelloworld . &#xA;</code></pre>&#xA;&#xA;<p>and then tag is as follows:</p>&#xA;&#xA;<pre><code>docker tag microservicehelloworld docker.io/library/microservicehelloworld&#xA;</code></pre>&#xA;&#xA;<p>and finally you will be able to push it:</p>&#xA;&#xA;<pre><code>docker push docker.io/library/microservicehelloworld&#xA;</code></pre>&#xA;
35200403,35198346,5766732,2016-02-04T11:57:26,"<p>It is OK to return 404 in both cases. As 4xx codes are client relevant codes, it is also OK to return content even if there was an error.</p>&#xA;&#xA;<p>Now, deciding what kind of 404 it was can be decided based on the body of the response. Remember, that the response should carry a <code>mime-type</code> that is compatible with the <code>Accept</code> header the client supplied. So if the client ""knows"" your specific error-describing format, your server can answer with a more detailed description. </p>&#xA;&#xA;<p>This way both the server can decide whether the client would understand a detailed response with the 404, <strong>and</strong> the client also understands when it just got a regular 404, or one with a message it can process.</p>&#xA;&#xA;<p>This would be both semantically correct, and compatible with HTTP.</p>&#xA;"
45506085,45468971,5766732,2017-08-04T11:49:36,"<p>Yes, there are alternatives. One of them is called <a href=""http://scs-architecture.org/"" rel=""nofollow noreferrer"">Self-Contained Systems</a>, which is a ""stricter"" form of microservices.</p>&#xA;&#xA;<p>The main idea being, that every service should be able to fulfill its role without needing communication with others for the business logic (no synchronous calls during execution to other services). That is what it means to be truly decoupled.</p>&#xA;&#xA;<p>Now obviously services do not exist in a vacuum, so how to we make them play together. There are two preferred ways which still avoid the problems you are facing.</p>&#xA;&#xA;<p><strong>Offline/Asynchronous Communication</strong></p>&#xA;&#xA;<p>Data can still flow between services if the data is asynchronous. Asynchronous does not only mean that it should go though a queue or something like that. It means that it is <em>offline</em>. It does not happen during the execution of the ""business-logic"".</p>&#xA;&#xA;<p>So persisting a report, as it is part of the logic, would not be allowed. You can however archive reports later (offline) to a data warehouse system or something. That ""main"" functionality still works if this offline replication fails or is not available for some time for example.</p>&#xA;&#xA;<p><strong>Frontend composition</strong></p>&#xA;&#xA;<p>If you are already using REST, you probably have web-interfaces and other services with links in them.</p>&#xA;&#xA;<p>If you want to present some reports, you can directly link to those reports available at their respective services. You don't really have to copy reports over to some ""central"" system.</p>&#xA;&#xA;<p>So basically the services are composed through links in the UI.</p>&#xA;&#xA;<p>That means of course, all services should host their own UIs too.</p>&#xA;&#xA;<p><strong>Anti-patterns to SCSs</strong></p>&#xA;&#xA;<p>As you said, if you have something like a ""product"", and you build a separate service for that, you <em>will</em> have coupling. Since all others will probably depend on ""product"", you just created a dependency to all.</p>&#xA;&#xA;<p>In SCS, we don't create services for ""things"", like <em>product</em> or <em>user</em>, we create services for functionality like ""search"", ""shopping cart"", ""configurator"", etc. Each of them have their own ""product"" view. The cool part is, they usually all have different concepts of what a product is, so there is very little duplication or redundancy.</p>&#xA;&#xA;<p>The remaining redundancy is then handled by offline data streams as discussed above.</p>&#xA;"
46826115,46804482,5766732,2017-10-19T08:53:02,"<p>If you already have Grafana, you can try this panel:</p>&#xA;&#xA;<p><a href=""https://grafana.com/plugins/jdbranham-diagram-panel"" rel=""nofollow noreferrer"">https://grafana.com/plugins/jdbranham-diagram-panel</a></p>&#xA;&#xA;<p>You can define a graph of your architecture, and assign metrics to the nodes based on which you can color them red / green.</p>&#xA;&#xA;<p>You would need some metrics about the health of the nodes. You can use <a href=""https://prometheus.io/"" rel=""nofollow noreferrer"">Prometheus</a> for example to collect some standard Java indicators like memory and cpu usage of the JVM. Or you can publish custom indicators if you want.</p>&#xA;"
47391200,47380189,5766732,2017-11-20T11:40:59,"<p>I participated in a similar transition, with quite a few mistakes along the way. Here are some things I would do as a central governing body:</p>&#xA;&#xA;<p><strong>1. Create architectural independence first</strong></p>&#xA;&#xA;<p>I think the biggest mistake is just letting old SOAP services be their own thing. It won't work. Second mistake is letting Microservices be Data CRUD services (like Product, Customer, etc.). That won't work either.</p>&#xA;&#xA;<p>Those things will just create a lot of synchronous interdependencies and a lot more problems for you!</p>&#xA;&#xA;<p>I would invest in an architecture where interdependencies are minimized. Reduce the need for synchronous communication as much as possible. And I don't mean use MQ, but main functions of a microservice should work with other services down.</p>&#xA;&#xA;<p>That requires a whole new type of decomposition that won't be along the lines of old SOAP services. So this is hard work, but avoids a lot of (exponental) problems later. Check out <a href=""http://scs-architecture.org/"" rel=""nofollow noreferrer"">Self-Contained Systems</a>.</p>&#xA;&#xA;<p><strong>2. Protocol governance</strong></p>&#xA;&#xA;<p>Especially if you are transitioning to RESTful HTTP, I would set rules for:</p>&#xA;&#xA;<ul>&#xA;<li>Linking format standard (so all applications can be crawled uniformly)</li>&#xA;<li>Linking best practices (all resources <em>have</em> to be reachable through links, urls should not be hard-coded, etc.)</li>&#xA;<li>Documentation standard (how to document <a href=""http://whatisrest.com/uniform_contract_elements/media_types"" rel=""nofollow noreferrer"">Media-Types</a>)</li>&#xA;<li>Versioning Media-Types</li>&#xA;<li>And importantly, an automatic way to mark a version obsolete after a non-backwards compatible change. And a standard grace period after which these are removed (the time they have to be kept alive). Either by calendar interval, or number of releases, etc.</li>&#xA;</ul>&#xA;&#xA;<p>There is no one way of doing either of these, so you have to come up with all of these, then enforce them.</p>&#xA;&#xA;<p>I would stay away of requiring a specific product (like Swagger), and let these decisions with the teams.</p>&#xA;&#xA;<p>If you are just looking for JSON-RPC and not REST, then some of the above points may be irrelevant to you.</p>&#xA;&#xA;<p><strong>3. Infrastructure-like things</strong></p>&#xA;&#xA;<p>Create unified standard for authentication and authorization. Again, I would make those as product-independent as possible, and not require synchronous communication.</p>&#xA;&#xA;<p>For example define to use <a href=""https://jwt.io/"" rel=""nofollow noreferrer"">Json Tokens</a>. Those things can be used ""offline"", without communication to anybody, and can contain assertions about a user that help with authorization as well.</p>&#xA;&#xA;<p>Define security constraints, like communication encryption of certain messages also. Again, I would just require the ""what"" not ""how"".</p>&#xA;&#xA;<p><strong>4. Continuous supervision</strong></p>&#xA;&#xA;<p>I would perhaps create a team for architectural supervision. It is hard to create a proper architecture, it is even harder to change it without falling for quick and dirty solutions projects sometimes demand, creating sneaky dependencies and hidden issues.</p>&#xA;&#xA;<p>These people need to be hands-on domain experts <em>and</em> architects and have to ultimately be <em>responsible</em> for the functioning of the whole landscape.</p>&#xA;&#xA;<p>Well, that's my improvised list of things, HTH..</p>&#xA;"
51003159,51000111,5766732,2018-06-23T16:43:46,"<p>Short answer: 4 - Some other way.</p>&#xA;&#xA;<p>The problem you're having is a direct consequence of how you designed your microservices. Usually (this depends on the use-case of course), it is a <em>bad idea</em> to have services be ""CRUD"" services for different entities, without any business logic.</p>&#xA;&#xA;<p>Why is this a bad idea? For the same reasons it is a bad idea to do the same in a monolith. It creates dependencies, makes the microservices depend on each other, ultimately resulting in a solution that has the worst of both worlds (microservices and monoliths).</p>&#xA;&#xA;<p>This is further reinforced by your requirements to get all customers with all categories. If you design your services the right way, you would not need to extract the full dataset from them.</p>&#xA;&#xA;<p><strong>The right way to do it</strong>: Design your services around business functions, instead of data. If both customer and category are need for a function, then those should be actually together and form a ""microservice"" with the appropriate function (you didn't actually mention any).</p>&#xA;&#xA;<p>Try to separate different aspects of data to form different functions, even if it means some data may be redundant.</p>&#xA;"
51468101,51463528,5766732,2018-07-22T18:31:22,"<p>The point of a microservices architecture is to split your problem domain into (technically, organizationally and semantically) <em>independent</em> parts. Making the ""microservices"" glorified (apified) tables actually creates more problems than it solves, if it solves any problem at all.</p>&#xA;&#xA;<p>Here are a few things to do first:</p>&#xA;&#xA;<ul>&#xA;<li>List architectural constraints (i.e. the reason for doing microservices). Is it separate scaling ability, organizational problems, making team independent, etc.</li>&#xA;<li>List business-relevant boundaries in the problem domain (i.e. parts that theoretically don't need each other to work, or don't require synchronous communication).</li>&#xA;</ul>&#xA;&#xA;<p>With that information, here are a few ways to fix the problem:</p>&#xA;&#xA;<ol>&#xA;<li>Restructure the services based on business boundaries instead of technical ones. This means <em>not</em> using tables or layers or other technical stuff to split functions. Services should be a complete vertical slice of the problem domain.</li>&#xA;<li>Or as a work-around create a third system which aggregates data and can create reports.</li>&#xA;<li>Or if you find there is actually no reason to keep the microservices approach, just do it in a way you are used to.</li>&#xA;</ol>&#xA;"
51522747,51520654,5766732,2018-07-25T15:32:12,"<p>If both <code>S1</code> and <code>S2</code> are ""under your control"", than you are much better off designing them in a way that doesn't require distributed transactions. After all, microservices are supposed to be <em>independent</em> services. They are obviously not independent, if they must share a transaction.</p>&#xA;&#xA;<p>If only one of those is under your control and the other one isn't, then the simplest way is to order the calls in a way that won't require a rollback on the service that is not under your control. If you are lucky, you might even order calls in a way that don't require any rollbacks, not even in your service. Remember, you don't have to solve distributed transactions in general, just solve the actual use-case you face!</p>&#xA;"
45053896,45047011,5766732,2017-07-12T09:34:33,"<p>If your ETL process works on individual records (some parallelize-able units of computation), then there are a lot of options you could go with, here are a few:</p>&#xA;&#xA;<p><strong>Messaging System-based</strong></p>&#xA;&#xA;<p>You could base your processing around a messaging system, like <a href=""https://kafka.apache.org/"" rel=""nofollow noreferrer"">Apache Kafka</a>. It requires a careful setup and configuration (depending on durability, availability and scalability requirements of your specific use-cases), but may give you a better fit than a relational db. </p>&#xA;&#xA;<p>In this case, the ETL steps would work completely independently, and just consume some topics, produce into some other topics. Those other topics are then picked up by the next step, etc. There would be no direct communication (calls) between the E/T/L steps.</p>&#xA;&#xA;<p>It's a clean and easy to understand solution, with independent components.</p>&#xA;&#xA;<p><strong>Off-the-shelf processing solutions</strong></p>&#xA;&#xA;<p>There are a couple of OTS solutions for data processing/computation and transformation: <a href=""https://flink.apache.org/"" rel=""nofollow noreferrer"">Apache Flink</a>, <a href=""http://storm.apache.org/"" rel=""nofollow noreferrer"">Apache Storm</a>, <a href=""http://spark.apache.org/"" rel=""nofollow noreferrer"">Apache Spark</a>.</p>&#xA;&#xA;<p>Although these solutions would obviously confine you to one particular technology, they may be better than building a similar system from scratch.</p>&#xA;&#xA;<p><strong>Non-persistent</strong></p>&#xA;&#xA;<p>If the actual data is streaming/record-based, and it is not required to persist the results between steps, you could just get away with long-polling the HTTP output of the previous step.</p>&#xA;&#xA;<p>You say it is just too much data, but that data doesn't have to go to the database (if it's not required), and could just go to the next step instead. If the data is produced continuously (not everything in one batch), on the same local network, I don't think this would be a problem.</p>&#xA;&#xA;<p>This would be technically very easy to do, very simple to validate and monitor.</p>&#xA;"
43249081,43246560,5766732,2017-04-06T07:56:08,"<p>There is a different solution that ""side-steps"" this trade-off.</p>&#xA;&#xA;<p>Your question suggests that you are thinking more in the ""old"" ""service-oriented"" approach. That is, services are basically data-oriented services that supply data. Like ""Inventory"", ""Session"", ""Customer"", etc.</p>&#xA;&#xA;<p>An alternative approach would be, and this is quite similar to DDD bounded contexts, to decompose the application based on business areas. This results in a completely different architecture, in which data is not separated from the functions that work on it. Kinda like Object-Orientation.</p>&#xA;&#xA;<p>This would result in the Shirt-Configurator having its own database with all the relevant information, including sessions, inventory, whatever. Also, including UI.</p>&#xA;&#xA;<p>Another application could be the Checkout. The checkout could be a completely independent application, with only getting URLs back to the Shirt-Configurator for getting a proper presentation. The Checkout application would not have to call or even know the Shirt-Configurator.</p>&#xA;&#xA;<p>And so on... </p>&#xA;&#xA;<p>More on this style of architectures: <a href=""http://scs-architecture.org/"" rel=""nofollow noreferrer"">http://scs-architecture.org/</a></p>&#xA;"
43850855,43850359,5766732,2017-05-08T14:55:19,"<p>The obvious answer if you don't know how to communicate without coupling is not to communicate then.</p>&#xA;&#xA;<p>I really mean that. You should design your services in a way that does not require synchronous communication with other services to fulfill a business case. Doing otherwise, as you noted, leads to runtime coupling.</p>&#xA;&#xA;<p>Obviously if you have a ""product-service"", that already suggests this is something pretty much every other service will need. You baked coupling into the architecture by cutting it up in a specific way.</p>&#xA;&#xA;<p>Specifically in this case: the ""sales"" service should have all the data for the report, so it does not have to communicate. You might find that this data is actually not needed elsewhere, so there would be no real duplication of data.</p>&#xA;&#xA;<p>Have a look at these guys: <a href=""http://scs-architecture.org/"" rel=""nofollow noreferrer"">http://scs-architecture.org/</a>. They have a lot of good ideas how (and why) to avoid such couplings, and how to design independent services, or at least only ""offline"" dependent ones.</p>&#xA;&#xA;<p>Obviously this is not for everything. Most notably Netflix is doing coupling and ""synchronous"" calls, that's why they have all the cool frameworks for these sorts of things. But they also have a specific use-case, which might not be the same as yours.</p>&#xA;"
46316588,46315996,5766732,2017-09-20T08:06:14,"<p>You've said:</p>&#xA;&#xA;<blockquote>&#xA;  <p>built some Micro Services on current DB Structure</p>&#xA;</blockquote>&#xA;&#xA;<p>I think that is fundamentally the wrong approach to take, I would urge you to think about alternatives. The problem with this approach is, it will lead to the same level of coupling and interdependencies you have now. You are just introducing an additional HTTP layer above an SQL one, and may be pushing logic then into the UI.</p>&#xA;&#xA;<p>If you want to have long-term maintainability you will have to minimize interdependencies between the microservices. Basically microservices need to be able to do their job independently of other microservices.</p>&#xA;&#xA;<p>Take a look at <a href=""http://scs-architecture.org/"" rel=""nofollow noreferrer"">Self-Contained Systems</a>.</p>&#xA;&#xA;<p>Admittedly just mirroring an SQL interface (CRUD) through HTTP is easier to do, and there are frameworks that do that for you if you want, but it ultimately does not make the architecture better.</p>&#xA;"
38201589,38168658,5766732,2016-07-05T10:52:52,"<p>If you are using RESTful HTTP, you already have a persistent, bookmarkable identification of resources, <strong>URL</strong>s (URIs, IRIs if you want to be pedantic). Those are the IDs that you can use to refer to some entity in another microservice.</p>&#xA;&#xA;<p>There is no need to introduce another layer of IDs, be it country codes, or database ids. Those things are internal to your microservice anyway and should be transparent for all clients, including other microservices.</p>&#xA;&#xA;<p>To be clear, I'm saying, you can store the <strong>URI</strong> to the country in the attractions service. That <strong>URI</strong> should not change anyway (although you might want to prepare to change it if you receive permanent redirects), and you have to recall that <strong>URI</strong> anyway, to be able to include it in the attraction representation.</p>&#xA;&#xA;<p>You don't really need any ""business key"" for favorites either, other than the <strong>URI</strong> of the attraction. You can bookmark that <strong>URI</strong>, just as you would in a browser.</p>&#xA;&#xA;<p>I would imagine if there is an auth-service, there are <strong>URI</strong>s also for identifying individual users. So in a ""favorites"" service, you could simply link the User <strong>URI</strong> with Attraction <em>URI</em>s.</p>&#xA;"
48902356,48900551,5766732,2018-02-21T09:30:59,"<p>I've witnessed two ESB rollouts at different companies, in both cases with the same noble goals of just helping with some monitoring and authentication, providing ""better"" access to legacy systems. In both cases in just 1-2 years the ESB became a single point of failure, a bottleneck for change, and generally a roadblock for all projects.</p>&#xA;&#xA;<p>ESBs are just too <em>convenient</em> not to use them. First, you'll just add some special routing for a message you want to send to some system, then you just quickly solve translating some xml message to another format, because you can. Then you add some more XSLTs or whatever to cover for a version update that would be too expensive to fix in the client system. And so on...</p>&#xA;&#xA;<p>Before long, you <em>will</em> have business logic on there. All teams will have to coordinate with the ESB team for all rollouts, new messages or even changes in message formats. It <strong>will kill the independence</strong> (the low coupling) of your teams.</p>&#xA;&#xA;<p>The point of the Microservices architecture, as you pointed out, is to enable <strong>autonomous operation</strong> not just for the service, but <strong>for its team as well</strong>. This enables quick change. Ideally this would mean:</p>&#xA;&#xA;<ul>&#xA;<li>Avoid any synchronization points with other teams, be it for testing, rollout, configuration or operation (i.e. you have to go cross-functional and do DevOps, etc.)</li>&#xA;<li>Avoid synchronization points runtime with other services. This means avoiding synchronous calls regardless of technology. You should only do fire-and-forget, never request-response even if the response comes at later point in time.</li>&#xA;<li>Avoid dependencies to other teams. This means avoid code-sharing (of code that has business-logic or business-related objects)</li>&#xA;</ul>&#xA;&#xA;<p>Basically, you should be able to keep operating your microservice (and roll out new versions) even if the rest of the company shut theirs down and went on vacation.</p>&#xA;&#xA;<p>Of course that is an ""idealized"" scenario, but an ESB most definitely goes against all goals above. It is a synchronization point, a centralized dependency both runtime and organizationally.</p>&#xA;"
48925210,48921774,5766732,2018-02-22T10:35:34,"<p>First of all, you don't have to have a Microservices architecture. I really mean it! If you were <em>ordered</em> by management/architect to do it, and it doesn't solve any real problems you are having, you are probably right for pushing back.</p>&#xA;&#xA;<p>That being said, and with the disclaimer that I don't know the exact requirements of your application, <strong>having ""things"" as bounded context is a smell</strong>. So having ""Customers"", ""Applications"", ""Documents"", etc. as services is very likely the wrong approach.</p>&#xA;&#xA;<p>Bounded contexts should not be CRUD operations on a specific entity. They should be completely independent (or as independent as possible) ""vertical"" parts of the whole application. Preferably with their own Database <em>and</em> GUI. They should also operate independently of each other, not requiring input from other services for own decisions.</p>&#xA;&#xA;<p>It is the complete opposite of data-centric design, where tables/fields and relations are the core concepts. Here, functionality is the core concept. You would have to split your application along functionality to arrive at a good separation.</p>&#xA;&#xA;<p>I could imagine a document management system having these idependent bounded contexts / services: <em>Search</em>, <em>Workflow</em>, <em>Editing</em>, etc.</p>&#xA;&#xA;<p>Here is how you would think about it: <em>Search</em> does not require any (synchronous) input from any other service. It may receive regular, even near-time updates with new documents, but that does not impact it's main feature: searching already indexed documents. The GUI is also independent, something like one google-like page with a search box maybe. It can deliver results independently, and would <em>link</em> back to the <em>Workflow</em> or <em>Editing</em> apps when you click on a result.</p>&#xA;&#xA;<p>The others would be similarly independent. Again, the point is to split the services in a way that makes them work independently. If you don't have that, you will only make things worse with Microservices.</p>&#xA;"
47174954,47162798,5766732,2017-11-08T08:37:33,"<p>An alternative 3rd solution is to not make microservices just tables. The problem with just putting ""things"" with CRUD functionality in separate services is that most of the time, they will have a lot of cross-dependencies. This is the problem you are having now.</p>&#xA;&#xA;<p>You could make services that align with the functionality instead of the data. Make a ""search"" service, a ""shopping cart"" service, ""billing"" service, etc. All those things usually need different aspects of the same ""thing"" (like product). I can imagine the categories for example to be only relevant for the ""search"", but not for the other two.</p>&#xA;&#xA;<p>Some guys already wrote about this approach <a href=""http://scs-architecture.org/"" rel=""nofollow noreferrer"">here, named Self-Contained Systems</a>.</p>&#xA;"
48892469,48799257,2901325,2018-02-20T19:04:48,<p>The IP assignment is a kubernetes not an istio feature and so you'd have to look there for how to configure it.</p>&#xA;&#xA;<p>Istio should work either way - is there an issue you are seeing ?</p>&#xA;
51986363,51986244,4939853,2018-08-23T12:55:39,"<p>If your service discovery is accessible from the outside, you need to add some security to it, and HTTPS will only protect from man-in-the-middle attacks, so it is not enought.</p>&#xA;&#xA;<p>If you use Eureka with the spring cloud starter (@EnableEurekaServer), you could use spring security to protect your server.</p>&#xA;&#xA;<p>For a simple exemple you could add :</p>&#xA;&#xA;<pre><code>security:&#xA;  user:&#xA;    name: admin&#xA;    password: password&#xA;</code></pre>&#xA;&#xA;<p>And declare eureka like this on the spring-boot client side :</p>&#xA;&#xA;<pre><code>eureka:&#xA;  client:&#xA;    serviceUrl:&#xA;      defaultZone: http://admin:password@localhost:8002/eureka&#xA;</code></pre>&#xA;&#xA;<p>You can also use oauth, and all the others security protocol that spring offers.</p>&#xA;"
38699198,36876367,2061133,2016-08-01T12:41:29,"<p>There are two key areas that you need to think about. </p>&#xA;&#xA;<p>The first is securing your cluster and the management API / capabilities. <a href=""https://azure.microsoft.com/en-gb/documentation/articles/service-fabric-secure-azure-cluster-with-certs/"" rel=""nofollow"">This can be achieved using certificates</a>. I know this is a link-only-answer but it's too much to paste in and rewrite. You should secure your communication between nodes with a cert and then the client (read only admin) and admin ""interfaces"" with additional certs (don't re-use the same one you used for your cluster). </p>&#xA;&#xA;<p>Once you have done this you can be confident in the <strong>security of your cluster</strong>. Now you want to host a WebAPI on your cluster and have it talk to an existing Cloud Service. The requirement here is to <strong>secure your application</strong>.</p>&#xA;&#xA;<p>The standard WebAPI security options are now available to you. I would recommend <a href=""http://bitoftech.net/2014/12/15/secure-asp-net-web-api-using-api-key-authentication-hmac-authentication/"" rel=""nofollow"">shared key security via HMAC</a> for it's simplicity and non-reliance on any further infrastructure bar you having to securely store your keys. <a href=""http://pathberiya.blogspot.co.uk/2011/02/2-legged-oauth-to-secure-restful.html"" rel=""nofollow"">Two legged OAuth</a> is also an option if you have OAuth infrastructure in place. Of course you should run you API over TLS. </p>&#xA;&#xA;<p>In short, focus separately on securing your cluster ""infrastructure"" and your application. </p>&#xA;&#xA;<p><a href=""http://gonzowins.com/2015/12/13/microservices-azureservicefabric-p1/"" rel=""nofollow"">I found the following useful list here:</a> </p>&#xA;&#xA;<ol>&#xA;<li>Expose all the Web APIs over HTTPS by using an Application Gateway</li>&#xA;<li>Apply IP Filters, so only services from the corporate network can call the Web APIs, we will do this by applying Network Security Groups.</li>&#xA;<li>Secure the Service Fabric Nodes, so they don’t expose RDP endpoints. Only a Jumpbox VM will be accessible by RDP.</li>&#xA;<li>Add Web Application Firewalls to apply more advanced and granular threat / intrusion detection.</li>&#xA;</ol>&#xA;&#xA;<p>These requirements can be applied by leveraging Azure Infrastructure related capabilities (Application Gateway, Network Security Groups, Web Application Firewalls and Security Centre). In addition to these requirements, you will very likely want to:</p>&#xA;&#xA;<ol>&#xA;<li>Add authentication / authorization capabilities to your Web APIS.</li>&#xA;<li>Manage Data in a secure way, potentially you might need to encrypt the data (actors and collections).</li>&#xA;</ol>&#xA;"
42411466,42400613,2061133,2017-02-23T09:18:06,"<p>How are you calling it on the local cluster - through PInvoke? A couple of ideas to help: </p>&#xA;&#xA;<p>You could use <a href=""https://msdn.microsoft.com/en-us/library/w29wacsy(v=vs.100).aspx"" rel=""nofollow noreferrer"">COM Interop</a>: </p>&#xA;&#xA;<p><code>tlbimp YourComDll.dll /out: YourDotNetWrapper.dll</code></p>&#xA;&#xA;<p>Then use <code>YourDotNetWrapper.dll</code> from you Service Fabric service. You may be able to get away with ""xcopy"" style deployment of the COM component, i.e. include it as a resource in your project and have it copied to the output folder e.g. <code>bin/Release</code>.</p>&#xA;&#xA;<p>If it still needs to be registered, you can register it in code with <a href=""https://msdn.microsoft.com/en-us/library/system.diagnostics.process.start(v=vs.110).aspx"" rel=""nofollow noreferrer"">Process.Start(""regsvr32..."")</a> or <a href=""https://limbioliong.wordpress.com/2011/08/11/programmatically-register-com-dlls-in-c/"" rel=""nofollow noreferrer"">via code (PInvoke)</a> - thought you may need to elevate privileges on Service Fabric. </p>&#xA;&#xA;<p>You would need a mechanism to register it only once to avoid spamming the registry. Maybe a <code>Reliable Dictionary of [NodeName],[IsOurComRegistered]</code>, or just checking if it is registered before registering. </p>&#xA;&#xA;<p>Good luck!</p>&#xA;"
43295680,43295417,2197837,2017-04-08T15:04:20,"<p>Spring boot by default looks for file application.yml. If u have different profiles in your application it can also look for application-{profilename}.yml. This is the default convention followed.</p>&#xA;&#xA;<p>spring.config.name property is used to override this default behaviour. When u override this property with register-service then spring boot looks for a file register-service.yml and loads config from that.</p>&#xA;&#xA;<p>So your eureka server url which is given in the register-service.yml file may not be available in the default application.yml file. Hence when u change the value Spring boot may not be avle to find the Eureka server url. </p>&#xA;&#xA;<p>Keep the names unchanged, as much as possible. If u have config file as register-service.yml then keep the spring.config.name=register-service. If you change this value then u need to create the new file with config.name value and then add eureka configuration to that again.</p>&#xA;"
43668553,43664192,2197837,2017-04-27T21:55:49,"<p>Microservices separation needs thinking from functional perspective(Z-axis scaling) rather than technical perspective. When you say each individual service, that may not mean every api should be places as a different service, it can be thought as group of api which constitute a logical service to the user, that can be separately developer/tested/deployed/managed.&#xA;When you have monolithic apps which do everything like user management, inventory management, order management etc. it makes sense to break these modules into microservices so that the complete flow is build by these services interacting to each other. Also Think from the aspect of scaling, Do you need to scale the individual services depending on the requirement.</p>&#xA;&#xA;<p>But in your case user management and role management may not be of big use when exposed as two different services. user, roles may be co-existing entities which share same database and functionality to achieve. </p>&#xA;"
43721451,43721197,2197837,2017-05-01T15:15:31,<p>The idea of making micro services is mainly to build services that are developed/ deployed/scaled independently. When you try to deploy multiple microservices inside a same container/jvm then you may not be able to leverage all these benefits. Also you CI/CD/integration testing may be hard. Try using embedded containers or container technologies like Docker which ensures complete isolation of the microservices.</p>&#xA;&#xA;<p>The decision also depends on what is the deployment environment of yours. If its cloud going for Docker would be a good idea</p>&#xA;
44057173,44056521,2197837,2017-05-18T20:25:00,"<p>When You have the same request and response, retrieved to browser and mobile clients there is no point in creating two diff controllers or services. Keep your app simple with one controller to do the job. With this your service just sees the mobile and web client in same way.</p>&#xA;&#xA;<p>Whenever there is increase in load that has to be handled by the app, you can go for horizontal scaling, using a routing, load balancer service like zuul, nginx.&#xA;Just scale up/down the instances behind the load balancer according to the load you need to handle.</p>&#xA;"
34093199,34077752,2069069,2015-12-04T16:41:31,"<p>Looks like you've found a bug - we are investigating. In the interim, it should work if you build using msbuild.</p>&#xA;"
37307830,36599435,2069069,2016-05-18T18:47:56,"<ol>&#xA;<li><p>For visualization, you can dump the data out, build some visualization UI into the service, or build a separate visualization service that pulls from other services. Assuming you're pushing data off to an external store for <a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-disaster-recovery/"" rel=""nofollow"">disaster recovery</a>, you can also visualize through that store. That said, providing tools to visualize the state in your services is something we'd like to enable in the future.</p></li>&#xA;<li><p>Your primary protection against data corruption is <a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-reliable-services-backup-restore/"" rel=""nofollow"">backup and restore</a>. If you end up in a bad state, you create a new instance of the service and restore a last-known good data set.</p></li>&#xA;</ol>&#xA;"
29445953,28930710,4172877,2015-04-04T11:39:17,"<p>I assume you have a means to safely access the product queue across all worker services. Given that, one simple way to avoid conflict could be using global queues per product next to the main queue</p>&#xA;&#xA;<pre><code>// Queue[X] is the queue for product X&#xA;// QueueMain is the main queue &#xA;DoWork(ProductType X)&#xA;{&#xA;  if (Queue[X].empty())&#xA;  {&#xA;    product = QueueMain().pop()&#xA;    if (product.type != X)&#xA;    {&#xA;      Queue[product.type].push(product) &#xA;      return;&#xA;    }&#xA;  }else&#xA;  {&#xA;     product = Queue[X].pop()&#xA;  }&#xA;&#xA;  //process product...&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>The access to queues need to be atomic </p>&#xA;"
34428880,34424706,635981,2015-12-23T04:54:42,"<p>To simplify the problem, let's say you have:</p>&#xA;&#xA;<ul>&#xA;<li>two applications: <code>A</code> and <code>B</code></li>&#xA;<li>one shared collection: <code>Posts</code></li>&#xA;<li>one <a href=""https://stackoverflow.com/questions/27189093/how-to-multiple-meteor-apps-on-single-mongodb-same-collections"">shared database</a> via <code>MONGO_URL</code></li>&#xA;</ul>&#xA;&#xA;<hr>&#xA;&#xA;<h2>Quick and Dirty</h2>&#xA;&#xA;<p>There's nothing complex about this solution - just copy the collection definition from one app to the next:</p>&#xA;&#xA;<p><strong>A/lib/collections.js</strong></p>&#xA;&#xA;<pre><code>Posts = new Mongo.Collection('posts');&#xA;</code></pre>&#xA;&#xA;<p><strong>B/lib/collections.js</strong></p>&#xA;&#xA;<pre><code>Posts = new Mongo.Collection('posts');&#xA;</code></pre>&#xA;&#xA;<p>This works well in cases where you just need the collection name.</p>&#xA;&#xA;<hr>&#xA;&#xA;<h2>More Work but Maintainable</h2>&#xA;&#xA;<p>Create a shared <a href=""http://dweldon.silvrback.com/local-packages"" rel=""nofollow"">local package</a> for your <code>Posts</code> collection. In each app: <code>meteor add posts</code>.</p>&#xA;&#xA;<p>This is a little more complex because you'll need to create a pacakge, but it's better for cases where your collection has a <a href=""http://dweldon.silvrback.com/models"" rel=""nofollow"">model</a> or other extra code that needs to be shared between the applications. Additionally, you'll get the benefits of creating a package, like testing dependency management, etc.</p>&#xA;"
50783948,50783648,3398989,2018-06-10T13:00:41,"<p>I have realized my mistake, my application.yml file was mis-configured hence the eureka server endpoint was incorrect. The <code>defaultZone</code> value was <code>localhost:8761</code> instead of <code>http://localhost:8761/eureka/</code>. Immediately I changed it the server was discovered by the sidecar application. Here is the correctly configured <code>application.yml</code> file:</p>&#xA;&#xA;<pre><code>server:&#xA;port: 9100&#xA;spring:&#xA;    application:&#xA;      name: vets-sidecar&#xA;eureka:&#xA;  instance:&#xA;    hostname: localhost&#xA;  client:&#xA;    serviceUrl:&#xA;      defaultZone: http://localhost:8761/eureka/&#xA;sidecar:&#xA;  port: 9102&#xA;  health-uri: http://localhost:${sidecar.port}/health&#xA;</code></pre>&#xA;"
50189521,49588249,3398989,2018-05-05T12:38:38,"<p>You may also take advantage of <code>polyglot sidecar</code> support that enables integration of other technologies with <code>Eureka Server</code> for discovery. For example, non-JVM languages may enable sidecar by creating a Spring Boot application with <code>@EnableSidecar</code>. This effectively triggers <code>@EnableCircuitBreaker</code>, <code>@EnableDiscoveryClient</code>, and <code>@EnableZuulProxy</code> support. The  application should run on the same host as the non-JVM application. More details are provided at <a href=""https://cloud.spring.io/spring-cloud-netflix/multi/multi__polyglot_support_with_sidecar.html"" rel=""nofollow noreferrer"">this Spring document</a>. You may also take a look at an example implementation <a href=""https://github.com/BarathArivazhagan/spring-cloud-sidecar-sample"" rel=""nofollow noreferrer"">here</a></p>&#xA;"
49116512,49116339,6884942,2018-03-05T17:47:55,"<p>Maybe you can run multiple instances of node,of course thats need to be a diferent version. Run in diferent ports and match with proxy_pass.&#xA;I Might help you.</p>&#xA;"
42201275,42199241,613198,2017-02-13T10:01:29,"<p>More efficient than REST over HTTP will be, from least efficient to most efficient:</p>&#xA;&#xA;<ul>&#xA;<li>REST over HTTP/2</li>&#xA;<li>WebSocket</li>&#xA;<li>TCP sockets</li>&#xA;<li>UDP packets</li>&#xA;</ul>&#xA;&#xA;<p>But this difference may be completely irrelevant if your miscroservices are actually doing anything useful and don't spend most of their time handling the HTTP headers.</p>&#xA;&#xA;<p>For example, when you spend half a second waiting for the database and then return a megabyte of JSON then adding few additional lines of HTTP headers can be even not measurable.</p>&#xA;&#xA;<p>You need to profile your code and test it before doing premature optimization and keep in mind that some of those ways to communicate more efficiently in terms of sending less bytes, can be much less efficient in terms of development time, maintenance and debugging. Keep in mind that nothing is easier to inspect and debug than text based protocols like good old HTTP/1.1 that you can talk to using netcat or anything else that handles plain text.</p>&#xA;"
43498801,43498534,613198,2017-04-19T14:35:40,"<blockquote>&#xA;  <p>Should I use only one package.json, for all micro-services, resulting >in only one node_modules folder?</p>&#xA;</blockquote>&#xA;&#xA;<p>You shouldn't worry about having multiple node_modules directories when you're doing microservices because one of the points of microservices is to be able to put them on separate hosts or in separate containers and they won't be able to share their dependencies anyway.</p>&#xA;&#xA;<p>Every microservice should be a separate service. You should avoid tight coupling and possible leaky abstractions that can result from fighting against the microservice architecture and joining them together.</p>&#xA;&#xA;<p>Of course microservices is not the only architecture that works in practice but you want microservices then every microservice should be completely independent. Otherwise it's not microservices.</p>&#xA;&#xA;<p>If there is any common code that you can share among the services then put it in a module that would be required by all or some of the services as needed. You can keep private modules on npm, you can host a private npm registry, or you can install that module directly from GitHub or GitLab private repos. All you need to do to add a private (or public) module hosted on GitHub would be to run:</p>&#xA;&#xA;<pre><code>npm install user/repo --save&#xA;</code></pre>&#xA;&#xA;<p>where <code>user</code> is your user (or organization) name on GitHub and <code>repo</code> is the name of the repository. Keep in mind that to install such a module if it's in a private repo the ssh key used on that machine would have to belong to a user that has read access to the repo. You can also use deploy keys on GitHub for more flexibility.</p>&#xA;&#xA;<p>You can even simplify loading external dependencies in your services by creating a separate module that includes all of the dependencies and exposes them to your services all at once. E.g. if you create a module called <code>dependencies</code> that looks like this:</p>&#xA;&#xA;<pre><code>module.exports = {&#xA;  _: require('lodash'),&#xA;  P: require('bluebird'),&#xA;  fs: require('mz/fs'),&#xA;  // ...&#xA;};&#xA;</code></pre>&#xA;&#xA;<p>Then you can use all of those modules in your microservices like this:</p>&#xA;&#xA;<pre><code>const { _, P, fs } = require('dependencies');&#xA;</code></pre>&#xA;&#xA;<p>When you have a large number of small microservices, especially when those are split into multiple files, this can simplify things quite a bit.</p>&#xA;"
42810767,42810364,613198,2017-03-15T13:09:21,"<blockquote>&#xA;  <p>Is there any way to track the data from the first moment it enters the application till the end, moving between the microservices without having to pass a generate id between all the methods.</p>&#xA;</blockquote>&#xA;&#xA;<p>No, because none of your microservices could know which of the other requests are related to the current one, unless you generate some id and pass it between all of them.</p>&#xA;&#xA;<p>If you add some request ID that is optional, then the microservise that don't have it in the request may generate it randomly (using a UUID for example) and then include it in all of the logs and pass it in requests to all other microservices, that in turn would use the same ID instead of generating a random one.</p>&#xA;&#xA;<p>Without such an ID you will not have enough context to group all of the requests together, especially between different microservices. Depending on the framework you're using it can be very easy to write a middleware to handle that for you. Since you didn't say which framework you're using it's impossible to give you a more specific answer.</p>&#xA;"
42958778,42958561,613198,2017-03-22T17:30:00,"<p>This is a very reasonable project. You can access any RESTful service from an Android app. You can even go realtime and use WebSocket or TCP or whatever you want. Your service just needs to be accessible over the Internet.</p>&#xA;&#xA;<p>Using a good framework will help you with that. Some frameworks to consider: Express, Hapi, Restify, LoopBack, ActionHero. More options here: <a href=""http://nodeframework.com/"" rel=""nofollow noreferrer"">http://nodeframework.com/</a></p>&#xA;&#xA;<p>The usual REST architecture looks like this: Your server listens for HTTP connections from your client, using REST conventions. The data in requests can be passed as JSON or form-urlencoded. The data returned in responses is usually JSON. Your client connects to various endpoints and gets responses.</p>&#xA;&#xA;<p>But to have microservices you actually need to have multiple Node applications, each responsible for a fraction of the functionality. Otherwise you have just a service, not microservices.</p>&#xA;&#xA;<p>When you have multiple Node apps each listening on a different port, or running on a different host, then you can use some reverse proxy server in front of them to dispatch the requests to the right microservice, so that your client wouldn't have to know your internal architecture. What is usually used for that is nginx but you could also write another Node app to act as a proxy, it's up to you.</p>&#xA;"
45446769,39126454,810830,2017-08-01T20:05:00,"<p>I would advise against option 1 as it adds a lot of unnecessary burden on the signing service.</p>&#xA;&#xA;<p>The signing service can share its public key. The services that need that key can simply fetch it on startup and use it to validate JWTs. That being said, you need to make sure that the signing service is who it says it is by using HTTPS for instance.</p>&#xA;"
30623424,30621628,97614,2015-06-03T14:45:50,"<p>Why do you think that BDD and integration testing are different?</p>&#xA;&#xA;<p>BDD just means driving your design through the desired behaviour, usually expressed through a set of acceptance tests.</p>&#xA;&#xA;<p>These tests may be 'integration tests' which involve many [micro]services or they may be tests which specify the desired behaviour of a single service, or a single class in that service. Ideally there will be a mix of tests at all these levels. The important thing is that you specify the behaviour you want and use this to drive the development.</p>&#xA;&#xA;<p>How your system is implemented is, to some degree, irrelevant as long as it exhibits the expected behaviour. For the high level tests that treat the system as a black box, this is true and the lower you go and the closer you you get to the actual code this becomes less true (as you are effectively testing the implementation at that point).</p>&#xA;&#xA;<p>So I would focus on the behaviour expected from the new features and write the specifications for these acceptance tests first, then implement your services to fulfil the required behaviour adding lower level tests as needed in a pragmatic way, bearing in mind that the lower level the tests go the more likely they are to be fragile and to need to be changed as you change your implementation.</p>&#xA;&#xA;<p><strong>EDIT</strong></p>&#xA;&#xA;<p>Based on your question edit.</p>&#xA;&#xA;<p>I don't agree that BDD tests should test only the business logic. In fact it is usual that BDD tests are more focussed on testing the system as a whole, with all the parts integrated together. Having said that BDD is just a style of testing by specifying the desired behaviour, and can be applied to any level of the application. You could test a single class by specifying the behaviour using Gherkin syntax, and we sometimes do this. we also specify the expected behaviour of the whole system using Gherkin and the expected behaviour of our services individually. These tests will naturally be a slightly different format depending on the level we are targeting.</p>&#xA;&#xA;<p>For the system tests we might have specification like this:</p>&#xA;&#xA;<pre><code>Scenario: user can perform action A&#xA;   Given I am a user with access to some feature A&#xA;   And feature A is enabled for the user&#xA;   When I call perform action A with parameters 'Bob' and 'John'&#xA;   Then A 'BobJohn' is created&#xA;   And notifications are sent to the current user&#xA;</code></pre>&#xA;&#xA;<p>for individual services we might have tests like</p>&#xA;&#xA;<pre><code>Scenario: create messages are handled correctly&#xA;   Given the service is set up&#xA;   When a message arrives to create a 'BobJohn'&#xA;   Then a new entry is added to the database with the key 'BobJohn'&#xA;   And an outgoing notification message for 'BobJohn' is created&#xA;</code></pre>&#xA;&#xA;<p>For individual classes we might have tests like</p>&#xA;&#xA;<pre><code>Scenario: Notifier class should send notifications via all users preferred means&#xA;    Given the current user wants notification by Twitter&#xA;    And the current user who wants notification by email&#xA;    When I send the notification 'BobJohn' to the current user&#xA;    Then the twitter notifier should be invoked with 'BobJohn'&#xA;    And the email notifier should be invoked with 'BobJohn'&#xA;</code></pre>&#xA;&#xA;<p>These are all BDD style tests but they test different aspects of the system. </p>&#xA;"
42339417,42331661,1543502,2017-02-20T08:11:15,"<p>While you can only have one App Engine app per project, an App Engine app can host <a href=""https://cloud.google.com/appengine/docs/python/an-overview-of-app-engine#services_the_building_blocks_of_app_engine"" rel=""nofollow noreferrer"">multiple services</a>, each of which has several versions of code deployed.</p>&#xA;"
51835645,51835569,1744718,2018-08-14T07:11:04,"<p>You can still create bundles in symfony4 though its not recommended by best practices. see <a href=""https://symfony.com/doc/current/best_practices/creating-the-project.html"" rel=""nofollow noreferrer"">https://symfony.com/doc/current/best_practices/creating-the-project.html</a></p>&#xA;"
42411858,36792713,3581006,2017-02-23T09:37:15,"<p>I am not quite sure if I fully understand your question.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Is there a pattern that enables this kind of 'on-demand-loading' of components and their templates in ng2?</p>&#xA;</blockquote>&#xA;&#xA;<p>Well there is on-demand-loading of modules, using lazy loaded modules, see: <a href=""https://angular.io/docs/ts/latest/guide/ngmodule.html#!#lazy-load"" rel=""nofollow noreferrer"">https://angular.io/docs/ts/latest/guide/ngmodule.html#!#lazy-load</a>&#xA;I highly recommend reading this whole guide, it is really helpful. </p>&#xA;&#xA;<blockquote>&#xA;  <p>but they should be able to share services and libraries.</p>&#xA;</blockquote>&#xA;&#xA;<p>Angulars guide suggests a shared module and a core module for that. Actually I think that is really the best way to do it. Just scroll down on the link above.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Each component is able to communicate its capabilities (in the form of a NG2 component) as well as the protocol implementation and the necessary GUI elements (HTML/CSS) via a package sent over the very same websocket connection.</p>&#xA;</blockquote>&#xA;&#xA;<p>This is the part that I am unsure about, like I don't know if I understand correctly what you mean. You don't want to load a component through a websocket connection right? If so, then why? &#xA;If you mean only communication for every component with the same websocket: I'd recommend a service doing that, which would be part of the Core Module and hence be a Singleton. The components can then access this service and therefore they are always connected to the same websocket.</p>&#xA;"
48744662,48743223,5296462,2018-02-12T10:50:30,"<p>When modeling aggregates, the rule is that one aggregate can refer to any other aggregate only by its identity. One aggregate cannot hold object reference to any other aggregate, otherwise consistency boundary is broken.</p>&#xA;&#xA;<p>So if you model your system with regards to DDD rules, your microservices will be clearly isolated and you'll have no issues.</p>&#xA;"
30910936,30908725,2187087,2015-06-18T09:02:29,"<blockquote>&#xA;  <p>How should I do, to create a new billing ? In fact, when somebody post&#xA;  a new billing on the microservice, he sends the user too. Does it mean&#xA;  that I need to have a UserEntity in my Billing service AND my User&#xA;  Service ? So the billing service will be able to marshall the request,&#xA;  meaning code duplication between the two services ? Or should I do&#xA;  something else ?</p>&#xA;</blockquote>&#xA;&#xA;<p>The user that the billing service need is not the same one in the user service. Usually, the user's identity is all the consumer need to post a new billing. If the billing service need more information of the user, it may query from the user service. There may be some code duplicates here, but the code plays different roles in each service which means they can evolve without disrupting each other.  Some questions may explain further here: <a href=""https://stackoverflow.com/questions/29728001/bounded-contexts-sharing-a-same-aggregate/29728284#29728284"">Bounded contexts sharing a same aggregate</a>, <a href=""https://stackoverflow.com/questions/25010902/handling-duplication-of-domain-logic-using-ddd-and-cqrs"">Handling duplication of domain logic using DDD and CQRS</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>Then, is this the role of the front end (API consumer) to make 2&#xA;  requests (one for billing, and one for the user related to the&#xA;  billing) to get the ressource ? Or should the BillingService get the&#xA;  User before responding to the front ?</p>&#xA;</blockquote>&#xA;&#xA;<p>I think it brings the most flexibility to let API consumer navigate the links. What if the consumer is not interested in the owner detail?</p>&#xA;&#xA;<blockquote>&#xA;  <p>I have seen in an article, that it's a good thing to use amqp / bus&#xA;  when dealing with microservice, to know if a ressource exists, or to&#xA;  check if it exists. Now we need a service container/registry to&#xA;  dynamically discover other services. In my case I use Zookeeper. But&#xA;  how can I do to tell Zookeeper ""give me the location of the service(s)&#xA;  related to the ressource with hateoas links :&#xA;  <a href=""http://80.80.80.80:7000/users/123456789"" rel=""nofollow"">http://80.80.80.80:7000/users/123456789</a>"" ? Am I missing an important&#xA;  information in my hateoas schema ?</p>&#xA;</blockquote>&#xA;&#xA;<p>Not quite understand this one. If the consumer has the link like ""<a href=""http://80.80.80.80:7000/users/123456789"" rel=""nofollow"">http://80.80.80.80:7000/users/123456789</a>"" already, it can access the resource directly. Why should it ask the zookeeper? I think the zookeeper helps the billing service assemble the URI for owner. For example, the billing service tell the zookeeper ""Give me the location of the service related to user resource"".</p>&#xA;"
33306085,26529567,2187087,2015-10-23T15:25:40,"<p>Pagination and filtering are doable even if there is only one table, right?</p>&#xA;&#xA;<p>So I think the question is about how to join tables between micro-services.</p>&#xA;&#xA;<p>I think people use micro-services will avoid joining tables between micro-services as much as possible. If they can't, maybe the tables should not be separated in different micro-services at all.</p>&#xA;&#xA;<p>On the other hand, sometimes, you don't need joining tables to achieve your goals. For example, let's say you have two tables:</p>&#xA;&#xA;<pre><code>-- from hotel information service&#xA;create table t_hotel (&#xA;    id VARCHAR(40) not null,&#xA;    name varchar(50) not null,&#xA;    location varchar(50) not null,&#xA;    CONSTRAINT pk_hotel PRIMARY KEY (id)&#xA;);&#xA;&#xA;-- from hotel comment service&#xA;create table t_hotel_comment (&#xA;    id VARCHAR(40) not null,&#xA;    hotel_id varchar(40) not null,&#xA;    content varchar(50) not null,&#xA;    CONSTRAINT pk_hotel_comment PRIMARY KEY (id)&#xA;);&#xA;</code></pre>&#xA;&#xA;<p>Now you want to implement a page showing hotel list, each row displays its comment count.</p>&#xA;&#xA;<pre><code> ____________________________&#xA;| name | location | comments |&#xA;| foo  | foooooo  |        2 |&#xA;| bar  | barrrrr  |        3 |&#xA; ----------------------------&#xA;</code></pre>&#xA;&#xA;<p>You may want to implement with a join query or with two api calls:</p>&#xA;&#xA;<ol>&#xA;<li>Call hotel information service to list the hotels.</li>&#xA;<li>For each hotel, call hotel comment service to sum the comments.</li>&#xA;</ol>&#xA;&#xA;<p>Maybe you have a concern about the N+1 performance issue, then you may cache the comments count in t_hotel:</p>&#xA;&#xA;<pre><code>-- from hotel information service&#xA;create table t_hotel (&#xA;    id VARCHAR(40) not null,&#xA;    name varchar(50) not null,&#xA;    location varchar(50) not null,&#xA;    comments numeric not null,&#xA;    CONSTRAINT pk_hotel PRIMARY KEY (id)&#xA;);&#xA;</code></pre>&#xA;&#xA;<p>Every time, hotel comment service receive a comment, it publish a HotelCommentedEvent</p>&#xA;&#xA;<pre><code>HotelCommentedEvent {&#xA;    ""comment_id"": ""id"",&#xA;    ""hotel_id"": ""foo"",&#xA;    ""content"": ""bar""&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>And hotel information service update its cache with this event. Therefore, you can implement this feature with a single table query.</p>&#xA;"
33305409,33228178,2187087,2015-10-23T14:54:52,"<p><strong>For the latency part</strong>, <a href=""https://github.com/dreamhead/moco"" rel=""nofollow"">Moco</a> maybe is what you are looking for.</p>&#xA;&#xA;<p>The following is from <a href=""https://github.com/dreamhead/moco/blob/master/moco-doc/apis.md#latency"" rel=""nofollow"">API</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>Latency</p>&#xA;  &#xA;  <p>@Since 0.7</p>&#xA;  &#xA;  <p>Sometimes, we need a latency to simulate slow server side operation.</p>&#xA;  &#xA;  <p>@Since 0.10.1</p>&#xA;  &#xA;  <p>It's easy to setup latency with time unit.</p>&#xA;  &#xA;  <p>Java API  </p>&#xA;</blockquote>&#xA;&#xA;<pre><code>server.response(latency(1, TimeUnit.SECONDS));   &#xA;</code></pre>&#xA;&#xA;<blockquote>&#xA;  <p>JSON </p>&#xA;</blockquote>&#xA;&#xA;<pre><code>{  &#xA;    ""request"" : {&#xA;        ""text"" : ""foo""&#xA;    },&#xA;    ""response"" : {&#xA;        ""latency"": {&#xA;            ""duration"": 1,&#xA;            ""unit"": ""second""&#xA;        }&#xA;    }&#xA; }&#xA;</code></pre>&#xA;&#xA;<p><strong>For the ""thousands of requests per second on cheap hardware"" part</strong>, why do you need this feature? Do you just want to simulate a timeout or something else?</p>&#xA;"
46322779,46321680,4745807,2017-09-20T12:59:19,"<p>There are no right or wrong answer but in a monolith, you will probably end up with a structure like this (Domain Driven - Profile Domain, BankAccountDomain): </p>&#xA;&#xA;<ul>&#xA;<li><p>main</p>&#xA;&#xA;<ul>&#xA;<li>profile&#xA;&#xA;<ul>&#xA;<li>model&#xA;&#xA;<ul>&#xA;<li>Profile.java</li>&#xA;<li>ProfileRequest.java</li>&#xA;<li>ProfileResponse.java</li>&#xA;</ul></li>&#xA;<li>ProfileController.java</li>&#xA;<li>ProfileService.java</li>&#xA;<li>ProfileRepository.java</li>&#xA;</ul></li>&#xA;<li>bankaccount&#xA;&#xA;<ul>&#xA;<li>model&#xA;&#xA;<ul>&#xA;<li>BankAccount.java</li>&#xA;<li>BankAccountRequest.java</li>&#xA;<li>BankAccountResponse.java</li>&#xA;</ul></li>&#xA;<li>BankAccountController.java</li>&#xA;<li>BankAccountService.java</li>&#xA;<li>BankAccountRepository.java</li>&#xA;</ul></li>&#xA;</ul></li>&#xA;<li><p>test</p></li>&#xA;</ul>&#xA;&#xA;<p>What you can do if you go to the extreme of microservice have each domain as a seperate microservice</p>&#xA;&#xA;<p>You can use grade or maven to have different projects (microservice) within the same repo if thats what you are looking for. </p>&#xA;&#xA;<p><a href=""https://docs.gradle.org/current/userguide/multi_project_builds.html"" rel=""nofollow noreferrer"">https://docs.gradle.org/current/userguide/multi_project_builds.html</a></p>&#xA;"
38689735,38687434,1411407,2016-08-01T01:04:50,"<p>Generally speaking, there are 2 approaches to implementing service discovery:</p>&#xA;&#xA;<ol>&#xA;<li>with reverse-proxy / api-gateway. This approach provides faster update propagation. When your service is deployed / redeployed / undeployed all changes can be immediately handled by reverse-proxy, so its configuration always reflects the state of your microservices. However, there is a performance impact - all requests, including internal should go through reverse-proxy component. More details on this approach <a href=""https://memz.co/api-gateway-microservices-docker-node-js/"" rel=""nofollow"">https://memz.co/api-gateway-microservices-docker-node-js/</a></li>&#xA;<li>with DNS. This approach provides slower updates, as every component (essentially, every http client used to call discoverable components) needs to revalidate its DNS cache, which may take some time (it can be configured with TTL of corresponding DNS entry). Additionally, it assumes that every http client implementation will respect that TTL value. As a first approximation, we can assume that TTL can be set as low as 60 seconds, and so, it will take no longer than that for configuration changes to take effect. More details on this approach <a href=""https://memz.co/service-discovery-microservices-skydns-docker/"" rel=""nofollow"">https://memz.co/service-discovery-microservices-skydns-docker/</a></li>&#xA;</ol>&#xA;"
36439313,31546631,1411407,2016-04-06T00:18:33,"<p>If you like getting your hands dirty, you could quite easily implement your own simplified API Gateway. I believe this approach perfectly fits into microservice paradigm - implement simple service with limited functionality that does only one thing, but does it well. </p>&#xA;&#xA;<p>I've written a tutorial on this subject (implementing simple API Gateway for Dockerized microservices with Node.js). My example is about 100 lines of JavaScript code, it uses <a href=""https://www.npmjs.com/package/node-docker-monitor"" rel=""nofollow"">node-docker-monitor</a> to listen to Docker events and <a href=""https://www.npmjs.com/package/http-proxy"" rel=""nofollow"">http-proxy</a> to handle HTTP requests from the clients.</p>&#xA;&#xA;<p><a href=""https://memz.co/api-gateway-microservices-docker-node-js/"" rel=""nofollow"">https://memz.co/api-gateway-microservices-docker-node-js/</a></p>&#xA;&#xA;<p>or alternative solution with SkyDNS and Nginx</p>&#xA;&#xA;<p><a href=""https://memz.co/reverse-proxy-nginx-docker-microservices/"" rel=""nofollow"">https://memz.co/reverse-proxy-nginx-docker-microservices/</a></p>&#xA;"
36439578,35756663,1411407,2016-04-06T00:51:22,"<p>I believe, API Gateway is a reverse proxy that can be configured dynamically via API and potentially via UI, while traditional reverse proxy (like Nginx, HAProxy or Apache) is configured via config file and has to be restarted when configuration changes. Thus, API Gateway should be used when routing rules or other configuration often changes. To your questions:</p>&#xA;&#xA;<ol>&#xA;<li>It makes sense as long as every component in this sequence serves its purpose.</li>&#xA;<li>Differences are not in feature list but in the way configuration changes applied.</li>&#xA;</ol>&#xA;&#xA;<p>Additionally, API Gateway is often provided in form of SAAS, like <a href=""http://apigee.com/about/"" rel=""noreferrer"">Apigee</a> or <a href=""https://tyk.io/"" rel=""noreferrer"">Tyk</a> for example.</p>&#xA;&#xA;<p>Also, here's my tutorial on how to create a simple API Gateway with Node.js <a href=""https://memz.co/api-gateway-microservices-docker-node-js/"" rel=""noreferrer"">https://memz.co/api-gateway-microservices-docker-node-js/</a></p>&#xA;&#xA;<p>Hope it helps.</p>&#xA;"
40525635,40525468,3640655,2016-11-10T10:45:20,"<p>I was facing same problem, so I added</p>&#xA;&#xA;<pre><code>@Bean&#xA;public Docket docket() {&#xA;    return new Docket(DocumentationType.SWAGGER_2)&#xA;                .groupName(""name"")&#xA;                .directModelSubstitute(LocalDateTime.class, String.class)&#xA;                .directModelSubstitute(LocalDate.class, String.class)&#xA;                .directModelSubstitute(LocalTime.class, String.class)&#xA;                .directModelSubstitute(ZonedDateTime.class, String.class)&#xA;                .apiInfo(apiInfo())&#xA;                .select()&#xA;                .paths(paths())&#xA;                .build();&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>in docket configuration.</p>&#xA;&#xA;<p><code>directModelSubstitute</code> makes swagger to treat <code>LocalDate</code> as <code>String</code> class</p>&#xA;"
43490667,43489589,2138959,2017-04-19T08:42:45,"<p>When you create your application with .NET Core you bootstrap things in <code>Main</code> method and then register services and middle-ware in <code>Startup</code> class. So before you start web host you can also create and start your messaging services:</p>&#xA;&#xA;<pre><code>public class MessageListener&#xA;{&#xA;    public void Start()&#xA;    {&#xA;        // Listen to rabbit QM and all the things.&#xA;    }&#xA;}&#xA;&#xA;public static void Main(string[] args)&#xA;{&#xA;    // Listen to messages.&#xA;    var messaging =  MessageListener();&#xA;    messaging.Start();&#xA;&#xA;    // Configure web host and start it.&#xA;    var host = new WebHostBuilder()&#xA;        ...&#xA;        .Build();&#xA;    host.Run();&#xA;}&#xA;</code></pre>&#xA;&#xA;<p><strong>UPDATE:</strong></p>&#xA;&#xA;<p>In ASP.NET Core 2.0 <code>IHostedService</code> interface was introduced to achieve same goal and they mention this scenario in their <a href=""https://blogs.msdn.microsoft.com/webdev/2017/08/25/asp-net-core-2-0-features-1/"" rel=""noreferrer"">announcement</a>. Here is <a href=""https://www.stevejgordon.co.uk/asp-net-core-2-ihostedservice"" rel=""noreferrer"">an example</a> of how to implement one.</p>&#xA;"
47894284,47888172,918608,2017-12-19T19:55:41,<p>Why not have a single serialization format and let each service deserialize the payload for their use case? Templating with something like Velocity or Freemarker seems like a specific concern independent of the data used to populate the template. Maybe focus on broadcasting the raw data.</p>&#xA;
47894341,47889886,918608,2017-12-19T19:59:33,"<p>You should consider keeping the migrations with the respective code repositories. Service A should have its own set of migrations independent of Service B. This will allow you to deploy Service A and migrate A's schema without any bearing on Service B. </p>&#xA;&#xA;<p>Also, you should consider having no common tables. Common tables can have serious drawbacks. If Service A needs to modify User in a way that breaks Service B, you have created a distributed monolith. </p>&#xA;&#xA;<hr>&#xA;&#xA;<p>UPDATE 1:</p>&#xA;&#xA;<p>Building an audit log might not need strong referential integrity. You could consider soft foreign keys instead.</p>&#xA;&#xA;<p>Much of how you design your microservices relies on the domain. If a <code>User</code> is an authenticated user, then you should first tackle the cross-cutting concern of authentication. You may choose for each microservice to require an authentication token such as a jwt to determine who the authenticated user is and whether or not they are authorized to perform some action. Then you could simply use the user's id in the audit log.</p>&#xA;&#xA;<p>As for whether or not a user ""falls under the service's bounded context"", it likely does not. In other words, how are update against <code>User</code> bound to <code>Service A</code>? You probably do not consider the user to be subordinate to service A, nor would you want to update the user via actions against service A. </p>&#xA;"
49494004,49490466,8735568,2018-03-26T14:34:12,"<p>After My Exploration I find out Solution for this problem for loading global variables and application variables including database configuration. The best way we can use that is - spring cloud config server externalized configuration.</p>&#xA;&#xA;<p>We can create a microservice for spring cloud config server. In config server we can create our variables and configuration in two ways.</p>&#xA;&#xA;<ol>&#xA;<li>Configurations from GIT Link reference</li>&#xA;<li>Using local file system / Environment variables.</li>&#xA;</ol>&#xA;&#xA;<p>Links To refer</p>&#xA;&#xA;<ol>&#xA;<li><a href=""https://cloud.spring.io/spring-cloud-config/single/spring-cloud-config.html"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-config/single/spring-cloud-config.html</a></li>&#xA;<li><a href=""https://cloud.spring.io/spring-cloud-static/spring-cloud-config/1.3.3.RELEASE/multi/multi__spring_cloud_config_server.html"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-static/spring-cloud-config/1.3.3.RELEASE/multi/multi__spring_cloud_config_server.html</a></li>&#xA;<li><a href=""https://github.com/spring-cloud/spring-cloud-config"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-config</a></li>&#xA;</ol>&#xA;&#xA;<p><strong>Here I followed using local file system.</strong> </p>&#xA;&#xA;<p>Need to create Config folder under src/main/resources. And create different profiles by following naming convention,</p>&#xA;&#xA;<p>db,properties , db-test.properties , db-prod.properties , db-dev.properties.&#xA;I created for example for different development environment. Like we can create any profiles for variables and configuration.</p>&#xA;&#xA;<p>And add following in application.properties for config server</p>&#xA;&#xA;<pre><code>server.port=8888&#xA;spring.profiles.active=native&#xA;</code></pre>&#xA;&#xA;<p>Add config server dependency in pom.xml file of config server,</p>&#xA;&#xA;<pre><code>&lt;dependency&gt;&#xA;    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&#xA;    &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt;&#xA;&lt;/dependency&gt;&#xA;</code></pre>&#xA;&#xA;<p>Add the following into main application run class,</p>&#xA;&#xA;<pre><code>@SpringBootApplication&#xA;@EnableConfigServer&#xA;public class ConfigServerApplication {&#xA;&#xA;    public static void main(String[] args) {&#xA;    SpringApplication.run(ConfigServerApplication.class, args);&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p><strong>And also create client microservice project by adding pom.xml dependency,</strong></p>&#xA;&#xA;<pre><code>&lt;dependency&gt;&#xA;        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&#xA;        &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;&#xA;&lt;/dependency&gt;&#xA;</code></pre>&#xA;&#xA;<p>Add following line in application.properties file for setting client to receive configuration from server,</p>&#xA;&#xA;<pre><code>server.port=8080&#xA;spring.application.name=db&#xA;spring.cloud.config.uri=localhost:8888&#xA;</code></pre>&#xA;&#xA;<p>Finally run your client project by specifying profile ,</p>&#xA;&#xA;<pre><code>java -jar -Dsping.profiles.active=&lt;profile&gt; &lt;jar_name&gt;.jar&#xA;</code></pre>&#xA;&#xA;<p>Thanks In advance</p>&#xA;"
46962195,46961616,8016720,2017-10-26T19:18:30,"<p>Do not go overboard trying to check for every service, every dependency, etc. in your health check. Basically think of your health check as a Go / No Go test so that the load balancer knows if the service is running.</p>&#xA;&#xA;<p>Load balancers will not recover failed instances. They will just take your service offline. Auto Scaling Groups can recover failed instances by creating new instances and terminating failed instances. CloudWatch can monitor your instances and report problems and cause events to happen (e.g. rebooting).</p>&#xA;&#xA;<p>You can implement more comprehensive tests that run internal to your server and that chose a reporting / recovery path. Examples might include sending an SNS notification to your email or cell phone account, rebooting the server, etc.</p>&#xA;&#xA;<p>Amazon has a number of services to help monitor, report and manage services. Look into CloudWatch for monitoring, SNS or SES for reporting, ASG for auto scaling, etc.</p>&#xA;&#xA;<p>Think thru what type of fault tolerance, high availability and recovery strategy you need for your service. Then implement an approach that is simple enough so that the monitoring itself does not become a point of failure.</p>&#xA;"
47042416,47042162,8016720,2017-10-31T18:38:00,"<p>If you upload your source code to your own S3 bucket before creating the Lambda function, then you will know the path to your source code. Note that simply editing the code in S3 will not update your Lambda function. See link below. Amazon will copy your code from your bucket to Amazon's S3 bucket.</p>&#xA;&#xA;<p>If you upload your code in the AWS console when you create the Lambda function, then it is stored in an Amazon S3 location that you cannot access.</p>&#xA;&#xA;<p>The end result is that your code is located in a private Amazon S3 bucket that you cannot access.</p>&#xA;&#xA;<p><a href=""http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html"" rel=""nofollow noreferrer"">AWS Lambda Function Code</a></p>&#xA;"
42546694,42546246,498256,2017-03-02T04:08:29,"<p>You need network policy applied in K8s</p>&#xA;&#xA;<p><a href=""https://kubernetes.io/docs/user-guide/networkpolicies/"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/user-guide/networkpolicies/</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints.</p>&#xA;</blockquote>&#xA;"
47649137,47647560,2989136,2017-12-05T08:30:21,"<p>I'd say it depends on the situation. If you use a shared package, this will introduce a coupling between the two projects. This makes sense, if both of the project build up on the same data classes and therefore will have the same dto objects to work with. Ideally you would have your own nexus which simplifies the usage of the shared artefact.&#xA;Otherwise, if only a few classes are redundant I probably would implement it in each sevice separately, which decouples them too.</p>&#xA;&#xA;<p>I am afraid that you need to decide which one the right solution is for your project.</p>&#xA;"
50663173,50663047,1947216,2018-06-03T04:02:08,"<p>If you mean ""can I deploy service without WAR?"", then the answer is no, because the service should be a part of some WAR. If you mean ""should every microservice be put to a separate WAR"", the answer is not so simple. WAR should contain a single microservice. And dont think of microservice as a technically single end point, single REST or SOAP service, it can consist of several such services/endpoints.</p>&#xA;"
42610390,42582463,900944,2017-03-05T15:46:25,"<p>The way I'd monitor such systems with Prometheus is by instrumented the applications that talk to S3/SQS.  See this code for an example of how we instrumented an application that talks to S3: <a href=""https://github.com/weaveworks/cortex/blob/master/chunk/chunk_store.go#L177"" rel=""nofollow noreferrer"">https://github.com/weaveworks/cortex/blob/master/chunk/chunk_store.go#L177</a></p>&#xA;&#xA;<p>We use these histograms (scraped and aggregated by Prometheus) to alert us when PUTs or GETs to S3 and failing, or taking too long.</p>&#xA;"
50866762,46742274,4706128,2018-06-14T22:24:50,"<p>It seems to me that they just didn't draw the events returning from the event processors, perhaps because they may not specifically be events (like some sort of a callback) or because they may not be <em>normal</em> events (perhaps events that only go back to the mediator and are not visible to any other subscribers), depending on what you use as your mediator.  This part seems to indicate something like that:</p>&#xA;&#xA;<blockquote>&#xA;  <p>The event mediator can be implemented in a variety of ways. Understand each implementation option to ensure that the solution you&#xA;  choose for the event mediator matches your needs.</p>&#xA;  &#xA;  <p>The simplest and most common implementation of the event mediator is&#xA;  through open source integration hubs such as Spring Integration,&#xA;  Apache Camel, or Mule ESB. Event flows in these open source&#xA;  integration hubs are typically implemented through Java code or a DSL&#xA;  (domain-specific language). For more sophisticated mediation and&#xA;  orchestration, you can use BPEL (business process execution language)&#xA;  coupled with a BPEL engine such as the open source Apache ODE. BPEL is&#xA;  a standard XML-like language that describes the data and steps&#xA;  required for processing an initial event. For very large applications&#xA;  requiring much more sophisticated orchestration (including steps&#xA;  involving human interactions), you can implement the event mediator&#xA;  using a business process manager (BPM) such as jBPM.</p>&#xA;  &#xA;  <p>Understanding your needs and matching them to the correct event&#xA;  mediator implementation is critical to the success of any event-driven&#xA;  architecture using this topology. Using an open source integration hub&#xA;  to do very complex business process management orchestration is a&#xA;  recipe for failure, just as is implementing a BPM solution to perform&#xA;  simple routing logic.</p>&#xA;</blockquote>&#xA;&#xA;<p>They mentioned Spring as a possible implementation - I've never used it, but looking at the documentation (<a href=""https://docs.spring.io/spring-integration/docs/2.0.0.RC1/reference/html/service-activator.html"" rel=""nofollow noreferrer"">here</a>) I see the concept of a reply channel:</p>&#xA;&#xA;<blockquote>&#xA;  <p>...when the service method returns a non-null value, the endpoint will attempt to send the reply message to an appropriate reply channel.</p>&#xA;</blockquote>&#xA;&#xA;<hr>&#xA;&#xA;<p>The goal is to send one or more messages off to be processed asynchronously, then to send other messages off when the results come back.  I don't think it matters too much at the pattern level exactly how those results come back (function callback, 'response' event, web API call, whatever) - that will depend on your specific infrastructure.</p>&#xA;&#xA;<p>To me it sounds quite a bit like the Saga pattern (<a href=""http://microservices.io/patterns/data/saga.html"" rel=""nofollow noreferrer"">link</a>).  In that, the Saga (or Mediator) knows the steps needed to complete some task and maintains some state about the progress of that task.</p>&#xA;&#xA;<p>It fires off commands to be processed and listens for responses.  When a response (event) comes in, it updates its state, then uses its state to determine what command(s) need to be fired off next.</p>&#xA;&#xA;<p>That continues until either A) the process is completed, or B) a step in the process fails (in which case it might reverse direction and start firing compensating commands to 'undo' the prior actions).</p>&#xA;&#xA;<p>Using the diagram below from the post your referenced, the ""thoughts"" of the saga/mediator might be along the lines of...</p>&#xA;&#xA;<ol>&#xA;<li>Relocation event, so fire off the Change Address command and wait.</li>&#xA;<li>AddressChanged event received. Now I've changed the address, but I haven't recalced the quote or updated the claims, so I'll send those off (they don't conflict, so send them both) and wait.</li>&#xA;<li>ClaimsUpdated event received.  Still need the recalc before moving forward, so keep waiting.</li>&#xA;<li>QuoteRecalced event received.  Now that I've updated the address, recalced the quote, and updated the claims, I can send off the Adjust Claims command, and wait.</li>&#xA;</ol>&#xA;&#xA;<p>... and so on.</p>&#xA;&#xA;<p>You would maybe want to add persistence (so it can pick up where it left off in case of a crash), idempotent event processors (so replay of events doesn't cause issues), and/or timeouts (to not get stuck waiting forever if a response event gets missed/lost).</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/YR0Sy.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YR0Sy.jpg"" alt=""Diagram""></a></p>&#xA;"
32751314,32750861,5369936,2015-09-23T23:58:47,"<p>Confirm that kingdomGroup is initialized properly?&#xA;secondly, confirm the datasource that you're using, and that it has data as expected.</p>&#xA;"
40740751,40719381,1766224,2016-11-22T11:32:41,"<p>It sounds like you want to use the RPC pattern? To follow the protocol, you should publish the response based on the <code>ReplyTo</code> or <code>ReplyToAddress</code> from the <code>BasicProperties</code>. That way, it is up to the caller (requester) to decided where the response is expected to be published. In my opinion it might be overkill to declare a dedicated exchange for one message type. For higher performance, you could use the <a href=""https://www.rabbitmq.com/direct-reply-to.html"" rel=""nofollow noreferrer"">direct reply to</a> feature. There are <a href=""http://www.rabbitmq.com/devtools.html"" rel=""nofollow noreferrer"">a lot of high level clients</a> that helps you deal with some of these things.</p>&#xA;"
32649268,28607400,2293072,2015-09-18T10:03:00,"<p>What about creating one more microservice - SessionProvider ?&#xA;The service will be responsible for creating and save session states and variables, every session will be identified by unique session id, other services may interact with the SessionProvider through this id.</p>&#xA;"
45461078,45449806,8405585,2017-08-02T12:35:43,"<p>It is not necessary that you need to have 10 front end services one for each backend service but try to have as many front end service as possible to fully utilize the advantages of microservices.</p>&#xA;&#xA;<p>I don't think separating the front end based on the role is a good idea. Separation should be done based on the domain, products &amp; sub products.</p>&#xA;&#xA;<p>I am not completely aware of what those 10 services are so it is very difficult to say how to separate the front end service.</p>&#xA;"
30289168,30288968,1133649,2015-05-17T16:22:08,"<p>It is hard to advice on the Solution N level, but certain problems can be avoided by the following advices:</p>&#xA;&#xA;<ol>&#xA;<li><p>Use globally unique identifiers for entities. For example, by assigning key values some kind of URI.</p></li>&#xA;<li><p>The global ids also simplify updates, because you track what has actually changed, the name or the entity. (entity has one-to-one relation with global URI)</p></li>&#xA;<li><p>CAP theorem says you can choose only two from CAP. Do you want a CA architecture? Or CP? Or maybe AP? This will strongly affect the way you distribute data.</p></li>&#xA;<li><p>For ""sort and filter"" there is MapReduce approach, which can distribute the load of figuring out those things.</p></li>&#xA;<li><p>Think carefully about the balance of normalization / denormalization. If your services operate on URIs, you can have a service which turns URIs to labels (names, descriptions, etc), but you do not need to keep the redundant information everywhere and update it. Do not do preliminary optimization, but try to keep data normalized as long as possible. This way, worker may not even need the building name but it's global id. And the microservice looks up the metadata from another microservice. </p></li>&#xA;<li><p>In other words, minimize the number of keys, shared between services, as part of separation of concerns.</p></li>&#xA;<li><p>Focus on the underlying model, not the JSON to and from. Right modelling of the data in your system(s) gains you more than saving JSON calls.</p></li>&#xA;</ol>&#xA;&#xA;<p>As for NoSQL, take a look at Riak database: it has adjustable CAP properties, IIRC. Even if you do not use it as such, reading it's documentation may help to come up with suitable architecture for your distributed microservices system. (Of course, this applies if you have essentially parallel system)</p>&#xA;"
43463781,25812816,1082449,2017-04-18T04:49:21,"<p>The other answers here are great, but if you want to go a bit further up the stack and easily add custom logging / security / arbitrarily complex shutdown logic, then adding a shutdown hook via a dropwizard admin <a href=""http://www.dropwizard.io/0.9.2/docs/manual/core.html#tasks"" rel=""nofollow noreferrer"">Task</a> is a nice pattern.</p>&#xA;&#xA;<p>Just code up your shutdown Task with any logic you like</p>&#xA;&#xA;<pre><code>import io.dropwizard.servlets.tasks.Task;&#xA;&#xA;public class ShutdownTask extends Task {&#xA;&#xA;  public ShutdownTask() {&#xA;    super(""shutdown""); // the task name, used in the endpoint to execute it &#xA;  }&#xA;&#xA;  public void execute(&#xA;    ImmutableMultimap&lt;String, String&gt; immutableMultimap,&#xA;    PrintWriter printWriter&#xA;  ) throws Exception {&#xA;    // kill the process asynchronously with some nominal delay&#xA;    // to allow the task http response to be sent&#xA;    new Timer().schedule(new TimerTask() {&#xA;      public void run() {&#xA;        // any custom logging / logic here prior to shutdown&#xA;        System.exit(0);&#xA;      }&#xA;    }, 5000);&#xA;  }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Register the task in <code>Application.run()</code></p>&#xA;&#xA;<pre><code>environment.admin().addTask(new ShutdownTask());&#xA;</code></pre>&#xA;&#xA;<p>And then execute it via a POST to the following endpoint on the admin port</p>&#xA;&#xA;<pre><code>http://localhost:&lt;ADMIN PORT&gt;/tasks/shutdown&#xA;</code></pre>&#xA;"
48756656,48753245,3782993,2018-02-12T22:33:41,"<p>Checkout WAF. It stands for web application firewall and is available as AWS service. Follow these steps as guidance:</p>&#xA;&#xA;<ol>&#xA;<li>Create a WAF ACL.</li>&#xA;<li>Add ""String and regex matching"" condition for your private endpoints.</li>&#xA;<li>Add ""IP addresses"" condition for your IP list/range that are allowed to access private endpoints.</li>&#xA;<li>Create a rule in your ACL to Allow access if both conditions above are met.</li>&#xA;<li>Assign ALB to your WAF ACL.</li>&#xA;</ol>&#xA;&#xA;<p><br></p>&#xA;&#xA;<hr>&#xA;&#xA;<p><strong>UPDATE:</strong></p>&#xA;&#xA;<p>In this case you have to use external facing ALB in a public subnet as mentioned by Dan Farrell in comment below.</p>&#xA;"
45025741,43326844,5756241,2017-07-11T04:58:26,"<p>I had the same problem. Downgrading <code>springfox-swagger-ui</code> to 2.2.2 fixed my problem (as mentioned in the comments by <a href=""https://stackoverflow.com/questions/43326844/swagger2-for-springboot-microservice-do-no-produce-response-on-ui#comment73889994_43326844"">sailor</a>. </p>&#xA;"
43156548,43142821,6413649,2017-04-01T11:56:15,"<p>Should choose the right side of the structure, on the grounds that the deployment of the left side of the architecture model is tight coupling is not conducive to a module according to the actual needs of the business expansion capacity.</p>&#xA;"
40775838,28500066,7202641,2016-11-23T23:30:37,"<p>I assume you have a Jenkins-user on the server and this user is the owner of the Jenkins-service:</p>&#xA;&#xA;<ol>&#xA;<li>log in on the server as root.</li>&#xA;<li>run <code>sudo visudo</code></li>&#xA;<li>add ""jenkins  ALL=(ALL) NOPASSWD:ALL"" at the end (jenkins=your Jenkins-user)</li>&#xA;<li>Sign In in Jenkins and choose your jobs and click to configure</li>&#xA;<li>Choose ""Execute Shell"" in the ""Post build step""</li>&#xA;<li>Copy and paste this:</li>&#xA;</ol>&#xA;&#xA;<pre class=""lang-bash prettyprint-override""><code>   service=myapp&#xA;   if ps ax | grep -v grep | grep -v $0 | grep $service &gt; /dev/null&#xA;   then&#xA;       sudo service myapp stop&#xA;       sudo unlink /etc/init.d/myapp&#xA;       sudo chmod +x /path/to/your/myapp.jar&#xA;       sudo ln -s /path/to/your/myapp.jar /etc/init.d/myapp&#xA;       sudo service myapp start &#xA;    else&#xA;       sudo chmod +x  /path/to/your/myapp.jar&#xA;       sudo ln -s  /path/to/your/myapp.jar /etc/init.d/myapp&#xA;       sudo service myapp start &#xA;    fi&#xA;</code></pre>&#xA;&#xA;<p>Save and run your job, the service should start automatically.</p>&#xA;"
51207490,51206924,3994193,2018-07-06T09:42:34,<p>you can see user command 'docker ps' or 'docker container ls' to know information about all the containers which are currently running.&#xA;To see information about all container including stopped use command 'docker container ls -a'</p>&#xA;&#xA;<p>In case you are looking for more information please elaborate what you are expecting?</p>&#xA;
26231059,26218391,3931985,2014-10-07T07:48:15,"<p>Your approach is quite good, and since soon I'll have to face the same issues like you - I can give you my ideas so far. I'm pretty sure that to </p>&#xA;&#xA;<blockquote>&#xA;  <p>create a sound testing strategy to cover this kind of system set up</p>&#xA;</blockquote>&#xA;&#xA;<p>can't be squeezed-in in one post. So the big picture looks like this (to me) - you're in the middle of the <a href=""http://en.wikipedia.org/wiki/Enterprise_application_integration"" rel=""nofollow"">Enterprise application integration</a> process, the fundamental basis to be test covered will be the <a href=""http://en.wikipedia.org/wiki/Data_migration"" rel=""nofollow"">Data migration</a>. Maybe you need to consider the concept of <a href=""http://en.wikipedia.org/wiki/Service-oriented_architecture"" rel=""nofollow"">Service-oriented architecture</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>generic platform using shared components </p>&#xA;</blockquote>&#xA;&#xA;<p>since it'll enable you to provide application functionality as services to other applications. Here  indirect benefit will be that SOA involves dramatically simplified testing. Services are autonomous, stateless, with fully documented interfaces, and separate from the cross-cutting concerns of the implementation. There are a lot of resources like this <strong><a href=""http://msdn.microsoft.com/en-us/library/cc194885.aspx"" rel=""nofollow"">E2E testing</a></strong> or <a href=""http://wso2.com/library/articles/2014/04/how-to-efficiently-test-service-oriented-architecture/"" rel=""nofollow"">efficiently testing SOA</a>. </p>&#xA;"
33407393,33399988,3931985,2015-10-29T06:33:59,"<p>In my experience with mSOA architecture, I've never seen</p>&#xA;&#xA;<blockquote>&#xA;  <p>MULTIPLE (datasource per instance)</p>&#xA;</blockquote>&#xA;&#xA;<p>to be used. Even if you plan to load it heavily, the most common DBs by nature support multi-threading access. Usually the bottleneck (or slowest part) of a DB system is the disk. We had to scale our clusters several times (relatively cheap if you are in the cloud, but scalability can also become an issue, as more threads will be required to manage and execute the scaled DB system). Keep in mind that some RDBMS use a temporary DB (tempdb) that is used by all the DBs on that instance for sorting, hashing, temporary variables, etc. Multithreading and splitting up this tempdb files can be used to improve the throughput of the tempdb, thereby improving overall server performance. </p>&#xA;&#xA;<p>Since now I work with <a href=""http://www.orchardproject.net/"" rel=""noreferrer"">Orchard</a>, I have to say that there are some corner cases, when your actions over one instance are not completely (and timely) synced. This causes access over resources to be denied (right after registration of the event) even after correct authentication.</p>&#xA;&#xA;<blockquote>&#xA;  <p>I plan to hide multiple instances behind Load Balancer</p>&#xA;</blockquote>&#xA;&#xA;<p>This is a proper design for your App servers, so utilizing a DB cluster should be suitable as well. Aiming at full answer - you can consider <a href=""https://en.wikipedia.org/wiki/Data_warehouse"" rel=""noreferrer"">DWH</a>, in case you have a lot of services and you want to be able to do some data mining and analysis from all their DBs.</p>&#xA;"
31718333,31717615,3931985,2015-07-30T08:06:34,"<p>In my experience when dealing with mSOA it always comes to <a href=""http://www.1keydata.com/datawarehousing/data-warehouse-definition.html"" rel=""nofollow"">Data Warehouse</a> solution in the end. And this is the natural choice if you have a dedicated DB (cluster) per micro-service. After all the business should be able to use that info from your domain. Even <a href=""https://en.wikipedia.org/wiki/Data_Vault_Modeling"" rel=""nofollow"">Data Vault Modeling</a> will be a good fit here. </p>&#xA;"
38554679,38554037,5047634,2016-07-24T17:18:45,"<p>Two microservices should not share same data model. You can create a service which is solely responsible for managing data persistence, and can use it in other services.</p>&#xA;"
44453850,41903352,7672402,2017-06-09T09:12:20,"<p>Assuming the Micro service has a REST Endpoint over HTTP. In that case you've to use HttpTask which is a system task. Httptask makes a Http call and the response is available as task output. Pls refer to the below link:<a href=""https://netflix.github.io/conductor/metadata/systask/#http"" rel=""nofollow noreferrer"">HttpTask</a></p>&#xA;&#xA;<p>Pls remember to set the SchemaVersion as 2 for the WorkflowDef which contains HttpTask. You would also need a corresponding <a href=""https://netflix.github.io/conductor/metadata/#task-definition"" rel=""nofollow noreferrer"">Task type</a> registered. </p>&#xA;"
36264400,36157778,5066066,2016-03-28T14:28:01,"<p>I ran in to exactly the same problem using ServiceProxy with a Stateless Service. I have a Stateful Services that works well, but the Stateless was receiving: </p>&#xA;&#xA;<pre><code>System.Fabric.FabricException: Invalid partition key/ID '{0}'  for selector {1}&#xA;</code></pre>&#xA;&#xA;<p>As Allan T mentions above, you also need to use the third constructor overload for ServiceProxy.Create():</p>&#xA;&#xA;<p>Instead of: </p>&#xA;&#xA;<pre><code>ServiceProxy.Create&lt;IMasterDataMService&gt;(0, new Uri(""fabric:/DataServiceFabric/MasterDataMService""));&#xA;</code></pre>&#xA;&#xA;<p>use:</p>&#xA;&#xA;<pre><code>ServiceProxy.Create&lt;IMasterDataMService&gt;(new Uri(""fabric:/DataServiceFabric/MasterDataMService""));&#xA;</code></pre>&#xA;&#xA;<p>...simply, do not specify the partition. That made the error go away for me.</p>&#xA;&#xA;<p><a href=""https://msdn.microsoft.com/en-us/library/mt628402.aspx"" rel=""nofollow"">https://msdn.microsoft.com/en-us/library/mt628402.aspx</a></p>&#xA;"
32550474,32532736,416850,2015-09-13T14:06:26,"<p>You might take a hint from the 12-factor App, and call them releases (<a href=""http://12factor.net/build-release-run"" rel=""nofollow"">http://12factor.net/build-release-run</a>)</p>&#xA;&#xA;<p>You then deploy a versioned release.</p>&#xA;"
38790741,38790603,3828818,2016-08-05T13:39:53,"<h3>Inside of a microservice</h3>&#xA;&#xA;<p>If possible, the microservice should not be reliant on any external resource, meaning the database should be inside the same service. However, if you for some reason must have your database as an external resource, and you cannot give a connection string directly to it. I guess you would use the client discovery pattern, but then I feel it isn't microservices anymore as they would not really be self-contained.</p>&#xA;&#xA;<h3>Between microservices</h3>&#xA;&#xA;<p>In my opinion you should keep your microservices self-contained. They should not depend on a ""database""-service, but rather if there is information available to a service available on the message bus, it can take action. If that information originates from the database or not is not relevant for the sake of that service. But that requires some sort of message queue.</p>&#xA;&#xA;<p>If you have to rely on http requests or other non-centralized communication I agree, the client discovery pattern seems suitable.</p>&#xA;"
42219096,42199241,7369554,2017-02-14T06:12:50,"<p>The above 2 answers are perfectly fine. I would want to know if you're looking forward to using Docker so as to support your microservices efficiently and have more visibility and security. &#xA;So, far it is being used as the most popular microservices architecture component.</p>&#xA;&#xA;<p>This is a good read: <a href=""https://medium.com/aws-activate-startup-blog/using-containers-to-build-a-microservices-architecture-6e1b8bacb7d1#.qztlom227"" rel=""nofollow noreferrer"">https://medium.com/aws-activate-startup-blog/using-containers-to-build-a-microservices-architecture-6e1b8bacb7d1#.qztlom227</a></p>&#xA;"
38734817,38734740,301650,2016-08-03T05:08:30,"<p>Make sure that your <code>logback.xml</code> file is in the classpath of your application, e.g. lives in the root of your packaged jar file. Then specifying just <code>logback.xml</code> should work.</p>&#xA;"
46360415,46333040,5152338,2017-09-22T08:41:06,"<p>Think of <code>codeship-services.yml</code> as a superset of <code>docker-compose.yml</code>, in the sense that <code>codeship-services.yml</code> has additional options that Docker Compose doesn't provide. Other than that, they are totally identical. Both build images the same way, and both can start all containers at once.</p>&#xA;&#xA;<p>That being said, I agree with Moema's answer that it is often better to have both files in your project and optimize each of them for their environment. Caching, for example, can only be configured in <code>codeship-services.yml</code>. For our images, caching makes a huge difference for build times, so we definitely want to use it. And just like Moema, we need a lot of auxiliary tools on CI that we don't need locally (AWS CLI, curl, test frameworks, ...). On the other hand, we often run multiple instances of a service locally, which is not necessary on Codeship. </p>&#xA;&#xA;<p>Having both files in our projects makes it much easier for us to cover the different requirements of CI and local development.</p>&#xA;"
33659605,31190685,317522,2015-11-11T20:51:29,"<p>Microservice is an architectural style that is poorly defined. There are several blog posts, essays, and now a book (Building Microservices by Sam Newman) that try to explain microservices. But it became a buzzword and expected characteristics (e.g., ""developed by a team that can be fed by 1 or 2 pizzas"") vary from one author to another.&#xA;If we filter out the noise, I think the distinctive factor is that for microservices the deployment unit should contain only one service or just a few cohesive services. &#xA;I explore this point in a <a href=""https://insights.sei.cmu.edu/saturn/2015/11/defining-microservices.html"" rel=""nofollow"">short blog post</a>.</p>&#xA;&#xA;<p>In your example, all else being equal, you would have distinct microservices, such as:</p>&#xA;&#xA;<ul>&#xA;<li>account (self registration, password reset, login, etc.)</li>&#xA;<li>catalog (items with their price, description, and stock information, and search capabilities)</li>&#xA;<li>purchase order </li>&#xA;</ul>&#xA;&#xA;<p>These services are expected to communicate among themselves using http (REST) and/or events via a message broker. And your design should minimize communication between microservices. How to do that? One idea is to design microservices around DDD bounded contexts. In any case, microservices is not a good idea in all cases, and you should be aware of the <a href=""https://insights.sei.cmu.edu/saturn/2015/11/microservices-beyond-the-hype-what-you-gain-and-what-you-lose.html"" rel=""nofollow"">tradeoffs involved</a>.</p>&#xA;"
33662057,33041733,317522,2015-11-11T23:52:58,"<p>This is a very important question because a few people get lured by all the buzz around microservices, and there are tradeoffs to consider. So, what are the pros and cons of microservices (when compared with the monolithic model)? </p>&#xA;&#xA;<p><em>Pros:</em></p>&#xA;&#xA;<ul>&#xA;<li><strong>Deployability</strong>: more agility to roll out new versions of a service due to shorter build+test+deploy cycles. Also, flexibility to employ service-specific security, replication, persistence, and monitoring configurations.</li>&#xA;<li><strong>Reliability</strong>: a microservice fault affects that microservice alone and its consumers, whereas in the monolithic model a service fault may bring down the entire monolith.</li>&#xA;<li><strong>Availability</strong>: rolling out a new version of a microservice requires little downtime, whereas rolling out a new version of a service in the monolith requires a typically slower restart of the entire monolith.</li>&#xA;<li><strong>Scalability</strong>: each microservice can be scaled independently using pools, clusters, grids. The deployment characteristics make microservices a great match for the elasticity of the cloud.</li>&#xA;<li><strong>Modifiability</strong>: more flexibility to use new frameworks, libraries, datasources, and other resources. Also, microservices are loosely-coupled, modular components only accessible via their contracts, and hence less prone to turn into a big ball of mud. Dynamic discovery and binding via a registry (e.g., Apache ZooKeeper, Netflix Eureka) is sometimes used for location transparency.</li>&#xA;<li><strong>Management</strong>: the application <em>development</em> effort is divided across teams that are smaller and work more independently.</li>&#xA;<li><strong>Design autonomy</strong>: the team has freedom to employ different technologies, frameworks, and patterns to design and implement each microservice, and can change and redeploy each microservice independently</li>&#xA;</ul>&#xA;&#xA;<p><em>Cons:</em></p>&#xA;&#xA;<ul>&#xA;<li><strong>Deployability</strong>: deployment becomes more complex with many jobs, scripts,    transfer areas, and config files for deployment.</li>&#xA;<li><strong>Performance</strong>: services more likely need to communicate over the network, whereas services within the monolith may benefit from local calls. Also, if the microservice uses dynamic discovery, the registry lookup is a performance overhead.</li>&#xA;<li><strong>Availability</strong>: if you use a registry for dynamic discovery, unavailability of the registry may compromise the consumer-service interaction.</li>&#xA;<li><strong>Modifiability</strong>: changes to the contract are more likely to impact consumers deployed elsewhere, whereas in the monolithic model consumers are more likely to be within the monolith and will be rolled out in lockstep with the service. Also, mechanisms to improve autonomy, such as eventual consistency and asynchronous calls, add complexity to microservices.</li>&#xA;<li><strong>Testability</strong>: automated tests are harder to setup and run because they may span different microservices on different runtime environments.</li>&#xA;<li><strong>Management</strong>: the application operation effort increases because there are more runtime components, log files, and point-to-point interactions to oversee.</li>&#xA;<li><strong>Memory use</strong>: several classes and libraries are often replicated in each microservice bundle and the overall memory footprint increases.</li>&#xA;<li><strong>Runtime autonomy</strong>: in the monolith the overall business logic is collocated. With microservices the logic is spread across microservices. So, all else being equal, it's more likely that a microservice will interact with other microservices over the network--that interaction decreases autonomy. If the interaction between microservices involves changing data, the need for a transactional boundary further compromises autonomy. The good news is that to avoid runtime autonomy issues, we can employ techniques such as eventual consistency, event-driven architecture, CQRS, cache (data replication), and aligning microservices with DDD bounded contexts. These techniques are not inherent to microservices, but have been suggested by virtually every author I've read. </li>&#xA;</ul>&#xA;&#xA;<p>Once we understand <a href=""https://insights.sei.cmu.edu/saturn/2015/11/microservices-beyond-the-hype-what-you-gain-and-what-you-lose.html"">these tradeoffs</a>, there's one more thing we need to know to answer the other question: which is better, microservices or monolith? <em>We need to know the non-functional requirements (quality attribute requirements) of the application. Once you understand how important is performance vs scalability, for example, you can weigh the tradeoffs and make an educated design decision.</em></p>&#xA;"
33678191,30213456,317522,2015-11-12T17:59:57,"<p><em>What doesn't make sense:</em></p>&#xA;&#xA;<ul>&#xA;<li><strong>distributed transactions with REST services</strong>. REST services by definition are stateless, so they should not be participants in a transactional boundary that spans more than one service. Your user registration use case scenario makes sense, but the design with REST microservices to create User and Wallet data is not good.    </li>&#xA;</ul>&#xA;&#xA;<p><em>What will give you headaches:</em> </p>&#xA;&#xA;<ul>&#xA;<li><strong>EJBs with distributed transactions</strong>. It's one of those things that work in theory but not in practice. Right now I'm trying to make a distributed transaction work for remote EJBs across JBoss EAP 6.3 instances. We've been talking to RedHat support for weeks, and it didn't work yet. </li>&#xA;<li><strong>Tho-phase commit solutions in general</strong>. I think the <a href=""https://en.wikipedia.org/wiki/Two-phase_commit_protocol"" rel=""nofollow noreferrer"">2PC protocol</a> is a great algorithm (many years ago I implemented it in C with RPC). It requires comprehensive fail recovery mechanisms, with retries, state repository, etc. All the complexity is hidden within the transaction framework (ex.: JBoss Arjuna). However, 2PC is not fail proof. There are situations the transaction simply can't complete. Then you need to identify and fix database inconsistencies manually. It may happen once in a million transactions if you're lucky, but it may happen once in every 100 transactions depending on your platform and scenario. </li>&#xA;<li><strong>Sagas (Compensating transactions)</strong>. There's the implementation overhead of creating the compensating operations, and the coordination mechanism to activate compensation at the end. But compensation is not fail proof either. You may still end up with inconsistencies (= some headache).  </li>&#xA;</ul>&#xA;&#xA;<p><em>What's probably the best alternative:</em></p>&#xA;&#xA;<ul>&#xA;<li><strong>Eventual consistency</strong>. Neither ACID-like distributed transactions nor compensating transactions are fail proof, and both may lead to inconsistencies. Eventual consistency is often better than ""occasional inconsistency"". There are different design solutions, such as: &#xA;&#xA;<ul>&#xA;<li>You may create a more robust solution using asynchronous communication. In your scenario, when Bob registers, the API gateway could send a message to a NewUser queue, and right-away reply to the user saying ""You'll receive an email to confirm the account creation."" A queue consumer service could process the message, perform the database changes in a single transaction, and send the email to Bob to notify the account creation. </li>&#xA;<li>The User microservice creates the user record <em>and</em> a wallet record <em>in the same database</em>. In this case, the wallet store in the User microservice is a replica of the master wallet store only visible to the Wallet microservice. There's a data synchronization mechanism that is trigger-based or kicks in periodically to send data changes (e.g., new wallets) from the replica to the master, and vice-versa.</li>&#xA;</ul></li>&#xA;</ul>&#xA;&#xA;<p><em>But what if you need synchronous responses?</em></p>&#xA;&#xA;<ul>&#xA;<li><strong>Remodel the microservices</strong>. If the solution with the queue doesn't work because the service consumer needs a response right away, then I'd rather remodel the User and Wallet functionality to be collocated in the same service (or at least in the same VM to avoid distributed transactions). Yes, it's a step farther from microservices and closer to a monolith, but will save you from some headache. </li>&#xA;</ul>&#xA;"
33893798,29117570,317522,2015-11-24T12:31:00,"<p>So, how is orchestration of microservices different from orchestration of old SOA services that are not “micro”? Not much at all. </p>&#xA;&#xA;<p>Microservices usually communicate using http (REST) or messaging/events. Orchestration is often associated with orchestration platforms that allow you to create a scripted interaction among services to automate workflows. In the old SOA days, these platforms used WS-BPEL. Today's tools don't use BPEL. Examples of modern orchestration products: Netflix Conductor, Camunda, Zeebe, Azure Logic Apps, Baker. </p>&#xA;&#xA;<p>Keep in mind that orchestration is a compound pattern that offers several capabilities to create complex compositions of services. Microservices are more often seen as services that should not participate in complex compositions and rather be more autonomous. </p>&#xA;&#xA;<p>I can see a microservice being invoked in an orchestrated workflow to do some simple processing, but I don’t see a microservice being the orchestrator service, which often uses mechanisms such as compensating transactions and state repository (dehydration). </p>&#xA;"
46502476,34442192,1239122,2017-09-30T11:56:53,"<p>The API gateway doesn't introduce a single point of failure any more than a load balancer does. Any serious API gateway should be able to run in high availability mode removing the single point of failure.</p>&#xA;&#xA;<p>The API gateway encourages good documentation &amp; planning within teams. Some API gateways allow you to import Swagger Specifications <a href=""https://swagger.io/"" rel=""nofollow noreferrer"">https://swagger.io/</a> in order to create the API.</p>&#xA;&#xA;<p>Some gateways allow you to create virtual endpoints to mock responses of an upstream target. That way, if your service is not available quite yet, you can still code to it, and switch to the targets when ready.</p>&#xA;&#xA;<p>API gateways should be able to round robin load balance your upstream targets, negating the necessity for adding a dedicated load balancer. You can also configure your gateway to periodically hit a healthcheck endpoint, and automatically remove targets from LB if service is not available.</p>&#xA;&#xA;<p>Gateways will handle auth for you. Whether that be via JWT, Oauth, Simple, Open etc. Your developers can concentrate on building their microservices. Your microservices can be micro. The gateway would sit at the edge of the infrastructure, and handle security for you.</p>&#xA;"
42487772,42487685,5502802,2017-02-27T14:07:01,"<p>You can create a package from one of your apps and then reference the package in the other app.</p>&#xA;&#xA;<p><a href=""https://docs.npmjs.com/getting-started/creating-node-modules"" rel=""noreferrer"">https://docs.npmjs.com/getting-started/creating-node-modules</a></p>&#xA;"
27874244,27857790,2425939,2015-01-10T08:19:51,"<p>I use a setup with consul to connect tomcat containers to an apache http server (using mod_jk ). Consul is similar to etcd, i.e. it allows to register and discover services. This might be applicable to your question, but you are not restricted to consul.</p>&#xA;&#xA;<p>Every time a new tomcat container is started, I assign a distinct public port to that container, register the tomcat container in consul with information about its IP and Ports and fire an event (the script is run on the docker host, and is reduced for readability)</p>&#xA;&#xA;<pre><code>#!/bin/bash&#xA;INTERNAL_PORT=8009&#xA;source ~/ports.properties&#xA;TOMCAT_PORT=$(( TOMCAT_PORT + 1))&#xA;echo ""TOMCAT_PORT=$TOMCAT_PORT"" &gt; ~/ports.properties&#xA;&#xA;CONTAINER_ID=$(docker run -d -p $TOMCAT_PORT:8009 -v `pwd`$WAR_DIR:/webapps rossbachp/tomcat8)&#xA;echo ""Container started, CONTAINER_ID:  $CONTAINER_ID""&#xA;&#xA;IP_ADDRESS=$(docker inspect -f '{{.NetworkSettings.IPAddress}}' $CONTAINER_ID )&#xA;echo ""Container IP_ADDRESS:               $IP_ADDRESS ""&#xA;&#xA;echo ""Register Container in Consul""&#xA;curl -X PUT -d '{""ID"": ""'$CONTAINER_ID'"",""Name"":""'$CLUSTER_NAME'"", ""Tags"": [ ""IP_'$IP_ADDRESS'"", ""PORT_'$INTERNAL_PORT'""],""Port"":'$TOMCAT_PORT'}' localhost:8500/v1/agent/service/register &#xA;&#xA;echo ""Fire Event""&#xA;consul event -name ""TomcatServiceUp""&#xA;</code></pre>&#xA;&#xA;<p>In consul (on the docker host) I have defined a watch for the event ""TomcatServiceUp"" in File /etc/consul.d/bootstrap/watchTomcatServiceUp.json that executes a script </p>&#xA;&#xA;<pre><code>{&#xA;""watches"":[    {&#xA;""type"":""event"",&#xA;""name"":""TomcatServiceUp"",&#xA;""handler"": ""/home/dude/docker/docker-http-withmodjk/callbackTomcatServiceUpEvent.sh""&#xA;   }  ]&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>and the script callbackTomcatServiceUpEvent.sh queries the services (mainly IPAddress and Port), creates a new workers.properties file, copies this file to the http docker instance (to its volume) and gracefully restarts the http server (in the docker container).</p>&#xA;&#xA;<pre><code>#!/bin/bash&#xA;SERVICE=$(curl localhost:8500/v1/agent/services)&#xA;java -jar /home/dude/docker/JSonParser.jar ""$SERVICE"" &gt;&gt; /tmp/workers.properties&#xA;cp workers.properties /home/dude/docker/docker-http-withmodjk/mod_jk_conf&#xA;# http graceful restart&#xA;</code></pre>&#xA;&#xA;<p>Could you use an approach where your services register themselves in consul (or etcd) and discover each other via events and service lookup. Or use nginx to handle the events and do the service lookup?</p>&#xA;"
38049395,38048683,4723795,2016-06-27T08:42:45,"<p>After researching together with my teammates it has occured, that not only <code>return 301 url;</code> succesfuly redirects user. Although the documentation says avoid using <code>rewrite</code> what has worked is:</p>&#xA;&#xA;<pre><code>server {&#xA;    ...&#xA;    rewrite ^/$ https://$http_host/dir/ permantent;&#xA;    ...&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>The <code>permantent</code> statement is responsible for acting as a regular redirection.</p>&#xA;&#xA;<p><strong>Important:</strong>&#xA;In nginx <code>^</code> is the begining of a regular expression and of the incomming string and <code>$</code> sign is a normal <code>regex</code> sign which stands for <code>theEndOfString</code>. So  regex <code>^/$</code> means match <code>iff</code> url equals <code>/</code>.</p>&#xA;&#xA;<p><strong>Credits:</strong></p>&#xA;&#xA;<p>Hopefully this will save one's weekend-time and money.</p>&#xA;"
49288362,49284804,5125459,2018-03-14T22:04:20,"<p>Yes, Each Microservice should have its own database and if any other Microservice needs data owned by another microservice, then they do it using an API exposed by Microservices. <strong>No, it's not at all necessary</strong> to have the Microservice and its database to be hosted on the same server. For Example - A Microservice can be hosted on-premise and its database can live in the cloud like AWS DynamoDB or RDS.</p>&#xA;"
49041568,49036468,5125459,2018-03-01T02:10:34,"<p>There are a lot of classic patterns available to solve such problems in Microservices. You have 2 microservices - 1 for User (Microservice A) and 1 for Device (Microservice B). The fundamental principle of a microservice is to have a separate database for each of the microservice. If any microservice wants to talk to each other (or to get data from another microservice), they can but they would do it using an API. Another way for communication between 2 microservices is by events. When something happens in Microservice A, it will raise an event and push it to a central event store or a message queue and Microservice B would subscribe to some or all of the events emitted by A.</p>&#xA;&#xA;<p>I guess in your domain, A would have methods like - Add/Update/Delete a User and B would have Add/Update/Delete a device. Each user can have its own unique id and other data fields like Name, Address, Email etc. Each device can have its own unique id, a user id, and other data fields like Name, Type, Manufacturer, Price etc. Whenever you ""Add"" a device, you can send a POST request or a command (if you use CQRS) to Device Microservice with the request containing data about device + user-id and it could raise an event called ""DeviceAdded"". It can also have events corresponding to Update and Delete like ""DeviceUpdated"" and ""DeviceRemoved"". The microservice A can subscribe to events - ""DeviceAdded"", ""DeviceRemoved"", and ""DeviceUpdated"" events emitted by B and whenever any such event is raised, it will handle that event and denormalize that event into its own little database of Devices (Which you can call UserRelationships). In future, it can listen to events from other microservices too (so your pattern here would be extensible and scalable). </p>&#xA;&#xA;<p>So now to get all devices owned by a user, all you have to do is make an end-point in User Microservice like ""http://{microservice-A-host}:{port}/user/{user-id}/devices"" and it will return you a list of the devices by querying for user-id in its own little database of UserRelationships which you must have been maintaining through events. </p>&#xA;&#xA;<p>Good Reference is here: <a href=""https://www.nginx.com/blog/event-driven-data-management-microservices/"" rel=""nofollow noreferrer"">https://www.nginx.com/blog/event-driven-data-management-microservices/</a></p>&#xA;"
49185945,49171571,5125459,2018-03-09T03:03:04,"<p>Well, You are correct that there will be a lot of calls if you choose to go with option 1 or 2. If you choose 3, You can use a search technology like ElasticSearch (which can be used both as a search technology as well as data storage as documents) OR you can use SOLR technology backed by a database of your choice. Whenever you add a Book to the Books Microservice, raise an event which carries data about books and your search service would denormalize this event data into JSON documents and store it in Elasticsearch. Similarly, listen to events from Author Microservice and denormalize them into the database. </p>&#xA;&#xA;<p>In order to avoid a lot of calls, what you can also do is maintain all data about Books in Author Microservice (along with data about Author) and all data about Authors in Books Microservice (along with data about Books). There will be a lot of redundancy (and there will be problems like data synchronization) but at least now you won't have to do any cross-service calls. </p>&#xA;"
49205776,48942153,5125459,2018-03-10T05:43:00,"<p>Not at all. You can have well a very well defined microservices-styled architecture without CQRS and Event Sourcing. CQRS and Event Sourcing is a solution for intra-microservice design. You can choose to implement all or some of your microservices using CQRS and Event Sourcing. </p>&#xA;&#xA;<p>Let's see how Event Sourcing may help you. Event Sourcing is an alternative to restoring your current state of an entity using events instead of an ORM like Entity Framework or Hibernate and a SQL database. Suppose you have a microservice to store data about Books. If you use SQL, you would have controllers and end-points to Create, Update, and Delete a book and store those books in a SQL table. If you want to update that book, then in order to get the current state you would go back to SQL table and query for that book (by its id) and then your ORM would convert that table representation into a book object (object‑relational impedance mismatch problem) and then you would apply the changes and save the changed book object back into SQL table. As an alternative, you can store events for the books objects in a NoSQL database like MongoDB or maybe an event store. Now in order to update the book, first you would want to restore the current state and you can do that by getting back all the events related to this book and replaying these events to restore the current state. Your events became a source of truth and you completely avoided the bottleneck of ORM mapping and SQL joins. Events are stored as JSON documents and are usually ultra-fast.</p>&#xA;&#xA;<p>Now, coming to CQRS - CQRS is purely a pattern for separation of concerns. You would use CQRS to bifurcate your read-side from the write-side. End-points related to write-side like create, update, and delete live in one service and end-point for read-side live in another service. The advantage you get here is independent scaling, deploying, maintenance and many more. If your application is read-intensive, then have multiple numbers of instances deployed for read-side service. </p>&#xA;&#xA;<p>If you want to learn more, feel free to PM me. Good Luck!</p>&#xA;"
49145600,49113488,5125459,2018-03-07T06:34:27,"<p>In the Microservices world, each Microservice owns a set of functionalities and the data manipulated by these functionalities. If a microservice needs data owned by another microservice, it cannot directly go to the database maintained/owned by the other microservice rather it would call an API exposed by the other microservice. </p>&#xA;&#xA;<p>Now, regarding the placement of data, there are various options - you can store data owned by a microservice in a NoSQL database like MongoDB, DynamoDB, Cassandra (it really depends on the microservice's use-case) OR you can have a different table for each micro-service in a single instance of a SQL database. BUT remember, if you choose a single instance of a SQL Database with multiple tables, then there would be no joins (basically no interaction) between tables owned by different microservices. </p>&#xA;&#xA;<p>I would suggest you start small and then think about database scaling issues when the usage of the system grows. </p>&#xA;"
48289416,48279479,5125459,2018-01-16T20:35:28,"<p>In addition to @Constantin Galbenu's answer, I would like to put in my two cents. </p>&#xA;&#xA;<p>I would strongly advise you to look at a microservices pattern called <strong>""BFF"" (Backend-For-Frontend)</strong> pattern. Instead of having a thick API gateway doing all the work, you can have an API per use-case. For Example: In your case, you can an API called ""CreateMovieBFFHandler"" which would receive the POST request from front-end and then this guy would coordinate with other things in the system like message queues, events etc. to track the status of the submitted request. UI might have a protocol with this BFFhandler that if the response doesn't come back in X seconds, then the front-end would consider it as failure and if this handler is able to get a successfully processed messaged from message queue or ""MovieCreated"" event for this key, then it could send a 200 OK back and then you can redirect the page to call write side and then populate the UI. </p>&#xA;&#xA;<p>Useful Link: <a href=""https://samnewman.io/patterns/architectural/bff/"" rel=""nofollow noreferrer"">https://samnewman.io/patterns/architectural/bff/</a></p>&#xA;"
48306796,48294450,5125459,2018-01-17T17:29:12,"<p>In such scenario, think about C.A.P. Theorem. According to Wikipedia, ""the CAP theorem states that in the presence of a network partition, one has to choose between consistency and availability. Note that consistency, as defined in the CAP theorem, is quite different from the consistency guaranteed in ACID database transactions.""</p>&#xA;&#xA;<p>Since you have 2 microservices, so your system definitely needs to be partition tolerant and you are left with either A (Availability) or C (Consistency). If you want to go with C, then your system will suffer in availability terms. When a request comes into Microservice A, then you should not send back a success message to the client until A gets back a response from Microservice B that data has been successfully stored. This way you can achieve consistency by sacrificing availability. </p>&#xA;"
48373974,48370641,5125459,2018-01-22T02:37:57,"<p>REST is not the only style available for microservice-to-microservice (aka inter-process communication/IPC) but it is the <strong>most popular</strong> IPC technology used by applications based on <strong>microservices-styled architecture</strong>. There are other technologies like <a href=""https://thrift.apache.org/"" rel=""nofollow noreferrer""><strong>Apache Thrift</strong></a> and <a href=""https://grpc.io/"" rel=""nofollow noreferrer""><strong>gRPC</strong></a> which can serve the same purpose. Essentially all of these technologies are industry-grade implementations of <a href=""https://en.wikipedia.org/wiki/Remote_procedure_call"" rel=""nofollow noreferrer"">RPC</a> (Remote Procedure Call). </p>&#xA;&#xA;<p>I highly recommend you to read this <a href=""https://www.nginx.com/blog/building-microservices-inter-process-communication/"" rel=""nofollow noreferrer"">link</a> by Microservices Expert - Chris Richardson</p>&#xA;"
48157043,48144298,5125459,2018-01-08T19:41:30,"<p>Bounded Context is an independent domain. Let's imagine it as a different department in a company. Something like separation of concerns. An independent context in which all the stakeholders (Business Analysts, Testers, Developers, Business Folks) have the same definition of terms used in the Bounded Context. You can then have a separate microservice corresponding to each Bounded Context. For Example: If you in Insurance Domain, then you can have bounded context like Customer, Quote, Policy etc. and can have a micro-service for each of them.</p>&#xA;"
46812123,46807757,1331539,2017-10-18T14:06:38,"<p>you dont have an out-of-the-box solution in AWS, although it is possible with some effort as described in <a href=""https://aws.amazon.com/es/blogs/compute/service-discovery-an-amazon-ecs-reference-architecture/"" rel=""nofollow noreferrer"">https://aws.amazon.com/es/blogs/compute/service-discovery-an-amazon-ecs-reference-architecture/</a></p>&#xA;&#xA;<p>You may also install Zuul + Ribbon + Eureka or Nginx + Consul and use ALB to distribute traffic among Zuul or Nginx</p>&#xA;"
46660557,46657234,1331539,2017-10-10T07:08:12,"<p>I think you need to implement the <a href=""http://www.enterpriseintegrationpatterns.com/patterns/messaging/BroadcastAggregate.html"" rel=""nofollow noreferrer"">Scatter-Gather</a> pattern. With spring framework you can achieve that using <a href=""https://projects.spring.io/spring-integration/"" rel=""nofollow noreferrer"">Spring Integration</a> and its <a href=""https://docs.spring.io/spring-integration/docs/4.3.10.RELEASE/reference/html/messaging-routing-chapter.html#scatter-gather"" rel=""nofollow noreferrer"">Scatter-Gather implementation</a></p>&#xA;"
46302494,46283367,1331539,2017-09-19T14:00:33,"<p>The main difference is that Hystrix opens the circuit (it is an analogy to electrical circuits) when it detects an error and does not invoke downstream services until some time has elapsed. This behaviour prevents an avalanche of errors in cascading. It is similar to a smart traffic light that turns red and doesn't let you pass because it knows that you are going to have an accident a little later. After a configurable time, the circuit is closed again.&#xA;You can see 'Circuit opened / closed' at Hystrix dashboard:</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/zjZis.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zjZis.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>It is also well explained by Chris Richardson at <a href=""http://microservices.io/patterns/reliability/circuit-breaker.html"" rel=""nofollow noreferrer"">pattern - circuit breaker</a></p>&#xA;"
46302759,40856925,1331539,2017-09-19T14:12:33,"<p>As explained by Chris Richardson in <a href=""http://microservices.io/patterns/data/database-per-service.html"" rel=""nofollow noreferrer"">pattern database per service</a>, </p>&#xA;&#xA;<blockquote>&#xA;  <p>Instances of the same service should share the same database</p>&#xA;</blockquote>&#xA;"
46316662,46315996,1331539,2017-09-20T08:10:52,"<p>The great advantage of the micro-services is that you can adopt your migration project with an incremental orientation, so I would try to identify the individual functionalities that could be migrated and allow you to develop a quick-win, avoid a big-ban approach.</p>&#xA;&#xA;<p>Perhaps the greatest challenge you face is to know the proper granularity of your first microservice. How big or small it must be.</p>&#xA;&#xA;<p>A good point of reference is that a microservice should try to follow the <a href=""https://en.wikipedia.org/wiki/Single_responsibility_principle"" rel=""nofollow noreferrer"">principle of single responsibility</a>, and in general all <a href=""https://en.wikipedia.org/wiki/SOLID_(object-oriented_design)"" rel=""nofollow noreferrer"">SOLID principles</a> also apply.</p>&#xA;&#xA;<p>If you DB is fat you'll probably have many foreing keys, and you have to plan carefully how to face the <a href=""http://microservices.io/patterns/data/database-per-service.html"" rel=""nofollow noreferrer"">database per service pattern</a> </p>&#xA;&#xA;<p>This article may also be helpful <a href=""https://dzone.com/articles/how-small-should-microservices-be"" rel=""nofollow noreferrer"">'How Small Should Microservices Be?'</a></p>&#xA;"
46316993,46311488,1331539,2017-09-20T08:27:05,"<p>ESBs are solutions that have been implemented in many companies to ensure application interoperability and traceability. However, they are heavy solutions that do not allow to scale horizontally, usually the ESBs adopt a configuration of two nodes, active-active or active-passive. </p>&#xA;&#xA;<p>On the other hand, services in ESB are not usually deployed individually but in deployment packages alongside other services, making delivery management and testing more complicated. </p>&#xA;&#xA;<p>Microservices are designed to be developed and deployed individually, and so that within a cloud infrastructure can easily scale horizontally increasing the number of instances dynamically. </p>&#xA;&#xA;<p>Interoperability between applications has moved into the background at present, as today's communication by web services is virtually commonplace, although some middleware that solves connectivity may still be needed in some very old infrastructures.</p>&#xA;"
46467620,46464455,1331539,2017-09-28T11:06:50,"<p>You may use a Gateway, some alternatives are </p>&#xA;&#xA;<ul>&#xA;<li><a href=""http://tyk.io"" rel=""nofollow noreferrer"">Tyk</a></li>&#xA;<li><a href=""https://getkong.org/"" rel=""nofollow noreferrer"">Kong</a></li>&#xA;<li><a href=""https://spring.io/guides/gs/routing-and-filtering/"" rel=""nofollow noreferrer"">Netflix/Zuul</a></li>&#xA;</ul>&#xA;&#xA;<p>There is a nice article at <a href=""https://thenewstack.io/api-gateways-age-microservices/"" rel=""nofollow noreferrer"">https://thenewstack.io/api-gateways-age-microservices/</a></p>&#xA;"
46337673,46333428,1331539,2017-09-21T07:15:44,"<p>You may try with urlPattern instead of url, replace </p>&#xA;&#xA;<pre><code>url '/check/1234567'&#xA;</code></pre>&#xA;&#xA;<p>for </p>&#xA;&#xA;<pre><code>urlPattern '/check/[0-9]+'&#xA;</code></pre>&#xA;"
46615875,46615008,1201235,2017-10-07T01:39:43,"<p>Spring Boot integrates well with <a href=""http://projects.spring.io/spring-cloud/"" rel=""nofollow noreferrer"">Spring Cloud</a>.  And Spring Cloud provides Eureka (for service discovery) as well as Hystrix (for circuit breaker patterns).  Also, Spring Cloud Stream to provide event driven patterns</p>&#xA;&#xA;<p>Very easy to use with Spring Boot</p>&#xA;"
49064428,49036468,3503187,2018-03-02T07:22:01,<p>With microservices there is nothing wrong with both the options. However the <code>device</code> api <em>makes more sense</em> and further I'll prefer </p>&#xA;&#xA;<pre><code>GET ../device/{userId}/devices&#xA;</code></pre>&#xA;&#xA;<p>over</p>&#xA;&#xA;<pre><code>GET ../device?user_id=123&#xA;</code></pre>&#xA;&#xA;<p>There are two reasons:</p>&#xA;&#xA;<ol>&#xA;<li>As userId should already be there with devices service you'll save one call to user service. Otherwise it'll go like Requester -> User service -> Device Service</li>&#xA;<li>You can use POST ../device/{userId}/devices to create new device for particular user. Which looks more restful then parameterized URL.</li>&#xA;</ol>&#xA;
49065066,49062631,3503187,2018-03-02T08:10:04,"<p>In multi node architecture where a distributed queue is being used the general approach is to have locking mechanism. &#xA;Consider you have an object/request to process which is initially put into the queue. Any of the instances can pick that for processing. Your logic should be like:</p>&#xA;&#xA;<ol>&#xA;<li>Attempt to acquire lock on the object. If successful process it. This will ensure tt a time only one instance will pick.</li>&#xA;<li>Process the request and move it to another consistent state, so that when another instance picks it, it should have all the information</li>&#xA;<li>Release the lock.</li>&#xA;</ol>&#xA;&#xA;<p>In this case there will be no issue if any instance picks the response in the second step you mentioned.</p>&#xA;&#xA;<p>Another approach, along with locking goes like this:</p>&#xA;&#xA;<ol>&#xA;<li>Try to lock the request. If successful.</li>&#xA;<li>If successful, remove it from the queue</li>&#xA;<li>Do the processing and move to another consistent state.</li>&#xA;<li>Put the object back to queue and release the lock.</li>&#xA;</ol>&#xA;&#xA;<p>Of course as pointed by @Constantin there should be a link to map the response from B to the original object. Consistent state is another thing you should care about when an object can be handled by multiple instances. There shouldn't be any information which resides in a particular instance's memory.</p>&#xA;"
49082212,49080999,3503187,2018-03-03T08:26:38,"<p>You are trying to go away from monolith and the approach you are taking is very common, to take out part from monolith which can be converted into a microservice. Monolith starts to shrink over time and you have more number of MSs.</p>&#xA;&#xA;<p>Coming to your question of data duplicacy, yes this is a challenge and some data needs to be duplicated but this vary case to case and difficult to say without looking into application.</p>&#xA;&#xA;<p>You may expose API so monolith can get/create the data if needed and I strongly suggest not to sacrifice or compromise <em>data model of microservice</em> to avoid duplicacy, because MS will be going to more important than your monolith in future. Keep in mind you should avoid adding any new code to the monolith and even if you have to, for data ask the MS instead of the monolith.</p>&#xA;"
50809596,50801690,3503187,2018-06-12T05:07:57,"<p>As already pointed by @Vadim in the comment it ultimately it is the desginer's or developer's job to decide what to use.</p>&#xA;&#xA;<p>My two cents from experience, in long run it is always <em>good to use schema generation scripts</em> and there are lots of opensource libraries available.&#xA;For instance in java we have <a href=""http://www.liquibase.org/"" rel=""nofollow noreferrer"">Liquibase</a> and <a href=""https://flywaydb.org/"" rel=""nofollow noreferrer"">Flyway</a>.</p>&#xA;&#xA;<p>The reason why I am saying this is, your DB will undergo lot of changes in long run. Hibernate can easily handle creating and modifying table and column changes, but sometimes for example when you add a new column you may want to fill the existing records for which you may need to write custom sqls.</p>&#xA;&#xA;<p>Similarly from time to time you may want to update records from back-end which is difficult to achieve using hibernate.</p>&#xA;&#xA;<p>I have observed that <em>DB creation</em> generally is part of pre-deploy scripts and schema generation happens during application startup.</p>&#xA;&#xA;<p>My advise is to use some schema generation tool for schema migration and use hibernate for schema validation so that the two remain in sync.</p>&#xA;"
49642861,49610008,3503187,2018-04-04T04:35:29,"<blockquote>&#xA;  <p>It is not able to get those header parameter which microservice-B is trying to fetch.</p>&#xA;</blockquote>&#xA;&#xA;<p><code>Swagger</code> on its own can't call the authenticator service and add the obtained token to another request's header.</p>&#xA;&#xA;<p>You can modify the <code>Docket</code> object to accept additional parameter in header as below:</p>&#xA;&#xA;<pre><code>docket.globalOperationParameters(&#xA;    Collections.singletonList(new ParameterBuilder()&#xA;      .name(""Authorization"")&#xA;      .description(""Bearer [token]"")&#xA;      .modelRef(new ModelRef(""string""))&#xA;      .parameterType(""string"")&#xA;      .required(true)&#xA;      .build()&#xA;    )&#xA;);&#xA;</code></pre>&#xA;&#xA;<p>This will allow Swagger UI to show additional field to accept token (see image below). You need to obtain the token by yourself and can put in this field.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/YzPbn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YzPbn.png"" alt=""enter image description here""></a>Hope this helps.</p>&#xA;"
48456001,48440783,307795,2018-01-26T04:57:25,"<p>Yes, I think this is a very common use case.  You develop version service-a, version 1, and you want to deploy service-a, version 2, (canary or blue/green)  You can deploy both versions, and register both versions with Eureka, and traffic will be sent to both versions. After you verify version 2, you can shut down version 1.</p>&#xA;"
51844454,51842770,9688659,2018-08-14T15:00:47,"<p>Breaking a monolith up into proper microservices with appropriate boundaries for your domain is certainly more of an art than a science.  The prerequisite to taking on such a task is a thorough understanding of your domain and the interactions within, and you won't get it right the first time.  One of points that Evans makes in his book on Domain-Driven Design is that for any sufficiently complex domain, the domain model continually evolves because your understanding of the domain is continually evolving; you will understand it a little better tomorrow than you do today.  That said, don't be afraid to start when you have an understanding that is ""good enough"" and be willing to adapt/evolve your model.</p>&#xA;&#xA;<p>I don't know your domain, but it sounds to me like you need to first figure out in which <a href=""https://martinfowler.com/bliki/BoundedContext.html"" rel=""nofollow noreferrer"">bounded context</a> <code>Customer</code> primarily belongs.  Yes, you want to minimize duplication of domain logic, and though it may not fit completely and neatly into a single service, to the extent that you make one service take primary responsibility for accessing, persisting, manipulating, validating, and ensuring the integrity of a <code>Customer</code>, the better off you'll be.</p>&#xA;&#xA;<p>From your question, I see two possibilities:</p>&#xA;&#xA;<ul>&#xA;<li>The <code>Account Services</code> bounded context is the primary stakeholder in <code>Customer</code>, and <code>Customer</code> has non-trivial ties to other <code>Account Services</code> entities and services.  It's difficult to draw clear boundaries around a <code>Customer</code> in isolation.  In this case, <code>Customer</code> belongs in the <code>Account Services</code> bounded context.</li>&#xA;<li><code>Customer</code> is an independent enough concept to merit its own microservice.  A <code>Customer</code> can stand alone.  In this case, <code>Customer</code> belongs in its own bounded context.</li>&#xA;</ul>&#xA;&#xA;<p>In either case, great care should be taken to ensure that the <code>Customer</code>-specific domain logic stays centralized in the <code>Customer</code> microservice behind strong boundaries.  Other services might use <code>Customer</code>, or perhaps a light-weight (even read-only) <code>CustomerView</code>, but their interactions should go through the <code>Customer</code> service to the extent that they can.</p>&#xA;&#xA;<p>In your question, you indicate that the <code>Payments</code> bounded context will need access to <code>Customer</code>, but it might just need a light-weight version.  It should communicate with the <code>Customer</code> service to get that light-weight object.  If, during Payments processing you need to update the <code>Customer</code>'s billing address for example, <code>Payments</code> should call into the <code>Customer</code> microservice telling it to update its billing address.  <code>Payments</code> need not know anything about <em>how</em> to update a <code>Customer</code>'s billing address other than the single API call; any domain logic, validation, firing of domain events, etc... that need to happen as part of that operation are contained within the <code>Customer</code> microservice.</p>&#xA;&#xA;<p>Regarding your second question: it's true that atomic transactions become more complex/difficult in a distributed architecture.  Do some reading on the Saga pattern: <a href=""https://blog.couchbase.com/saga-pattern-implement-business-transactions-using-microservices-part/"" rel=""nofollow noreferrer"">https://blog.couchbase.com/saga-pattern-implement-business-transactions-using-microservices-part/</a>.  Also, Jimmy Bogard is currently in the midst of a blog series called &#xA;<a href=""https://lostechies.com/external/?guid=urn:uuid:df26ea12-e99c-15d6-d131-6aaa57bcc48c"" rel=""nofollow noreferrer"">Life Beyond Distributed Transactions: An Apostate's Implementation</a> that may offer some good insights.</p>&#xA;&#xA;<p>Hope this helps!</p>&#xA;"
39260553,39260387,2809427,2016-08-31T23:31:23,"<p>The simplest way is to use the environment variables <code>SOMATA_REGISTRY_HOST</code> (default ""127.0.0.1"") and <code>SOMATA_REGISTRY_PORT</code> (default 8420) when running your script:</p>&#xA;&#xA;<pre><code>SOMATA_REGISTRY_HOST=55.44.33.21 node test.js&#xA;</code></pre>&#xA;&#xA;<p>The somata Client constructor also lets you connect to specific registries with the options <code>registry_host</code> and <code>registry_port</code>:</p>&#xA;&#xA;<pre><code>var client = new somata.Client({&#xA;    registry_host: '55.44.33.21',&#xA;    registry_port: 5858&#xA;})&#xA;</code></pre>&#xA;&#xA;<p><strong>Note:</strong> To allow connections from remote hosts, <code>somata-registry</code> will have to be run with its binding port as ""0.0.0.0"" instead of the default ""127.0.0.1"", which can be accomplished with the <code>-h</code> flag or <code>SOMATA_REGISTRY_BIND_HOST</code> environment variable when starting the registry. The <code>-p</code> flag and  <code>SOMATA_REGISTRY_BIND_PORT</code> are also available for listening on a custom port.</p>&#xA;&#xA;<p><code>somata-registry -h 0.0.0.0</code>&#xA;or&#xA;<code>SOMATA_REGISTRY_BIND_HOST=0.0.0.0 somata-registry</code></p>&#xA;&#xA;<p>And of course you'll need access to the host and port from the remote machine.</p>&#xA;"
47757398,47757342,520275,2017-12-11T16:38:13,"<p>I usually end up <a href=""https://docs.docker.com/compose/extends/#multiple-compose-files"" rel=""nofollow noreferrer"">having multiple yml files</a>. </p>&#xA;&#xA;<p>Then you can add several <em>-f</em> flags to <em>docker-compose</em> command to indicate which services to run.</p>&#xA;&#xA;<p>For instance, having:</p>&#xA;&#xA;<ul>&#xA;<li>docker-compose.yml: containing basic services</li>&#xA;<li>docker-compose-additional-services.yml: containing additional/optional services.</li>&#xA;</ul>&#xA;&#xA;<p>You can execute to start:</p>&#xA;&#xA;<pre><code> docker-compose -f docker-compose.yml -f docker-compose-additional-services.yml up&#xA;</code></pre>&#xA;&#xA;<p>And it will start all services.</p>&#xA;&#xA;<p>Note, that all services are merged as if they were in a single file, so you can reference (<em>depends_on</em>, <em>link</em>) services from one file to the other.</p>&#xA;"
47567587,47566892,5440084,2017-11-30T06:58:01,"<p>Well, it's a matter of choice. I've seen some services which use embedded tomcat in production and some services which rely on standalone tomcat container which is directly provided by the platform. Some platforms might not have a java runtime, in such scenarios you're bound to use embedded containers for tomcat or jetty.&#xA;The key difference would be that standalone containers can host multiple java deployments whereas for single deployments you can stick to the embedded ones. And yeah reliability and performance won't be a huge concern, although I believe that standalone containers are designed to be more scalable. From my personal experience, embedded deployments are easier to manage, since we can custom configure the tomcat setting specific to that deployment (might be the answer to your last question)</p>&#xA;"
45424588,45424413,3018068,2017-07-31T20:17:39,"<p>The ""best"" way is kind of a vague question. </p>&#xA;&#xA;<p>If you just want access to Postgres, you can use a package like <a href=""https://www.npmjs.com/package/pg"" rel=""nofollow noreferrer"">pg</a>.</p>&#xA;&#xA;<p>If you want a little more flexibility in your queries, I'd suggest <a href=""http://knexjs.org"" rel=""nofollow noreferrer"">KnexJS</a>. That's a fun way to write a few queries. </p>&#xA;&#xA;<p>A stored procedure might be ""fast"" or whatever you're looking for it to be, but if you don't know your domain for the microservice, you'll be spending a lot of time updating that stored procedure and your nodejs package won't matter that much. </p>&#xA;&#xA;<p>Lots of unknowns here, but I'd suggest just using knex (or an ORM if that kind of modeling will help you) until you reach millions of users, and then see if optimizing your microservice with stored functions is the right path forward at that time. You might have a few other concerns by then : )</p>&#xA;"
44554715,44554638,2313300,2017-06-14T21:04:55,"<p>One solution is endpoint versioning. Assuming your microservices are REST APIs, you can instruct your teams to increment versions of API endpoints if any change they would make to that endpoint would break it. Follow a cycle of deprecation and after 6 months remove support for the older endpoints. It gives you time to switch over to the newer versions and not break anything relying on the older versions.</p>&#xA;&#xA;<p>Another solutions is, instead of microservices talking to microservices, have an orchestration layer. That is, some manager of microservices which abstracts away any microservice to microservice communication. The applications which would be consumers of microservices then become consumers of the orchestration layer, which determines which microservice api's need to be called. This still ends up causing broken contract issues if the API contract is changed, but you can fix it at the orchestration layer where it is all centralized instead of coordinating cross team microservice changes.</p>&#xA;"
37154530,37143689,150868,2016-05-11T06:22:33,"<p>This is because you upgraded the JHipster Registry to the 2.0.0 version. This new version is secured, and that's why you have 401 ""unauthenticated"" errors, as you didn't provide any login/password to connect.</p>&#xA;&#xA;<p>3 solutions:</p>&#xA;&#xA;<ul>&#xA;<li>Stay with the older JHipster Registry version, that matches your JHipster version.</li>&#xA;<li>Add a login/password (by default it's admin/admin) to your Eureka and Spring Cloud Config URLs. The detailed documentation on securing the JHipster Registry is <a href=""http://jhipster.github.io/microservices-architecture/"" rel=""nofollow"">here</a> - it is incomplete at the time of this writing, but I will update it shortly</li>&#xA;<li>Update to JHipster v3.2.1, that is meant to be used with JHipster Registry v2.0.0, and where everything will be automatically configured.</li>&#xA;</ul>&#xA;"
39876332,39876079,150868,2016-10-05T14:12:40,"<p>2 questions in one...</p>&#xA;&#xA;<ol>&#xA;<li><p>It really depends on your configuration, version number, etc... So it's hard to tell, but if your microservices get registered in the gateway (in the admin/gateway screen) then everything should be fine.</p></li>&#xA;<li><p>The idea is that you use the ""docker-compose"" subgenerator to run the registry, the services you are not working on, and maybe also the gateway. Then, you work normally (without Docker) on the services (and gateway) you want to work on. Gulp, Spring Boot devtools will work as usual, of course. And those services should register in the registry, allowing you to have the full architecture working. So basically we put inside Docker everything you are not working on, and then you keep working normally for the services you want to develop.</p></li>&#xA;</ol>&#xA;"
51696113,51682257,10178900,2018-08-05T16:19:15,"<p>It is not running on any random port. Port number is specified in the project properties. When you are debugging in visual studio, you can set the port number by going to project properties -> debug -> App URL. &#xA;When you deploy the web api on IIS Server, it will run on the port you have specified when configuring the web api.&#xA;Now when you know the web api (with port number) in advance, you can hard-code the url or put it in app settings file of your react app. Hope this helps.</p>&#xA;"
51943094,51939287,1659476,2018-08-21T06:49:20,"<p>In theory, you could indeed deploy each of your microservices as a completely separate Heroku app, as you suggested. </p>&#xA;&#xA;<p>However, depending on your requirements, a MUCH simpler, possibly better and almost definitely cheaper approach may be to deploy them as separate microservices in one <a href=""https://www.heroku.com/languages"" rel=""nofollow noreferrer"">polyglot Heroku app</a>, using <a href=""https://www.heroku.com/dynos"" rel=""nofollow noreferrer"">Heroku dynos</a>.</p>&#xA;&#xA;<p>For example, you could run your Rails API as the <a href=""https://devcenter.heroku.com/articles/dynos#dyno-configurations"" rel=""nofollow noreferrer"">web dyno</a> of your single app. In that case you might want it to also serve your front-end framework.</p>&#xA;&#xA;<p>You should consider using <a href=""https://www.heroku.com/postgres"" rel=""nofollow noreferrer"">Heroku Postgres</a> as a managed <a href=""https://www.techopedia.com/definition/29431/database-as-a-service-dbaas"" rel=""nofollow noreferrer"">DBaaS</a>. It will be a breeze to <a href=""https://devcenter.heroku.com/articles/heroku-postgresql#connecting-in-rails"" rel=""nofollow noreferrer"">connect your Rails web dyno to your Heroku Postgres instance</a>.</p>&#xA;&#xA;<p>You might then want to define each of your python scripts as a separate <a href=""https://devcenter.heroku.com/articles/procfile"" rel=""nofollow noreferrer"">process type in your Procfile</a>. You need to do so if you need them to be ""always on"". &#xA;Alternatively, depending on your requirements, you might want to consider using <a href=""https://devcenter.heroku.com/articles/one-off-dynos"" rel=""nofollow noreferrer"">one-off dynos</a> for your python scripts. &#xA;At any rate, your python scripts will run on separate dynos.</p>&#xA;&#xA;<p>Note that each of your process types can be <a href=""https://devcenter.heroku.com/articles/scaling"" rel=""nofollow noreferrer"">separately scaled</a>.</p>&#xA;&#xA;<p>One thing you need to consider is how your microservices interact (if you need them to do so). There are many ways to approach this, but note that only your Web dyno instances can listen on http/s traffic. See <a href=""https://devcenter.heroku.com/articles/background-jobs-queueing"" rel=""nofollow noreferrer"">here</a> for some ideas on this.  </p>&#xA;"
42838255,42836979,1659476,2017-03-16T15:24:27,"<p>You can only have 1 web process type. You can horizontally scale your web process to run on multiple dynos (""horizontal scalability""), however you will need to upgrade to at least standard-1x dyno types to do that (i.e. you can only run 1 web dyno instance if you are using free or hobby dyno types).</p>&#xA;&#xA;<p>However, in addition to your web process, you can instantiate multiple additional process types (e.g. ""worker"" processes). These will NOT be able to listen on HTTP/S requests from your clients, but can be used for offloading long-running jobs from your web process.</p>&#xA;&#xA;<p>So, if you map each of your 4-6 microservices to a different Process Type in your Procfile, and if your microservices are not themselves web servers, you might be able to make do with hobby dynos.</p>&#xA;"
51711364,51697673,9524052,2018-08-06T15:46:50,"<blockquote>&#xA;  <p>Is API gateway a microservice I built for my app that can&#xA;  automatically scale? Or is it already a built-in function of&#xA;  kubernetes?</p>&#xA;</blockquote>&#xA;&#xA;<p>Kubernetes does not have its own API-gateway service. It has an Ingress controller, which operates as a reverse proxy and exposes Kubernetes resources to the outside world. And Services, which load-balance traffic between Pods linked to them.</p>&#xA;&#xA;<p>Also, Kubernetes provides an auto-scaling according to the resources consumed by Pods, memory usage or CPU utilization and some custom metrics. It is called Horizontal Pod Autoscaler, and you can read more about it <a href=""https://medium.com/google-cloud/kubernetes-horizontal-pod-scaling-190e95c258f5"" rel=""nofollow noreferrer"">here</a> or in the <a href=""https://kubernetes.io/v1.1/docs/user-guide/horizontal-pod-autoscaler.html"" rel=""nofollow noreferrer"">official documentation</a>. </p>&#xA;&#xA;<blockquote>&#xA;  <p>Service Discovery: Is this a problem with kubernetes at all? Does kubernetes automatically figure out this for you?</p>&#xA;</blockquote>&#xA;&#xA;<p>Service Discovery is not a problem in Kubernetes, it has an entity called Services responsible for this. For more information, you can look through the <a href=""https://kubernetes.io/docs/tutorials/services/"" rel=""nofollow noreferrer"">link</a>.</p>&#xA;&#xA;<p>Your other questions refer more to the architecture of your application.</p>&#xA;"
52037386,51976057,6444628,2018-08-27T10:32:57,"<p>In MySQL there are more than one way to achieve this. </p>&#xA;&#xA;<p>The method you choose depends on many variables, such your actual pagination method (whether you just want to have ""previous"" and ""next"" buttons, or actually want to provide range from 1...n, where n is total number of matching records divided by your your per page records count); also on database design, planned growth, partitioning and/or sharding, current and predicted database load, possible hard query limits (for example if you have years worth of records, you might require end user to choose a reasonable time range for the query (last month, last 3 months, last year, and so on...), so they don't overload the database with unrestricted and too broad queries, etc..&#xA;<hr>&#xA;To paginate:&#xA;<br></p>&#xA;&#xA;<ul>&#xA;<li>using simple <strong><em>previous</em></strong> and <strong><em>next</em></strong> buttons, you&#xA;can use the simple <strong>LIMIT [<em>START_AT_RECORD</em>,]&#xA;<em>NUMBER_OF_RECORDS</em></strong> <a href=""https://stackoverflow.com/a/52013836/6444628"">method</a>, as <a href=""https://stackoverflow.com/users/1766831/rick-james"">Rick&#xA;James</a> proposed&#xA;above.</li>&#xA;<li>using (all) page numbers, you need to know the number of matching&#xA;records, so based on your page size you'd know how many total pages&#xA;there'd be.</li>&#xA;<li>using a mix of two methods above. For example you could present a few clickable page numbers (previous/next 5 for example), as well as <em>first</em> and <em>last</em> links/buttons.</li>&#xA;</ul>&#xA;&#xA;<p>If you choose one of the last two options, you'd definitely need to know the total number of found records. </p>&#xA;&#xA;<p>As I said above, there is more than one way to achieve the same goal. The choice must be made depending on the circumstances. Below I'm describing couple simpler ideas:</p>&#xA;&#xA;<ul>&#xA;<li><p>FIRST:<br>&#xA;If your solution is session based, and you can persist the session, then you can use a <a href=""https://dev.mysql.com/doc/refman/5.6/en/create-temporary-table.html"" rel=""nofollow noreferrer""><strong>temporary table</strong></a> into which you could select only <em>order_id</em> (assuming it's a primary key in the orders table). Optionally, if you want to get the counts (or otherwise filter) per customer, you can also add the second column as <em>customer_id</em> next to <em>order_id</em> from the orders table.<br><br> Once you have propagated the temporary table with minimum data, you can just easily count rows from the temporary table and create your pagination based on that number. &#xA;Now as you start displaying pages, you only select the subset of these rows (using the <strong>LIMIT</strong> method above), and <em>join</em> the corresponding records (the rest of the columns) from <em>orders</em> on temporary table <em>order_id</em>.<br><br>&#xA;This has two benefits: <br>1) Browsing records page-by-page would be fast, as it's not querying the (presumably) large orders table any more.<br>2)You're not using aggregate queries on the orders table, as depending on the number of records, and the design, these would have pretty bad performance, as well as potentially impacting the performance of other concurrent users.&#xA;<br><br>Just bear in mind that the initial temporary table creation would be a bit slower query. But it'd definitely be even more slower if you didn't restrict temporary table to only essential columns.<br>Still, it's really advisable that you set a reasonable maximum hard limit (number of temporary table records, or some time range) for the initial query<br><br></p></li>&#xA;<li><p>SECOND:&#xA;This is my favourite, as with this method I've been able to solve customers' huge database (or specific queries) performance issues on more than one occasion. And we're talking about going from 50-55 sec query time down to milliseconds. This method is especially immune to database scalability related slow downs.&#xA;<br><br>&#xA;The main idea is that you can pre-calculate all kinds of aggregates (be that cumulative sum of products, or number of orders per customer, etc...). For that you can create an additional table to hold the aggregates (count of orders per customer in your example). &#xA;<br><br>And now comes the most important part:<br>&#xA;You <strong>must</strong> use custom database <a href=""https://dev.mysql.com/doc/refman/5.6/en/trigger-syntax.html"" rel=""nofollow noreferrer"">triggers</a>, namely in your case you can use <strong><em>ON INSERT</em></strong> and <strong><em>ON DELETE</em></strong> triggers, which would update the aggregates table and would increase/decrease the order count for the specific customer, depending on whether an order was added/deleted. Triggers can fire either before or after the triggering table change, depending on how you set them up. <br>&#xA;Triggers have virtually no overhead on the database, as they only fire quickly once per (inserted/deleted) record (unless you do something stupid and for example run <strong>COUNT(...)</strong> query from some big table, which would completely defeat the purpose anyway)<br><br>I usually go even more granular, by having counts/sums per customer per month, etc... &#xA;<br><br>&#xA;When done properly it's virtually impossible for aggregate counts to go out of sync with the actual records. If you application enables order's customer_id change, you might also need to add <strong><em>ON UPDATE</em></strong> trigger, so the customer id change for order would automatically get reflected in the aggregates table.&#xA;<br><br>&#xA;Of course there are many more ways you can go with this. But these two above have proven to be great. It's all depending on the circumstances...&#xA;<br><br>&#xA;I'm hoping that my somewhat abstract answer can lead you on the right path, as I could only answer based on the little information your question presented...</p></li>&#xA;</ul>&#xA;"
45169030,45168622,2851412,2017-07-18T14:01:32,"<ol>&#xA;<li>write a gem and include it in the different services.</li>&#xA;<li>if they run on the same machines, another possibility is env vars.</li>&#xA;<li>Keep it in a DB/storage, i.e. memchache/redi. Load the relevant constants on each service init.</li>&#xA;</ol>&#xA;"
50936614,50935969,1234260,2018-06-19T20:40:52,"<p>Application clustering is different to database clustering. You cannot ""divide"" data among the 5 instances of application services since all application instances require a similar set of data to function (unless your application is designed to work on a subset of the data, i.e. each application instance is used to serve a specific list of countries, then you might be able to break the data up by country). </p>&#xA;&#xA;<p>You can look into clustering at the database level for ideas on how you can cluster at the SQL level: <a href=""https://www.brentozar.com/archive/2012/02/introduction-sql-server-clusters/"" rel=""nofollow noreferrer"">https://www.brentozar.com/archive/2012/02/introduction-sql-server-clusters/</a> .</p>&#xA;"
39173025,39172786,1375316,2016-08-26T18:55:48,"<p>Depending on how complicated your service is, you could build it directly off of alpine.  An alpine node box at its most basic looks like this:</p>&#xA;&#xA;<pre><code>FROM alpine:latest&#xA;RUN apk update &amp;&amp; apk add nodejs &amp;&amp; rm -rf /var/cache/apk/*&#xA;</code></pre>&#xA;&#xA;<p>This image is less than 25 MB.  It also installs <code>npm</code> of course, so you can install other dependencies or just mount <code>package.json</code> into the working directory, or however else you might want to handle that.</p>&#xA;"
39475510,39044130,64852,2016-09-13T17:10:01,"<p>I am active within the MicroProfile community on a daily basis (and primary author of the <a href=""http://microprofile.io/faq"" rel=""noreferrer"">FAQ</a> on MicroProfile.io). As Gimby mentions, check out the FAQ.</p>&#xA;&#xA;<p>Basically, the <a href=""http://jcp.org"" rel=""noreferrer"">JCP</a> is a standards organization and doing innovation using a process designed for standardizing technology is a no-no. The idea of MicroProfile is to do rapid innovation with Enterprise Java and Microservices that leads to technology standardization. To James' point, yes, it is getting off the ground using Java EE technologies so it will feel more natural for Java EE developers.</p>&#xA;&#xA;<p>Going forward, however, MicroProfile will not be limited to Java EE technologies. It may adopt other non-Java EE frameworks or even create new APIs. Once features/technologies are mature enough, we'll look to standardize them. If you like the idea of standards around Java microservices and want to participate in innovating-before-standardizing, then join the <a href=""https://groups.google.com/forum/#!forum/microprofile"" rel=""noreferrer"">MicroProfile google group</a> and feel free to get involved in doing so. You can have direct input and even contribute code that will eventually be part of a standard.</p>&#xA;&#xA;<p>Hope this helps.</p>&#xA;"
43701003,43700905,657237,2017-04-29T22:18:03,"<p>DHCP solves this problem. When a host boots it sends a broadcast DHCP message. The DHCP server responds with many values, one of which is the location of DNS servers. </p>&#xA;&#xA;<p>In the case of micro services, the host OS (or container host) will be configured for DNS via DHCP. The microservice code uses the OS DNS functions to resolve addresses.</p>&#xA;&#xA;<p><a href=""https://en.m.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol"" rel=""nofollow noreferrer"">https://en.m.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol</a></p>&#xA;"
47258699,47258026,5311317,2017-11-13T07:01:45,"<p>@wizard I would share here something .</p>&#xA;&#xA;<p>If you have complex calculation or any type of calculation do it in db end if feasible. </p>&#xA;&#xA;<p>The best approach&#xA;<code>postgres</code></p>&#xA;&#xA;<p>to do this type of process is to make function (PL\SQL) block . If you are using postgres create all calculation in postgres .</p>&#xA;&#xA;<p>This is why node.js was design , no calculation in node.js end . </p>&#xA;&#xA;<p><code>mongodb</code></p>&#xA;&#xA;<p>if you are working with mongodb with this approach then i will recommend you to go with</p>&#xA;&#xA;<p><code>Stored JavaScript in MongoDB</code></p>&#xA;"
48598364,47050984,1849429,2018-02-03T14:20:32,"<p>It is important to that you also use <code>$request-&gt;session()</code>, otherwise it will not work. </p>&#xA;"
33135799,33125508,1408085,2015-10-14T21:21:06,"<p>Service Discovery component usually has a notification component. It is not a load balancer eventhough some might have the capability to do so. It can notify registered clients about changes, for example a loadbalancer going down.</p>&#xA;&#xA;<p>A client can query a service discovery/registry to get a load balancer that is running. Whereas a load balancer does not noitfy a client when it is down.</p>&#xA;"
51200539,41036330,1213525,2018-07-05T22:11:13,"<p>I don't know your exact use case but you could consider taking a look at Hashicorp Vault. It has a <a href=""https://www.vaultproject.io/docs/secrets/pki/index.html"" rel=""nofollow noreferrer"">PKI secrets plugin</a> which lets you storing public/private key pairs. If it fits your use case this might be a good read, as at least someone else has thought about storing it securely.</p>&#xA;&#xA;<p>The nice thing about vault is that in theory (this depends on the plugin that's being used) you could create a situation where the private keys are completely managed by vault and your messages are being authenticated with the vault. This will make it hard for an attacker to get your private keys. However, you might end up needing to write your own vault plugin to get this completely working with your application.</p>&#xA;"
41464518,41127782,307797,2017-01-04T13:02:46,"<p>This is a current, known limitation of composite-builds. It is discussed in <a href=""https://github.com/gradle/gradle/issues/947"" rel=""nofollow noreferrer"">this topic</a>.</p>&#xA;"
45497067,45489456,1754709,2017-08-04T01:48:27,"<p>To share a simplified version of my experience with this. To consider if running dependencies (<code>B</code>, <code>C</code> and <code>D</code>) on a remote docker engine is even worth the hassle, one of the following must (normally) be true:</p>&#xA;&#xA;<ul>&#xA;<li>The amount of resources used by the depending services is not practical to run on a single developers computer.</li>&#xA;<li>Initializing data for the depending services is too much of a hassle because of the size or other factors that makes it too burdensome to run locally.</li>&#xA;<li>Data used by depending services raises privacy concerns</li>&#xA;</ul>&#xA;&#xA;<p>What you potentially lose by using the remote approach is a bit more scary. </p>&#xA;&#xA;<ul>&#xA;<li>Developers cannot easily control what version of the depending services are running. This is especially important if these services are custom so they can downgrade or upgrade versions as issues are discovered or fixed. </li>&#xA;<li>It can also be an issue during the transition to newer versions of 3rd party services as not all developers may be working on a branch that support it.</li>&#xA;<li>Also, not having the mobility to quickly jump back to older branches/releases to fix or expore an issue and then integrate/test that into the current branch can be really frustrating when you need it the most (cases when time is an issue!)</li>&#xA;</ul>&#xA;&#xA;<p>There are many other points to add to these lists, but these were the important ones for us.</p>&#xA;&#xA;<p><strong>We ended up with a hybrid approach.</strong></p>&#xA;&#xA;<p>Developers will run everything locally for most tasks. We trimmed down the data needed for the depending services for local development so they could spin up locally in a couple of minutes. Making a dev environment fully ""offline"" is such a huge advantage. If a centralised system breaks down your developers are quickly reduced to a horde of zombies roaming the building during the downtime. They also have the ability to crank up their laptop on the train home and debug some weird issue if needed, then commit that and let the CI system chew through tests and whatnot while they move on with their personal lives.</p>&#xA;&#xA;<p>In addition we started some VMs with docker engine running the depending services. These have (mostly) <code>live</code> and <code>dev</code> name prefixes (and others if needed) and contain snapshots from the live environment. Developers can use a separate compose setup for these if needed. This can be practical when devs are trying to pin down issue caused by bad data or code that just scale badly with larger data sets.</p>&#xA;&#xA;<p>What never changes is that <code>A</code> will <strong>always run on the developers computer</strong>. If someone for some reason have other needs, we spin up a new VM with docker engine, some data snaphots(s) and the depending services. This is a <strong>fully automated process</strong>, so this requires a well-established and efficient pipeline. If I chose to start a personal setup, the host name could have a prefix with my username.</p>&#xA;&#xA;<p>I'd say if developers can run everything locally, then save yourself from a lot of work and do just that. Find smart ways to make all depending services run smoothly within a couple of minutes.</p>&#xA;&#xA;<p><strong>Data dependencies and privacy concerns</strong></p>&#xA;&#xA;<p>I'll inject this point here as well since way too many have neglected this part.</p>&#xA;&#xA;<p>Now that GDPR and Privacy Shield will most likely put even more pressure on privacy concerns in 2018 (at least of you store data about EU citizens) your company will have to take this seriously or possibly face huge fines and/or customers abandoning you. This adds a bit more work.</p>&#xA;&#xA;<ul>&#xA;<li>All images for the local services contains generated data or a transformed subset of live data that cannot possibly be used to identify an individual.</li>&#xA;<li>The remote <code>dev</code> and <code>live</code> hosts also contain transformed data to hide identities, but simplified a bit to greatly speed up the process.</li>&#xA;<li>Only a small subset of the developers have access to the live system and these are also the only ones that is allowed to run their own VM with live data (they are given a unique client certificate for that host).</li>&#xA;</ul>&#xA;&#xA;<p>Developers travel with their laptops on a daily basis and even bring them home. No data containing any form of information about an individual will ever end up on a developers laptop.</p>&#xA;"
48465228,48464383,262742,2018-01-26T16:13:41,"<p>Definitely you could do that, especially in a small project or demo project. </p>&#xA;&#xA;<p>Let's assume you are doing web application.&#xA;For complex enterprise applications, most time you would like to expose certain fields and certain format of data, with one more attraction of data object, you can avoid change entity fields and database table fields. </p>&#xA;"
44720595,36157778,3410518,2017-06-23T11:40:36,"<p>In my case, none of the above worked. Or more precisely, my solution was a combination/variation of the above solutions</p>&#xA;&#xA;<p>I needed to do 2 things: First, make sure I have registered my services: </p>&#xA;&#xA;<pre><code>  protected override IEnumerable&lt;ServiceReplicaListener&gt; CreateServiceReplicaListeners()&#xA;    {&#xA;        return new[] { new ServiceReplicaListener(context =&gt; this.CreateServiceRemotingListener(context)) };}&#xA;</code></pre>&#xA;&#xA;<p>And second, refer to the very first service (""magic"" value of 0)</p>&#xA;&#xA;<pre><code>        ServiceProxy.Create&lt;IMasterDataMService&gt;(new Uri(""fabric:/DataServiceFabric/MasterDataMService""), new ServicePartitionKey(0));&#xA;</code></pre>&#xA;&#xA;<p>I hope this helps</p>&#xA;"
27845584,27839789,528726,2015-01-08T17:02:07,"<blockquote>&#xA;  <p>What is the simplest way I can change this setup to allow for parallel processing?</p>&#xA;</blockquote>&#xA;&#xA;<p>If you can upgrade the code on the server, or add middle-man code, then the simplest way is a queue.</p>&#xA;&#xA;<p>If you prefer just client-side, with no middle-man and no client-to-client talk, and some occasional redundancy is ok, then here are some ideas.</p>&#xA;&#xA;<ol>&#xA;<li><p>Reduce collisions by using shuffle </p>&#xA;&#xA;<ul>&#xA;<li>If it's ok for your server to receive a DELETE for a non-existent object</li>&#xA;<li>And the ""process item"" cost+time is relatively small</li>&#xA;<li>And the process is order-independent</li>&#xA;<li><p>Then you could shuffle the items to reduce collisions:</p>&#xA;&#xA;<pre><code>items.shuffle.each do |item|&#xA;  process item&#xA;</code></pre></li>&#xA;</ul></li>&#xA;<li><p>Check that the item exists by using HEAD</p>&#xA;&#xA;<ul>&#xA;<li>If your server has the HEAD method</li>&#xA;<li>And has a way to look up one item</li>&#xA;<li>And the HTTP connection is cheap+fast compared to ""process item""</li>&#xA;<li><p>Then you could skip the item if it doesn't exists:</p>&#xA;&#xA;<pre><code>items.each do |item|&#xA;  next if !&lt;HEAD provider/items/id&gt;&#xA;</code></pre></li>&#xA;</ul></li>&#xA;<li><p>Refresh the items by using a polling loop</p>&#xA;&#xA;<ul>&#xA;<li>If the items are akin to you polling an ongoing pool of work</li>&#xA;<li>And are order independent</li>&#xA;<li>And the GET request is idempotent, i.e. it's ok to request all the items more than once</li>&#xA;<li>And the DELETE request returns a result that informs you the item did not exist</li>&#xA;<li><p>Then you could process items until you hit a redundancy, then refresh the items list:</p>&#xA;&#xA;<pre><code>loop do&#xA;  items = &lt;GET provider/items&gt;&#xA;  if items.blank?&#xA;    sleep 1&#xA;    next&#xA;  end&#xA;  items.each do |item|&#xA;    process item&#xA;    &lt;DELETE provider/items/#{item.id}&gt;&#xA;    break if DELETE returns a code that indicates ""already deleted""&#xA;  end&#xA;end&#xA;</code></pre></li>&#xA;</ul></li>&#xA;<li><p>All of the above combined using a polling loop, shuffle, and HEAD check.</p>&#xA;&#xA;<ul>&#xA;<li>This is surprisingly efficient, given no queue, nor middle-man, nor client-to-client talk.</li>&#xA;<li><p>There's still a rare redundant ""process item"" that can happen when multiple clients check if an item exists then start processing it; in practice this is near-zero probability, especially when there are many items.</p>&#xA;&#xA;<pre><code>loop do&#xA;  items = &lt;GET provider/items&gt;&#xA;  if items.blank?&#xA;    sleep 1 &#xA;    next&#xA;  end&#xA;  items.shuffle do |item|&#xA;    break if !&lt;HEAD provider/items/id&gt;&#xA;    process item&#xA;    &lt;DELETE provider/items/#{item.id}&gt;&#xA;    break if DELETE returns a code that indicates ""already deleted""&#xA;  end&#xA;end&#xA;</code></pre></li>&#xA;</ul></li>&#xA;</ol>&#xA;"
36543127,36541906,472150,2016-04-11T08:08:53,"<p>Have you already looked at projects like <a href=""https://zookeeper.apache.org/"" rel=""noreferrer"">ZooKeeper</a>, <a href=""https://coreos.com/etcd/"" rel=""noreferrer"">etcd</a> or <a href=""https://www.consul.io/"" rel=""noreferrer"">Consul</a>? These can provide facilities to manage your configuration settings and service discovery.</p>&#xA;"
47258433,47258026,1390678,2017-11-13T06:41:28,"<p>On can do calulations in (datamodel / class) in javascript </p>&#xA;&#xA;<p>1 : you can create a javascript class</p>&#xA;&#xA;<pre><code>   function sales(){&#xA;    let self = this&#xA;    this.address = '';&#xA;    this.personname = '';&#xA;    this.totalVat = 0&#xA;    this.items = [];&#xA;    this.calculateTax = function(){&#xA;       let tax = ...compute items array and set &#xA;       self.totalVat = tax&#xA;     }   &#xA;   }&#xA;</code></pre>&#xA;&#xA;<h2>Note :</h2>&#xA;&#xA;<p><strong>you can use this data model if you are going to use it only on ui</strong></p>&#xA;&#xA;<p>else if </p>&#xA;&#xA;<p><strong>you want to use it on server then you can use same datamodel on server</strong></p>&#xA;&#xA;<p>esle if</p>&#xA;&#xA;<p><strong>you want to use on both server and ui then</strong></p>&#xA;&#xA;<p>(you can maintain only 1 copy of this model shared by server and ui , here you can use require.js bundling to take help of sharing one datamodel , also there are nice articles that show how to share js datamodels)</p>&#xA;&#xA;<p><strong>in Case Of (you want to use on both server and ui then)</strong></p>&#xA;&#xA;<p>You mentioned database , you create mechanism to store formula in database and you can generate sales_datamodel.js on server whenever any changes are done in datamodel in db</p>&#xA;"
39998895,39990666,808278,2016-10-12T12:41:59,"<p>Greg Young's <a href=""https://geteventstore.com/"" rel=""nofollow"">Event Store</a> appears to fit your criteria.</p>&#xA;&#xA;<ul>&#xA;<li>Stores your data as a series of immutable events over time.</li>&#xA;<li>Claims to be benchmarked at 15,000 writes per second and 50,000 reads per second.</li>&#xA;</ul>&#xA;"
46444025,46321751,2906198,2017-09-27T09:19:11,"<p>you can easily use two methods to secure your model.</p>&#xA;&#xA;<p>1.From model.json file</p>&#xA;&#xA;<p>inside your model model.json you can include ACL objects.</p>&#xA;&#xA;<pre><code> ""acls"": [{&#xA;    ""accessType"": ""EXECUTE"",&#xA;    ""principalType"": ""ROLE"",&#xA;    ""principalId"": ""$authenticated"",&#xA;    ""permission"": ""ALLOW""&#xA;  }, {&#xA;    ""accessType"": ""*"",&#xA;    ""principalType"": ""ROLE"",&#xA;    ""principalId"": ""$everyone"",&#xA;    ""permission"": ""DENY""&#xA;  }]&#xA;</code></pre>&#xA;&#xA;<p>2.Using operational hooks in model.js file&#xA; you can use operational hooks to manipulate security.</p>&#xA;&#xA;<p><em>example</em> </p>&#xA;&#xA;<pre><code> Template.observe('access', function (ctx, next) {&#xA;    if(ctx.options.team){&#xA;      var teamId = ctx.options.team.teamId;&#xA;     ctx.query.where= ctx.query.where  || {or :[ {user_created : 0},{teamId : teamId}]}   ;&#xA;    next();&#xA;    }&#xA;&#xA;  });&#xA;</code></pre>&#xA;&#xA;<p>hope this will be helpful.</p>&#xA;"
42836701,40448015,1270870,2017-03-16T14:20:02,"<p>Your design seems OK. We are also building our microservice project using API Gateway approach. All the services including the Gateway service(GW) are <strong>containerized(we use docker)</strong> Java applications(<strong>spring boot</strong> or <strong>dropwizard</strong>). Similar architecture could be built using nodejs as well. Some topics to mention related with your question: </p>&#xA;&#xA;<ul>&#xA;<li><strong>Authentication/Authorization:</strong>  The GW service is the single entry point for the clients. All the authentication/authorization operations are handled in the GW using <strong>JSON web tokens(JWT)</strong> which has nodejs libray as well. We keep authorization information like user's roles in the JWT token. Once the token is generated in the GW and returned to client, at each request the client sends the token in HTTP header then we check the token whether the client has the required role to call the specific service or the token has expired. In this approach, you don't need to keep track user's session in the server side. Actually there is no session. The required information is in the JWT token.</li>&#xA;<li><strong>Service Discovery/ Load balance:</strong> We use <strong>docker</strong>, <strong>docker swarm</strong> which is a docker engine clustering tool bundled in docker engine (after docker v.12.1). Our services are docker containers. Containerized approach using docker makes it easy to deploy, maintain and scale the services. At the beginning of the project, we used <strong>Haproxy, Registrator</strong> and <strong>Consul</strong> together to implement service discovery and load balancing, similar to your drawing. Then we realized, we don't need them for service discovery and load balancing as long as we create a <strong>docker network</strong> and deploy our services using <strong>docker swarm</strong>. With this approach you can easily create isolated environments for your services like <em>dev,beta,prod</em> in one or multiple machines by creating different networks for each environment. Once you create the network and deploy services, service discovery and load balancing is not your concern. In  same docker network, each container has the DNS records of other containers and can communicate with them. With docker swarm, you can easily scale services, with one command. At each request to a service, docker distributes(load balances) the request to a instance of the service.</li>&#xA;</ul>&#xA;"
42837345,42061684,1270870,2017-03-16T14:45:53,"<p>I think you are in the wrong direction by using a big application server do deploy each service, if I got your point correct. </p>&#xA;&#xA;<p>In microservice approach, it is better to keep services small in size and use embedded servers, like embedded tomcat or jetty. </p>&#xA;"
42905139,42899966,1270870,2017-03-20T13:41:16,"<p>It is better to keep all services outside of the world(<strong>isolation on network level</strong>) except one service which is the <strong>single-entry point</strong> for your clients where authentication/authorization handled. So you only need to implement security related operations/validations on that service than let client's request pass through other services.</p>&#xA;&#xA;<p>Since you isolate your services from the world and <strong>no security</strong> implemented on rest of the services, you don't need token or validation for inter-service communication.</p>&#xA;"
40243833,37711051,96855,2016-10-25T15:23:22,"<p>Not your typical CRUD app but <a href=""https://github.com/deis/workflow"" rel=""nofollow"">Deis</a> (a PaaS) uses REST APIs mostly to communicate between services. <a href=""https://github.com/peatio/peatio"" rel=""nofollow"">Peatio</a> has a bunch of services that communicate asynchronously through a message queue.</p>&#xA;"
47641885,42499778,4560215,2017-12-04T21:01:04,"<p>First, you’ll want to create a directory where you can hold the docker compose yaml file and the ssl certificates. </p>&#xA;&#xA;<pre><code>├── docker-compose.yml&#xA;└── ssl&#xA;    ├── cert.key&#xA;    └── cert.pem&#xA;</code></pre>&#xA;&#xA;<p>In the docker-compose.yml mount the volumes inside the container something like this and copy the cert:</p>&#xA;&#xA;<pre><code>version: '2'&#xA;&#xA;services:&#xA;  image: hub:application-service&#xA;    volumes:&#xA;      - ./ssl:/etc/ssl&#xA;</code></pre>&#xA;"
36535745,36519652,6997335,2016-04-10T21:26:30,"<p>There is the caveat consultants answer of 'it depends'. Everyone has a different description of what microservices are, but for me they are small independently deployable services that do one thing well (i generally subscribe to <a href=""http://martinfowler.com/articles/microservices.html"" rel=""noreferrer"">fowlers definition of microsevices</a>).</p>&#xA;&#xA;<p>Actually I don't have a problem with microservices sharing storage (be it databases, blob storage etc.) as long as the microservices fall within the same 'service' boundary. What i mean by service boundary is a logical grouping of small services all collaborating to achieve some goal or business capability. For me its this logical grouping which forms the 'service' as viewed from outside the team. One service 'the user service' is often made up of many small microservices. </p>&#xA;&#xA;<p>For example you might have a 'User' service that consists of a number of microservices. One to capture interest from a website and store it in a DB, one used by the website to process address changes, one to run a timed job and issue events to other services in the system. </p>&#xA;&#xA;<p>But you would not want the Document Generation (reporting service?) reading and writing from the User DB, in this case (like Stavreva said) event communication between services would be the way to go.</p>&#xA;&#xA;<p>Only you with your business domain knowledge can decide if two microservices belong within this same service, one tip is to look at the business capability that the microservice is contributing towards.</p>&#xA;"
43144691,43100199,7655255,2017-03-31T15:42:20,"<p>Your User Location Service is registred in Zuul. So, all the times your filter is going to be executed when you got caugth in an infinite loop.</p>&#xA;&#xA;<p>There are two approaches here:</p>&#xA;&#xA;<ul>&#xA;<li>Do not pass again to the gateway and call direct User Location Service</li>&#xA;<li>OR, Create a filter rule in your ZUll Authentication Filter to not consider the User Location Service. You can use the URL Path Context to exclude this route to be executed</li>&#xA;</ul>&#xA;"
50723336,50718353,7655255,2018-06-06T14:48:07,"<p>If you are using express to implement your API backend server you can use the express-winston to intercept the request and response of each call.</p>&#xA;&#xA;<p>In the example bellow a use the express winston to log out the request and response into the console.</p>&#xA;&#xA;<pre><code>const winston = require('winston');&#xA;const expressWinston = require('express-winston');&#xA;&#xA;/**&#xA; * Custom Wiston Express Middleware to log all requests and responses in the console.&#xA; */&#xA;module.exports = async() =&gt; {&#xA;    // Creating middleware&#xA;    expressWinston.requestWhitelist.push('body');&#xA;    expressWinston.responseWhitelist.push('body');&#xA;    const wistonMiddleware = expressWinston.logger({&#xA;        transports: [&#xA;            new winston.transports.Console({&#xA;                json: true,&#xA;                colorize: true&#xA;            })&#xA;        ],&#xA;        // optional: control whether you want to log the meta data about the request (default to true)&#xA;        meta: true,&#xA;        // optional: customize the default logging message. E.g. ""{{res.statusCode}} {{req.method}} {{res.responseTime}}ms {{req.url}}""&#xA;        msg: 'HTTP {{req.method}} {{req.url}}',&#xA;        // Use the default Express/morgan request formatting, with the same colors.&#xA;        // Enabling this will override any msg and colorStatus if true. Will only output colors on transports with colorize set to true&#xA;        expressFormat: true,&#xA;        // Color the status code, using the Express/morgan color palette (default green, 3XX cyan, 4XX yellow, 5XX red). Will not be recognized if expressFormat is true&#xA;        colorStatus: true,&#xA;        ignoreRoute: function (req, res) {&#xA;            return false;&#xA;        } // optional: allows to skip some log messages based on request and/or response&#xA;    });&#xA;&#xA;    return wistonMiddleware;&#xA;};&#xA;</code></pre>&#xA;&#xA;<p>But, you can you other winston modules to log in a different way like to elastic search: <a href=""https://github.com/vanthome/winston-elasticsearch"" rel=""nofollow noreferrer"">https://github.com/vanthome/winston-elasticsearch</a></p>&#xA;"
42933650,42899966,7655255,2017-03-21T17:00:02,"<p>When we´re working with API security the most used protocol is OAuth 2.0. </p>&#xA;&#xA;<p>When dealing with client-service authentication/ authorization the most fit access token provisioning would be by Authorization Code Flow.</p>&#xA;&#xA;<p>However, when dealing with service-to-service authentication / authorization the most fit access token provisioning would be by JWT Authorization Flow. Is this case the the requestor need to genereate a JWT token using a private key.</p>&#xA;&#xA;<p>But if want to keep simple and the network restrictions are in place you could use a simple client credantials flow in OAuth.</p>&#xA;&#xA;<p><strong><em>There are a interresting topic about handle batch processing with OAuth 2.0 in the nordicapi blog:</em></strong></p>&#xA;&#xA;<p><a href=""http://nordicapis.com/how-to-handle-batch-processing-with-oauth-2-0/"" rel=""nofollow noreferrer"">http://nordicapis.com/how-to-handle-batch-processing-with-oauth-2-0/</a></p>&#xA;&#xA;<p>For more information check the links bellow:</p>&#xA;&#xA;<p><a href=""http://websystique.com/spring-security/secure-spring-rest-api-using-oauth2/"" rel=""nofollow noreferrer"">http://websystique.com/spring-security/secure-spring-rest-api-using-oauth2/</a></p>&#xA;&#xA;<p><a href=""http://blog.monkey.codes/how-to-use-jwt-and-oauth-with-spring-boot/"" rel=""nofollow noreferrer"">http://blog.monkey.codes/how-to-use-jwt-and-oauth-with-spring-boot/</a></p>&#xA;"
32955061,30800908,516439,2015-10-05T18:08:03,"<p>The Dropwizard manual gives code examples of <a href=""https://dropwizard.github.io/dropwizard/manual/client.html#man-client-jersey"" rel=""nofollow"">how to add configuration for and then build a Jersey client in your app</a>. </p>&#xA;&#xA;<p>The Jersey documentation itself has details (and code examples) of <a href=""https://jersey.java.net/documentation/latest/client.html#d0e4463"" rel=""nofollow"">how to use a Client to make a request</a>.</p>&#xA;&#xA;<p>As a slightly unrelated suggestion, I like to write a client for each service I write (as a separate maven module) that other libraries (and indeed other Dropwizard services) can then use to communicate with the service. That lets me encapsulate all the details of interacting with the service (e.g. how to construct paths, what classes to marshal the results as) in one place, so I can present a nice, simple POJO interface to the outside world. Note this means sharing model representations between the client and the service, pretty much as per <a href=""https://dropwizard.github.io/dropwizard/manual/core.html#organizing-your-project"" rel=""nofollow"">the suggestion in the Dropwizard docs</a>.</p>&#xA;&#xA;<p>An example client might looks something like the following:</p>&#xA;&#xA;<pre class=""lang-java prettyprint-override""><code>public class MyClient {&#xA;    private static final String RESOURCE_PATH = ""resource-path"";&#xA;    private static final DateTimeFormatter FORMATTER = ISODateTimeFormat.dateTimeNoMillis();&#xA;&#xA;    private final Client jerseyClient;&#xA;    private final String targetUrl;&#xA;&#xA;    public MyClient(Client jerseyClient, String targetUrl) {&#xA;        this.jerseyClient = jerseyClient;&#xA;        this.targetUrl = targetUrl;&#xA;    }&#xA;&#xA;    public List&lt;CustomModelClass&gt; getSomeResource(Interval someParam) {&#xA;        WebTarget webResource = jerseyClient.target(targetUrl).path(RESERVATIONS_PATH);&#xA;&#xA;        webResource = webResource.queryParam(""startTime"", someParam.getStart().toString(FORMATTER));&#xA;        webResource = webResource.queryParam(""endTime"", someParam.getEnd().toString(FORMATTER));&#xA;&#xA;        Invocation.Builder invocationBuilder = webResource.request(MediaType.APPLICATION_JSON_TYPE);&#xA;        Response response = invocationBuilder.get();&#xA;&#xA;        return response.readEntity(new GenericType&lt;List&lt;CustomModelClass&gt;&gt;(){});&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
39500385,39224930,1572439,2016-09-14T22:07:27,"<p>For start I want also to subscribe to the last part of the answer of Gaël:</p>&#xA;&#xA;<p><strong>think about why you're migrating?</strong></p>&#xA;&#xA;<p>Personally I am at the moment in a migration process. I start in 2015 a JHipster monolith app (at that time that was the only option :) ) which I still develop and add new features. For my monolith I decide to migrate to microservice because we gone increase the team and want to go with a DDD in the future. I must admit that there is some overhead at the begin and the learning curve is quite steep but in the end the results are very rewarding especially if you believe in CI (y)</p>&#xA;&#xA;<p>This is how I migrate my monolith:</p>&#xA;&#xA;<ol>&#xA;<li>be sure that you have all your sources commited and sync with your VSC (I use git as DVCS)</li>&#xA;<li>without any changes just run the jhipster generator and overwrite all the old sources</li>&#xA;<li>make a git diff to have an overview of the files that are generated from jhipster and which you have modified&#xA;&#xA;<ol>&#xA;<li>if you have not changed the format of the files that jhipster generates it should be just some files in webapp folder and configuration file</li>&#xA;<li>if you have differences only because of formatting I will recommend to check the code and then update your base code of your monolith app</li>&#xA;<li>the target is to have a few as possible differences when regenerating the the monolith app with the jhipster generator (is better to have fewer files to check when migrating to microservices)</li>&#xA;</ol></li>&#xA;<li>at this moment I imply that you are on clean workspace (i.e. all your changes are sync with the VCS) and if you will run a yo jhipster you will have as few as possible file to recheck manually</li>&#xA;<li>in the root folder of the app there is a <strong><em>.yo-rc.json</em></strong> file&#xA;&#xA;<ol>&#xA;<li>in that file you should change the <strong><em>applicationType</em></strong> from <strong><em>monolith</em></strong> to <strong><em>getaway</em></strong> and <strong><em>authenticationType</em></strong> from what you have to <strong><em>jwt</em></strong> e.g.</li>&#xA;</ol></li>&#xA;</ol>&#xA;&#xA;<p>.yo-rc.json</p>&#xA;&#xA;<pre><code>""jhipsterVersion"": ""3.5.1"",&#xA;""serverPort"": ""8080"",&#xA;""applicationType"": ""gateway"",&#xA;""jhiPrefix"": ""jhi"",&#xA;</code></pre>&#xA;&#xA;<ol start=""6"">&#xA;<li>after merging the new generated files you should have now the gateway of the microservice (it can be that you need to delete some classes depending on which authenticationType your monolith use to have)&#xA;&#xA;<ol>&#xA;<li>personally I am working now on moving some of the responsibilities(all the staff that the old monolith did) which exist in the gateway to migrate to sand alone microservices</li>&#xA;<li>the migration of the services mentioned in 6.1 is something that goes parallel with adding new features to the app and those will be added as new microservices</li>&#xA;</ol></li>&#xA;</ol>&#xA;&#xA;<p>My recommendation is to go in small steps/increments and it will be nice if you have a CI so that you can have asap also a feedback about your migration ;)</p>&#xA;&#xA;<p>Good luck.&#xA;Cheers, duderoot</p>&#xA;"
46464593,46464455,3084632,2017-09-28T08:36:42,"<p>Keep it simple. Do it at the firewall level</p>&#xA;&#xA;<p>Whitelist the IP(s) that you want to be able to make requests, reject the rest</p>&#xA;"
32396621,26331854,1319512,2015-09-04T11:08:42,"<p>Since the protocol of choice when working with microservices is HTTP, I would recommend .NET Web API for the API layer. Since microservices respect the 4 tenants of SOA, you will want to make sure that every service has its own data storage and that every ""service"" can be deployed independent. I would say that the most import aspect when setting up an architecture is related to the API contracts, because every service can have it's own implementation.</p>&#xA;&#xA;<p>.NET Web API + NServiceBusdo provide a nice foundation for an architecture based on microservices.</p>&#xA;"
38725271,38615132,1319512,2016-08-02T15:55:38,"<p>There's at least 3 types of things you will need to monitor: the host where the service is deployed, the component itself and the SLAs and some of them depend on the software stack you're using as well as the architecture. </p>&#xA;&#xA;<p>With that said, you could for example use <a href=""https://en.wikipedia.org/wiki/Nagios"" rel=""nofollow"">Nagios</a> to monitor the hardware where the services are deployed, <a href=""http://www.splunk.com/"" rel=""nofollow"">Splunk</a> for the services metrics/SLAs as well as for any errors that might occur. You can also use <a href=""https://en.wikipedia.org/wiki/Simple_Network_Management_Protocol"" rel=""nofollow"">SNMP</a> packages in case something goes wrong and you have a more sophisticated support structure, this would be yours triggers. Without knowing how your infrastructure/services are set up it is complicated to go into deeper details.</p>&#xA;"
50269392,50239555,1319512,2018-05-10T09:14:09,"<blockquote>&#xA;  <p>My question is: What's the boundary of shared items for microservices and how bad is that?</p>&#xA;</blockquote>&#xA;&#xA;<p>Up until a few years ago it was complicated to get the boundaries a microservice defined because there was simply no agreement on how to archieve that, but Evans sorted that out a few years ago:</p>&#xA;&#xA;<p><a href=""https://www.youtube.com/watch?v=yPvef9R3k-M"" rel=""nofollow noreferrer"">GOTO 2015 • DDD &amp; Microservices: At Last, Some Boundaries! • Eric Evans</a></p>&#xA;&#xA;<p>Microservices also follow the four tenants of SOA and the same 9 fallacies of distributed system are to take in consideration nevertheless their business scopes are different. Bear in mind that a microservice architecture should follow a Shared-nothing sort of architecture, so services don't really share entities, what they do is subscribe to messages, typically in a bus, and store local copies of the pieces of data they are interested in. This obviously introduce another concept called eventual consistency and depending on your business requirements,that might or might not if in your overall design.</p>&#xA;"
35379287,35379246,1393496,2016-02-13T11:30:08,"<p>Two potential options:</p>&#xA;&#xA;<p>Option 1 - Use a common codebase and deploy multiple instances of it (one for each microservice). You could follow 12factor Apps Methodology and just load a different set of environment variables for each deployed instance. <a href=""http://12factor.net/"" rel=""nofollow"">http://12factor.net/</a></p>&#xA;&#xA;<p>Option 2 - Split your django code up into self contained applications then have a project for each microservice that just includes these projects and a url config.</p>&#xA;&#xA;<p>I can expand further if this is helpful?</p>&#xA;"
48785563,48525262,6511448,2018-02-14T11:01:15,"<p>The problem occurs because Visual studio container is unable to connect to docker for windows and solution for this is to open the visual studio 2017 from Docker CLI using following command.&#xA;<code>/c/Program\ Files\ \(x86\)/Microsoft\ Visual\ Studio/2017/Community/Common7/IDE/devenv.exe C:\\PATH\\TO\\MY\\SOLUTION.sln</code></p>&#xA;&#xA;<p>Here:</p>&#xA;&#xA;<pre><code>C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\devenv.exe&#xA;</code></pre>&#xA;&#xA;<p>is the location of my devenv.exe file and 2nd parameter </p>&#xA;&#xA;<pre><code>C:\\PATH\\TO\\MY\\SOLUTION.sln&#xA;</code></pre>&#xA;&#xA;<p>shows the path of solution file.</p>&#xA;&#xA;<p>For further details of this solution, click <a href=""https://stackoverflow.com/questions/45869766/how-to-get-docker-toolbox-to-work-with-net-core-2-0-project"">Here</a>.</p>&#xA;"
47147639,47146939,3236440,2017-11-06T23:36:53,"<p>Yes, you should create a new keystore for new environment as the (Common Name) CN field in keystore should match to your dev/test/prod environment. I assume your CN is localhost or loopback address when you created one for your local environment.</p>&#xA;&#xA;<p>Alternatively, if you want to match based on wildcards such as *.test.abc.com you can do it using Subject Alternative Names(SAN). </p>&#xA;"
35300778,28572202,2581592,2016-02-09T19:40:20,"<p>I just had the same error. Just be attentive you're adding ""server your-file-name.yml"" to the program arguments but not VM options ;-)</p>&#xA;"
42200054,41800036,8160856,2017-02-13T08:56:21,"<p>I have found the solution after long research. I am sharing these links to help others looking this question later.</p>&#xA;&#xA;<p>Basically, we need to add a column for foreign key. For my example in question, We need to add b_id instead of B object. Then we set B's id. When we need B, we will fetch it by using that id.</p>&#xA;&#xA;<p><a href=""https://www.quora.com/How-do-I-handle-Foreign-Keys-with-a-Microservices-architecture"" rel=""nofollow noreferrer"">https://www.quora.com/How-do-I-handle-Foreign-Keys-with-a-Microservices-architecture</a></p>&#xA;&#xA;<p><a href=""http://microservices.io/patterns/data/database-per-service.html"" rel=""nofollow noreferrer"">http://microservices.io/patterns/data/database-per-service.html</a>﻿</p>&#xA;"
32908073,32887039,5371736,2015-10-02T13:18:11,"<p>If you would like to have HATEOAS in place but do not want to use spring-data-rest you could still fall back to <a href=""http://projects.spring.io/spring-hateoas/"" rel=""nofollow"">Spring HATEOAS</a></p>&#xA;&#xA;<p>It is internally used by spring-data-rest and gives you the ability to create a HATEOAS-style REST API. </p>&#xA;"
38790799,38786207,5371736,2016-08-05T13:43:27,"<p>You could use a feign <code>ErrorDecoder</code> </p>&#xA;&#xA;<p><a href=""https://github.com/OpenFeign/feign/wiki/Custom-error-handling"" rel=""noreferrer"">https://github.com/OpenFeign/feign/wiki/Custom-error-handling</a></p>&#xA;&#xA;<p>Here is an example</p>&#xA;&#xA;<pre><code>public class MyErrorDecoder implements ErrorDecoder {&#xA;&#xA;    private final ErrorDecoder defaultErrorDecoder = new Default();&#xA;&#xA;    @Override&#xA;    public Exception decode(String methodKey, Response response) {&#xA;        if (response.status() &gt;= 400 &amp;&amp; response.status() &lt;= 499) {&#xA;            return new MyBadRequestException();&#xA;        }&#xA;        return defaultErrorDecoder.decode(methodKey, response);&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>For spring to pick up the ErrorDecoder you have to put it on the ApplicationContext:</p>&#xA;&#xA;<pre><code>@Bean&#xA;public MyErrorDecoder myErrorDecoder() {&#xA;  return new MyErrorDecoder();&#xA;}&#xA;</code></pre>&#xA;"
39136774,39135928,917336,2016-08-25T04:37:58,"<p>It depends on what parts of the stack you want to integrate Hazelcast. We have a Eureka discovery plugin which makes it possible to discover other Hazelcast nodes. You can put Hystrix in front of Hazelcast calls but remember those are fault tolerant, so they might are re-run. I never tried Governator or Zuul but I think there was a user to successfully integrate the latter one with Hazelcast.</p>&#xA;"
43227291,41174987,1791065,2017-04-05T09:34:26,"<p>In HATEOAS URIs are discoverable (and not documented) so that they can be changed. That is, unless they are the very entry points into your system (<a href=""https://www.w3.org/Provider/Style/URI.html"" rel=""nofollow noreferrer"">Cool URIs</a>, the only ones that can be hard-coded by clients) - and you shouldn't have too many of those if you want the ability to evolve the rest of your system's URI structure in the future. This is in fact one of the most <a href=""https://morethancoding.com/2011/09/07/uri-construction-give-it-a-rest/"" rel=""nofollow noreferrer"">useful</a> features of REST.</p>&#xA;&#xA;<p>For the remaining non-Cool URIs, they can be changed over time, and your API documentation should spell out the fact that they should be discovered at runtime through hypermedia traversal.</p>&#xA;&#xA;<p>That being said, in your scenario that link would be a Cool URI, and not relative to the current API (because it may reside on a different machine/domain etc). Unless you're using some <a href=""https://www.consul.io/"" rel=""nofollow noreferrer"">discovery tool</a>, you're going to have to hardcode that link, and thus lose the benefit of discoverability.</p>&#xA;"
39325321,39323491,1791065,2016-09-05T07:01:45,"<p>Usually you'll end up with something like</p>&#xA;&#xA;<pre><code>- Solution&#xA;    - IntegrationTestsFolder&#xA;          -IntegrationTestsProject1&#xA;          -IntegrationTestsProject2&#xA;    - BusinessLayerProject&#xA;    - BusinessLayerTestProject&#xA;</code></pre>&#xA;&#xA;<p>So you'll have the tests near the targeted code.</p>&#xA;&#xA;<p>You're unlikely to have as many Integration test projects as you have unit, and they're probably not mapped 1 to 1 with the source projects, so you might want to nest them all inside a <code>IntegrationTests</code> folder.</p>&#xA;&#xA;<p>As for versioning, do you really double/version everything? If you just end up with a couple of different versioned classes, you can probably target them individually in the actual Unit/Integration test.</p>&#xA;"
40733965,40733857,3364697,2016-11-22T04:48:35,"<p>Yes, Nginx can be a deployment and a service (of loadbalancer or externalIP type) and can forward to upstream services. </p>&#xA;&#xA;<p>You might have to frequently change the nginx.conf though (when you add/remove services), so I would recommend using a ConfigMap to keep your nginx.conf and mounting that as a volume in your deployment. Refer: <a href=""http://kubernetes.io/docs/user-guide/configmap/"" rel=""nofollow noreferrer"">http://kubernetes.io/docs/user-guide/configmap/</a> and scroll down to consume configmap via volumes.</p>&#xA;&#xA;<p>Another thing to keep in mind is that if you delete and create a service that is referred to in nginx.conf as an upstream service, you'll have to restart your deployment because nginx resolves all service DNS labels when nginx starts.</p>&#xA;"
44897315,44884316,7038809,2017-07-04T04:57:46,"<p>Pact-JVM supports Message Pacts, which encapsulate a message that is consumed (one way) over some mechanism, normally a message queue. The idea is to test the consumer code can consume the message via a consumer test, and then verify that the provider generates an appropriate message. The actual message queue is not used in the test.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/FKsw6.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/FKsw6.png"" alt=""Contract over a message queue""></a></p>&#xA;&#xA;<p>This was originally developed to apply contract tests for micro-services communicating over a Kafka message queue.</p>&#xA;&#xA;<p>The tests are done in two parts, just like Request-Response Pact tests, except the Consumer reads the message during the consumer pact test and if successful a pact file is written. Then the provider code is called to generate a message, and that is compared to what is in the pact file.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/I54gJ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/I54gJ.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>The relevant sections of the Pact-JVM docs are:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://github.com/DiUS/pact-jvm/blob/master/pact-jvm-provider-gradle/README.md#verifying-a-message-provider-version-2212"" rel=""noreferrer"">Gradle Plugin</a></li>&#xA;<li><a href=""https://github.com/DiUS/pact-jvm/blob/master/pact-jvm-provider-maven/README.md#verifying-a-message-provider-version-2212"" rel=""noreferrer"">Maven Plugin</a></li>&#xA;<li><a href=""https://github.com/DiUS/pact-jvm/blob/master/pact-jvm-provider-junit/README.md#example-of-amqp-message-test"" rel=""noreferrer"">JUnit Example</a></li>&#xA;</ul>&#xA;"
43577928,43567411,7038809,2017-04-24T00:16:08,"<p>Pact-mock-service is a general mock server built into the pact libraries to support mocking out the other dependency in an integration during a consumer test. If you use any of the consumer test support libraries, you do not need to use it directly.</p>&#xA;&#xA;<p>pact-jvm-server is a controllable server that bundles the Pact-mock-service and allows you to setup and tear down mock servers via HTTP requests. It exists for people who can not,or do not wish to use the consumer test support libraries.</p>&#xA;&#xA;<p>For people using Maven, there is a <a href=""https://github.com/DiUS/pact-jvm/tree/master/pact-jvm-provider-maven"" rel=""nofollow noreferrer"">plugin</a> provided as part of the pact-jvm project that can do provider verification tests and publish to a pact broker. For the consumer tests, they just run as JUnit tests so you don't need any Maven specific plugin.</p>&#xA;&#xA;<p>Of the two links you posted, the first is an example project using a spring-boot application, and the second is a maven plugin that provides publishing to a pact broker only.</p>&#xA;"
35034143,34316241,1555545,2016-01-27T10:01:10,"<p>In a perfect world example, the service that are responsible to send ""publish"" a real time push notifications should be separated from other services. Since the micro service is a set of narrowly related methods, and there is no relation between the authentication ""user"" service, and the realtime push notification service. And to a deep break down, the authentication actually is a separate service, this only FYI, There might be a reason you did this way.</p>&#xA;&#xA;<p>How the service would communicate? There is actually many ways how to implement the internal communication between the services, MQ solution, which could add more technology to your stack, like Rabbit MQ, Beanstalk, Gearman, etc...</p>&#xA;&#xA;<p>And also you can do the communication on top of HTTP protocal, but you need to consider that the HTTP call will add more cost.</p>&#xA;&#xA;<p>The perfect solution is that each service will have to interfaces to execute on their behalf, HTTP interface and an MQ interface (console)</p>&#xA;"
30755076,30739851,49241,2015-06-10T11:33:51,"<p>The question is really about coupling. How coupled are your various services? 'Standalone' implies a level of loose coupling, but in order to interoperate, <em>some</em> level of coupling is required. The challenge is to couple as loosely as possible whilst maintaining the interoperability that will allow your components to work together to achieve your desired outcome.</p>&#xA;&#xA;<p>A coupling via message contracts is a loose coupling. This only requires that each of your services knows and understands the 'shape' of the data (messages) that passes between them. The services have no need to share any code, hosts or even platforms.</p>&#xA;&#xA;<p>When using an ESB, this shared knowledge of messages is the only thing that your services need to 'know' about each other. Depending on the type of transport you use, you may also need to give each service knowledge of the location (URI) of the other services, but again, this is a very loose coupling (spatial) and can be loosened further by, e.g. DNS. If you choose to use a brokered transport, e.g. centralised queuing technology like RabbitMQ, then this spatial coupling can be removed, but this comes with other trade-offs.</p>&#xA;&#xA;<p>Bear in mind that MassTransit is not the only option available in the .NET space. E.g. a popular choice is <a href=""http://particular.net/nservicebus"">NServiceBus</a> (full disclosure: I work for Particular Software and NSB is one of our products).</p>&#xA;"
36081993,36080524,6078553,2016-03-18T10:29:16,"<p>I'm afraid that for RabbitMQ at least you will need a client. RabbitMQ implements the AMQP protocol, as opposed to the HTTP protocol used by web servers. As Sergio mentioned above, Rails is a web framework, so it doesn't have AMQP support built into it. You'll have to use an AMQP client such as Bunny in order to subscribe to a Rabbit queue from within a Rails app.</p>&#xA;"
29108891,28581644,3992665,2015-03-17T20:17:16,"<p>Indeed with Apigee you can throttle the response. This can be done in two ways;</p>&#xA;&#xA;<ol>&#xA;<li>With a 'Spike Arest', which smoothes out the requests over time, for instance if you define an amount of requests per second, its smoothed over milliseconds, and when an amount of reuqests defined per minute it is smoother per second.</li>&#xA;<li>A Qouta defines a maximum amount of requests per minute, day, etc. After that requests are rejected.</li>&#xA;<li>Apigee also provides a 'concurrent rate' policy which does exactly that; tracks the amount of concurrent calls, and rejects in case more are sent. (HTTP 503).</li>&#xA;</ol>&#xA;&#xA;<p>You might like:&#xA;<a href=""http://apigee.com/docs/api-services/content/comparing-quota-spike-arrest-and-concurrent-rate-limit-policies"" rel=""nofollow"">http://apigee.com/docs/api-services/content/comparing-quota-spike-arrest-and-concurrent-rate-limit-policies</a></p>&#xA;&#xA;<p>Also, in case you want to keep it custom, check out Volos, or Apigee 127, which is a great open source project created by Apigee as well.</p>&#xA;&#xA;<p>hope this helps.</p>&#xA;&#xA;<p>Actually, on a final note; I'd solve the root cause; why do you get 1mio requests? Or is that expected bevahior?</p>&#xA;"
50558583,50555353,131930,2018-05-28T03:02:13,"<blockquote>&#xA;  <p>The problem is what if the broker M is down?</p>&#xA;</blockquote>&#xA;&#xA;<p>If the broker is down, then A and B can't use it to communicate.</p>&#xA;&#xA;<p>What A and B should do in that scenario is going to depend very much on the details of your particular application/use-case.  </p>&#xA;&#xA;<p>Is there useful work they <em>can</em> do in that scenario? </p>&#xA;&#xA;<p>If not, then they might as well just stop trying to handle any work/transactions for the time being, and instead just sit and wait for M to come back up.  Having them do periodic pings/queries of M (to see if it's back yet) while in this state is a good idea.</p>&#xA;&#xA;<p>If they can do something useful in this scenario, then you can have them continue to work in some sort of ""offline mode"", caching their results locally in anticipation of M's re-appearance at some point in the future.  Of course this can become problematic, especially if M doesn't come back up for a long time -- e.g. </p>&#xA;&#xA;<ul>&#xA;<li>what if the set of cached local results becomes unreasonably large, such that A/B runs out of space to store it?  </li>&#xA;<li>Or what if A and B cache local results that will both apply to the same data structure(s) within M, such that when M comes back online, some of A's results will overwrite B's (or vice-versa, depending on the order in which they reconnect)?  (This is analogous to the sort of thing that source-code-control servers have to deal with after several developers have been working offline, both making changes to the same lines in the same file, and then they both come back online and want to commit their changes to that file.  It can get a bit complex and there's not always an obvious ""correct"" way to resolve conflicts)</li>&#xA;<li>Finally what if it was something A or B ""said"" that caused M to crash in the first place?  In that case, re-uploading the same requests to M after it comes back online might only cause it to crash again, and so on in an infinite loop, making the service perpetually unusable.  (In this case, of course, the proper fix would be to debug M)</li>&#xA;</ul>&#xA;&#xA;<p>Another approach might be to try to avoid the problem by having multiple redundant brokers (e.g. M1, M2, M3, ...) such that as long as at least one of them is still available, productive work can continue.  Or perhaps allow A and B to communicate with each other directly rather than through an intermediary.</p>&#xA;&#xA;<p>As for whether this sort of thing would best be handled by threads or reactive programming, that's a matter of personal preference -- personally I prefer reactive programming, because the multiple-threads style usually means blocking-RPC-operations, and a thread that is blocked inside a blocking-operation is a frozen/helpless thread until the remote party responds (e.g. if M takes 2 minutes to respond to an RPC request, then A's RPC call to M cannot return for 2 minutes, which means that the calling thread is unable to do anything at all for 2 minutes).  In a reactive approach, A's thread could also be doing other things during this period (such as pinging M to make sure it's okay, or contacting a backup broker, or whatever) during that 2 minute period if it wanted to.</p>&#xA;"
43501440,43497790,1882,2017-04-19T16:39:07,"<p>As mentioned in the other answer, look at the request/response sample.</p>&#xA;&#xA;<p>Also, review the documentation for the request client usage:</p>&#xA;&#xA;<p><a href=""http://masstransit-project.com/MassTransit/usage/request-response.html"" rel=""nofollow noreferrer"">http://masstransit-project.com/MassTransit/usage/request-response.html</a></p>&#xA;"
39692633,39686227,201306,2016-09-25T23:02:37,<p>In my personal opinion DB migration shouldn't be a microservice responsibility.</p>&#xA;&#xA;<p>Some time ago I read a lot about Blue/Green deployment and DB migrations and our final decision was to trigger DB migrations when needed.</p>&#xA;&#xA;<p>I don't have the links right now but they are easy googlable.</p>&#xA;&#xA;<p>I mentioned Blue/Green deployment since microservices based approach and containers makes it's much simpler so if you are going this direction you probably should consider Blue/Green deployment and it will be much harder if you decide to run migrations from containers.</p>&#xA;&#xA;<p>You can also consider implementing some circuit breaker like approach so microservice will stop accessing DB in case of DB schema problems. </p>&#xA;
41086984,41086281,12890,2016-12-11T14:00:19,"<p>To find the right contexts, borders and communication channels is imho one of the most difficult parts of a microservice architecture. It is about finding the data you need, how the relationships are and which service is responsible for what (responsible means the only one allowed to change it). Have a look at the <a href=""http://www.martinfowler.com/articles/microservices.html"" rel=""nofollow noreferrer"">Blog from Martin Fowler</a>.</p>&#xA;&#xA;<p>Microservices is not modules. Each service should be an independent service regarding development and deployment. And yes, they may communicate to each other but a client may also communicate to them individually. The Microservice approach is also about using the right tool for the problem. So each service can be implemented in a different programming language. They can use different kind of storage like RDMBS, NoSQL or Key-Value store. An they will be scaled individually - many instances for ConnectionsDetails and fewer for Reservations e.g.</p>&#xA;&#xA;<p>What will happen if one service is not available? Each service should be as fault tolerant as possible and try to decrease it's service gracefully if nothing else is possible. You should think about minimising the needed communication between the services by choosing the right borders, make data independent and maybe introduce caching. Don't forget about the <a href=""https://en.wikipedia.org/wiki/CAP_theorem"" rel=""nofollow noreferrer"">CAP theorem</a>, a microservice approach makes it more visible. Here are some <a href=""http://www.slideshare.net/ufried/patterns-of-resilience"" rel=""nofollow noreferrer"">slides about resilience</a> that may help. Do not share the same database or replicate everything between services.</p>&#xA;&#xA;<p>""how should modules communicate if they do different things?"". You should choose a language independent way of communication and depending on your problem a synchronous or asynchronous method. As a language independent format JSON or XML are most common. Synchronous communication can be based on REST, asynchronous communication on messaging. The authentication (""Client"") is typically a REST service, sending the booked tickets via Email is more a message driven asynchronous service.</p>&#xA;"
47935761,47935575,6767450,2017-12-22T04:14:24,"<p>I am not good at AWS, but by my understanding, your question sounds like your are mixing the deployment structure and source code structure. I think you can write shared source code for both, and deploy dlls(not dll in AWS?) seperately. I think it's not good to call API gateway.</p>&#xA;"
46662171,46660853,2382426,2017-10-10T08:38:09,"<p>Please have a look at the numerous tutorials that exists for spring boot and dockerizing this application. </p>&#xA;&#xA;<p><a href=""https://spring.io/guides/gs/spring-boot-docker/"" rel=""nofollow noreferrer"">Here</a> is one which explains every step that is necessary</p>&#xA;&#xA;<ol>&#xA;<li>Build Application as Jar File</li>&#xA;<li>Create your docker image with Dockerfile&#xA;In this dockerfile you create an environment like you would have a new setup linux server and you define what you need for software to run your application: like java. Have a look at existing images like <a href=""https://hub.docker.com/r/anapsix/alpine-java/"" rel=""nofollow noreferrer"">anapsix/alpine-java</a>.</li>&#xA;<li>Now think of what you need to do to start your app in this environment: java -jar --some-options -location-of-your-jar.jar </li>&#xA;<li>Make sure to be able to reach your app by exposing the docker port so that you can see that is runs.</li>&#xA;</ol>&#xA;&#xA;<p>As I sad if these instruction is not helpful for you, then please read tutorials for docker and dockerizing spring boot applications.</p>&#xA;"
50810406,50810342,32090,2018-06-12T06:19:15,<p>Make the 3rd party dependencies that are not going to get upgraded soon available on the classpath. jBoss's Wildfly uses the notion of modules allowing to strip down the war to the minimum making the deployment fast.</p>&#xA;
50554275,40669943,962650,2018-05-27T16:35:24,"<p>Hystrix may also be usefull if you refactor your monolith application to split it into several microservices.&#xA;When putting it into production, you may want to leave your old monolith code for some time as an alternate response. So if microservices are unavailable, just the old code will be executed, so basically you reduce risks. If everything works fine, you may just remove old code from monolith and keep using microservices.</p>&#xA;&#xA;<p>By extending HystrixCommand class this can be done easily.</p>&#xA;&#xA;<pre><code>public class MicroserviceCommand extends HystrixCommand&lt;String&gt;&#xA;{&#xA;    @Override&#xA;    protected String run()&#xA;    {&#xA;        //return response from your new microservice&#xA;    }&#xA;&#xA;    @Override&#xA;    protected String getFallback()&#xA;    {&#xA;        //microservice is not available, &#xA;        //so execute old code which was not removed from application yet&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
45945174,45934621,2472282,2017-08-29T17:36:25,"<p>Currently this isn't possible with API.AI's webhook design.  I'd recommend setting up a proxy service that unpacks the webhook requests from API.AI, inspects the action and sends the proper request to the proper microservice endpoint and then forwards the response back to API.AI once the microservice has returned its result:&#xA;<a href=""https://i.stack.imgur.com/Xelc0.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Xelc0.jpg"" alt=""enter image description here""></a></p>&#xA;"
41243005,35065875,248922,2016-12-20T13:01:21,"<p>The grpc people at <a href=""https://github.com/grpc/"" rel=""nofollow noreferrer"">https://github.com/grpc/</a> are currently building a js <a href=""https://github.com/grpc/grpc/issues/8682"" rel=""nofollow noreferrer"">implementation</a>.</p>&#xA;&#xA;<p>The repro is at <a href=""https://github.com/grpc/grpc-web"" rel=""nofollow noreferrer"">https://github.com/grpc/grpc-web</a> (gives 404 ->) which is currently (2016-12-20) in early access so you need to <a href=""https://docs.google.com/forms/u/0/d/15iRDHoP-VBenc4hWgKn7bk7IirJLgs0uh88nw1vi_Hc"" rel=""nofollow noreferrer"">request access</a>.</p>&#xA;"
38575873,38471362,6228827,2016-07-25T19:23:02,"<p>So in Sync Gateway you have the concept of CHANNELS , JSON Docs &amp; USERS.</p>&#xA;&#xA;<p>In the micro service model you probably want to separate data into different parts when:</p>&#xA;&#xA;<ul>&#xA;<li>You want to use a specific sub-set of data in the future for other<br>&#xA;projects. </li>&#xA;<li>You may want that data to have a sync function for a&#xA;specific  purpose.</li>&#xA;<li><p>You might want to keep things simple and you do not want to manage a very large and&#xA;complicated sync function to manage all the data types.</p></li>&#xA;<li><p>You have very specific security concerns. So have one sync gateway database&#xA;for public information and a second one for very private/sensitive&#xA;data.</p></li>&#xA;</ul>&#xA;"
48671832,48667874,5723,2018-02-07T19:30:01,<p>If you want to make sure that the token is valid and current on every request then you will have to ask the data source (your Authentication Provider) for that information on every request.</p>&#xA;&#xA;<p>This can be done in a middleware so that your main code can stay clean.</p>&#xA;
43645682,43350278,3218267,2017-04-26T22:39:32,"<p>You should version your services independently, one of the benefits of a microservices architectures is that you can update and deploy small parts of the system without any downtime to the other components (in theory). If you version component A and B when you make an update to component C, you will end up having to deploy all the components when only one has changed.</p>&#xA;"
39562606,39561186,1384539,2016-09-18T20:56:36,"<p>EDIT: The best approach is using Graphs to map the dependencies for a given service. I've just put an example using Dependency Injection framework because them already do that. Feel free to use it, or create your own.</p>&#xA;&#xA;<p>Using any Dependency Injection tool (Unity, Simple Injector, Autofac, Structure Map, Ninject) will inject the dependencies directly. Here's an example using Unity:</p>&#xA;&#xA;<pre><code>var container = new UnityContainer();&#xA;container.RegisterType&lt;IServiceA, ServiceA&gt;();&#xA;container.RegisterType&lt;IServiceB, ServiceB&gt;();&#xA;</code></pre>&#xA;&#xA;<p>When you ask for ServiceB instance, it will inject ServiceA on it:</p>&#xA;&#xA;<pre><code>var serviceB = container.Resolve&lt;IServiceB&gt;();&#xA;</code></pre>&#xA;&#xA;<p>More info about Unity: <a href=""https://msdn.microsoft.com/en-us/library/dn178463(v=pandp.30).aspx"" rel=""nofollow"">https://msdn.microsoft.com/en-us/library/dn178463(v=pandp.30).aspx</a></p>&#xA;"
34159154,34158847,1384539,2015-12-08T15:03:03,"<p>To your first question, you don't need to do 100 queries, just one with the array of your 100 documents, like the following:</p>&#xA;&#xA;<p>db.collection.find( { _id : { $in : [1,2,3,4] } } );</p>&#xA;&#xA;<p><a href=""https://stackoverflow.com/a/7713461/1384539"">https://stackoverflow.com/a/7713461/1384539</a></p>&#xA;"
38712133,38711908,1384539,2016-08-02T05:06:56,"<p>In my opinion and according to what I've read so far about microservices architecture, the idea of each service having it's own persistence mechanism is to create an independent service that could be entirely rewritten, if needed, without affecting the consumers. (as long as you keep the request/response contract for the client).</p>&#xA;&#xA;<p>This is the <strong>Decentralized Data Management.</strong>  characteristic, and making microservices using the same database, you won't achieve that.</p>&#xA;&#xA;<p>For further read:</p>&#xA;&#xA;<p><a href=""http://martinfowler.com/articles/microservices.html"" rel=""nofollow"">http://martinfowler.com/articles/microservices.html</a></p>&#xA;"
42052770,38989659,5842523,2017-02-05T13:42:29,"<p>I faced the same problem. In addition to removing direct dependency 'javax.servlet:servlet-api:2.5' (as suggested by @Stephane Nicoll), you also need to exclude it from the dependencies that includes 'javax.servlet:servlet-api:2.5'<br>&#xA;Following command is handy to find where it is coming from<br>&#xA;<code>mvn dependency:tree -Dverbose -Dincludes=javax.servlet</code><br>&#xA;That should fix the problem.</p>&#xA;"
49810397,32529742,2959707,2018-04-13T06:14:03,"<p>Transaction across microservices can become complex and can slow down the system, one of the best ways to solve the problem of distributed transactions is to avoid them completely.&#xA;If you avoid distributed transactions across microservices then you will not end up in such situation.</p>&#xA;&#xA;<p>If at all you have to implement distributed transactions across microservices then I think there are a couple of ways :</p>&#xA;&#xA;<p><strong>Two-phase commit protocol &#xA;**Eventual Consistency</strong></p>&#xA;&#xA;<p>In your case, I would recommend using message bus and flag to communicate among services , So if legal service adds the data into legal database put a lock on that record and send message on message bus , user service when it is up it will pick the message and update database at its end  and send ack message onto the message bus, once ack message is received remove the lock otherwise delete/rollback the record after certain time duration. This looks complex but reliable and failure-proof solution in your case. </p>&#xA;"
36674676,36615117,1979483,2016-04-17T09:14:17,"<p>At our company, we built a dedicated service responsible for unique ids generation. And every other services grap the ids they need from there.</p>&#xA;&#xA;<p>These generated ids couldn't be used as an order number but I think it's shouldn't be used for this job anyway. If you need to sort by created date, it's better to have a created_date field.</p>&#xA;&#xA;<p>One more thing that is used to bug my mind with this approach is that the primary resource might be persisted after the other resource that rerefence it by the id. For example, a insert user, and insert user address request payload are sent asynchronously. The insert user payload contains a generated unique id, and user address payload contains that id as foreign reference back to user. The insert user address might be proccessed before the insert user request, but it's totally fine. I think it's called eventual consistency.</p>&#xA;"
47838552,47566338,8012379,2017-12-15T19:12:23,"<p>You can find a working example <a href=""https://github.com/indrabasak/spring-consul-example"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;&#xA;<h3>Consul Configuration KV Store</h3>&#xA;&#xA;<p>You need to store your properties in the Consul KV store either from Consul UI or from the command line. <strong>The Consul Agent will not load your properties from the file system</strong>. To load the properties from the command line, you can use the following command once the Consul Agent is up and running. The YAML data can be read from a file by prefixing the file name with the @ symbol.</p>&#xA;&#xA;<pre><code>./consul kv put config/application/data @spring-boot-consul.yml&#xA;</code></pre>&#xA;&#xA;<p>where <code>config/application/data</code> is the key name.</p>&#xA;&#xA;<p>If the data is successfully written in the KV, you should get the following response,</p>&#xA;&#xA;<pre><code>Success! Data written to: config/application/data&#xA;</code></pre>&#xA;&#xA;<p>You can also fetch the properties from the KV by using the following command,</p>&#xA;&#xA;<pre><code>$ ./consul kv get config/application/data&#xA;cassandra:&#xA;  host: 127.0.0.1:9042,127.0.0.2:9042&#xA;  user: my_user&#xA;  password: my_pass&#xA;</code></pre>&#xA;&#xA;<p>You can also view the properties from the Consul Web UI,</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/wvVsz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wvVsz.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<h3>Changes to bootstrap.yml</h3>&#xA;&#xA;<p>You need to modify your <code>bootstrap.yml</code> slightly. Here are the changes:</p>&#xA;&#xA;<ul>&#xA;<li><code>prefix</code> value to <code>config</code></li>&#xA;<li><code>defaultContext</code> value to <code>application</code></li>&#xA;<li>Added <code>format</code> to <code>yaml</code></li>&#xA;<li><p>Added <code>data-key</code> by the name of <code>data</code> to fetch the YAML blob.</p>&#xA;&#xA;<pre><code>spring:&#xA;  profiles: default&#xA;  cloud:&#xA;    consul:&#xA;      host: localhost&#xA;      port: 8500&#xA;      config:&#xA;        enabled: true&#xA;        prefix: config&#xA;        defaultContext: application&#xA;        data-key: data&#xA;        profileSeparator: '::'&#xA;        format: yaml&#xA;  application:&#xA;    name: spring-boot-consul&#xA;</code></pre></li>&#xA;</ul>&#xA;&#xA;<h3>Changes to ConsulConfiguration</h3>&#xA;&#xA;<pre><code>@Configuration&#xA;@RefreshScope&#xA;public class ConsulConfiguration {&#xA;&#xA;    @Value(""${cassandra.host}"")&#xA;    private String cassandraHost;&#xA;&#xA;    @Value(""${cassandra.user}"")&#xA;    private String userName;&#xA;&#xA;    @Value(""${cassandra.password}"")&#xA;    private String password;&#xA;&#xA;    @PostConstruct&#xA;    public void postConstruct() {&#xA;        // to validate if properties are loaded&#xA;        System.out.println(""** cassandra.host: "" + cassandraHost);&#xA;        System.out.println(""** cassandra.user: "" + userName);&#xA;        System.out.println(""** cassandra.password: "" + password);&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<h3>Changes to Application class,</h3>&#xA;&#xA;<pre><code>@EnableRetry&#xA;@RefreshScope&#xA;@EnableDiscoveryClient&#xA;@EnableAutoConfiguration&#xA;@EnableConfigurationProperties&#xA;@SpringBootApplication&#xA;@ComponentScan(""com.test.*"")&#xA;public class SpringBootConsulApplication {&#xA;&#xA;    public static void main(String[] args) {&#xA;        ...&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;"
26251381,26247474,412763,2014-10-08T07:28:55,"<p>Apache Camel is not really an ESB (<a href=""http://camel.apache.org/is-camel-an-esb.html"" rel=""nofollow"">unless you want it to be</a>), but rather a language/framework to connect ""stuff"" in a message oriented fashion.</p>&#xA;&#xA;<p>If you feel you can use a concise syntax and flexible swiss army knife to connect ""stuff"" in your microsservices, sure - use Apache Camel. If you rather solve your integration code in other ways, do so.</p>&#xA;"
27878112,27865238,412763,2015-01-10T15:56:59,"<p>I would avoid dependencies in the parent pom. It's awkward if one of your (independent) microservices would want some other things. It's weird to have the parent know of each microservice.</p>&#xA;&#xA;<p>You can stick with dependencyManagement though to suggest default versions/scopes if you want. A parent pom is, non the less, very convenient to define plugins, repositories and the like.</p>&#xA;&#xA;<p>Instead, I would group a common set of dependencies into a specific artifact(s), that may be only a single pom with dependencies. Then you can depend on, say ""com.example/common-db-dependencies/1.2"" to include a standard set of database dependencies, like hibernate, apache derby and JPA specs. (or whatever you're using). A service does not use JPA/SQL could avoid that dependency all together.</p>&#xA;&#xA;<p>Don't entangle yourself though. It's easy to overwork dependency structures if you're trying to cover each case. So, only try to standardize things that really get used by a majority of the services.</p>&#xA;"
27877969,27865814,412763,2015-01-10T15:42:36,"<p>There is nothing that says you architecture is not a MS architecture just because you're using magento and PHP. But, you have to consider a few things:</p>&#xA;&#xA;<ul>&#xA;<li>Think in terms of always being able to rewrite any of the services in any language and deploy somewhere the total system should just continue to work.</li>&#xA;</ul>&#xA;&#xA;<p>If your services are just transformation/interface very tightly linked to magento and you cannot simply rewrite them in java/C#/ruby easily, then I guess you do not have a MS architecture.</p>&#xA;&#xA;<p>For PHP deployable artifacts, you typically have some packaging or versioning strategy around your service. Even though ""deploy"" in PHP is typically just swapping a folder of .php files. And you should not really share code/config between different services. You can even look at <a href=""http://deployer.in/"">deployment tools for PHP</a> if you want to take an extra step.</p>&#xA;"
33034734,33033834,412763,2015-10-09T09:31:31,"<p>The Microservice pattern does not suggest you move every single service you have to it's own deployable. Only move self sustaining pieces of logic that will benefit from it's own release cycle. I.e. if your RatingEngine needs rating-logic updates weekly, but the rest of your system is pretty stable - it will likely benefit from beeing a service of it's own.</p>&#xA;&#xA;<p>And yes - Microservices adds complexity, but not really boiler plate code of HTTP servers. There are a lot of frameworks around to deal with that. <a href=""http://vertx.io/"" rel=""nofollow"">Vert.x</a> is one good. Others are Spring Boot, Apache Camel etc. A complete microservice setup could look like this with Vert.x.</p>&#xA;&#xA;<pre><code> public class RatingService extends AbstractVerticle implements RatingEngine{&#xA;  public void start() {&#xA;    vertx.createHttpServer().requestHandler(req -&gt; {&#xA;      req.response()&#xA;        .putHeader(""content-type"", ""application/json"")&#xA;        .end(computeCurrentRating().encodePrettily());&#xA;    }).listen(8080);&#xA;   }&#xA;&#xA;   @Override&#xA;   public int getRating(){&#xA;        return 4; // or whatever.&#xA;   }&#xA;&#xA;   protected JsonObject computeCurrentRating(){&#xA;     return new JsonObject().put(""rating"", getRating());&#xA;   }&#xA; }&#xA;</code></pre>&#xA;&#xA;<p>Even the Java built-in framework JAX-RS helps making a microservice in not too many lines of code.</p>&#xA;&#xA;<p>The really hard work with microservices is to add error-handling logic in the clients. Some common pitfalls</p>&#xA;&#xA;<ul>&#xA;<li><p><strong>Microservice may go down</strong> If call to RatingService gives connection refused exception - can you deal with it? Can you estimate a ""rating"" in client to not prevent further processing? Can you reuse old responses to estimate the rating? .. Or at least - you need to signal the error to support staff.</p></li>&#xA;<li><p><strong>Reactive app?</strong> How long can you wait for a response? A call to in memory methods will return within nano seconds, a call to an external HTTP service may take seconds or minutes depending on a number of factors. As long as the application is ""reactive"" and can continue to work without a ""Rating"" - and present the rating for the user once it's available - it's fine. If you are waiting for a blocking call to rating service, more than a few millisec. - response time becomes an obstacle. It's not as convenient/common to make reactive apps in Java as in node.js. A reactive approach will likely trigger a remake of you entire system.</p></li>&#xA;<li><p><strong>Tolerant client</strong> Unit/integration testing a single project is easy. Testing a complex net of microservices is not. The best thing you can do about it is to make your client call less picky. Schema validations etc. are actually bad things. In XML use single XPaths to get data you want from the response, not more not less. That way, a change in the microservice response will not require updates of all clients. JSON is a bit easier to deal with than XML in this aspect.</p></li>&#xA;</ul>&#xA;"
28339738,28319278,412763,2015-02-05T08:55:14,"<p>One way is to use HTTPS for internal MS communication. Lock down the access (using a trust store) to only your services. You can share a certificate among the services for backend communication. Preferably a wildcard certificate. Then it should work as long as your services can be adressed to the same domain. Like *.yourcompany.com.</p>&#xA;&#xA;<p>Once you have it all in place, it should work fine. HTTPS sessions does imply some  overhead, but that's primarily in the handshake process. Using keep-alive on your sessions, there shouldn't be much overhead with encrypted channels.</p>&#xA;&#xA;<p>Of course, you can simply add some credentials to your http headers as well. That would be less secure.</p>&#xA;"
40706043,40691082,1662299,2016-11-20T15:49:50,"<p>Eric Evans on GOTO 2015 conference (  <a href=""https://youtu.be/yPvef9R3k-M"" rel=""nofollow"">https://youtu.be/yPvef9R3k-M</a>) and I 100% agree with him, answered to your question. Microservice scope should be one or maybe more Bounded Context(s). Including its supporting classes for persistence, REST/HTTP API, etc. &#xA;As I understood, the microservice is deployment wrapper over Bounded Context, with adding the isolation, scaling and resilient aspects.&#xA;As you wrote, you didn't apply Strategic Design to define bounded context. So its time to check, before tearing the app to parts.</p>&#xA;"
46195863,46193708,6643803,2017-09-13T11:04:56,"<p>It just impossible what you want, you have 2 differents applications, each entity has its own life in its context. Imagine the case where a service is down, how would you do? </p>&#xA;&#xA;<p>If a microservice is closely linked to another one, you should revise your architecture. </p>&#xA;&#xA;<p>To solve this kind of problem, add an identifier in each entity to identify which Tag belongs to an Entry and vice-versa, you can request your data using those identifiers.</p>&#xA;"
42539716,42539505,1207899,2017-03-01T18:52:45,"<p>You should make that endpoint only receive Http Post and then you should put all of the product ids in the body of your request. Something like this:</p>&#xA;&#xA;<p>{&#xA;  ""ids"" : [], (an array with all your ids in it)&#xA;  ""limit"": 10,&#xA;  ""sortBy"": ""price""&#xA;}</p>&#xA;&#xA;<p>not sure why you have the field ""limit"", I would think that what controls the limit is how many ids you pass in.</p>&#xA;"
51492917,49999709,3791707,2018-07-24T07:31:22,"<p>Here's what I ended up using - might not be the proper, optimal solution but it worked as I needed:</p>&#xA;&#xA;<p>In the <code>OAuth2AuthenticationService</code>, I catch the exception (so that it doesn't bubble up as a 500 Internal Error)</p>&#xA;&#xA;<pre><code> public ResponseEntity&lt;OAuth2AccessToken&gt; authenticate(HttpServletRequest request, HttpServletResponse response,&#xA;                                                      Map&lt;String, String&gt; params) {&#xA;    try {&#xA;        [...]&#xA;        OAuth2AccessToken accessToken = authorizationClient.sendPasswordGrant(username, password);&#xA;        [...]&#xA;        return ResponseEntity.ok(accessToken);&#xA;    } catch (HttpClientErrorException ex) {&#xA;        ObjectMapper mapper = new ObjectMapper();&#xA;        try {&#xA;            // Throw the original exception from auth-service to keep its message (i18n code used by the frontend in my case)&#xA;            throw mapper.readValue(ex.getResponseBodyAsString(), OAuth2Exception.class);&#xA;        } catch (IOException e) {&#xA;            // If it's not a OAuth2Exception, let the error get thrown&#xA;            throw ex;&#xA;        }&#xA;    } catch (Exception ex) {&#xA;        // If it's not a OAuth2Exception, let the error get thrown&#xA;        throw ex;&#xA;    }&#xA;</code></pre>&#xA;&#xA;<p>In the <code>AuthResource.authenticate()</code> method, I catch these OAuth2Exception, wrap them in a custom AuthenticationFailureException class and let the <code>ExceptionTranslator</code> return the right object to the frontend (note the BadRequest Status and the message payload):</p>&#xA;&#xA;<pre><code>@ExceptionHandler(AuthenticationFailureException.class)&#xA;public ResponseEntity&lt;Problem&gt; handleBadRequestAlertException(AuthenticationFailureException ex, NativeWebRequest request) {&#xA;    Problem problem = Problem.builder()&#xA;        .withStatus(BAD_REQUEST)&#xA;        .with(""message"", ex.getMessage())&#xA;        .build();&#xA;    return create(ex, problem, request);&#xA;}&#xA;</code></pre>&#xA;"
35804522,30648096,3794523,2016-03-04T19:29:08,"<p>I don't know about any existing tool rather I'm just putting my thought on where to do it.</p>&#xA;&#xA;<p>From what the OP describe, they are already building a micro services architecture using Netflix stack. There should be a repository to config the name (or URL) for each of the services and the 'config server' or 'service registry' will read from that. To me, that's the perfect place to put the reference to each of the micro-service's documentation under their own entries. This way you get the benefit of maintaining the documentation and code at same place, plus you could potentially also collect run time information like instance/connections count if you hook into the config/registry server.</p>&#xA;"
51027449,50986816,3143003,2018-06-25T15:51:53,"<p>Good question. My answer to this is, introduce synchronous flows in the system. </p>&#xA;&#xA;<p>I am using rabbitMq so i don't know about kafka but you should search for kafka's synchronous flow.</p>&#xA;&#xA;<p>WebSockets does seem one overkiil.</p>&#xA;&#xA;<p>Hope that helps.</p>&#xA;"
45970675,45964527,3604315,2017-08-30T22:33:58,"<p>I would suggest you the first option. It's the same Id that's used in the other context, in this way you don't need to create something just for sharing the reference around.</p>&#xA;&#xA;<p>About the second, are any background reasons that you would not share the real Id but instead a generated key? If yes (you say nothing about this), then the second would be better because you ""protect"" what you don't want to share. This requires a way, on the CustomerMicroservice, to get the user from the generated key (if you need this). </p>&#xA;&#xA;<p>Edit:</p>&#xA;&#xA;<p>With ""reasons"" I mean not operations that require the Id (this would be a bit weird) but, for example, the User needs to access that information for interacting with a call center. In this case a human readable value it's far better than a Guid. </p>&#xA;"
52077110,51962515,1387612,2018-08-29T12:09:44,"<p><code>hostPort</code> equal to 0 means your application will be stared at random port assigned by Mesos. See <a href=""https://stackoverflow.com/a/37822870/1387612"">this answer</a></p>&#xA;&#xA;<p>You don't need to keep <code>hostPort</code> and <code>containerPort</code> equal. <code>containerPort</code> must match the port application is listening at. <code>hostPort</code> might be any port (that's why you should use 0 for it). Containerizer will then pass packets sent to/from your <code>hostPost</code> to <code>containertPort</code> inside the container.</p>&#xA;"
38976850,38970723,1387612,2016-08-16T13:55:20,"<p><strong>Take a look at <a href=""https://stackoverflow.com/a/37822870/1387612"">this answer</a>.</strong></p>&#xA;&#xA;<p>If you use marathon-lb then there is no need to pass a port because it's a proxy and it will know where service is just by name.</p>&#xA;&#xA;<p>If you use mesos-dns you should make a SRV request to get ip and port. In node you can do it with <a href=""https://nodejs.org/api/dns.html#dns_dns_resolvesrv_hostname_callback"" rel=""nofollow noreferrer""><code>dns.resolveSrv(hostname, callback)</code></a> but your DNS must be exposed on defaul (53) port and supports SRV request (mesos-dns supports it).</p>&#xA;"
40022565,40010594,1387612,2016-10-13T13:36:08,"<p>Consul by default do not deregister unhealthy services instead marks them as critical.&#xA;From Consul 0.7 there is special option (<code>deregister_critical_service_after</code>) that allows you to define time after unhealthy service will be deregstered</p>&#xA;&#xA;<p>From Consul <a href=""https://github.com/hashicorp/consul/blob/v0.7.0/CHANGELOG.md"" rel=""noreferrer"">0.7 Changelog</a></p>&#xA;&#xA;<blockquote>&#xA;  <p><strong>Automatic Service Deregistration:</strong> Added a new&#xA;  <code>deregister_critical_service_after</code> timeout field for health checks&#xA;  which will cause the service associated with that check to get&#xA;  deregistered if the check is critical for longer than the timeout.&#xA;  This is useful for cleanup of health checks registered natively by&#xA;  applications, or in other situations where services may not always be&#xA;  cleanly shutdown. <a href=""https://github.com/hashicorp/consul/issues/679"" rel=""noreferrer"">GH-679</a></p>&#xA;</blockquote>&#xA;&#xA;<p>If you are usign Marathon then you can consider using <a href=""https://github.com/allegro/marathon-consul"" rel=""noreferrer"">allegro/marathon-consul</a> it will deregister task when its dead</p>&#xA;"
42553905,42527724,1387612,2017-03-02T11:14:43,"<p>Take a look at <a href=""https://stackoverflow.com/a/37822870/1387612"">this answer</a></p>&#xA;&#xA;<p>DNS could provide information about servce port when you make <a href=""https://en.wikipedia.org/wiki/SRV_record"" rel=""nofollow noreferrer"">DNS SRV</a> request. It's not compatible with most of clients so you need to manually. Mesos DNS has <a href=""https://github.com/mesosphere/mesos-dns/blob/1810c09811dfeb8c9e72ee6c58102313f74e0e1c/docs/docs/naming.md#srv-records"" rel=""nofollow noreferrer"">whole section dedicate to SRV records</a></p>&#xA;&#xA;<p>Below is an example from docs:</p>&#xA;&#xA;<blockquote>&#xA;  <h2>SRV Records</h2>&#xA;  &#xA;  <p>An SRV record associates a service name to a hostname and an IP port.&#xA;  For task <code>task</code> launched by framework <code>framework</code>, Mesos-DNS generates an SRV record for service name <code>_task._protocol.framework.domain</code>, where <code>protocol</code> is <code>udp</code> or <code>tcp</code>.&#xA;  For example, other Mesos tasks can discover service <code>search</code> launched by the <code>marathon</code> framework with a lookup for lookup <code>_search._tcp.marathon.mesos</code>:</p>&#xA;</blockquote>&#xA;&#xA;<pre><code>$ dig _search._tcp.marathon.mesos SRV&#xA;&#xA;; &lt;&lt;&gt;&gt; DiG 9.8.4-rpz2+rl005.12-P1 &lt;&lt;&gt;&gt; _search._tcp.marathon.mesos SRV&#xA;;; global options: +cmd&#xA;;; Got answer:&#xA;;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 33793&#xA;;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0&#xA;&#xA;;; QUESTION SECTION:&#xA;;_search._tcp.marathon.mesos.   IN SRV&#xA;&#xA;;; ANSWER SECTION:&#xA;_search._tcp.marathon.mesos.    60 IN SRV 0 0 31302 10.254.132.41.&#xA;</code></pre>&#xA;&#xA;<blockquote>&#xA;  <p>Mesos-DNS supports the use of a task's DiscoveryInfo for SRV record generation.&#xA;  If no DiscoveryInfo is available then Mesos-DNS will fall back to those ""ports"" resources allocated for the task.&#xA;  The following table illustrates the rules that govern SRV generation:</p>&#xA;</blockquote>&#xA;"
26622688,26611387,1133660,2014-10-29T04:14:07,"<p>You will create <em>aggregate root</em> if you follow DDD principles wether you're using JPA or not.  This is one of the very fundamental building blocks in DDD.  From the book Eric Evan's DDD book:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Aggregates mark off the scope within which invariants have to be&#xA;  maintaned at every stage of lifecycle.  The following patterns,&#xA;  factories and repositories, operate on aggregates.</p>&#xA;</blockquote>&#xA;&#xA;<p>DTO and detached entity are related to JPA (technical constraints).  An <em>aggregate root</em> is also an <em>entity</em>.  When the <em>aggregate root</em> become unmanaged (by persistence context), it is called <em>detached entity</em>.</p>&#xA;&#xA;<p>Perhaps your question can be rephrased into: should I return <em>aggregate root</em> as <em>detached entity</em> or DTO?  The answer is subjective and depends on your environment.</p>&#xA;&#xA;<p>The benefit of returning <em>aggregate root</em> as <em>detached entity</em> is you don't need to create a new DTO class.  You can also call methods owned by the <em>aggregate root</em>.  The disadvantage is you usually won't populate the complete object graph for performance reason because some <em>aggregate roots</em> can have a very deep hierarchy.  This will lead to lazy loading exception if not handled properly.</p>&#xA;&#xA;<p>Returning DTO instead of <em>aggregate root</em> is considered as more robust design.  You will need to create a new DTO class for every <em>'use case'</em> of the <em>aggregate root</em>.  This maybe too cumbersome for small system, but if you're using DDD, I believe your requirement is complex.</p>&#xA;"
50300985,50284055,484444,2018-05-11T22:22:02,"<p>1)&#xA;Dataflow does not offer inspection to intermediary results. If a frontend wants more progress about an element being processed in a Dataflow pipeline, custom progress reporting will need to be built into the Pipline.</p>&#xA;&#xA;<p>One idea, is to write progress updates to a sink table and output molecules to that at various parts of the pipeline. I.e. have a BigQuery sink where you write rows like [""element_idX"", ""PHASE-1 DONE""]. Then a frontend can query for those results. (I would avoid overwriting old rows personally, but many approaches can work). </p>&#xA;&#xA;<p>You cand do this by consuming the PCollection in both the new sink, and your pipeline's next step.</p>&#xA;&#xA;<p>2)&#xA;Is your Microservice architecture using a ""Pipes and filters"" pipeline style approach? I.e. each service reads from a source (Kafka/RabbitMQ) and writes data out, then the next consumes it?</p>&#xA;&#xA;<p>Probably the best way to do setup one a few different Dataflow pipelines, and output their results using a Pub/Sub or Kafka sink, and have the next pipeline consume that Pub/Sub sink. You may also wish to sink them to a another location like BigQuery/GCS, so that you can query out these results again if you need to.</p>&#xA;&#xA;<p>There is also an option to use Cloud Functions instead of Dataflow, which have <a href=""https://cloud.google.com/functions/docs/calling/"" rel=""nofollow noreferrer"">Pub/Sub and GCS triggers</a>. A microservice system can be setup with several Cloud Functions.</p>&#xA;"
45935685,45751211,8289628,2017-08-29T09:40:30,"<p>I hope I've understood what you asked, but if not that the following at least contributes something to your thinking. </p>&#xA;&#xA;<p>The way I understand your question it is a very practical one, i.e. ""How do I organize the solutions and projects to minimize the risk that a developer breaks a service contract without understanding the impact it would have on consumers of that service?""</p>&#xA;&#xA;<p>In an ideal world, developers don't do that. If they make breaking changes to a service, they know they've done it, and it's a new version. That being said, I get your point - it's nice to have some sort of safeguard against that kind of human error since developers are only perfect most of the time.</p>&#xA;&#xA;<p>If you're working in an environment, where you're creating microservices that other teams depend on and vice-versa, there's no way around a disciplined and structured approach, i.e. proper governance. A full test environment that mirrors production, where all services are running and you can test that your service integrates well with the services it depends on doesn't hurt either.</p>&#xA;&#xA;<p>If you're in control of all the services that need to communicate with each other, there's (at least in my opinion) no reason why you shouldn't have a single solution where all your service projects are included and where your integration test project starts all the services it needs to and checks the integrations between them are working as expected.</p>&#xA;&#xA;<p>Your individual microservices shouldn't know anything about the outside world, apart from contracts/interfaces, and you should have Unit Tests that test every microservice in isolation, mocking out dependencies.</p>&#xA;&#xA;<p>If one solution becomes too large, you could break it into smaller pieces, to one solution for each service that includes the service and its nearest neighbours. In your example, that could be a solution that contains ThreeDSecureService, CreditCardService and CreditCardService.IntegrationTests (which tests CreditCardService which in turn invokes ThreeDSecureService).&#xA;That way, when a developer is working on ThreeDSecureService, the integration tests will let the developer know that he/she has broken the integration.&#xA;That still requires the developer to remember to actually run the Integration tests of course.</p>&#xA;&#xA;<p>The end goal as I see it, is to incorporate a gate into your build flow, that ensures that developers are not able to publish changes that would cause integration tests to fail, coupled with automatic deployment. That will ensure that basic integration errors do not make it into production.</p>&#xA;"
34278617,33431076,2544732,2015-12-14T23:41:03,"<p>I believe that your problem should solve itself by putting the project inside their own vhost. When projects live in their own vhost you will separate the files more easier.</p>&#xA;&#xA;<p>This way you can have the .env fils in different folders alltogether.</p>&#xA;&#xA;<p>I believe that what you are experiencing is that the application are bootstrapping at the same time therefore the variables are being used together.</p>&#xA;&#xA;<p>If you are using Homestead as your VM, try creating two domains and putting your project separated and when your laravel application needs to call lumen call it by host name and it should work. (Note only one vhost can be called localhost)</p>&#xA;"
49563892,49561411,7199068,2018-03-29T19:43:22,"<p>What you need to do is creating several modules (normally Maven modules), each of them is a SpringBoot application with its own application context and its own configuration, directories structure, etc.</p>&#xA;&#xA;<p>IntelliJ has a concept of Module which matches very well the Maven module concept.</p>&#xA;&#xA;<p>If you go to File, Project Structure, Project Settings, Modules you can see a very nice view of the current modules (in your case it should be only one). You can add or remove modules there. (Also by just clicking File-New Module, but the Project Structure View is more useful).</p>&#xA;&#xA;<p>If you decide going the Maven way, you can also create your structure in disk and its POMs and import maven project in IntelliJ. </p>&#xA;&#xA;<p>In any case, keep in mind that you will like to deploy every microservice as a separate and autonomous deployable unit.  </p>&#xA;"
51446312,46425554,4252764,2018-07-20T15:55:53,"<p>Thank you for the answers, but emphasize in the question is on microservice (MS) architecture and reusing defined entity POJOs from one MS in another as POJOs. From what I've read on microservices it's closely related to another question - should MSs share any common functionality and classes at all, or be completely independent? It seems there is no definite agreement on it, and also no definite answer, or widely accepted pattern, to this.</p>&#xA;&#xA;<p>From my recent experience here is what I adopted, and it works well so far.&#xA;Have common functionality across MSs - yes, in form of a commons project added as dependency to all MSs, with its dependencies set as optional. Share entity classes (expose them in commons) - no.</p>&#xA;&#xA;<p>The main reason is that entity classes are closely related to data store for particular MS. And as the established rule is that MSs shouldn't share data stores, then it makes sense not to share entity classes for those data stores. It helps MSs to be more independent, and freedom to manage their data store in their own way. It means some more typing to add additional DTO classes and conversion between them, but it's a trade-off worth taking to retain MS independence. Reasons Christian Beikov and  Maksim Gumerov mentioned apply as well.</p>&#xA;&#xA;<p>What we do share (put in commons) are some common functionality and helper classes (for cloud, discovery, error handling, rest and json configuration...), and pure DTOs, where T is transfer between MSs (rest entities or message payloads).</p>&#xA;"
51419441,51419192,4457053,2018-07-19T09:48:29,"<p>It is not a good idea to share classes between microservices, if you want to replace microservice A, you'll have to adapt Microservice B. &#xA;Every Service must implement its own data classes which holds the fields which are needed for the service.</p>&#xA;&#xA;<p>MicroService A and MicroService B both can contain a class Foo but this classes can be different by its fields. Perhaps both contain the field 'id' and 'name' but only Microservice A also needs a field 'date' to do his work. </p>&#xA;"
45058590,45056729,344669,2017-07-12T13:08:52,<p>It doesn’t replace any kubernetes core components. ISTIO is add-on for kubernetes to manage the micro service.&#xA;In the IBM article you can use ISTO for routing instated of Ingress Controller. </p>&#xA;
40685465,40660618,1721437,2016-11-18T20:29:48,"<p>I'll answer the points that are not opinion based:</p>&#xA;&#xA;<blockquote>&#xA;  <ol>&#xA;  <li>Elastic Beanstalk gets subdomain on application creation. This&#xA;  subdomain should be used by API Gateway with integration type: AWS&#xA;  service, in action configuration - Am I right?</li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p>No, AWS service integration would only apply if you were actually calling the <strong>Elastic Beanstalk service</strong>. You would be calling your own beanstalk <em>instance</em> so you would use HTTP integration.</p>&#xA;&#xA;<blockquote>&#xA;  <ol start=""2"">&#xA;  <li>What would represent a single microservice? An Elastic Beanstalk's application is a specific scalable microservice?</li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p>This is up to you, but as mentioned in comments, many customers choose to do this via Lambda functions rather than beanstalk applications. Using Lambda has the benefit that you do not need to managed the scaling of your beanstalk application.</p>&#xA;&#xA;<blockquote>&#xA;  <ol start=""4"">&#xA;  <li>Test environment: What structure should I use in test environment (or&#xA;  staging env.)? I think about creating separate VPC with another&#xA;  Elastic Beanstalk and other Amazon services.</li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p>Just a note here, API Gateway cannot contact resources in your VPC currently. Any beanstalk instance would need to be publically accessible.</p>&#xA;&#xA;<blockquote>&#xA;  <ol start=""5"">&#xA;  <li>Test environment and API Gateway: How should I set up an API Gateway? It should allow clients to access the microservices in test environment if request has specific subdomain, like: test.mydomain.com/hello_world/say_hello. I'm not sure how to use API Gateway in CI/CD to make it fast and simple, without manual copying some configuration from test stage to the production stage. (I'm not expecting any complex solution, only some hints about what components, parts, concepts could I use for it. More details I'll find on my own).</li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p>You should take a look at <a href=""http://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html"" rel=""noreferrer"">stage variables</a>. This would allow you to use basic configuration with differences between dev/test/prod stored in these variables.</p>&#xA;"
42830426,42816444,100957,2017-03-16T09:47:44,"<p>I think your question includes a few concerns that may or may not be separate</p>&#xA;&#xA;<ol>&#xA;<li>running the microservices with lifecylce operations</li>&#xA;<li>health checks - is my service alive </li>&#xA;<li>telemetry</li>&#xA;</ol>&#xA;&#xA;<p>There are several solutions available where 2 and 3 build on 1 :</p>&#xA;&#xA;<ol>&#xA;<li>Orchestration systems like Kubernetes or the OpenShift distribution of it, take your µ-service, but it in an Docker image and then schedule it to run inside a cluster. There are other solutions like Docker-swarm that do similar things</li>&#xA;<li>Kubernetes and OpenShift can regularly ""ping"" your service. Be it via http or by invocation of a command. If this shows the service is unhealthy, then it is killed and a new instance deployed by the system. Similar to this liveness probe, there is also a readiness probe that can decide from what point in time on your service is ready to receive requests</li>&#xA;<li>There is a ton of metrics system out there that can retrieve telemetry data from the systems and the services. OpenShift for example comes with Hawkular Metrics out of the box. Data gathered can be graphed or alerted upon.</li>&#xA;</ol>&#xA;"
39425599,39425569,37213,2016-09-10T11:32:39,<p>JMS not part of Java SE.  You need Java EE or Spring.  </p>&#xA;&#xA;<p>I'm not sure queues are the best way to solve the problem.</p>&#xA;&#xA;<p>I'd recommend Spring Boot.  </p>&#xA;&#xA;<p>Subscription maintenance isn't something that the queue would do. </p>&#xA;&#xA;<p>A simple REST service would manage this nicely.  You'll need a persistence layer to save subscription information.</p>&#xA;
35875626,35875489,1087837,2016-03-08T18:53:36,"<p>Short answer IMO: No.</p>&#xA;&#xA;<p>A microservice might not even <strong>have</strong> an API. It could be a scheduled job processor for instance. It can be started, runs perhaps nightly and does some work. No API.</p>&#xA;&#xA;<p>Suggested reading on Microservices (old but good): <a href=""http://martinfowler.com/articles/microservices.html"" rel=""nofollow"">http://martinfowler.com/articles/microservices.html</a></p>&#xA;"
46924477,46924175,1153885,2017-10-25T05:15:43,<ol>&#xA;<li>Most payment gateways support REST based APIs. So it doesn't matter what the underlying technology of your application is.</li>&#xA;<li>It is possible to host both PHP &amp; Node.js applications in a single machine as long as they map to different ports.</li>&#xA;<li>Microservices often communicate with REST based APIs. So communication between the 2 applications above should be possible with or without a message broker like RabbitMQ. Whether you should have this mechanism in place for your scenario is debatable.</li>&#xA;</ol>&#xA;
48526839,43542042,9290333,2018-01-30T17:04:42,"<p>Try to place Rabbitmq or Redis in front of GELF.</p>&#xA;&#xA;<p>You'll want to split the filtering from the ingestion in a centralized manner, add several Logstash shippers or just have a way to buffer new logs from any type of slower parsing. You can split the original log.conf into two files depending if they’re reading into Redis or grabbing from the queue, parsing and sending to ES.</p>&#xA;"
39027368,38507565,1568605,2016-08-18T20:56:23,"<p>Change your MobileSecurityConfiguration</p>&#xA;&#xA;<pre><code>@Inject&#xA;private DiscoveryClient discoveryClient;&#xA;&#xA;private String getKeyFromAuthorizationServer() {&#xA;          List&lt;String&gt; services = discoveryClient.getServices();&#xA;&#xA;          HttpEntity&lt;Void&gt; request = new HttpEntity&lt;Void&gt;(new HttpHeaders());&#xA;          String value = (String) this.keyUriRestTemplate&#xA;              .exchange(""http://uaa/oauth/token_key"", HttpMethod.GET, request, Map.class).getBody()&#xA;              .get(""value"");&#xA;&#xA;          return value;&#xA;      }&#xA;</code></pre>&#xA;&#xA;<p>so basically that one line : </p>&#xA;&#xA;<pre><code>List&lt;String&gt; services = discoveryClient.getServices();&#xA;</code></pre>&#xA;&#xA;<p>fix that problem.</p>&#xA;"
50395716,50392109,1054558,2018-05-17T15:50:56,"<blockquote>&#xA;  <p>Understanding Cassandra - can it replace RDBMS?</p>&#xA;</blockquote>&#xA;&#xA;<p>The short answer here, is ""NO.""  Cassandra is not a simple drop-in replacement for a RDBMS, when you suddenly need it to scale.</p>&#xA;&#xA;<blockquote>&#xA;  <p>While these concepts make sense to me, I'm struggling to see how this would fit most long-term database needs.</p>&#xA;</blockquote>&#xA;&#xA;<p>It fits long-term database needs <strong>if</strong> you're applying it to the right use case.</p>&#xA;&#xA;<p><strong>DISCLAIMER</strong>: I <em>am</em> a bit of a Cassandra zealot.  I've used it for a while, made minor contributions to the project, been named a ""Cassandra MVP,"" and even co-authored a book about it.  I think it's a <em>great</em> piece of tech, and you can do amazing things with it.</p>&#xA;&#xA;<p>That being said, there are a lot of things that it's just not good at:</p>&#xA;&#xA;<ul>&#xA;<li><p><strong>Query flexibility</strong>.  The tradeoff you make for spreading rows across multiple nodes to meet operational scale, is that you <em>have</em> to know your query patterns ahead of time, and then follow them strictly.  The idea, is that you want to have all queries served by a single node.  And you'll have to put some thought into your data model to achieve that.  Unbound queries (<code>SELECT</code>s without <code>WHERE</code> clauses) become the enemy.</p></li>&#xA;<li><p><strong>Updating data in-place</strong>.  Plan on storing values by a key, but then updating them a lot (ex: status)?  Cassandra is not a good fit for that.  This is because Cassandra has a log-based storage engine which doesn't overwrite anything...it just <em>obsoletes</em> it.  So your previous values are still there, and still take up space and compute resources.</p></li>&#xA;<li><p><strong>Deleting Data</strong>.  Deleting data in the distributed database world is tricky.  After all, how do you replicate <em>nothing</em> to another node?  Cassandra's answer to that problem, is to use a structure called a <strong>tombstone</strong>.  Tombstones take up space, can slow performance, and need to stay around long enough to replicate (making their removal tricky).</p></li>&#xA;<li><p><strong>Maintaining Data Consistency</strong>.  Being highly-available and partition tolerant, Cassandra embraces the concept of ""eventual consistency.""  So it should come as no surprise that it really wasn't designed to be consistent.  It has a lot of mechanisms which will <em>help</em> keep data consistent, but they are far from perfect.  Plus, there really isn't a way to know <em>for sure</em> if your data is in sync or not.</p></li>&#xA;</ul>&#xA;&#xA;<blockquote>&#xA;  <p>If data is redundant across several different tables...how is it managed and kept consistent across those many tables? Are materialized views the answer in this case?</p>&#xA;</blockquote>&#xA;&#xA;<p>Materialized views are something that I'd continue to stay away from for the foreseeable future.  They're ""experimental"" for a reason.  Basically, once they're out of sync, the only way to get them back in sync is to rebuild them.</p>&#xA;&#xA;<p>I coach my dev teams on keeping their query tables (tables containing the same data, just keyed differently) in sync with <code>BATCH</code> statements.  In fact, BATCH is a misnomer as it probably should have bene named ""ATOMIC"" instead.  Because of its name, it is heavily mis-used, and its mis-use can lead to problems.  But, it does keep mutations applied atomically, so that does help.</p>&#xA;&#xA;<p>Basically, scrutinize your database requirements.  If Cassandra doesn't cut it, then try to find one which does.  CockroachDB (or one of the other NewSQLs) might be a better fit for what you're talking about.  It tries to be a drop-in for Postgres, and it scales with some Cassandra-like mechanisms, so it might be worth looking into.</p>&#xA;"
50714003,40467382,2237022,2018-06-06T06:55:37,"<p>Thanks for the Answers guys,&#xA;Coming to the answer,i have implemented my own way of authorization.&#xA;I had come up with a design where User -> Roles -> Resources -> Permissions&#xA;Here the resources are individual parts where every user has some permissions upon using a resource and role has set of defined set of resources with some permissions like </p>&#xA;&#xA;<pre><code>read_only,read_create,read_update etc&#xA;</code></pre>&#xA;&#xA;<p>Each user can have any number of roles, thus user having a permission to access a specific resource.And i perform this check for each action using</p>&#xA;&#xA;<pre><code>before_action&#xA;</code></pre>&#xA;&#xA;<p>Thanks,&#xA;Suresh</p>&#xA;"
34929594,34909182,2633566,2016-01-21T16:50:26,"<p>This is a very general question, but as a guideline I would suggest starting small and minimal, and expanding only if necessity dictates it. </p>&#xA;&#xA;<p>So what is the minimal setup you need? Probably Host, Logic (including business, model, DAL etc., they can be just under different folders) and Tests. </p>&#xA;&#xA;<p>Start from that. See how that works. See if you really need anything more. Learn and iterate. That's part of the mindset you can really use when moving into the microservices way of thinking (and there's some agile connection there...). </p>&#xA;"
44874884,31088764,2633566,2017-07-02T20:20:34,"<p>We've recently open sourced our .NET microservices framework, that covers a couple of the needed patterns for microservices. I recommend at least taking a look to understand what is needed when you go into this kind of architecture. &#xA;<a href=""https://github.com/gigya/microdot"" rel=""nofollow noreferrer"">https://github.com/gigya/microdot</a></p>&#xA;"
29741870,29689630,2633566,2015-04-20T07:40:24,"<p>This is a big subject indeed. When moving into microservices, you need to remember that a lot of your headaches are going to be operational (devops oriented) rather than ""code"". </p>&#xA;&#xA;<p>Also, there's so much out there that at some point you will just need to make a decision, regardless if it's optimal. Things will change along the way in any case. Make the best decision you can at the moment, go with it, and tweak later.</p>&#xA;&#xA;<p>Regarding your questions, REST is a common practice. There are also other options (WCF, Avro, whatever). Give yourself a timeline for a decision. If you know REST and feel comfortable with REST, take the decision. Try to see if you can build it so you can change / add other protocols later, but don't make it delay you too much.</p>&#xA;&#xA;<p>Like Udi said, some architecture considerations are if you need to call this service sync or async (via a message bus). &#xA;Also, yes, think about service discovery. There are several options (zookeeper, consul).</p>&#xA;&#xA;<p>For some background and overview, try going through some of the resources:<br>&#xA;<a href=""http://blog.arkency.com/2014/07/microservices-72-resources/"" rel=""nofollow"">http://blog.arkency.com/2014/07/microservices-72-resources/</a></p>&#xA;&#xA;<p>This also gives a quick overview of microservices patterns:<br>&#xA;<a href=""http://microservices.io/patterns/microservices.html"" rel=""nofollow"">http://microservices.io/patterns/microservices.html</a></p>&#xA;&#xA;<p>But again, there's a lot of information and ways of doing things, don't get swamped. </p>&#xA;"
29594631,29460485,2633566,2015-04-12T21:04:34,"<p>This seems to be a very common and central question when moving into microservices. I wish there was a good answer for that :-)</p>&#xA;&#xA;<p>About the suggested pattern already mentioned here, I would use the term Data Denormalization rather than Polyglot Persistence, as it doesn't necessarily needs to be in different persistence technologies. The point is that each service handles its own data. And yes, you have data duplication and you usually need some kind of event bus to share data across services.</p>&#xA;&#xA;<p>There's another option, which is a sort of a take on the first - making the <em>search itself</em> as a separate service.    </p>&#xA;&#xA;<p>So in your example, you have the User service for managing users. The Purchases services manages purchases. Each handles its own data and only the data it needs (so, for instance, the Purchases service doesn't really need the user name, only the ID). And you have a third service - the Search Service - that consumes data produced by other services, and creates a search ""view"" from the combined data.</p>&#xA;"
42657205,42105805,2903646,2017-03-07T19:58:56,"<p>You can design your communications strategies in many ways. Obviously  you can use your API Gateway to do this (I assume that you are using this <a href=""http://microservices.io/patterns/apigateway.html"" rel=""nofollow noreferrer"">pattern</a>) but this isn't a good idea because to begin it is not  a good use of the pattern and for other hand many people think that the use of apirests should never be used to inter-communication of mservices. One a best approach to make this instead is base the communications  on asynchronous message-passing, to do this you can use <a href=""https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern"" rel=""nofollow noreferrer"">Publish–subscribe pattern</a>, but always you can take a look to <a href=""https://en.wikipedia.org/wiki/Event-driven_architecture"" rel=""nofollow noreferrer"">Event Pattern</a> very used too. </p>&#xA;&#xA;<p>This is a simple list of some articles that maybe could help you, take a look:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://www.nginx.com/blog/building-microservices-inter-process-communication"" rel=""nofollow noreferrer"">Building Microservices: Inter-Process Communication in a Microservices Architecture</a></li>&#xA;<li><a href=""https://www.innoq.com/en/blog/why-restful-communication-between-microservices-can-be-perfectly-fine/"" rel=""nofollow noreferrer"">Why RESTful communication between microservices can be perfectly fine</a></li>&#xA;<li><a href=""http://microservices.io/patterns/communication-style/messaging.html"" rel=""nofollow noreferrer"">Pattern: Messaging</a></li>&#xA;<li><a href=""http://howtocookmicroservices.com/communication/"" rel=""nofollow noreferrer"">Technical Approach of Inter-Service Communication</a></li>&#xA;</ul>&#xA;&#xA;<p>Is a lot of info if you don't have previous experience in this issues but with a bit of researching you will find soon a good strategy to use in you project.</p>&#xA;"
42607728,42607458,2903646,2017-03-05T11:19:59,"<p>I'm working also with an app deployed in App Engine that is microservices based and as I try to build this mServices to can run in any place (in spite of this is running of App Engine) my strategie is use  as few as possible Google tools. I preffer do unit tests based in raw request (also between mServices) to check everything, but of course it depends of each domain and each specific architecture implemented, but <em>this is only my opinion</em>.</p>&#xA;&#xA;<p>If you want talk with people with experience you can join to <a href=""https://googlecloud-community.slack.com/"" rel=""nofollow noreferrer"">Slack group</a> and vistit also community forum for users of Google App Engine in <a href=""https://groups.google.com/forum/#!forum/google-appengine"" rel=""nofollow noreferrer"">Google Groups</a>.</p>&#xA;&#xA;<p>Anyway please tell us how have you solved your problem when you do this because there isn't  too much info about in the net. </p>&#xA;"
33180751,33180090,1347422,2015-10-16T22:58:56,"<p>By default csrf protection is enabled. You'll have to configure all your pages to include the _csrf token for this to work&#xA;See here: <a href=""http://docs.spring.io/spring-security/site/docs/current/reference/html/csrf.html#csrf-include-csrf-token-in-action"" rel=""nofollow"">CSRF Spring docs</a></p>&#xA;&#xA;<p>You can always disable csrf protection.&#xA;If configuring in code:</p>&#xA;&#xA;<pre><code>@EnableWebSecurity&#xA;public class WebSecurityConfig extends&#xA;WebSecurityConfigurerAdapter {&#xA;&#xA;@Override&#xA;protected void configure(HttpSecurity http) throws Exception {&#xA;http&#xA;.csrf().disable();&#xA;}&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>or in XML:</p>&#xA;&#xA;<pre><code>&lt;http&gt;&#xA;    &lt;!-- ... --&gt;&#xA;    &lt;csrf disabled=""true""/&gt;&#xA;&lt;/http&gt;&#xA;</code></pre>&#xA;"
50739596,50736767,5939806,2018-06-07T11:03:18,"<hr>&#xA;&#xA;<p>I think that question covers so much ground and is so generic that it isn't possible to give you a proper answer. However, I thought I'd also check what's out there and you might try those:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""http://zoltanaltfatter.com/2016/03/11/dockerized-spring-boot-service-on-aws/"" rel=""nofollow noreferrer"">http://zoltanaltfatter.com/2016/03/11/dockerized-spring-boot-service-on-aws/</a></li>&#xA;<li><p><a href=""https://github.com/altfatterz/spring-boot-docker"" rel=""nofollow noreferrer"">https://github.com/altfatterz/spring-boot-docker</a></p></li>&#xA;<li><p><a href=""https://blog.cloudthat.com/implementation-of-microservices-with-spring-boot-on-aws-and-in-docker-part-1/"" rel=""nofollow noreferrer"">https://blog.cloudthat.com/implementation-of-microservices-with-spring-boot-on-aws-and-in-docker-part-1/</a></p></li>&#xA;</ul>&#xA;&#xA;<p>However, in order to help you properly, you should specify a little where your problems lay. </p>&#xA;"
45631293,45547556,1421918,2017-08-11T09:13:08,"<p>I would suggest dockerizing your microservices (easy with spring-boot), and then using ECS (Elastic Container Service) and ELB (Elastic Load Balancer) with application loadbalancers. (can be internal, or internet faced).</p>&#xA;&#xA;<p>ECS and ELB then utilizes your microservices <code>/health</code> endpoints when you deploy new versions.</p>&#xA;&#xA;<p>Then you could implement a more sophisticated <code>HealthIndicator</code> in spring-boot, to determine whether or not the application is healthy (and therefor ready to recieve incomming requests). Only when the new application is healthy, is it put into service, and the old one(s) are put to sleep.</p>&#xA;&#xA;<p>Then test all your business logic on a <code>test environment</code>, and because of Docker, you're running the exact same image on all environment, you shouldn't need to be running (any) tests when deploying to production. (Because it has already been tested, and if it boots up, you're good to go).</p>&#xA;"
48461673,48460986,122718,2018-01-26T12:47:10,"<p>I think you correctly identified downsides to the microservices approach! And there are no elegant solutions to these specific problems. You will have to eat the additional work and architecture deterioration that this brings.</p>&#xA;&#xA;<p>Concretely addressing your question now:</p>&#xA;&#xA;<blockquote>&#xA;  <p>The question I have is should it be ""Search"" that queries ""Product"" and ""Customer"", or should ""Product"" and ""Customer"" send data to ""Search"" ?</p>&#xA;</blockquote>&#xA;&#xA;<p>You seem to be looking for a data synchronization service. You want to decide between push and pull. You are concerned about data freshness and logic duplication.</p>&#xA;&#xA;<p>The key point here is that the source service cannot know about its consumers. This is to prevent an unwanted reverse dependency. This would break architectural isolation. Any data sync process that maintains this is fine. You can do what is most convenient.</p>&#xA;&#xA;<p>For example, you could make the data source expose two APIs:</p>&#xA;&#xA;<ol>&#xA;<li>An API to get the whole data set. This would be called periodically by the destination (e.g. nightly). It can also be used to seed the destination at will and to fix data errors there.</li>&#xA;<li>A feed of changes in the source database keyed by the date and time the change occurred. The destination can now poll that change feed very frequently (e.g. every few seconds or minutes) and apply the small delta that occurred.</li>&#xA;</ol>&#xA;&#xA;<p>You can even build a realtime change feed through a publish-subscribe middleware. Many message queue softwares can do that. The source would just send out changes to the middleware.</p>&#xA;&#xA;<p>Building all of this is conceptually simple but takes a lot of work. It also creates lots of ongoing work and increases the potential for bugs. Debugging becomes much harder. I have worked on systems like that.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>I'm going to add a subjective note: Microservices are not well understood by many teams. The downsides are often ignored. You identified a few of the downsides correctly and they are nasty! Given what I read on the web I believe many teams do not realize the mess they are getting themselves into. Managing disparate data stores can be a nightmare. This is not a one-time ""mess"" but an ongoing one.</p>&#xA;&#xA;<p>As an alternative I'd recommend using a common data store and building services simply as classes or projects that live in the same process. This gives you the microservices code structuring with the convenience of normal development. It also leaves a few of the upsides of microservices on the table.</p>&#xA;"
51346642,51312346,122718,2018-07-15T08:27:11,"<p>Remember <em>why</em> you are using microservices. Using them is not a goal on its own. They are a tool to achieve certain ends such as enforced modularization, independent deployment and more parallel development.</p>&#xA;&#xA;<p>There is no general guidance possible to their size. Making them as small as possible is a non-goal. Rather, find the <em>right</em> size. Not the smallest size possible.</p>&#xA;&#xA;<p>It therefore seems dubious to split the direct and asynchronous data updates into two services. There is simply no driving need.</p>&#xA;&#xA;<p>Since microservices should own their data it is natural to have both these concerns governed by a single microservice operating on that data.</p>&#xA;&#xA;<p>Also, it is a non-goal to avoid mixing communication methods.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Message consumers are better run as a service than inside something like a web project hosted in IIS</p>&#xA;</blockquote>&#xA;&#xA;<p>That is not technically true. For redundancy you need multiple consumers running on each queue. You can never rely on there being a single consumer or there being always at least one consumer. Therefore, a windows service gains you nothing. Doing this from inside a normal web application is not problematic in my view. It also potentially saves you the need for a windows service. They are hard to work with (deployment is a bit nasty).</p>&#xA;&#xA;<p>It feels to me you are overcomplicating things in order to meet certain abstract guidances.  Do not architect based on such blanket guidance. Architect with regard to the specific case.</p>&#xA;&#xA;<p>From what I have heard all you need is a single IIS hosted ASP.NET web application that does all of this including the data processing. But if you feel the need to introduce a service then you could consider splitting the data processing off into another ASP.NET web application which offers a service to the UI application. This service would include methods to retrieve and alter data as well as some kind of notification for asynchronous updates (like an event but across a service boundary).</p>&#xA;"
51249977,51249515,122718,2018-07-09T16:18:22,"<p>No production grade server has any issue with being a long running process. The only thing that can happen are leaks. These servers are tested for leaks. When you send 100k requests per second for a few minutes even a one byte leak becomes obvious.</p>&#xA;&#xA;<p>I'm not sure what you mean by sharing resources more efficiently. The server has fairly little performance impact on typical apps. Most apps actually do some work which overshadows the server overhead. The application code will always perform exactly the same way. Just the server overhead might be a little less or more.</p>&#xA;&#xA;<p>So it does not really matter for throughput which server you use if your application does meaningful work on each request. If you just benchmark a <code>return ""Hello World"";</code> then you measure the framework overhead which likely does not matter much.</p>&#xA;&#xA;<p>I recommend that you use the server type that is most convenient and productive to use.</p>&#xA;"
36741586,36705199,122718,2016-04-20T10:54:07,"<p>Put the work items into a relational database. You can still use queues to push work to workers but the queue items can now be empty. When a worker receives an item he know to look into the database instead. The content of the message is disregarded.</p>&#xA;&#xA;<p>That way messages are independent and idempotent. For queueing to work these two properties usually must hold.</p>&#xA;&#xA;<p>That way you can more easily sequence actions that actually are sequential. You can use transactions as well. </p>&#xA;&#xA;<p>Maybe you don't need queues at all. Maybe it is enough to have a fixed number of workers polling the database for work. This loses auto-scaling with queues, though.</p>&#xA;"
35520540,34578641,981598,2016-02-20T07:45:06,"<p>From my limited experience, I believe that it is overkill. If an attacker has access to listen in or interact with your internal network then there are most likely other issues which you should be contending with.</p>&#xA;&#xA;<p>This article on <a href=""https://auth0.com/blog/2015/09/13/an-introduction-to-microservices-part-2-API-gateway/"" rel=""nofollow"">auth0.com</a> explains the use of SSL only on connections to the external client. I also share this view and believe implementing SSL at an individual service level would get extremely difficult unless you where running some form of proxy such as HAProxy or Nginx on each individual host which is sub-optimal, especially if you're using a form of managed cluster like Kubernetes or Docker Swarm.</p>&#xA;&#xA;<p>My current thoughts are its fine to run SSL just for your edge services, ensure you lock down your AWS network using something like <a href=""https://isecpartners.github.io/Scout2/"" rel=""nofollow"">Scout2</a> and run unencrypted for inter-service communication on your lan.</p>&#xA;"
38168816,38168658,981598,2016-07-03T10:02:34,"<p>From the information you have provided, using a standalone favourite service sounds like the right option.</p>&#xA;&#xA;<p>A secondary simpler and quicker option might be to also to handle this on your user service which looks after the persistence of your users data as favourites are exclusive to a user entity.</p>&#xA;&#xA;<p>As for ID's, I haven't seen many reasons as to why this might be a bad idea? Your individual services are going need to store some identifying value for related data and the main issue here I feel is just keeping this ID field consistent across your different services. What you choose just needs to be reliable and predictable to keep things easy and simple as your system grows.</p>&#xA;"
40234142,40231507,1375326,2016-10-25T07:43:21,"<p>The <code>sandbox init</code> command currently expects that <code>docker-machine</code> is installed. On Linux, this is not the case. Instead, native <code>docker-engine</code> is used. So this is a bug and as a result, the <code>sandbox init</code> command is currently not supported on Linux. The PR <a href=""https://github.com/typesafehub/conductr-cli/pull/139"" rel=""nofollow"">https://github.com/typesafehub/conductr-cli/pull/139</a> is addressing this. Once the PR is merged the <code>sandbox init</code> command will be supported on all operation systems.</p>&#xA;&#xA;<p>Nevertheless, you don't need to wait until this issue has been fixed. The <code>sandbox init</code> command only checks that the VM you want to run ConductR on, in your case Ubuntu Virtual Box, has sufficient resources to start ConductR. All other commands are working on Linux. I would advise using a VM with at least 4 CPUs and 4 GiB of memory. ConductR alone does not need so many resources so it highly depends on what you want to run on ConductR and how many ConductR instances you want to run in your VM. The max heap of a ConductR core node itself is set to 256 MB of memory, the max heap for a ConductR agent node is set to 128 MB of memory. </p>&#xA;&#xA;<p><strong>Resource Example</strong></p>&#xA;&#xA;<ul>&#xA;<li>3 ConductR core nodes:  768  MB</li>&#xA;<li>3 Conductr agent nodes: 384  MB</li>&#xA;<li>ConductR total:         1152 MB</li>&#xA;</ul>&#xA;&#xA;<p>So in the case of a VM with 4 GiB you almost have 3 GiB available for the services you want to run on ConductR, e.g. Lagom, Cassandra, Akka, other Docker images.</p>&#xA;"
43030546,34439201,2035039,2017-03-26T15:16:11,"<p>I had exactly the same problem: linking to my consul container was not enough. However, the following did it for me: As stated <a href=""https://cloud.spring.io/spring-cloud-consul/"" rel=""nofollow noreferrer"">here</a>, the corresponding consul host and port configuration needs to be placed in <code>bootstrap.yml</code>, not <code>application.yml</code>.</p>&#xA;&#xA;<pre><code>spring:&#xA;  cloud:&#xA;    consul:&#xA;      host: consul&#xA;      port: 8500&#xA;</code></pre>&#xA;&#xA;<p>with the corresponding <code>docker-compose.yml</code>:</p>&#xA;&#xA;<pre><code>version: ""2.0""&#xA;&#xA;services:&#xA;  consul:&#xA;    image: consul:latest&#xA;    ports:&#xA;      - ""8500:8500""&#xA;my-service:&#xA;    build: path/to/dockerfile&#xA;    depends_on:&#xA;      - consul&#xA;    links:&#xA;      - consul:consul&#xA;</code></pre>&#xA;"
36742743,36291878,2635359,2016-04-20T11:43:22,"<p>I'm not a microservices expert, just started to get my feet wet in the microservices world. From what I've read until now, this could be handled in many ways, one of which as you mentioned is hard coding api-keys so that services recognise one another. But I <strong>never liked</strong> this idea personally - also using a user per service like you mentioned. A solution I <strong>really liked</strong> is using <strong><em>Oauth2</em></strong> for handling these scenarios - an interesting implementation I found is Gluu Server and I think <strong><em>client credentials grant type</em></strong> is what you're looking for - refer <strong><a href=""https://gluu.org/docs/integrate/oauth2grants/"" rel=""nofollow"">https://gluu.org/docs/integrate/oauth2grants/</a></strong>.</p>&#xA;&#xA;<p>Have fun :)</p>&#xA;"
48295960,48295080,4478047,2018-01-17T07:44:09,"<blockquote>&#xA;  <p>Is JSF appropriate for this task?</p>&#xA;</blockquote>&#xA;&#xA;<p>Sure</p>&#xA;&#xA;<blockquote>&#xA;  <p>And which tags can be used?</p>&#xA;</blockquote>&#xA;&#xA;<p>Any that will help you design the UI, just like with plain html. How you compose e.g. a 'full' page out of smaller pages is up to you. Do you want to use templates? You have to decide. You could use <code>ui:include</code> with a resource resolver, iframes but also use real (jsf based) portlets if that helps. But effectively this is not really a question for Stackoverflow.</p>&#xA;&#xA;<blockquote>&#xA;  <p>I could not find any similar approach using JSF!</p>&#xA;</blockquote>&#xA;&#xA;<p>That is because it mostly UIX <a href=""https://ux.stackexchange.com/"">https://ux.stackexchange.com/</a> initially. And when you know some of the basic features of a technology (facelets for jsf in this case), it is sort of 'logical' how to do it and you hence won't find lots of howtos or similar</p>&#xA;"
30909381,30908112,551263,2015-06-18T07:44:46,"<p>You microservice design is poor. You are modeling (<code>location</code> and <code>items</code>) 1 class = 1 microservice and this is not a good idea.</p>&#xA;&#xA;<p>You shoul modeling microservices like <code>Aggregate Roots</code> in <code>DDD</code>; even with its own bounded context. So, in your case, you should model an <code>Aggregate Root</code> with <code>location</code>,  <code>items</code> and <code>user</code> that allows to check domain rules at <code>item addition user action</code>. This could be, i.e., in your <code>Stock Context</code>.</p>&#xA;&#xA;<p>Of course, this doesn't mean that you should not have a <code>Wharehouse Context</code> in wich you can add, modify and/or delete <code>locations</code> and (if no need of depencies to check domain rules) the <code>Aggregate Root</code> is just <code>Location class</code>. But this is other microservice in another context. </p>&#xA;&#xA;<p>This <a href=""https://www.tigerteam.dk/2014/micro-services-its-not-only-the-size-that-matters-its-also-how-you-use-them-part-2/"">post</a> should help you. It will bring you a big A-HA! in your mind after reading it.</p>&#xA;"
45447258,45431599,4317279,2017-08-01T20:37:12,<p>You might implement this in different ways. The warehouse micro-service may consume data from the customer micro-service and enrich its response having everything in it for the presentation. Or the presentation may consist of several areas which are loaded from different micro-services each presenting its section.</p>&#xA;
44814546,44785897,4317279,2017-06-29T00:31:03,<p>If your JWT token is signed by the web microservice it may share the public key with other microservices like user-account microservice. This will allow other micro-services to do authentication using the existing JWT token but not generate a new one. &#xA;To use JWT correctly you need to ensure that JWT token has a limited expiration time and JWT token body doesn't contain any sensitive information. </p>&#xA;
39377147,39364466,57695,2016-09-07T18:48:39,"<p>Each Docker and JVM copy running uses memory. Normally having multiple JVMs on a single node would use shared memory, but this isn't an option with docker.</p>&#xA;&#xA;<p>What you can do is reduce the maximum heap size of each JVM.  However, I would allow at least 1 GB per docker image as overhead plus your heap size for each JVM. While that sounds like a lot of memory it doesn't cost so much these days.</p>&#xA;&#xA;<p>Say you give each JVM a 2 GB heap and add 1 GB for docker+JVM, you are looking needing a 64 GB server to run 20 JVMs/dockers.</p>&#xA;"
40216452,40216362,57695,2016-10-24T10:44:18,<p>I would use  common library for DTOs but you need to have serialization which is tolerant and allows version differences e.g. handle added/removed fields or a change of data types. If you share code you have to allow for the code to be running different a version in each service so that when you upgrade one service you are not forced to update any other.</p>&#xA;
51614359,51603557,6415199,2018-07-31T13:18:45,"<p>I took a quick look.  It looks like your gateway is NOT a ResourceServer, so it wouldn't accept an access token.</p>&#xA;&#xA;<p>Also note, you shouldn't put secrets like (including access tokens) on GitHub or StackOverflow. I'd recommend you replace the secrets associated with this post.</p>&#xA;"
51797037,38380827,3994207,2018-08-11T06:17:41,"<p>Sorry to reply 2 years later but it was my first hit on the search engine. You are mentioning sharing DAO, etc. but as much as I understand micro services, isn't it one of the main goals to don't share DAO at all. Does not this structure ruins actually your micro services architecture? Any feedback on this 2 years later? ;)</p>&#xA;&#xA;<p>But I do agree, that there is a really small code base which can be shared. But it's only some basic utils, exceptions base, etc. so really basic stuff which could be shared among any project in a company.</p>&#xA;&#xA;<p>I would never create a shared core code base or/and a deployment with dependencies to external jars. For my understanding the deployment of a micro services, based on java, must be as simple as ""java -jar application.jar"" ... makes local testing, CI, deployment and replacement of an application so easy</p>&#xA;&#xA;<p>Wondering about other opinions</p>&#xA;&#xA;<p>happy coding ;)</p>&#xA;"
38049314,38048683,6419039,2016-06-27T08:38:08,"<p>The = sign is invalid.</p>&#xA;&#xA;<pre><code>location / {&#xA;    return 301 https://$http_host/dir/;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>But that will end in a redirect loop, so be more specific:</p>&#xA;&#xA;<pre><code>location ~ ^[/]?$ {&#xA;    return 301 https://$http_host/dir/;&#xA;}&#xA;</code></pre>&#xA;"
51036245,51035930,4265824,2018-06-26T06:31:55,"<p>Add API version like </p>&#xA;&#xA;<pre><code>@RestController&#xA;@RequestMapping(""/API/V1"")&#xA;public class TestController {&#xA;&#xA;@RequestMapping(""/greeting"")&#xA;    public String greeting( {&#xA;        return ""welcome"";&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
49626621,49573552,5584992,2018-04-03T09:29:03,<p>You can use internal Loadbalancer for each service and create DNS records us it for app communication Also ECS and a service discovery feature that is useful in this scenario.</p>&#xA;
49668766,49666398,2726496,2018-04-05T09:25:48,"<p>I have found the error. Error was when registering with the Eureka Server, my services doesn't register with correct ip address. Because it is run on a docker container it doesn't register ip correctly.<br>&#xA;So when running docker I had to use <code>--network=host</code> part with docker run command. When I registered correctly with eureka everything works fine.</p>&#xA;"
30203787,29636094,1154144,2015-05-13T00:42:56,"<p>An alternate approach could be building another microservice that protects access to other services with authentication/authorization.&#xA;This relates to the API Gateway pattern, additional info here: <a href=""http://microservices.io/patterns/apigateway.html"" rel=""nofollow"">http://microservices.io/patterns/apigateway.html</a>.&#xA;Basically you'd have a single entry point to your system, and you could use oauth or json web tokens to handle client auth.</p>&#xA;&#xA;<p>Secure access between microservices could also be achieved using something like additional headers and token on http request (kind of ""internal"" auth).</p>&#xA;&#xA;<p>In my view, microservices shouldn't have this responsibility because you'd probably have to share/duplicate auth logic across your app.</p>&#xA;&#xA;<p>On another note, sharing IDs as ""foreign key"" is a good approach to decouple your related data.</p>&#xA;"
40666811,40657733,6793312,2016-11-17T23:20:46,<p>My solution is to simply share the DB between the two services using Spring Data Rest.</p>&#xA;
50295236,50176793,5551200,2018-05-11T15:06:38,"<p>I have built many such tests. I have thrown up some basic code on &#xA;<a href=""https://github.com/Vanlightly/RabbitMq-PoC-Code/tree/master/IntegrationTesting/RabbitMQTestExamples"" rel=""nofollow noreferrer"">Github here with .NET Core 2.0.</a></p>&#xA;&#xA;<p>You will need a RabbitMQ cluster for these automated tests. Each test starts by eliminating the queue to ensure that no messages already exist. Pre existing messages from another test will break the current test.</p>&#xA;&#xA;<p>I have a simple helper to delete the queue. In my applications, they always declare their own queues, but if that is not your case then you'll have to create the queue again and any bindings to any exchanges.</p>&#xA;&#xA;<pre><code>public class QueueDestroyer&#xA;{&#xA;    public static void DeleteQueue(string queueName, string virtualHost)&#xA;    {&#xA;        var connectionFactory = new ConnectionFactory();&#xA;        connectionFactory.HostName = ""localhost"";&#xA;        connectionFactory.UserName = ""guest"";&#xA;        connectionFactory.Password = ""guest"";&#xA;        connectionFactory.VirtualHost = virtualHost;&#xA;        var connection = connectionFactory.CreateConnection();&#xA;        var channel = connection.CreateModel();&#xA;        channel.QueueDelete(queueName);&#xA;        connection.Close();&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>I have created a very simple consumer example that represents your microservice. It runs in a Task until cancellation.</p>&#xA;&#xA;<pre><code>public class Consumer&#xA;{&#xA;    private IMessageProcessor _messageProcessor;&#xA;    private Task _consumerTask;&#xA;&#xA;    public Consumer(IMessageProcessor messageProcessor)&#xA;    {&#xA;        _messageProcessor = messageProcessor;&#xA;    }&#xA;&#xA;    public void Consume(CancellationToken token, string queueName)&#xA;    {&#xA;        _consumerTask = Task.Run(() =&gt;&#xA;        {&#xA;            var factory = new ConnectionFactory() { HostName = ""localhost"" };&#xA;            using (var connection = factory.CreateConnection())&#xA;            {&#xA;                using (var channel = connection.CreateModel())&#xA;                {&#xA;                    channel.QueueDeclare(queue: queueName,&#xA;                                    durable: false,&#xA;                                    exclusive: false,&#xA;                                    autoDelete: false,&#xA;                                    arguments: null);&#xA;&#xA;                    var consumer = new EventingBasicConsumer(channel);&#xA;                    consumer.Received += (model, ea) =&gt;&#xA;                    {&#xA;                        var body = ea.Body;&#xA;                        var message = Encoding.UTF8.GetString(body);&#xA;                        _messageProcessor.ProcessMessage(message);&#xA;                    };&#xA;                    channel.BasicConsume(queue: queueName,&#xA;                                        autoAck: false,&#xA;                                            consumer: consumer);&#xA;&#xA;                    while (!token.IsCancellationRequested)&#xA;                        Thread.Sleep(1000);&#xA;                }&#xA;            }&#xA;        });&#xA;    }&#xA;&#xA;    public void WaitForCompletion()&#xA;    {&#xA;        _consumerTask.Wait();&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>The consumer has an IMessageProcessor interface that will do the work of processing the message. In my integration test I created a fake. You would probably use your preferred mocking framework for this.</p>&#xA;&#xA;<p>The test publisher publishes a message to the queue.</p>&#xA;&#xA;<pre><code>public class TestPublisher&#xA;{&#xA;    public void Publish(string queueName, string message)&#xA;    {&#xA;        var factory = new ConnectionFactory() { HostName = ""localhost"", UserName=""guest"", Password=""guest"" };&#xA;        using (var connection = factory.CreateConnection())&#xA;        using (var channel = connection.CreateModel())&#xA;        {&#xA;            var body = Encoding.UTF8.GetBytes(message);&#xA;&#xA;            channel.BasicPublish(exchange: """",&#xA;                                    routingKey: queueName,&#xA;                                    basicProperties: null,&#xA;                                    body: body);&#xA;        }&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>My example test looks like this:</p>&#xA;&#xA;<pre><code>[Fact]&#xA;public void If_SendMessageToQueue_ThenConsumerReceiv4es()&#xA;{&#xA;    // ARRANGE&#xA;    QueueDestroyer.DeleteQueue(""queueX"", ""/"");&#xA;    var cts = new CancellationTokenSource();&#xA;    var fake = new FakeProcessor();&#xA;    var myMicroService = new Consumer(fake);&#xA;&#xA;    // ACT&#xA;    myMicroService.Consume(cts.Token, ""queueX"");&#xA;&#xA;    var producer = new TestPublisher();&#xA;    producer.Publish(""queueX"", ""hello"");&#xA;&#xA;    Thread.Sleep(1000); // make sure the consumer will have received the message&#xA;    cts.Cancel();&#xA;&#xA;    // ASSERT&#xA;    Assert.Equal(1, fake.Messages.Count);&#xA;    Assert.Equal(""hello"", fake.Messages[0]);&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>My fake is this:</p>&#xA;&#xA;<pre><code>public class FakeProcessor : IMessageProcessor&#xA;{&#xA;    public List&lt;string&gt; Messages { get; set; }&#xA;&#xA;    public FakeProcessor()&#xA;    {&#xA;        Messages = new List&lt;string&gt;();&#xA;    }&#xA;&#xA;    public void ProcessMessage(string message)&#xA;    {&#xA;        Messages.Add(message);&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Additional advice is:</p>&#xA;&#xA;<ul>&#xA;<li><p>If you can append randomized text to your queue and exchange names on each test run then do so to avoid concurrent tests interfering with each other</p></li>&#xA;<li><p>I have some helpers in the code for declaring queues, exchanges and bindings also, if your applications don't do that.</p></li>&#xA;<li><p>Write a connection killer class that will force close connections and check your applications still work and can recover. I have code for that, but not in .NET Core. Just ask me for it and I can modify it to run in .NET Core.</p></li>&#xA;<li><p>In general I think you should avoid including other microservices in your integration tests. If you send a message from one service to another and expect a message back for example, then create a fake consumer that can mock the expected behaviour. If you receive messages from other services then create fake publishers in your integration test project.</p></li>&#xA;</ul>&#xA;"
50456150,50454109,5551200,2018-05-21T20:26:44,"<p>Personally I am not a fan of using a message broker for RPC. It adds unnecessary complexity and overhead. </p>&#xA;&#xA;<p>How do you host your long-lived RabbitMQ consumer in your Users web service? If you make it some static singleton, in your web service how do you deal with scaling and concurrency? Or do you make it a stand-alone daemon process? Now you have two User applications instead of one. What happens if your Users consumer slows down, by the time it consumes the request message the Orders service context might have timed-out and sent another message or given up.</p>&#xA;&#xA;<p>For RPC I would suggest simple HTTP.</p>&#xA;&#xA;<p>There is a pattern involving a message broker that can avoid the need for a synchronous network call. The pattern is for services to consume events from other services and store that data locally in their own database. Then when the time comes when the Orders service needs a user record it can access it from its own database.</p>&#xA;&#xA;<p>In your case, your Users app doesn't need to know anything about orders, but your Orders app needs to know some details about your users. So every time a user is added, modified, removed etc, the Users service emits an event (UserCreated, UserModified, UserRemoved). The Orders service can subscribe to those events and store only the data it needs, such as the user address.</p>&#xA;&#xA;<p>The benefit is that is that at request time, your Orders service has one less synchronous dependency on another service. Testing the service is easier as you have fewer request time dependencies. There are also drawbacks however such as some latency between user record changes occuring and being received by the Orders app. Something to consider.</p>&#xA;&#xA;<p>UPDATE&#xA;If you do go with RabbitMQ for RPC then remember to make use of the message TTL feature. If the client will timeout, then set the message expiration to that period. This will help avoid wasted work on the part of the consumer and avoid a queue getting backed up under load. One issue with RPC over a message broker is that once a queue fills up it can add long latencies that take a while to recover from. Setting your message expiration to your client timeout helps avoid that.</p>&#xA;&#xA;<p>Regarding RabbitMQ for RPC. Normally we use a message broker for decoupling and durability. Seeing as RPC is a synchronous communication, that is, we are waiting for a response, then durability is not a consideration. That leaves us decoupling. The question is does that decoupling buy you anything over the decoupling you can do with HTTP via a gateway or Docker service names?</p>&#xA;"
38200052,35531784,1821792,2016-07-05T09:39:08,<p>A distributed data base definitely conflicts with the following basic principle of micro services:</p>&#xA;&#xA;<p>A micro service owns its data and exposes it via well defined interfaces.&#xA;No other micro service may access data owned by another micro service directly.</p>&#xA;&#xA;<p>So one solution in this case would be to have a token micro services or the last solution you have described.</p>&#xA;
38099639,38099204,4811873,2016-06-29T12:31:49,"<p>A very(!) significant ""fly in the ointment"" of this kind of design ... which requires careful advance thought on your part ... is: &ldquo;precisely <strong>what</strong> is meant by &lsquo;session&rsquo; information.&rdquo; In this architecture, &ldquo;everyone is <em>racing</em> with everyone else.&rdquo; If the session information is <em>updated</em>, you do not and basically cannot(!) know which of the agents knows about that change and which does not. To further complicate things, new requests are arriving asynchronously and will overlap other requests in unpredictable ways.</p>&#xA;&#xA;<p>Therefore, the Authorization Server must be exactly that ... and, no more. It validates <em>(authenticates ...)</em> the opaque token, and supplies a trustworthy description of what the request is <em>authorized</em> to do. But, the information that it harbors basically cannot change. And specifically, it cannot hold &ldquo;session state&rdquo; data in the <em>web server</em> sense of that term.</p>&#xA;&#xA;<p>Each microservice provider must maintain its own &ldquo;tote board&rdquo; *(my term ... &ldquo;its own particular subset of what in a web-server would be &lsquo;the session pool&rsquo;&rdquo;), and it is <em>desirable</em> but not always <em>feasible</em> that its board would be independent of the others. Almost certainly, it must use a central database (with transactions) to coordinate with other service-providers similarly situated. And still, if the truth is that the content of any of these &ldquo;totes&rdquo; is causally related to any other, you now have an out-of-sync issue between them.</p>&#xA;&#xA;<p>Although microservice architecture has a certain <i>academic</i> appeal, IMHO designs must be <strong>carefully</strong> studied to be certain that they are, in fact, compatible with this approach.</p>&#xA;"
41493226,37897876,6205025,2017-01-05T19:33:42,"<p>Each hit in the entry point ""/api/messagelist"" creates one new consumer with the request routing context.</p>&#xA;&#xA;<p>The first request will create the consumer and reply to the request. When the second message was published, that consumer will receive the message and will reply to the previous request (instance) and this was closed.</p>&#xA;&#xA;<p>I think that you misunderstood the event bus purpose and I really recommend you to read the documentation.&#xA;<a href=""http://vertx.io/docs/vertx-core/java/#event_bus"" rel=""nofollow noreferrer"">http://vertx.io/docs/vertx-core/java/#event_bus</a></p>&#xA;"
31064917,30995669,899937,2015-06-26T04:27:43,"<p>A lot of good questions! </p>&#xA;&#xA;<p>First of all, I want to answer your last question - multiple patterns are ok when you know what you're doing. It's fine to mix asynchronous queues, HTTP calls and even binary RPC - it depends on consistency, availability and performance requirements. Sometimes you can see a good fit for simple PubSub and sometimes you need to have distributed lock - microservices are different. </p>&#xA;&#xA;<p>Your example is simple: two microservices need to exchange some information. You chose asynchronous queue - fine, in this case they don't really need to know about each other. Queues don't expect any discovery between consumers. </p>&#xA;&#xA;<p>But we need service discovery in other cases! For example, backing services: databases, caches and <em>actually queues as well</em>. Without service discovery you probably hardcoded the URL to your queue, but if it goes down you have nothing. You need to have high availability - cluster of nodes replicating your queue, for example. When you add a new node or existing node crashed - you should not change anything, service discovery tool should understand that and update the registry.</p>&#xA;&#xA;<p><a href=""https://www.consul.io/"">Consul</a> is a perfect modern service discovery tool, you can just use custom DNS name for accessing your backing services and Consul will perform constant health checks and keep your cluster healthy. </p>&#xA;&#xA;<p>The same rule can be applied to microservices - when you have a cluster running service A and you need to access it from service B without any queues (for example, for HTTP call) you have to use service discovery to be sure that endpoint you use will bring you to the healthy node. So it's a perfect fit for Aggregator or Proxy patterns from the article you mentioned. </p>&#xA;&#xA;<p>Probably the most confusion is caused by the fact that you see ""hardcoded"" URLs in Zookeeper. And you think that you need to manage that manually. Modern tools like Consul or etcd allows you to avoid that headache and just rely on them. It's actually also achievable with Zookeeper, but it'll require more time and resources to have similar setup.</p>&#xA;&#xA;<p>PS: please remember about the most important rule in microservices - <a href=""http://martinfowler.com/bliki/MonolithFirst.html"">http://martinfowler.com/bliki/MonolithFirst.html</a></p>&#xA;"
29762027,29761872,899937,2015-04-21T02:51:44,"<ul>&#xA;<li><p>When performance or latency doesn't matter too much (yes, we don't&#xA;always need them) it's perfectly fine to just use simple RESTful APIs&#xA;for querying additional data you need. If you need to do multiple&#xA;calls to different microservices and return one result you can use&#xA;<a href=""http://microservices.io/patterns/apigateway.html"">API Gateway</a> pattern.</p></li>&#xA;<li><p>It's perfectly fine to have redundancy in <a href=""http://martinfowler.com/bliki/PolyglotPersistence.html"">Polyglot persistence</a> environments. For example, you can use messaging queue for your microservices and send ""update"" events every time you change something. Other microservices will listen to required events and save data locally. So instead of querying you keep all required data in appropriate storage for specific microservice.</p></li>&#xA;<li><p>Also, don't forget about caching :) You can use tools like <a href=""http://redis.io/"">Redis</a> or <a href=""http://memcached.org/"">Memcached</a> to avoid querying other databases too often.   </p></li>&#xA;</ul>&#xA;"
29461584,29460485,899937,2015-04-05T19:55:14,"<p>It's totally fine to keep appropriate data in different databases, it's called <a href=""http://martinfowler.com/bliki/PolyglotPersistence.html"" rel=""nofollow"">Polyglot Persistence</a>. Yes, you would like to keep user data and data about purchases separately and use message queue for sync. Millions of users seems fine to me, it's scalability, not design issue ;-)</p>&#xA;&#xA;<p>In case of search - you probably want to search more than just username, right? So, if you use message queue to update data between services you can also easily route this data to ElasticSearch, for example. And from ElasticSearch perspective it doesn't really matter what field to index - username or product title. </p>&#xA;"
29592221,29591967,899937,2015-04-12T17:16:35,"<p>Exactly, that's the beauty of microservices model! You can start thinking about microservices when you design your maven multi-module project, for example. Low coupling, clear separation of concerns, may be even asynchronous communication. When you feel more confident you extract them in into apps and run in a one host, next step - run in different hosts. It's up to you to decide how exactly they should be deployed, it's related to goals you want to achieve (fault-tolerance vs low latency, etc.) and DevOps resources you have (because more separation you have more maintenance you need).</p>&#xA;&#xA;<p>Regarding Java EE stack - nothing specific, just usual jar or war file running using <em>java -jar</em> or application servers like Tomcat. </p>&#xA;&#xA;<p>Another direction is to use tools like Docker + CoreOs / kubernetes / ..., Mesos + Marathon, etc., but they are suitable for any languages / frameworks in microservices.  </p>&#xA;&#xA;<p><strong>Edit:</strong></p>&#xA;&#xA;<p>Microservices can use a combination of synchronous (REST, SOAP) and asynchronous protocols (messaging queues like ActiveMQ, RabbitMQ, etc). It's up to you to decide how to combine them. My example: <a href=""http://labs.bench.co/2014/12/10/microservices-at-bench-intro"" rel=""nofollow"">labs.bench.co/2014/12/10/microservices-at-bench-intro</a></p>&#xA;"
41347642,41270649,7200353,2016-12-27T15:13:27,"<p>You can also consider the Forum Sentry API Security Gateway from Forum Systems (I work for Forum Systems).</p>&#xA;&#xA;<p>Based on your example use case, if each of the ""smaller request"" services are using the same protocols (e.g. HTTPS), message formats (e.g. JSON) and security characteristics (TLS, authentication, etc.) then the solution should be relatively straight forward.</p>&#xA;&#xA;<p>If each service is using different identity or message formats, say, then your API Gateway solution will also need to have strong capabilities around identity and message conversion. For example, one small request may require a BasicAuth header to authenticate to the service, whereas another small request may require a SAML assertion.</p>&#xA;"
41199101,41161769,7200353,2016-12-17T13:31:08,"<p>I don't have a full and clear picture of your authorization requirements.&#xA;I'm assuming a correlation between authenticated users and domain object/entity permissions.</p>&#xA;&#xA;<p>One option to consider is to define user attributes corresponding to your domain object/entity permissions, and implement an Attribute-based Access Control (ABAC) policy. </p>&#xA;&#xA;<p>The attributes are tied to and stored with the users identity in your repository, and retrieved when performing your authentication. </p>&#xA;"
47528693,47527983,3605090,2017-11-28T10:02:12,"<p>TLDR: Think components and modules separately and establish their ""touch points""</p>&#xA;&#xA;<p>Modules, as in your example, look like cross-cutting structure, which corresponds well enough to the recommended microservice practice. So, they can all be parts of a single microservice. And if you are going to use DDD, than you'll want to include a bounded context name in your package path.</p>&#xA;&#xA;<p>In my own source code I usually separate (at the top level) modules like <code>config</code> (to load and parse, well, config), <code>functional</code> for the functional core, domain model, <code>operational</code> for managing concurrency, Akka actors structure, monitoring and so on, and <code>adapters</code>, where all API, DB and MQ code lives. And, at last, module <code>app</code>, where all is launched and interfaces are bound to implementations. Also, you usually have some <code>utils</code> or <code>commons</code> for lower level boilerplate, algorithms and so on.</p>&#xA;&#xA;<p>In some architecture schools there is explicit separation between modules and components. While the former are parts of the source code structure, the latter are runtime units, which consume resources and live in their specific way.</p>&#xA;&#xA;<p>In your case microservices correspond to such components. These components can be run in the same JVM - and you get a monolith. Or they can be run in a separate JVM on a (maybe) separate host. Then you call them microservices.</p>&#xA;&#xA;<p>So, you need to:</p>&#xA;&#xA;<ul>&#xA;<li>make each component's source code autonomous so that it could be launched in a separate runtime space (like classloader, thread, threadpool, actor system subtree). Hence, you need some launcher to bring it to life. Then you'll be able to call it from your <code>public static void main(...)</code>.</li>&#xA;<li>introduce some modules in your code that would hold semantics of an individual component each. So that you could understand a component's scope from the code.</li>&#xA;<li>abstract communication between components, so that you could use adapters (source code modules) to talk over a network, or to use intra-JVM mechanisms like procedure call or Akka's message passing.</li>&#xA;</ul>&#xA;&#xA;<p>I should notice that on the lower levels you can use common source code modules in your components, so they can have some intersections in code. But on the higher level source code would be distinctive, so you can split it into modules according to components.</p>&#xA;&#xA;<p>You can use Akka and run each of your components in a supervision subtree, where the subtree's supervisor is your component's main actor. Then that main actor definition would be your component's main module. If you need to let components communicate, you should pass corresponding ActorRefs to adapters as a config param.</p>&#xA;&#xA;<p>You tell about centralising communication points, but in my opinion, if you stick to microservices paradigm and high level of autonomy for your components, then for every API call somebody has to own a contract. Enter different DDD bounded context interaction patterns. If you leave it in some centrally managed module, which every component should use, then that's a case of API governance. As long as you are the only maintainer, that may be convenient. But when different developers (or even teams) take their parts, you'll need to make this decision once again considering new conditions.</p>&#xA;&#xA;<p>When later you take components apart - then you'll pass URL to adapters instead of ActorRefs.</p>&#xA;"
34503672,34503547,1517410,2015-12-29T02:42:44,"<p>A <strong>HTTP415</strong> means <strong>Unsupported Media Type</strong>. What that means is that <code>isUsernameAvailable</code> expects input in JSON format, but that isn't what it is getting.</p>&#xA;&#xA;<p>Try explicitly adding <code>Content-Type: application/json</code> header to your HTTP request by doing the following:</p>&#xA;&#xA;<pre><code>HttpHeaders headers = new HttpHeaders();&#xA;headers.setContentType(MediaType.APPLICATION_JSON);&#xA;&#xA;HttpEntity&lt;String&gt; entity = new HttpEntity&lt;String&gt;(requestJson,headers);&#xA;restTemplate.put(uRL, entity);&#xA;</code></pre>&#xA;"
39941417,39788745,1517410,2016-10-09T08:10:07,"<p>Let's say your two systems are S1 and S2, where the following happens:</p>&#xA;&#xA;<ol>&#xA;<li>S1 sends a message to SQS. This step is asynchronous.</li>&#xA;<li>S2 creates the required entity.</li>&#xA;<li>S1 is expected to notify caller of the newly created entity id.</li>&#xA;</ol>&#xA;&#xA;<p>#3 is not straightforward, given the very nature of asynchronous systems. For e.g., you don't know when S2 will process your request.</p>&#xA;&#xA;<p>Possible solution:</p>&#xA;&#xA;<ol>&#xA;<li>S1 sends a message to SQS. SQS returns a <code>MessageId</code> in response. You return this <code>MessageId</code> to client.</li>&#xA;<li>Both S1 and S2 have a shared data store (for e.g., Memcache, MySQL etc.) which stores mapping of <code>MessageId</code> to entity-id. Whenever S2 process a message and creates a new entity, it updates this mapping in the shared data store.</li>&#xA;<li>Clients poll S1 with <code>MessageId</code>. S1 returns mapping from data store whenever it is non-null.</li>&#xA;</ol>&#xA;&#xA;<p>Documentation for <code>MessageId</code> from SQS's SendMessage API can be found <a href=""http://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SendMessage.html"" rel=""nofollow"">here</a>.</p>&#xA;"
49709097,49708997,6237026,2018-04-07T15:47:18,<p>You can create something like a currency conversion app with three microservices like these: </p>&#xA;&#xA;<ol>&#xA;<li>Limit service;</li>&#xA;<li>Exchange service;</li>&#xA;<li>Currency conversion service. </li>&#xA;</ol>&#xA;&#xA;<p>Limit service and currency conversion service can communicate with the database for retrieving the values of the limits and currencies conversion.</p>&#xA;&#xA;<p>For more info check github.com/in28minutes and look after a microservice repository.</p>&#xA;
48925896,48925813,6237026,2018-02-22T11:07:25,"<p>Here it's your response:</p>&#xA;&#xA;<p>In object-oriented programming, a God object is an object that knows too much or does too much. The God object is an example of an anti-pattern.</p>&#xA;&#xA;<p>A common programming technique is to separate a large problem into several smaller problems (a divide and conquer strategy) and create solutions for each of them. Once the smaller problems are solved, the big problem as a whole has been solved. Therefore a given object for a small problem need only know about itself. Likewise, there is only one set of problems an object needs to solve: its own problems.</p>&#xA;"
44172246,44157540,3819362,2017-05-25T04:25:49,"<p>You can adopt the second approach. &#xA;It's more suitable, simple and convenient.&#xA; Also it will reduce the complexities which you will face when you will integrate AngularJs as front-end in your Rails Application. Also you can test the application easily</p>&#xA;"
42572427,42487685,2772066,2017-03-03T06:49:28,"<p>You can use npm link feature.</p>&#xA;&#xA;<p>The linking process consists of two steps:</p>&#xA;&#xA;<ol>&#xA;<li>Declaring a module as a global link by running npm link in the module’s root folder</li>&#xA;<li>Installing the linked modules in your target module(app) by running npm link  in the target folder</li>&#xA;</ol>&#xA;&#xA;<p>This works pretty well unless one of your local modules depends on another local module. In this case, linking fails because it cannot find the dependent module. In order to solve this issue, one needs to link the dependent module to the parent module and then install the parent into the app.</p>&#xA;&#xA;<p><a href=""https://docs.npmjs.com/cli/link"" rel=""nofollow noreferrer"">https://docs.npmjs.com/cli/link</a></p>&#xA;"
50381061,50380259,9663586,2018-05-16T22:56:34,"<blockquote>&#xA;  <p>Based on your own experience, is there a good reason for these to be stored in a config file (or configmap)?</p>&#xA;</blockquote>&#xA;&#xA;<ul>&#xA;<li>As a general best practice guideline we tend to avoid ""magic things"" hardcoded in code. If even just for a off chance to be able to redirect it later, say, from dev service to qa service for some bug hunting or testing.</li>&#xA;</ul>&#xA;&#xA;<blockquote>&#xA;  <p>What do you do in your practice, and why? </p>&#xA;</blockquote>&#xA;&#xA;<ul>&#xA;<li>If it is remotely configurable - we place it in some form of config, being it config file, ConfigMap or Environment variable tossed around. We like more to have it exposed and not to have to change it than to hardcode it and later regret it... Depending on how often we anticipate change we place it (in ascending order) in: some config file bundled in docker image, ConfigMap with file content, ConfigMap with environment variable, Environment variable directly in container definition, Database field that app is reading. </li>&#xA;</ul>&#xA;"
44438506,44420585,4820834,2017-06-08T14:28:21,"<p>I'm also interested in this question. </p>&#xA;&#xA;<p>First of all, scenario that you described is very common. Users, videos and messages definitely three different microservices. There is no issue in how you broke down system into pieces. </p>&#xA;&#xA;<p>Secondly, there are multiple options, how to solve data sharing problem. Take a look at great article from auth0: <a href=""https://auth0.com/blog/introduction-to-microservices-part-4-dependencies/"" rel=""nofollow noreferrer"">https://auth0.com/blog/introduction-to-microservices-part-4-dependencies/</a></p>&#xA;"
46928741,35617996,2739810,2017-10-25T09:28:11,<p>For me this configuration works:</p>&#xA;&#xA;<pre><code>eureka:&#xA;  instance:&#xA;    nonSecurePortEnabled: false&#xA;    securePortEnabled: true&#xA;    securePort: ${server.port}&#xA;    statusPageUrl: https://${eureka.instance.hostname}:${eureka.instance.securePort}/info&#xA;    homePageUrl: https://${eureka.instance.hostname}:${eureka.instance.securePort}/&#xA;</code></pre>&#xA;
50869692,50856401,454754,2018-06-15T05:42:18,"<p>So if you have the reverse proxy enabled, and want to use that the endpoint on port 8939 is not interesting as you should access the reverse proxy using the proxy url</p>&#xA;&#xA;<p>As pointed out in the comments the format is <code>http://[FQDN]:[ReversPoxyPort]/[ApplicationName]/[ServiceName]/[Controller]</code> and typically the port is 19081 so the url becomes something like <code>myawesomeservicefabric.westeurope.cloudapp.azure.com:19081/FabricApp1/Service1/Values</code> </p>&#xA;&#xA;<p>As with all other things the port should be configured in the load balancer with a probe and a rule.</p>&#xA;&#xA;<p>I am not sure what the portal does with clusters now a days, but at some point it configured a network security group which might also be the cause of your issues.</p>&#xA;"
40010622,39990666,63074,2016-10-13T00:54:55,"<p><a href=""https://aws.amazon.com/dynamodb/"" rel=""nofollow"">Amazon's DynamoDB</a> can scale to meet very high TPS demands. It can certainly handle 10 to 100 of thousands of writes per second sustained if your schema is designed properly but it is not cheap. </p>&#xA;&#xA;<p>Your question is a bit vague about whether you need to be able to sustain tens of thousands of writes per second, or you need to be able to burst to tens of thousands of writes. It's also not clear how you intend to read the data.</p>&#xA;&#xA;<p>How big is a typical event/record?</p>&#xA;&#xA;<p>Could you batch the writes?</p>&#xA;&#xA;<p>Could you partition your writes?</p>&#xA;&#xA;<p>Have you looked into something like <a href=""https://aws.amazon.com/kinesis/firehose/"" rel=""nofollow"">Amazon's Kinesis Firehose</a>? With small events you could have a relatively cheap ingestion pipeline and then perhaps use S3 for long term storage. It would certainly be cheaper than DynamoDB.</p>&#xA;&#xA;<p>Azure offers similar services as well but I'm not as familiar with their offerings.</p>&#xA;"
39981369,39981115,63074,2016-10-11T15:46:38,"<p>I'd say it's idiomatic for micro-services to not expect compile-time, or build-time support for <em>boundary</em> checking or enforcement. </p>&#xA;&#xA;<p>Instead you would enforce these rules in two ways at run-time:</p>&#xA;&#xA;<ol>&#xA;<li>validation and error handling:  services must always expect incorrect input or abuse and be prepared to handle it gracefully, by returning proper error codes and throttling</li>&#xA;<li>take advantage of digital signatures to authenticate requests and enforce access policies</li>&#xA;</ol>&#xA;"
50183596,50180928,1898594,2018-05-04T22:01:39,"<p>This error is usually given when you are behind a proxy server.&#xA;If you are using a proxy you will probably have to authenticate the http client again.&#xA;Another possibility is to generate the Singleton instance again when it gives an error, for Example <code>Singleton.killInstance();</code> If your implementation is like this:</p>&#xA;&#xA;<pre><code>public class Singleton&#xA;{&#xA;   private static Singleton instance;&#xA;   private Singleton() {}&#xA;   public static Singleton instance&#xA;   {&#xA;      get &#xA;      {&#xA;         if (instance == null)&#xA;         {&#xA;            instance = new Singleton();&#xA;         }&#xA;         return instance;&#xA;      }&#xA;   }&#xA;  public static void Singleton killInstance()&#xA;  {&#xA;    instance == null&#xA;  }&#xA;}&#xA;</code></pre>&#xA;"
31104733,31104540,200291,2015-06-28T21:03:34,"<p>I don’t usually deal with microservices, and this might not be a good way of doing things, but here’s an idea:</p>&#xA;&#xA;<p>To restate the problem, the system consists of three independent-but-communicating parts: the frontend, the order-management backend, and the loyalty-program backend. The frontend wants to make sure some state is saved in both the order-management backend and the loyalty-program backend.</p>&#xA;&#xA;<p>One possible solution would be to implement some type of <a href=""https://en.wikipedia.org/wiki/Two-phase_commit_protocol"" rel=""noreferrer"">two-phase commit</a>:</p>&#xA;&#xA;<ol>&#xA;<li>First, the frontend places a record in its own database with all the data. Call this the <em>frontend record</em>.</li>&#xA;<li>The frontend asks the order-management backend for a transaction ID, and passes it whatever data it would need to complete the action. The order-management backend stores this data in a staging area, associating with it a fresh transaction ID and returning that to the frontend.</li>&#xA;<li>The order-management transaction ID is stored as part of the frontend record.</li>&#xA;<li>The frontend asks the loyalty-program backend for a transaction ID, and passes it whatever data it would need to complete the action. The loyalty-program backend stores this data in a staging area, associating with it a fresh transaction ID and returning that to the frontend.</li>&#xA;<li>The loyalty-program transaction ID is stored as part of the frontend record.</li>&#xA;<li>The frontend tells the order-management backend to finalize the transaction associated with the transaction ID the frontend stored.</li>&#xA;<li>The frontend tells the loyalty-program backend to finalize the transaction associated with the transaction ID the frontend stored.</li>&#xA;<li>The frontend deletes its frontend record.</li>&#xA;</ol>&#xA;&#xA;<p>If this is implemented, the changes will not necessarily be <em>atomic</em>, but it will be <em>eventually consistent</em>. Let’s think of the places it could fail:</p>&#xA;&#xA;<ul>&#xA;<li>If it fails in the first step, no data will change.</li>&#xA;<li>If it fails in the second, third, fourth, or fifth, when the system comes back online it can scan through all frontend records, looking for records without an associated transaction ID (of either type). If it comes across any such record, it can replay beginning at step 2. (If there is a failure in step 3 or 5, there will be some abandoned records left in the backends, but it is never moved out of the staging area so it is OK.)</li>&#xA;<li>If it fails in the sixth, seventh, or eighth step, when the system comes back online it can look for all frontend records with both transaction IDs filled in. It can then query the backends to see the state of these transactions—committed or uncommitted. Depending on which have been committed, it can resume from the appropriate step.</li>&#xA;</ul>&#xA;"
38661628,38627025,4664675,2016-07-29T14:44:19,"<p>So Docker for Mac just came out with a new beta version today. This seemed to have fixed my issue with connecting. Now I did make changes to source code when I found out it worked on my Windows pc.</p>&#xA;&#xA;<p>Here is the version of Docker for fix:</p>&#xA;&#xA;<pre><code>Client:&#xA; Version:      1.12.0&#xA; API version:  1.24&#xA; Go version:   go1.6.3&#xA; Git commit:   8eab29e&#xA; Built:        Thu Jul 28 21:04:48 2016&#xA; OS/Arch:      darwin/amd64&#xA; Experimental: true&#xA;&#xA;Server:&#xA; Version:      1.12.0&#xA; API version:  1.24&#xA; Go version:   go1.6.3&#xA; Git commit:   8eab29e&#xA; Built:        Thu Jul 28 21:04:48 2016&#xA; OS/Arch:      linux/amd64&#xA; Experimental: true&#xA;</code></pre>&#xA;&#xA;<p>And here is the compose file:</p>&#xA;&#xA;<pre><code>version: ""2""&#xA;services:&#xA;  staticfiles:&#xA;    build: ./files&#xA;    volumes:&#xA;      - /public&#xA;      - /views&#xA;      - /migrations&#xA;  databasefiles:&#xA;    build: ./databasefiles&#xA;    volumes:&#xA;      - /var/lib/postgresql/data&#xA;  db:&#xA;    build: ./postgres&#xA;    depends_on:&#xA;      - databasefiles&#xA;    volumes_from:&#xA;      - databasefiles&#xA;    environment:&#xA;      - POSTGRES_USER=inheritor&#xA;      - POSTGRES_DB=inheritor&#xA;  auth:&#xA;    build: ./auth&#xA;    expose:&#xA;      - ""8080""&#xA;    depends_on:&#xA;      - staticfiles&#xA;    volumes_from:&#xA;      - staticfiles:ro&#xA;    environment:&#xA;      - PORT=8080&#xA;      - DB_USER=inheritor&#xA;      - DB_NAME=inheritor&#xA;      - DB_HOST=db&#xA;      - DB_PORT=5432&#xA;      - MIGRATION_DIR=/migrations&#xA;    links:&#xA;      - db&#xA;  api:&#xA;    build: ./api&#xA;    environment:&#xA;      - PORT=8080&#xA;      - BASE_URL=https://example.org&#xA;      - AUTH_HOST=auth&#xA;      - AUTH_PORT=8080&#xA;      - VIEW_DIR=/views&#xA;      - PUBLIC_DIR=/public&#xA;    ports:&#xA;      - ""80:8080""&#xA;    volumes_from:&#xA;      - staticfiles:ro&#xA;    links:&#xA;      - auth&#xA;    depends_on:&#xA;      - staticfiles&#xA;</code></pre>&#xA;&#xA;<p>I did move services around but I do not see anything different that would change the communication between containers. This is just here incase others have same problem.</p>&#xA;"
40302098,40216232,2775651,2016-10-28T09:28:46,<p>Use @Async annotation on your service (or method).</p>&#xA;&#xA;<pre><code>@Service&#xA;@Async&#xA;public class EmailService &#xA;</code></pre>&#xA;&#xA;<p>Do not foget @EnableAsync</p>&#xA;
37229239,37159662,1209187,2016-05-14T16:37:01,"<p>As suggested by <a href=""https://stackoverflow.com/users/672859/patrick-grimard"">patrick-grimard</a> switching to Brixton and changing the code were needed fixed the issues. Working Solution is on <a href=""https://github.com/KenavR/spring-boot-microservices-example"" rel=""nofollow noreferrer"">Github</a>.</p>&#xA;&#xA;<p>Also changed the posted <code>id</code> from request param to request body, which also changed the way I add it to the request.</p>&#xA;&#xA;<p><strong>Service endpoint</strong></p>&#xA;&#xA;<pre><code>@RequestMapping(method = RequestMethod.POST, produces = ""application/json; charset=utf-8"")&#xA;public @ResponseBody Map&lt;String, String&gt; getTest(@RequestBody Map&lt;String, Long&gt; params) {&#xA;&#xA;    Map&lt;String, String&gt; response = new HashMap&lt;&gt;();&#xA;&#xA;    response.put(""name"", ""My Service"");&#xA;&#xA;    return response;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p><strong>RestTemplate creation</strong></p>&#xA;&#xA;<pre><code>@Configuration&#xA;public class PublicAPIConfiguration {&#xA;    @LoadBalanced&#xA;    @Bean&#xA;    RestTemplate restTemplate() {&#xA;        return new RestTemplate();&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p><strong>Calling service</strong></p>&#xA;&#xA;<pre><code>@Service&#xA;public class MyServiceService {&#xA;&#xA;    @Autowired&#xA;    private RestTemplate restTemplate;&#xA;&#xA;    private final String serviceUrl;&#xA;&#xA;    public MyServiceService() {&#xA;        this.serviceUrl = ""http://my-service"";&#xA;    }&#xA;&#xA;    public Map&lt;String, String&gt; getTest() {&#xA;&#xA;        Map&lt;String, Long&gt; vars = new HashMap&lt;&gt;();&#xA;        vars.put(""id"", 1L);&#xA;&#xA;        return restTemplate.postForObject(serviceUrl+""/test"", vars, Map.class);&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
33660690,33659658,1416035,2015-11-11T22:01:23,"<p>The number of parallel requests depends on both settings and hardware and you can't really count on the parallelism (or lack of) and order of execution by default.</p>&#xA;&#xA;<p>If you have a <em>real</em> need to isolate concurrency to in-order one-at-a-time you should use <a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-reliable-services-reliable-collections/"" rel=""nofollow"">IReliableQueue&lt;></a> or other <a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-reliable-services-quick-start/"" rel=""nofollow"">Reliable Collections</a></p>&#xA;&#xA;<p>Add two services to your Service Fabric app - a front-end that places items in the queue and another service that takes them and processes them. The best part is you could still go full parallel by deploying more consumers. </p>&#xA;"
44125013,44124914,1743971,2017-05-23T02:54:06,"<p>Assuming your are exposing services over An HTTP REST API, the general standard is to always base line your service urls with a version.</p>&#xA;&#xA;<p>Eg,</p>&#xA;&#xA;<pre><code>/v1/account/getUserInfo&#xA;</code></pre>&#xA;&#xA;<p>If you need to release a new version, expose it over:</p>&#xA;&#xA;<pre><code>/v2/account/getUserInfo&#xA;</code></pre>&#xA;&#xA;<p>Where v2 can run over a different branch of the codebase.</p>&#xA;"
44484835,44484346,2989411,2017-06-11T14:15:49,"<p>Basically, you need to use <code>sleep</code> command. But you can use it in a loop checking continuously if the port became available. But how to check availability of the port?</p>&#xA;&#xA;<p>One option is to use <code>netstat</code> command as suggested by Jack. The proper usage is:</p>&#xA;&#xA;<pre><code>netstat -tna | grep 'LISTEN\&gt;' | grep ':NNNN\&gt;'&#xA;</code></pre>&#xA;&#xA;<p>where <code>NNNN</code> is the port. To make that a condition to wait on, you can write following loop:</p>&#xA;&#xA;<pre><code>while ! netstat -tna | grep 'LISTEN\&gt;' | grep -q ':NNNN\&gt;'; do&#xA;  sleep 10 # time in seconds, tune it as needed&#xA;done&#xA;</code></pre>&#xA;&#xA;<p>Please mind the <code>-q</code> option in the last instance of <code>grep</code>.</p>&#xA;&#xA;<p>The other option is to check, if you can connect to port:</p>&#xA;&#xA;<pre><code>{&#xA;  while ! echo -n &gt; /dev/tcp/localhost/NNNN; do&#xA;    sleep 10&#xA;  done&#xA;} 2&gt;/dev/null&#xA;</code></pre>&#xA;&#xA;<p>Depending on the distribution you are using and the options bash has been compiled with, this method may or may not working.</p>&#xA;&#xA;<p>Another option to check if port is accessible is to use nc:</p>&#xA;&#xA;<pre><code>while ! nc -q0 localhost 2222 &lt; /dev/null &gt; /dev/null 2&gt;&amp;1; do&#xA;  sleep 10&#xA;done&#xA;</code></pre>&#xA;&#xA;<p>You can replace localhost with your hostname or ip address.</p>&#xA;"
43697450,43689879,5757027,2017-04-29T15:30:52,"<p>I'm going to assume you are developing a public internet facing application where your Micro Services are built using ASP.Net Core, and based on that I would suggest the following:</p>&#xA;&#xA;<p>When setting up SF, have it create two scale sets, one for your public facing micro services (""API Facade"" and GUI) and one for your internal Micro services.&#xA;It's always best to seperate ""internet facing code"" from ""internal code"".</p>&#xA;&#xA;<p>Have Identity Server issue a access token, containing the customer information, for your public ""Facade API"", and use it when calling your API.</p>&#xA;&#xA;<p>Your API facade service will act as a proxy for all your internal micro services and offload SSL/Authentication so you don't have to deal with this internaly. This is also where I would recommend that you do the actual audit logging. In your Facade API, create a correlation ID and add it as a header to any calls being made to a internal service. Add the correlation id to any logging being made. This will allow you to follow a API call all the way through your system and together with your audit log, you can see exacly what a user has been doing.</p>&#xA;&#xA;<p>For auditing, I can recomend <a href=""https://github.com/thepirat000/Audit.NET"" rel=""nofollow noreferrer"">Audit.Net</a>. With this you can add a attribute to your controller like this:</p>&#xA;&#xA;<pre><code>namespace MyWebApi.MyService.Controllers&#xA;{&#xA;  [Route(""api/[controller]"")]&#xA;  [AuditApi(EventTypeName = ""{controller}/{action} ({verb})"")]&#xA;  public class MyController : Controller&#xA;  {&#xA;  }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>and it will automaticly handle the audit for your. You can configure it to log blob storgate, file or whatever.</p>&#xA;&#xA;<p>(I am in no way associated with Audit.Net, I just like it.)</p>&#xA;"
29640822,29636899,2620515,2015-04-15T02:53:31,"<p>Things like validation, calculation, and persistence (CRUD) are all functions and not services.  By centralizing these tasks to one service you are tightly coupling all the other services to it, and lose the main benefit of SOA.  Making your services loosely coupled while retaining high cohesion should be your primary goal. I feel this design violates that.</p>&#xA;&#xA;<p>You want to design your services as vertical slices of related business functions, rather than with centralized generic services and layers.  A service should be comprised of components that are deployed throughout your enterprise from the database all the way to the UI.  Also each service should have it's own database or at least schema if that's not practical.  As soon as you have services sharing a database, then you become tightly coupled again.</p>&#xA;&#xA;<p>On a final note, if one of your services needs to do a simple CRUD insert, then that's all it should be.  No need to map it to a domain model first.  Save DDD for the business processes that have real invariants that need to be enforced.  It doesn't have to be an all or nothing approach.  Use the tool that makes sense for each use case.</p>&#xA;"
48902389,48824086,5246052,2018-02-21T09:32:28,"<p>We want the developers to login at the API gateway to and call services form there directly. I achieved this finally by adding a Filter that extends the basic <code>ZuulFilter</code>:</p>&#xA;&#xA;<pre><code>public class KeycloakAuthorizationFilter extends ZuulFilter&#xA;{&#xA;private static final String AUTHORIZATION_HEADER = ""Authorization"";&#xA;private static final String BEARER_TOKEN_TYPE = ""Bearer"";&#xA;private static Logger log = LoggerFactory.getLogger(KeycloakAuthorizationFilter.class);&#xA;&#xA;@Override&#xA;public Object run()&#xA;{&#xA;    log.info(""Adding authenticaton header..."");&#xA;    RequestContext ctx = RequestContext.getCurrentContext();&#xA;&#xA;    if (ctx.getZuulRequestHeaders().containsKey(AUTHORIZATION_HEADER)) return null;&#xA;&#xA;    KeycloakAuthenticationToken authentication = (KeycloakAuthenticationToken) SecurityContextHolder.getContext()&#xA;            .getAuthentication();&#xA;&#xA;    if(authentication == null)&#xA;    {&#xA;        log.error(""Could not load authenticaton from security context!"");&#xA;    }&#xA;    else if(authentication.isAuthenticated() == false)&#xA;    {&#xA;        log.error(""Not authenticated!"");&#xA;    }&#xA;    else&#xA;    {&#xA;        Object principal = authentication.getPrincipal();&#xA;        if(principal != null &amp;&amp; principal instanceof KeycloakPrincipal)&#xA;        {&#xA;            @SuppressWarnings(""unchecked"")&#xA;            KeycloakPrincipal&lt;KeycloakSecurityContext&gt; keycloakPrincipal = (KeycloakPrincipal&lt;KeycloakSecurityContext&gt;) authentication&#xA;                    .getPrincipal();&#xA;            log.info(String.format(""Constructing Header %s for Token %s"", AUTHORIZATION_HEADER, BEARER_TOKEN_TYPE));&#xA;            ctx.addZuulRequestHeader(AUTHORIZATION_HEADER, String.format(""%s %s"", BEARER_TOKEN_TYPE, keycloakPrincipal.getKeycloakSecurityContext().getTokenString()));&#xA;        }&#xA;    }&#xA;    return null;&#xA;}&#xA;&#xA;@Override&#xA;public boolean shouldFilter()&#xA;{&#xA;    return true;&#xA;}&#xA;&#xA;@Override&#xA;public int filterOrder()&#xA;{&#xA;    return 0;&#xA;}&#xA;&#xA;@Override&#xA;public String filterType()&#xA;{&#xA;    return ""pre"";&#xA;}&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>This is just working awesome and gives you a lot more flexibility.</p>&#xA;"
50910607,46131443,3150948,2018-06-18T13:26:42,"<p>Setting failsafe to false will just not throw the exception , this is something I would not recommend since this means even if you are not connected to your config server and unable to fetch the configuration your application will run which leads to an uncertain behavior (since you do not know where your properties are loaded), always better to <strong>fail fast</strong>.&#xA;The original cause of the problem is an exception or failing during fetching the remote environment from your config server, probably a timeout issue. </p>&#xA;"
45559386,45544777,4989284,2017-08-08T04:38:18,"<p>I believe this can be done using <a href=""https://www.confluent.io/blog/distributed-real-time-joins-and-aggregations-on-user-activity-events-using-kafka-streams/"" rel=""nofollow noreferrer"">kafka streams</a>, it is a pub/sub model where you can join streams based on a common key. You can either subscribe to a topic or add a data transformation service which joins multiple streams and pushes the joined/transformed data into a new stream. All services who wants to use this data can subscribe to this new stream. I recommend you go through this <a href=""https://kafka.apache.org/documentation/streams/"" rel=""nofollow noreferrer"">link</a></p>&#xA;"
52092732,52092546,485732,2018-08-30T08:45:20,"<p>Check out <a href=""https://github.com/rebus-org/Rebus/wiki"" rel=""nofollow noreferrer"">Rebus</a> library, that allows using different transport methods to send end receive messages in just a line of code (so in the future you can change it without effort). </p>&#xA;&#xA;<p>You could use <a href=""https://github.com/rebus-org/RebusSamples/tree/master/SqlAllTheWay"" rel=""nofollow noreferrer"">SQL Server</a> or try to develop your own transport method</p>&#xA;"
49709318,49548508,4812900,2018-04-07T16:09:13,"<p>I ended up proposing the below solution that is supported by our infra [rabbit mq and mongodb]. Here is the solution that was accepted after design review. Hope this helps someone who is having similar issues.</p>&#xA;&#xA;<ul>&#xA;<li>System receives order add / cancel event from Order Management.</li>&#xA;<li>If coupon code was used in order, We Upsert record into DB based on order ID.</li>&#xA;<li>Order Add Event- Track coupon code, orderId, discount, couponRuleId, updated etc. Upsert based on orderId and update couponUsed flag as “APPLIED”</li>&#xA;<li>Order cancel Event- Upsert orderId based on orderId and update couponUsed flag as “NOT_APPLIED”</li>&#xA;<li>Have  Mongo TTL on “coupon_usage_tracker” as 3 months / 6 months.</li>&#xA;<li>Trigger sync on COUPON_RULE_SUMMARY and COUPON_SUMMARY dashboard based on last by passing required data. Trigger sync mechanism is via MQ since we can have automated retry on failure via DLQ setup.&#xA;&#xA;<ul>&#xA;<li>If trigger sync fails for either COUPON_RULE_SUMMARY and COUPON_SUMMARY then it would be updated with correct value in next coupon code usage [Eventual consistency].</li>&#xA;<li>The config collections will not have mutable states and would be strictly having configuration. This lets us cache the configs.</li>&#xA;</ul></li>&#xA;</ul>&#xA;&#xA;<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""false"" data-babel=""false"">&#xD;&#xA;<div class=""snippet-code"">&#xD;&#xA;<pre class=""snippet-code-js lang-js prettyprint-override""><code>//COUPON_USAGE_TRACKER&#xD;&#xA;{&#xD;&#xA;    ""ruleId"": 948,&#xD;&#xA;    ""couponCode"": ""TENOFF"",&#xD;&#xA;    ""couponId"": 123,&#xD;&#xA;    ""order"": {&#xD;&#xA;        ""created_at"": """",&#xD;&#xA;        ""udpated_at"": """",&#xD;&#xA;        ""member_id"": 221930,&#xD;&#xA;        ""member_order_count"": 5,&#xD;&#xA;        ""publicId"": ""4asdff-23ewea-232"",&#xD;&#xA;        ""id"": 234123,&#xD;&#xA;        ""total"": {&#xD;&#xA;            ""discount_code"": ""TENOFF"",&#xD;&#xA;            ""discount"": ""10"",&#xD;&#xA;            ""grand_total"": 100,&#xD;&#xA;            ""sub_total"": 10&#xD;&#xA;        }&#xD;&#xA;    }&#xD;&#xA;}&#xD;&#xA;&#xD;&#xA;//COUPON_RULE_SUMMARY&#xD;&#xA;{&#xD;&#xA;    ""ruleId"": 948,&#xD;&#xA;    ""last_update_at"": """",&#xD;&#xA;    ""currentUsage"": 100,&#xD;&#xA;    ""currentBudget"": 1000&#xD;&#xA;}&#xD;&#xA;&#xD;&#xA;//COUPON_SUMMARY&#xD;&#xA;{&#xD;&#xA;    ""ruleId"": 948,&#xD;&#xA;    ""couponId"": 948,&#xD;&#xA;    ""last_update_at"": """",&#xD;&#xA;    ""currentUsage"": 100,&#xD;&#xA;    ""currentBudget"": 1000&#xD;&#xA; &#xD;&#xA;}</code></pre>&#xD;&#xA;</div>&#xD;&#xA;</div>&#xD;&#xA;</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/8AWkj.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8AWkj.jpg"" alt=""SYSTEM DESIGN FOR THE SOLUTION""></a></p>&#xA;"
50074727,50074667,1408868,2018-04-28T08:58:36,"<p>Consider docker as any software. if your OS is windows you install windows version of a software. if your is a linux distro then you install linux version of a software.</p>&#xA;&#xA;<p>So you need to install docker for windows afterwards you can install any docker image/container you want under your operating system. Could be windows, linux or anything else. </p>&#xA;"
36688990,36680157,1215076,2016-04-18T08:32:39,"<p>HTTP/1.0 working mode was to open a connection for each request, and close the connection after each response.</p>&#xA;&#xA;<p>Using HTTP/1.0 from remote clients and clients inside microservices (e.g. those in A that call B, and those in B that call C) should be avoided because the cost of opening a connection for each request can contribute for most of the latency.</p>&#xA;&#xA;<p>HTTP/1.1 working mode is to open a connection and then leave it open until either peer explicitly requests to close it. This allow for the connection to be reused for multiple requests, and it's a big win because it reduces the latency, it uses less resources, and in general it is more efficient.</p>&#xA;&#xA;<p>Fortunately nowadays both remote clients (e.g. browsers) and clients inside microservices support HTTP/1.1 well, or even HTTP/2.</p>&#xA;&#xA;<p>Surely browsers have connection pooling, and any decent HTTP client that you may use inside your microservices does also have connection pooling.</p>&#xA;&#xA;<p>Remote clients and microservices clients  should be using at least HTTP/1.1 with connection pooling.</p>&#xA;&#xA;<p>Regarding HTTP/2, while I am a big promoter of HTTP/2 for browser-to-server usage, for REST microservices calls inside data centers I would benchmark the parameters you are interested in for both HTTP/1.1 and HTTP/2, and then see how they fare. I expect HTTP/2 to be on par with HTTP/1.1 for most cases, if not slightly better.</p>&#xA;&#xA;<p>The way I would do it using HTTP/2 (disclaimer, I'm a <a href=""https://www.eclipse.org/jetty/"" rel=""noreferrer"">Jetty</a> committer) would be to <a href=""https://webtide.com/http2-with-haproxy-and-jetty/"" rel=""noreferrer"">offload TLS from remote clients using HAProxy</a>, and then use clear-text HTTP/2 between microservices A, B and C using <a href=""https://webtide.com/http2-support-for-httpclient/"" rel=""noreferrer"">Jetty's <code>HttpClient</code> with HTTP/2 transport</a>.</p>&#xA;&#xA;<p>I'm not sure AWS ELB already supports HTTP/2 at the time of this writing, but if it does not please be sure to drop a message to Amazon asking to support it (many others already did that). As I said, alternatively you can use HAProxy.</p>&#xA;&#xA;<p>For communication between microservices, you can use HTTP/2 no matter what is the protocol used by remote clients.&#xA;By using Jetty's <code>HttpClient</code>, you can very easily switch between the HTTP/1.1 and the HTTP/2 transports, so this gives you the maximum of flexibility.</p>&#xA;"
37615449,37615250,3070052,2016-06-03T13:16:40,"<p>Yes, I think it is wrong.</p>&#xA;&#xA;<p>Domain should match real business model and should not care how data is persisted. Even if data internally are stored in multiple tables, this should not affect domain objects in any way.</p>&#xA;&#xA;<p>When you are loading aggregate root, you should load related entities as well in one go. For example, this can easily be achieved with <code>Include</code> keyword in Entity Framework if you are on .NET. By loading all the data you ensure that you have full representation of business entity at any given time and you don't have to query database anymore.</p>&#xA;&#xA;<p>Any changes in related entities should be persisted together with aggregate root in one atomic operation (usually using transactions).</p>&#xA;"
50374486,50340234,2619748,2018-05-16T15:12:51,"<p>This isn't directly an answer but merely the solution I came up with to work around the problem.</p>&#xA;&#xA;<p>I switched to Flurl.Http which the latest version is compatible with both Standard and Framework. This is how I rewrote my SendEmail function,</p>&#xA;&#xA;<pre><code>public void SendEmail(MicroserviceMailMessage email)&#xA;{&#xA;    var url = _azureEndpoint;&#xA;    var transformedEmail = TransformEmail(email);&#xA;&#xA;    var content = new CapturedMultipartContent();&#xA;    content.AddJson(""email"", transformedEmail);&#xA;&#xA;    foreach (var attachment in email.Attachments)&#xA;    {&#xA;        var inline = attachment.ContentDisposition.Inline ? ""inline."" : """";&#xA;        content.AddFile($""attachments.{inline}{attachment.Name}"", attachment.ContentStream, attachment.Name);&#xA;    }&#xA;&#xA;    url.PostAsync(content).Wait();&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>It posts everything I need successfully, the files, and the JSON.</p>&#xA;"
50663140,50663047,9769061,2018-06-03T03:53:36,"<p>No, it's not mandatory to deployed war file, microservices can also be deployed as jar or war file on the server.</p>&#xA;&#xA;<p>you can also run your microservices as the jar file in external tomcat server&#xA;<a href=""https://spring.io/blog/2014/03/07/deploying-spring-boot-applications"" rel=""nofollow noreferrer"">Read this.</a></p>&#xA;"
35177012,35172625,1259109,2016-02-03T12:23:31,<p>You need the eureka serviceUrl in the bootstrap.yml (as well as the service id of the configserver).</p>&#xA;
45500187,39154613,1259109,2017-08-04T07:03:12,"<p>The <code>connectTimeout</code> and <code>readTimeout</code> in ribbon are passed down to the underlying HTTP client. They apply to the HTTP connection (not the HTTP request once the connection has been established). I'm not sure why you'd need to test it like this really, but it's going to be hard with a healthy server. For instance, for <code>connectTimeout</code>, you need one that can accept TCP connections but not finish the HTTP layer connection. For <code>readTimeout</code> you need one that makes a connection but then doesn't send any data (at all).</p>&#xA;"
36479595,36479452,1259109,2016-04-07T14:43:04,"<p>You need to configure the client app to be more robust if the config server is not ready yet when it starts. there's a section in the <a href=""http://cloud.spring.io/spring-cloud-static/spring-cloud.html#config-client-retry"" rel=""nofollow"">user guide</a>. TL;DR include spring-retry and set <code>spring.cloud.config.failFast=true</code>.</p>&#xA;"
29152337,29148547,1259109,2015-03-19T18:30:20,"<p>Your inner <code>@Configuration</code> classes need to be static. I'm surprised the app starts at all, and probably the whole of your <code>SecurityConfiguration</code> is actually not being used.</p>&#xA;&#xA;<blockquote>&#xA;  <p>It's possible to handle token request and refresh cycles by spring security without redirect to the authentication app?</p>&#xA;</blockquote>&#xA;&#xA;<p>Naturally. Did you read about the password and refresh_token grants in the spec? But in a web UI you are strongly advised to use the auth code grant (with the redirects), so that the user only enters his credentials in a trusted place.</p>&#xA;&#xA;<blockquote>&#xA;  <p>the user clicks on button and an event is written but the processing of this will be hours later. How can i process the event with the user credentials?</p>&#xA;</blockquote>&#xA;&#xA;<p>Refresh tokens might be the best approach. The event obviously needs to be secure since it will have to contain the refresh token.</p>&#xA;"
50522942,50430808,3226981,2018-05-25T06:38:58,"<p>When you are having complex business logic/complex relationships which can take more time to execute then you need to be cautious while using combination of <code>Lambda</code> and <code>API Gateway</code> for following reasons:</p>&#xA;&#xA;<ol>&#xA;<li><code>Api Gateway</code> is very useful in case of writing API only if you are sure that your APIs are fast and will bring the response in quick time. Currently, <code>API gateway</code> has the <strong><em>hard limit of timeout after 29 seconds</em></strong>. This means if your API takes more than 29 seconds then it will straight forwardly terminate that request.</li>&#xA;<li>Something similar goes with <code>lambda</code> as well. <code>Lambdas</code> are specially designed to handle smaller chunks of processing. Currently it supports <strong><em>maximum memory of 3008 MB and timeout of 5 minutes</em></strong>. After 5 minutes lambda will terminate the processing of the request.</li>&#xA;</ol>&#xA;&#xA;<p>So, your Option 1 has these limitations otherwise it's really easy and time+cost effective to use that combo along with <code>AWS Cognito</code>.</p>&#xA;&#xA;<p>Now let's see about your Option 2.</p>&#xA;&#xA;<p>Option 2 is widely used generally for monolithic applications as they are big, more complex and time consuming. </p>&#xA;&#xA;<p>If you are thinking to go with it then I would like to let you know that <code>API Gateway</code> won't be useful because of the timeout hard limit mentioned above. &#xA;In that case, Authentication can be done using widely available techniques like <code>JWT</code>, <code>OAuth</code>, <code>Basic Auth</code> or even <code>AWS Cognito</code>.</p>&#xA;&#xA;<p>So, in my opinion if you have microservice based architecture then there shouldn't be any issues going with Option 1 provided time performance is honored!</p>&#xA;&#xA;<p>Happy coding!</p>&#xA;"
30622678,30621628,4144997,2015-06-03T14:14:23,"<p>I think this article might help you:&#xA;<a href=""http://www.codingthearchitecture.com/2014/02/19/software_architecture_provides_boundaries_for_tdd_bdd_ddd_rdd_and_clean_code.html"" rel=""nofollow"">Software architecture provides boundaries for TDD, BDD, DDD, RDD and clean code</a></p>&#xA;"
44452714,43340819,213861,2017-06-09T08:16:30,"<p>You can use: <a href=""https://github.com/faceyspacey/react-universal-component"" rel=""nofollow noreferrer"">https://github.com/faceyspacey/react-universal-component</a> and load the component code as a string (which is what I assume you're doing if you're talking about retrieving it over ""ajax""), evaluate it, and return it from the <code>asyncComponent</code> function as a promise, or pass it to a callback passed to the function. It's like React Loadable, but doesn't need to work strictly with async imports. Check the docs. </p>&#xA;"
41576040,41566244,3625317,2017-01-10T18:40:57,"<p>Yes you can, what you usually do is</p>&#xA;&#xA;<p><strong>Alternative A:</strong></p>&#xA;&#xA;<p>create a busybox image and COPY your framework, expose the location as a volume <code>VOLUME /opt/framework/</code></p>&#xA;&#xA;<pre><code>FROM alpine&#xA;COPY framework /opt/framework&#xA;VOLUME /opt/framework&#xA;COPY busyscript.sh /usr/local/bin/busyscript&#xA;RUN chmod +x /usr/local/bin/busyscript&#xA;CMD [""busyscript""]&#xA;</code></pre>&#xA;&#xA;<p>While the <code>busyscript.sh</code> looks like</p>&#xA;&#xA;<pre><code>#!/bin/sh&#xA;#set -x&#xA;&#xA;pid=0&#xA;&#xA;# SIGTERM-handler&#xA;term_handler() {&#xA;  if [ $pid -ne 0 ]; then&#xA;    kill -SIGTERM ""$pid""&#xA;    wait ""$pid""&#xA;  fi&#xA;  exit 143; # 128 + 15 -- SIGTERM&#xA;}&#xA;&#xA;# setup handlers&#xA;# on callback, kill the last background process, which is `tail -f /dev/null` and execute the specified handler&#xA;trap 'kill ${!}; term_handler' SIGTERM&#xA;&#xA;echo ""Started code""&#xA;# wait forever&#xA;while true&#xA;do&#xA;  tail -f /dev/null &amp; wait ${!}&#xA;done&#xA;</code></pre>&#xA;&#xA;<p>Add this image as a service in your docker-compose.yml as lets say ""framework"", then, on the services you want them to consume, you add</p>&#xA;&#xA;<pre><code>volume_from&#xA;  - framework:ro&#xA;</code></pre>&#xA;&#xA;<p>Pros:</p>&#xA;&#xA;<ul>&#xA;<li>you can compile, build and deploy the framworks soley</li>&#xA;<li>there is more or less no runtime overhead for running this extra container</li>&#xA;</ul>&#xA;&#xA;<p>Con:</p>&#xA;&#xA;<ul>&#xA;<li>image-size overhead ( alpine, 30mb)</li>&#xA;</ul>&#xA;&#xA;<p><strong>Alternative B</strong>&#xA;You use one of your services as the ""framework base"", lets say service A, that means you copy the framework on that service ( one of the 2 consuming it ) and also again use <code>VOLUME /opt/framework</code> to expose it as volume</p>&#xA;&#xA;<p>in the service B, the same way, you mount the volume</p>&#xA;&#xA;<pre><code>serviceB:&#xA;  volume_from&#xA;    - serviceA:ro&#xA;</code></pre>&#xA;&#xA;<p>Pro:</p>&#xA;&#xA;<ul>&#xA;<li>no extra container</li>&#xA;</ul>&#xA;&#xA;<p>Con: </p>&#xA;&#xA;<ul>&#xA;<li>framework needs to be deployed with serviceA, no matter service A would need updates&#xA;&#xA;<ul>&#xA;<li>you have a dependency on A, does A need an update, all other containers need to be recreated due to the share</li>&#xA;</ul></li>&#xA;</ul>&#xA;"
49885639,43763479,3625317,2018-04-17T19:01:15,"<p>Seem like this issue has been identified by Hashicorp and addressed in <a href=""https://github.com/hashicorp/consul/blob/master/CHANGELOG.md#085-june-27-2017"" rel=""nofollow noreferrer"">https://github.com/hashicorp/consul/blob/master/CHANGELOG.md#085-june-27-2017</a> where <code>-disable-host-node-id</code> has been set to true by default, thus the node-id is no longer generated from the host hardware but a random uuid, which solves the issue i had running several consul nodes on the same physical hardware</p>&#xA;&#xA;<p>So the way i deployed was fine.</p>&#xA;"
45648854,45648115,3625317,2017-08-12T09:48:51,"<p>Its not part of your question to elaborate how to deal with shared libraries in Microservice-Ecosystems and what to avoid there, but if you like, you should read this up to get you at least a list for pros and cons of ""sharing"".</p>&#xA;&#xA;<p>Beside that, you can create a library container which only offers this library to be mounted.</p>&#xA;&#xA;<pre><code>version: ""2""&#xA;&#xA;services:&#xA;  shared:&#xA;    image: me/mysharelib&#xA;  m1:&#xA;    volume_from:&#xA;      - shared:ro&#xA;  m2:&#xA;    volume_from:&#xA;      - shared:ro&#xA;</code></pre>&#xA;&#xA;<p>while your mysharedlib image looks more or less like this</p>&#xA;&#xA;<pre><code>FROM busybox&#xA;&#xA;COPY bin/busyscript.sh /usr/local/bin/busyscript&#xA;&#xA;WORKDIR /your/lib/folder&#xA;VOLUME /your/lib/folder&#xA;&#xA;CMD [""busyscript""]&#xA;</code></pre>&#xA;&#xA;<p>and your busyscript is just a dummy like this</p>&#xA;&#xA;<pre><code>#!/bin/sh&#xA;#set -x&#xA;&#xA;pid=0&#xA;&#xA;# SIGTERM-handler&#xA;term_handler() {&#xA;  if [ $pid -ne 0 ]; then&#xA;    kill -SIGTERM ""$pid""&#xA;    wait ""$pid""&#xA;  fi&#xA;  exit 143; # 128 + 15 -- SIGTERM&#xA;}&#xA;&#xA;# setup handlers&#xA;# on callback, kill the last background process, which is `tail -f /dev/null` and execute the specified handler&#xA;trap 'kill ${!}; term_handler' SIGTERM&#xA;&#xA;echo ""Started DW php code""&#xA;# wait forever&#xA;while true&#xA;do&#xA;  tail -f /dev/null &amp; wait ${!}&#xA;done&#xA;</code></pre>&#xA;&#xA;<p>As you see, m1/m2 ... m10 mount the library which and it is truly shared across all microservices. </p>&#xA;&#xA;<p>Alternatives:&#xA;You can for sure use an private NPM packages or simply package the shared lib into the microservice m1..m10 during image build time.</p>&#xA;&#xA;<p>What describe above especially suits you well when you want to replace the shared library in the stack with very little overhead and want to ensure the library is in-sync for all container instancs</p>&#xA;"
51420644,51419192,2185829,2018-07-19T10:45:37,"<p>If you have classes that need to be in some of your <code>Microservices</code>, i think it's better to make a <code>shared library</code> and put your shared classes in that, then use your <code>shared library</code> in your <code>Microservices</code>. </p>&#xA;&#xA;<p>Actually i think it's a good idea to put classes that need to be in most of your <code>Microservices</code> in a <code>shared library</code> and use that library. But should be careful, because it may comes to <code>tight coupling</code> which isn't a good thing in <code>Microservices Architecture</code>. </p>&#xA;&#xA;<p>Personally i think some <code>Configuration</code> classes and some <code>Event models</code> that most of your <code>Microservices</code> use are good candidates. But i don't think sharing your <code>Service</code> classes between your <code>Microservices</code> are a good idea. Instead they should use each other's services as they are completely independent and are using external services.</p>&#xA;"
51406920,51392666,2185829,2018-07-18T16:29:06,"<p>When you are inside of a <code>Docker</code> container, <code>localhost</code> resolves to that container not <code>Docker Host</code>. So you should change the IP address of the <code>Config Server</code> to <code>Docker Host</code> IP address or your LAN IP address in your application configurations. Of course it will work if you have a running <code>Config Server</code> in your machine.</p>&#xA;"
51238338,51238009,2185829,2018-07-09T04:44:17,"<p>I think it's better that you have an endpoint that returns all the suppliers information and specify some optional query attributes to restrict the results for caller situation.</p>&#xA;&#xA;<p>in this case an attribute like supplierNames which is an array of String. <code>/suppliers?supplierNames=s1,supplierNames=s2,...</code> and you return all suppliers that has the supplierName in your supplierNames query attribute.</p>&#xA;"
37400186,35862481,1861733,2016-05-23T20:42:40,"<p>As far I know, you don't need to mirror User and Role domain in App 1, because your App2 + Spring Security REST plugin will be using JWT per default.</p>&#xA;&#xA;<p>All you have to do is save the token in App1 and send it in the header every time you send request to App2.</p>&#xA;"
44129294,43792085,7707749,2017-05-23T08:12:21,"<p>You're doing it wrong. You're defining <code>user</code>, but not using it.</p>&#xA;&#xA;<p>The following <b>example</b> greets a user who has signed in to the app with a personalized message and a link to sign out. If the user is not signed in, the app offers a link to the sign-in page for Google Accounts.</p>&#xA;&#xA;<p>If you use the <code>from google.appengine.api import users</code> module:</p>&#xA;&#xA;<pre><code>def get(self):&#xA;    user = users.get_current_user()&#xA;    if user:&#xA;        nickname = user.nickname()&#xA;        logout_url = users.create_logout_url('/')&#xA;        greeting = 'Welcome, {}! (&lt;a href=""{}""&gt;sign out&lt;/a&gt;)'.format(nickname, logout_url)&#xA;    else:&#xA;        login_url = users.create_login_url('/')&#xA;        greeting = '&lt;a href=""{}""&gt;Sign in&lt;/a&gt;'.format(login_url)&#xA;&#xA;    self.response.write('&lt;html&gt;&lt;body&gt;{}&lt;/body&gt;&lt;/html&gt;'.format(greeting))&#xA;</code></pre>&#xA;&#xA;<p>When creating a <code>user</code> you still need to check if it's empty or not. Plus <code>user</code> stores different values. So you just need to make a call to them and define them.</p>&#xA;&#xA;<p>If you have pages that require the user to be signed in in order to access, you can enforce this in your <a href=""https://cloud.google.com/appengine/docs/standard/go/config/appref#handlers_login"" rel=""nofollow noreferrer"">app.yaml</a> file.</p>&#xA;&#xA;<p>By default, your app will use Google Accounts for authentication. To choose another option, such as Google Apps domain, go to the <a href=""https://console.cloud.google.com/appengine/start?pli=1&amp;project=randombrowser-1493197222634"" rel=""nofollow noreferrer"">settings</a> page for your project in the Google Cloud Platform Console and click <code>Edit</code>. In the Google authentication <code>dropdown menu</code>, select the desired authentication type, and then click <code>Save</code>.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>You could however also use the <a href=""http://www.tipfy.org/"" rel=""nofollow noreferrer"">Tipfy</a> framework.</p>&#xA;"
43452024,43440170,48229,2017-04-17T13:01:10,"<p>Why not use a <a href=""https://docs.spring.io/spring/docs/current/spring-framework-reference/html/integration-testing.html#__testpropertysource"" rel=""nofollow noreferrer""><code>@TestPropertySource#properties</code></a>?</p>&#xA;&#xA;<blockquote>&#xA;  <p>The following example demonstrates how to declare inlined properties.</p>&#xA;</blockquote>&#xA;&#xA;<pre><code>@ContextConfiguration&#xA;@TestPropertySource(properties = { ""timezone = GMT"", ""port: 4242"" })&#xA;public class MyIntegrationTests {&#xA;    // class body...&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Note that from that <a href=""http://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/test/context/TestPropertySource.html"" rel=""nofollow noreferrer"">javadocs</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p><code>@TestPropertySource</code> is a class-level annotation</p>&#xA;</blockquote>&#xA;&#xA;<p>This means that you'll need to have different classes for the different configuration values you want to use.</p>&#xA;"
29835674,29821391,48229,2015-04-23T22:44:40,"<p>You could try not redirecting the user, but sending back the content you intended with some javascript to make the client browser to jump to the anchor on the after it loads, e.g. something like:</p>&#xA;&#xA;<pre><code>window.location.hash=""/some_path/"";&#xA;</code></pre>&#xA;"
39892128,39888380,6019685,2016-10-06T09:16:43,"<p>You can use the concept of intercept-url in spring security for this. </p>&#xA;&#xA;<ol>&#xA;<li>Java Based Configuration - h ttp://docs.spring.io/spring-security/site/docs/current/reference/htmlsingle/#authorize-requests</li>&#xA;<li>XML Based Configuration - h ttp://docs.spring.io/spring-security/site/docs/current/reference/htmlsingle/#ns-minimal <strong>and</strong> h ttp://docs.spring.io/spring-security/site/docs/current/reference/htmlsingle/#nsa-intercept-url</li>&#xA;</ol>&#xA;&#xA;<p>In your case, the URL to be intercepted is ""permission"". And in your particular case, you would want to dynamically configure the intercept URLs in spring security, reading them (permissions) from your database. You may look at the following answers to achieve the same by implementing a custom <strong>FilterInvocationSecurityMetadataSource</strong>. - </p>&#xA;&#xA;<ol>&#xA;<li><a href=""https://stackoverflow.com/questions/14215527/how-can-i-manage-spring-security-url-pattern-in-java-class-instead-of-xml-config"">how can I manage spring security url pattern in java class instead of xml config</a></li>&#xA;<li><a href=""https://stackoverflow.com/questions/6893061/how-to-dynamically-decide-intercept-url-access-attribute-value-in-spring-secur#6898824"">How to dynamically decide &lt;intercept-url&gt; access attribute value in Spring Security?</a></li>&#xA;</ol>&#xA;&#xA;<p><strong>P.S</strong>: Sorry for the broken links for spring documentation. StackOverflow does not allow me to post more than 2 links as I am a new contributor here. </p>&#xA;"
49779320,49777963,6226790,2018-04-11T15:44:07,"<p>Each docker-compose stands up its own network for the services it manages. As you proposed, using a shared bridge network would be the easiest way to ensure each different stack of services in separate docker-compose.yml would be able to connect to one another.</p>&#xA;&#xA;<p><a href=""https://docs.docker.com/compose/networking/#specify-custom-networks"" rel=""nofollow noreferrer"">You can tell docker-compose to look for and use a pre-existing network</a>. You can create this network with <code>docker network create</code> or maybe specify it in your service-a docker-compose.yml as a custom network with a static name:</p>&#xA;&#xA;<p>service-a docker-compose.yml</p>&#xA;&#xA;<pre><code>version: '3'&#xA;services:&#xA;  service-a:&#xA;    ...&#xA;&#xA;networks:&#xA;  mynetwork:&#xA;    driver: bridge&#xA;</code></pre>&#xA;&#xA;<p>Then use that network in your other docker-compose.yml files. You need to specify the network at the <em>top level</em> of the docker-compose.yml and then you can tell each service to use it:</p>&#xA;&#xA;<p>some other docker-compose.yml</p>&#xA;&#xA;<pre><code>version: '3'&#xA;services:&#xA;  web:&#xA;    ...&#xA;  networks:&#xA;    - mynetwork&#xA;&#xA;networks:&#xA;  mynetwork:&#xA;    external:&#xA;      name: mynetwork&#xA;</code></pre>&#xA;&#xA;<p>I only suggest the second option since you said any other app would need service-a running anyway.</p>&#xA;"
47315570,47314410,6226790,2017-11-15T19:17:55,"<blockquote>&#xA;  <p>If one docker affected by CPU or memory load, auto-scaling should create the copy of the particular docker only(not the whole task). How can I achieve this in ECS using Elastic Beanstalk?</p>&#xA;</blockquote>&#xA;&#xA;<p>ECS can only scale whole task definitions. This is defined at the ECS service level using <a href=""http://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html"" rel=""nofollow noreferrer"">service auto scaling</a>. Unfortunately I can't speak to how Elastic Beanstalk handles this, as I don't use it for container management.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Is it possible to create 10 services and each contains single task and single docker container to resolve this problem?</p>&#xA;</blockquote>&#xA;&#xA;<p>In short, yes, this is how you would solve your problem. Those 10 services can still coexist on as many or as few cluster instances as you like.</p>&#xA;"
41110500,41036545,3087974,2016-12-12T22:06:11,"<p>The simplest way is to use Map instead of custom POJO in case of agile structure:</p>&#xA;&#xA;<p>It is easy to read from JSON, e.g. using <code>JsonParser parser</code> (java docs <a href=""http://fasterxml.github.io/jackson-core/javadoc/2.1.0/com/fasterxml/jackson/core/JsonParser.html"" rel=""nofollow noreferrer"">here</a>):</p>&#xA;&#xA;<pre><code>Map&lt;String, Object&gt; fields =&#xA;      parser.readValueAs(new TypeReference&lt;Map&lt;String, Object&gt;&gt;() {});&#xA;</code></pre>&#xA;&#xA;<p>It is easy to write into MongoDB using <code>BasicDBObject</code> (java docs <a href=""https://api.mongodb.com/java/2.6/com/mongodb/BasicDBObject.html#BasicDBObject(java.util.Map)"" rel=""nofollow noreferrer"">here</a>):</p>&#xA;&#xA;<pre><code>DBCollection collection = db.getCollection(""tasks"");&#xA;collection.insert(new BasicDBObject(fields));&#xA;</code></pre>&#xA;&#xA;<p>You can even achieve it by wrapping <code>Map</code> with <code>Task</code> like this:</p>&#xA;&#xA;<pre><code>public class Task {&#xA;    private final Map&lt;String, Object&gt; fields;&#xA;&#xA;    public final double operand1;&#xA;    // And so on...&#xA;&#xA;    @JsonCreator&#xA;    public Task(Map&lt;String, Object&gt; fields) {&#xA;        this.fields = fields;&#xA;&#xA;        this.operand1 = 0; /* Extract desired values from the Map */&#xA;    }&#xA;&#xA;    @JsonValue&#xA;    public Map&lt;String, Object&gt; toMap() {&#xA;        return this.fields;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<hr>&#xA;&#xA;<p>It is also possible to use custom <code>JsonDeserializer</code> if it is required (<code>Task</code> must be annotated with <code>@JsonDeserialize(using = TaskDeserializer.class)</code> in that case):</p>&#xA;&#xA;<pre><code>public class TaskDeserializer extends JsonDeserializer&lt;Task&gt; {&#xA;    @Override&#xA;    public Task deserialize(JsonParser parser, DeserializationContext context)&#xA;            throws IOException, JsonProcessingException {&#xA;        Map&lt;String, Object&gt; fields = parser.readValueAs(new TypeReference&lt;Map&lt;String, Object&gt;&gt;() {});&#xA;        return new Task(fields);&#xA;}&#xA;</code></pre>&#xA;"
35699419,35684313,745574,2016-02-29T12:20:41,"<p>If you run a task on the root project, Gradle invokes the same task (if it exists) on all the subprojects. Just execute <code>runApp</code> from the root folder.</p>&#xA;&#xA;<p>In your case however, your runApp task might not exit because it starts a server, so execution will not move on to the next project. You can either enable parallel execution, or modify your tasks to run your server tasks in the background (using <code>ProcessBuilder</code>)</p>&#xA;"
44928327,44901467,944230,2017-07-05T13:56:52,"<p>Creating a proper authentication mechanism in a REST API is (i) not trivial, and (ii) already done in many many languages. Unless for learning purposes, I would suggest not to rewrite that.</p>&#xA;&#xA;<p>Depending on your backend language of choice, you can choose a framework to do exactly that. For example, in Python, I would suggest Django with Django Rest Framework.</p>&#xA;"
51600236,47793065,6521702,2018-07-30T18:33:57,"<p>Other than Iframe there is one more option is currently available is SPA framework, you can get the sample code from here</p>&#xA;&#xA;<p><a href=""https://github.com/PlaceMe-SAS/single-spa-angular-cli-examples"" rel=""nofollow noreferrer"">https://github.com/PlaceMe-SAS/single-spa-angular-cli-examples</a>&#xA;<a href=""https://github.com/joeldenning/simple-single-spa-webpack-example"" rel=""nofollow noreferrer"">https://github.com/joeldenning/simple-single-spa-webpack-example</a></p>&#xA;"
44414798,44414323,1094246,2017-06-07T13:59:28,"<p>I would build the application without any testing. Then I would test it using <code>docker run</code> so you can take the docker network advantages.</p>&#xA;&#xA;<p>Roughly this is more elegant than test in the middle of the build:</p>&#xA;&#xA;<ol>&#xA;<li><code>docker build -t my_app:1.0-early</code> your application in order to obtain an image. </li>&#xA;<li><code>docker run --network my_test_network my_app:1.0-early /run_test_cases.sh</code>. Return the properly exit code or text.</li>&#xA;<li>Depending on the success or not of the test, re tag: <code>docker tag my_app:1.0</code></li>&#xA;</ol>&#xA;&#xA;<p>You will need to have already created a docker network (<code>docker network create my_test_network</code>), or better use docker-compose.</p>&#xA;"
47367196,47366889,3465318,2017-11-18T14:34:05,"<p>I guess there is no way for that today. At least for 8 month of development with GCP, including Datastore, App Engine, Functions, Compute Engine, PubSub, I was not able to find much libraries for them. Including Elasticsearch indexing for Datastore out of the box. Today we are doing the same thing (indexing db entities in elasticsearch), and this is done as additional DB communication layer.</p>&#xA;"
51666145,51664743,5894758,2018-08-03T05:59:19,"<p>You can use kubernetes own dns.</p>&#xA;&#xA;<p><a href=""https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/</a></p>&#xA;&#xA;<p>So you can access your pod using pod dns;</p>&#xA;&#xA;<blockquote>&#xA;  <p>When enabled, pods are assigned a DNS A record in the form of</p>&#xA;  &#xA;  <p><strong>“pod-ip-address.my-namespace.pod.cluster.local”</strong></p>&#xA;</blockquote>&#xA;&#xA;<p>With service you can use</p>&#xA;&#xA;<blockquote>&#xA;  <p><strong>my-svc.my-namespace.svc.cluster.local</strong></p>&#xA;</blockquote>&#xA;"
45689120,45559979,2885735,2017-08-15T08:26:12,"<p>When you do server to server communication, you're not really acting on behalf of a user, but you're acting on behalf of the server itself. For that client credentials are used.</p>&#xA;&#xA;<p>Using curl for exemple : &#xA;curl acme:acmesecret@localhost:9999/oauth/token -d grant_type=client_credentials</p>&#xA;&#xA;<p>You should do the same with your http client and you will get the access token. use it to call other services.</p>&#xA;"
48973904,36137802,8788071,2018-02-25T13:19:51,"<p>How about if we have two <em>event stores</em>, and whenever a <em>Domain Event</em> is created, it is queued onto both of them. And the event handler on the query side, handles events popped from both the event stores.</p>&#xA;&#xA;<p>Ofcourse every event should be <strong><em>idempotent</em></strong>.&#xA;But wouldn’t this solve our problem of the event store being a single point of entry?</p>&#xA;"
30688753,30173267,25191,2015-06-07T00:05:00,"<p>If you embed the security logic into the other services, then you really can't call it a microservice architecture now can you? I also understand that having that extra and duplicitous server round trip for every other service can be a bit of a drag. Here are some viable alternatives for you to consider.</p>&#xA;&#xA;<p>Put all four of these microservices behind a firewall. Expose a public facing service that uses the security service to validate the incoming request and then call the other services if the request is valid for the given credentials. The other services always trust the caller which is a service owned and operated within your trusted environment.</p>&#xA;&#xA;<p>If this is a ""fire-and-forget"" use case and you feel uncomfortable about that public facing service having too much orchestration responsibilities, then consider this alternative. The public facing service sends the inbound request to an unauthorized queue in a message broker. The security service consumes from that queue and performs the authentication. If valid, then the security service queues the message up on an authorized queue. Any number of microservices after that consume from the authorized queue and perform their respective operation. </p>&#xA;"
45331450,45321939,94311,2017-07-26T15:32:35,"<p>When hosting them in the same process, it's possible to self-host the NServiceBus endpoint in the WebAPI code but this is not recommended as IIS will shut down the worker process after a period of inactivity</p>&#xA;&#xA;<blockquote>&#xA;  <p>IIS will not shutdown the process if you set the idletimeout to zero&#xA;  in the application pool settings.</p>&#xA;</blockquote>&#xA;&#xA;<p>Generally it is not recommended to run any background tasks(which in this case is) in IIS process as there is Application Domain recycle,Application Pool recyle settings all those things will come into picture. So <strong>going the Windows service(SelfHosted OWIN) is right approach in my opinion</strong>.</p>&#xA;&#xA;<p>I have not worked much on NServiceBus so cannot comment on problems part.But having looked at the NService Bus <a href=""https://docs.particular.net/samples/web/asp-web-application/"" rel=""nofollow noreferrer"">documentation</a>,I can see that it is recommended by them .</p>&#xA;"
34854997,34841907,1930087,2016-01-18T12:38:08,"<p>For APIs in Rails, see <a href=""http://edgeguides.rubyonrails.org/api_app.html"" rel=""nofollow"">http://edgeguides.rubyonrails.org/api_app.html</a>. I suggest to use a RESTful API, because REST uses the normal HTTP verbs and fits well to the Rails philosophy, see e.g. <a href=""https://www.airpair.com/ruby-on-rails/posts/building-a-restful-api-in-a-rails-application"" rel=""nofollow"">https://www.airpair.com/ruby-on-rails/posts/building-a-restful-api-in-a-rails-application</a></p>&#xA;"
36842139,36840448,3636071,2016-04-25T13:33:26,<p>Yes. It's a good idea to have wrapper around it.&#xA;It will help you to have different databases in future with minimal modification.</p>&#xA;&#xA;<p>In future you may replace it with lucene or you may want to go for any nosql database like mongodb.</p>&#xA;
44625770,44625192,7897191,2017-06-19T08:36:55,"<blockquote>&#xA;  <p>The whole application uses one topic.</p>&#xA;</blockquote>&#xA;&#xA;<p>I really see little benefit to this</p>&#xA;&#xA;<blockquote>&#xA;  <p>Each microservice uses its own topic.</p>&#xA;</blockquote>&#xA;&#xA;<p>If you need ordering between events that happened to the same entity, this is the way to go. e.g. events such as shopping-cart.product-added, shopping-cart.product-removed for the same shopping-cart ID for consistency should preserve ordering. That implies they go to the same partition, hence the same topic.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Each event type uses its own topic.</p>&#xA;</blockquote>&#xA;&#xA;<p>The benefit of this approach is type-safety, since you only get one type of messages in each topic, the deserialization and downstream handling is less error-prone. However, you can't preserve ordering between different events happening to the same entity.</p>&#xA;&#xA;<p>All in all, I'd suggest one topic per entity type (entity being something that has the events happen to, in DDD terms this would be called an aggregate instead), e.g. shopping-cart. If your services are so granular that you only have one entity type per service, this boils down to one topic per service.</p>&#xA;"
44748028,43950808,7897191,2017-06-25T15:51:26,"<p>I think there are 2 main forces at play here:</p>&#xA;&#xA;<ul>&#xA;<li>decoupling - that's why you have microservices in the first place and want a shared-nothing approach to data persistence</li>&#xA;<li>consistency requirement - if I understood correctly you're already fine with eventual consistency</li>&#xA;</ul>&#xA;&#xA;<p>The diagram makes perfect sense to me, but I don't know of any framework to do it out of the box, probably due to the many use-case specific trade-offs involved. I'd approach the problem as follows:</p>&#xA;&#xA;<p>The upstream service emits events on to the message bus, as you've shown. For the purpose of serialisation I'd carefully choose the wire format that doesn't couple the producer and consumer too much. The ones I know of are protobuf and avro. You can evolve your event model upstream without having to change the downstream if it doesn't care about the newly added fields and can do a rolling upgrade if it does.</p>&#xA;&#xA;<p>The downstream services subscribe to the events - the message bus must provide fault-tolerance. We're using kafka for this but since you chose AMQP I'm assuming it gives you what you need.</p>&#xA;&#xA;<p>In case of network failures (e.g. the downstream consumer cannot connect to the broker) if you favour (eventual) consistency over availability you may choose to refuse to serve requests that rely on data that you know can be more stale than some preconfigured threshold.</p>&#xA;"
44178524,44177783,7897191,2017-05-25T10:42:05,"<p>For a rest API you can consider the Kafka REST Proxy: <a href=""https://github.com/confluentinc/kafka-rest"" rel=""nofollow noreferrer"">https://github.com/confluentinc/kafka-rest</a>&#xA;Or course you can instead build your own using akka-http and akka-stream-kafka.</p>&#xA;&#xA;<p>As to windowing, I'm sure it can be done in akka streams but personally, I'd suggest using Kafka Streams as the first port of call:&#xA;<a href=""http://docs.confluent.io/current/streams/developer-guide.html#windowing"" rel=""nofollow noreferrer"">http://docs.confluent.io/current/streams/developer-guide.html#windowing</a></p>&#xA;&#xA;<p>I'm not sure what exactly you mean by returning the answer back, but if you follow the approach above, you can use use REST Proxy to consume the windowed-aggregated messages or you can build a REST service that queries the Kafka Streams state stores via the so-called ""interactive queries"". This post shows how to do it using javax.ws.rs: <a href=""https://www.confluent.io/blog/unifying-stream-processing-and-interactive-queries-in-apache-kafka/"" rel=""nofollow noreferrer"">https://www.confluent.io/blog/unifying-stream-processing-and-interactive-queries-in-apache-kafka/</a> but for a reactive application you can do the same using akka-http instead (I'm implementing this exact thing on one of my projects).</p>&#xA;"
41649190,41640621,495719,2017-01-14T10:57:25,"<p>From the question and comments I understand that you are trying to rearrange the blocks to enhance the performance of the system. As described by the diagrams you suggest that instead of microservice1 querying microservice2 the gateway would query microservice2 and then query microservice1 providing it the information from microservice2.</p>&#xA;&#xA;<p>As such I don't see how this would significantly increase the system performance but rather the change seems to just move the logic around.</p>&#xA;&#xA;<p>To remedy the situation the performance of the critical microservice2 should be enhanced. It can be done by profiling and optimizing the microservice2 software (vertical scaling) and/or you can introduce load balancing (horizontal scaling) and execute microservice2 on multiple servers. The design pattern to be utilized in this case is <a href=""http://cloudpatterns.org/design_patterns/service_load_balancing"" rel=""nofollow noreferrer"">Service Load Balancing pattern</a>.</p>&#xA;"
38464221,38460678,5613927,2016-07-19T16:47:03,"<p>Your needs are fit good by using JHipster in conjunction with a JHipster UAA.</p>&#xA;&#xA;<h2>general design</h2>&#xA;&#xA;<p>Your first part is to model your bounded contextes, such as ""HR"", ""SELECTION"" or ""CUSTOMER"". But as microservice backend. Every single item in your model, as a ""customer"", or a customers ""address"", or a ""HR"" entity, are called resources. These resources have to be divided wise across your services. </p>&#xA;&#xA;<h2>using the gateway</h2>&#xA;&#xA;<p>The JHipster gateway does 2 things for you. As first, it takes ALL of your microservices and expose them via a single API gateway. As a secondary feature it also offers you a user interface (here: in bootstrap + Angular). &#xA;This does not force you to use this interface. You can use ""jhipster:client"" generator, to generate the 2 clients for ""HR"" and ""SELECTION"" with JS+HTML only, and serve them using plaig old nginx, and refer to your one JHipster Gateway.</p>&#xA;&#xA;<h2>security</h2>&#xA;&#xA;<p>this is where the UAA option comes into play. Using the default JWT implementation will force you to login as an user (which is stored in the gateway), even if your ""CUSTOMER"" microservice is calling ""HR"" microservice.</p>&#xA;&#xA;<p>The UAA offers the option of using OAuth2 to enable all kinds of secure communication. So in this case you can make your ""CUSTOMER"" service ask ""HR"" more privileged data, then a user would be allowed to see, using FeignClients or general ""client credentials grant"".</p>&#xA;&#xA;<p>If your users are the ""CUSTOMERS"", so your JHipster UAA becomes your ""CUSTOMER"" microservice.</p>&#xA;&#xA;<p>if you have some time to wait <a href=""https://github.com/jhipster/generator-jhipster/pull/3662"" rel=""noreferrer"">this feature</a> is beeing merged, implementing all this will become as easy as declare communication interfaces similar to jpa repositories, which handle all the securing for you.</p>&#xA;&#xA;<h2>wrap up</h2>&#xA;&#xA;<p>look how you design your architecture. Use one gateway for all services (maybe without the ui), generate your UI later with <code>jhipster:client</code> and serve them on nginx, and all other things of your logic have to be realized as REST APIs inside small microservices, and take a look of how to use UAA.</p>&#xA;&#xA;<p>To learn how to use UAA the right way, I suggest take a look on <a href=""http://stytex.de/blog/2016/02/01/spring-cloud-security-with-oauth2/"" rel=""noreferrer"">my spring security article</a> and <a href=""https://github.com/ExtremeEnvironment"" rel=""noreferrer"">my example application</a>, which is already using the currently unmerged PR mentioned above. I had no time to write the official docs for this, yet. I also wrote a <a href=""http://stytex.de/blog/2016/03/25/jhipster3-microservice-tutorial/"" rel=""noreferrer"">general article</a>, how to use JHipster microservices, which you can read addtionally to the <a href=""https://jhipster.github.io/microservices-architecture/"" rel=""noreferrer"">official documentation</a>. </p>&#xA;&#xA;<p>Beyond this, you are free to ask me for further information!</p>&#xA;&#xA;<p>good luck</p>&#xA;"
40454812,40436389,5613927,2016-11-06T21:36:38,"<p>This is a question which has a wide range of answers, since there are several ways to go.</p>&#xA;&#xA;<p>The simplest way is to call the endpoints of your first app, which expose your team entities via REST API. This directly means, every time your second service needs to do something with a team entity, it retrieves one or more via HTTP. This currently is mostly covered in the uaa configuration (using JHipster uaa for authentication)</p>&#xA;&#xA;<p>With uaa, you can just define something very similar to a JPA repository:</p>&#xA;&#xA;<pre><code>@AuthorizedFeignClient(name = ""microservice1"")&#xA;public interface TeamClient {&#xA;   @RequestMapping(value = ""/api/teams/"", method = RequestMethod.GET)&#xA;   List&lt;Team&gt; findTeams();&#xA;&#xA;   @RequestMapping(value = ""/api/teams/{id}"", method = RequestMethod.GET)&#xA;   Team findTeam(@PathVariable(""id"") Long id);&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>It looks like the way you define repositories, but works with REST infernally. It also handles security stuff for you, so you can ensure only defined users or services may access your resources. More about this solution <a href=""http://jhipster.github.io/using-uaa/"" rel=""nofollow noreferrer"">here</a></p>&#xA;&#xA;<p>The advantage of this strategy is its simplicity and presence of ready to use implementations from spring and JHipster. The drawback is, that this can be quite low performance, when your design is forcing you to use such requests too often, which leads to a huge network load.</p>&#xA;&#xA;<p>Alternative ways of solving this is using event driven systems, like Spring Cloud Bus, Event-Sourcing, CQRS etc...however, these options are not directly supported by JHipster and needs some time to get in, as it is not trivial.</p>&#xA;"
51858114,51781316,5613927,2018-08-15T11:47:24,"<p>referring to your comment on my question you are getting ""Cannot GET /auth/login"", which is an answer from webpack-dev-server and not the spring application. That means that the dev server doesn't proxy your request to the backend at all.</p>&#xA;&#xA;<p>You can fix this error by changing the file ""webpack.dev.js"", where you should see something like this</p>&#xA;&#xA;<pre><code>    proxy: [{&#xA;        context: [&#xA;            '/uaa',&#xA;            /* jhipster-needle-add-entity-to-webpack - JHipster will add entity api paths here */&#xA;            '/api',&#xA;            '/management',&#xA;            '/swagger-resources',&#xA;            '/v2/api-docs',&#xA;            '/h2-console',&#xA;            '/auth' // that one is probaly missing!!&#xA;        ],&#xA;        target: 'http://127.0.0.1:8080',&#xA;        secure: false,&#xA;        headers: { host: 'localhost:9000' }&#xA;    }],&#xA;</code></pre>&#xA;&#xA;<p>it looks like there is no '/auth' in your proxy list. Add it and restart <code>yarn start</code> to make it work.</p>&#xA;"
51304364,51300620,5613927,2018-07-12T11:21:38,"<p>The problem is that docker is looking for your images appuaa, chipagames. That comes because of you haven't build them locally and docker is looking at the known repositories like hub.docker for it, without success.</p>&#xA;&#xA;<p>You should build your applications either with</p>&#xA;&#xA;<pre><code>./mvnw -Pprod package dockerfile:build&#xA;</code></pre>&#xA;&#xA;<p>or</p>&#xA;&#xA;<pre><code>./gradlew -Pprod build buildDocker&#xA;</code></pre>&#xA;&#xA;<p>and then try to <code>docker-compose up</code> again</p>&#xA;"
40739881,40735590,5613927,2016-11-22T10:49:24,"<h1>eureka</h1>&#xA;&#xA;<p>Unfortunately, this is not that easy, as Eureka favors availability over consistency, so it happens, that your service discovery consists of services, which are dead. A workaround I prefer to use with eurka is to set the MaxAutoRetries property higher for all application (what is easy, as JHipster Registry is a config server, so i share this for all), like</p>&#xA;&#xA;<pre><code>ribbon:&#xA;  ConnectTimeout: 3000&#xA;  ReadTimeout: 60000&#xA;  MaxAutoRetries: 10&#xA;</code></pre>&#xA;&#xA;<p>so this at least make your request succeed after time during a deployment. You will suffer some latency, as the first tries will go ahead to the old service, which is timing out...but no restarts are needed and your new services get online approx. 3 - 5 minutes after deploy (this is my experience...it may differ from case to case)</p>&#xA;&#xA;<h1>consul</h1>&#xA;&#xA;<p>As an alternative you might consider to switch to Hashicorps consul instead of eureka, which favors consistency, so you will be able to manage and sync your service discovery as soon as you deploy.</p>&#xA;&#xA;<p>Currently JHipster does only provide BETA support for consul, becaue it is not possible to run a fully secure installation, because of <a href=""https://github.com/spring-cloud/spring-cloud-consul/issues/201"" rel=""nofollow noreferrer"">this bug</a> where I am waiting for the review of <a href=""https://github.com/spring-cloud/spring-cloud-consul/pull/233"" rel=""nofollow noreferrer"">the fix</a></p>&#xA;"
39884905,39852947,5613927,2016-10-05T22:53:41,"<p>If you are using OAuth2 for internal security, I suggest to use oauth in order to access controll all your services, treating the presense of a token as something like a session.</p>&#xA;&#xA;<p>Consider you generated some access token in one of 4 grant types. You now can access any <code>ResourceServerConfigurerAdapter</code> secured spring cloud resource, by passing that token in a <code>Authorization: Bearer &lt;token&gt;</code> HTTP header, or as get parameter like <code>/service/endpoint?access_token=&lt;token&gt;</code>. </p>&#xA;&#xA;<p>If your token is related to a user (you can authorize clients without users!), you can access its details by getting <code>OAuth2Authentication</code> from securityContext. </p>&#xA;&#xA;<p>Furthermore, if your access tokens are JWTs, which is supported per default in spring cloud security, your neither have to provide a real token store on authorization server, nor you have to request the user endpoints on auth server from resource server to fetch user info, as it is shipped inside the JWT. </p>&#xA;&#xA;<p>With this, everything about your ""session"" (and its state) is stored inside the access token, which you can easily pass around, and scaled without replication mess. </p>&#xA;&#xA;<p>I don't thing you get your security more stateless than this ;)</p>&#xA;"
39743917,39742117,5613927,2016-09-28T09:51:16,"<p>I suggest to switch to <a href=""http://jhipster.github.io/using-uaa"" rel=""nofollow"">JHipster UAA</a> to solve this, as you are looking for <strong>service-to-service</strong> authorizastion. Furthermore, should have no own entitiy served by the gateway. Every single endpoint of your microservice, should be inside a own microservice. </p>&#xA;&#xA;<p>To sum up the use-case of UAA for you, you get a new service, which can authenticate users, to exchange username and password for a JWT, like the JWT authentication, but also internal client credentials, without a user. As this is part of spring-cloud-security, auditing works as well. Keep in mind, that the ""auditor"" in this case will be a service (or more precise, a oauth2 client used by the service), rather than a user.</p>&#xA;"
42450074,42449160,5613927,2017-02-24T23:37:26,"<p>There is no <strong>the right way</strong>, as it depends on your needs. In your mentioned article (i'm the author, thanks for the complement!), I described the general approach, which has a better work flow <a href=""http://stytex.de/blog/2016/09/15/jhipster-3-dot-7-secure-service-communication/"" rel=""nofollow noreferrer"">described here</a>, which makes it easier to implement but doesn't change the fact, you do more CRUD calls for one request as you increase your service communication chain.</p>&#xA;&#xA;<p>So, what might be wrong with that? While this is the most consistent approach (you always get the very recent state of the data), you got a lack of availability, as your store service is coupled to the skill service. If this fails and there is no advanced cache setup (such as with hystrix), you have 2 services failing if skill service crashes. Additionally, request will produce a bigger network overhead.</p>&#xA;&#xA;<p>Another approach is called event-sourcing, where your skill service informs in a messaging channel about changes of skill entities, so all consuming services can calculate the current state by applying that changelogs. While this leads to less network overhead and guaranteed availability, your data is ""eventually consistent"". </p>&#xA;&#xA;<p>For this you could take apache kafka (which is also supported by JHipster) and switch to event based entity communication. This approach is harder to implement, as there are no pre-built functions as they exist for REST-based and secure communication</p>&#xA;"
42493861,42478361,5613927,2017-02-27T19:09:35,"<p>as Gael told, this happens due to a bug in 4.0.6, where we switched to spring boot 1.5.1</p>&#xA;&#xA;<p>this changed the filter order. Just add</p>&#xA;&#xA;<pre><code>security:&#xA;  oauth2:&#xA;    resource:&#xA;      filter-order: 3&#xA;</code></pre>&#xA;&#xA;<p>to application.yml, or wait for the next release with a fix to that error</p>&#xA;"
42392859,42392301,5613927,2017-02-22T13:28:45,"<p>first...the UAA does NOT define a /login path...the login happens via OAuth 2.0 password grant in the gateway. </p>&#xA;&#xA;<p>The other point is, that you are reaching the UAA directly. This is not the Netflix way, as you point to a real host + port, instead of getting this info from zuul. As for zuul, you know the name of your uaa service (its ""uaa""), and zuul builds paths like ""host/serviceNameInUpperCase/path for service""</p>&#xA;&#xA;<p>so try <a href=""http://192.168.1.136:8080/uaa/api/"" rel=""nofollow noreferrer"">http://192.168.1.136:8080/uaa/api/</a>... instead of <a href=""http://192.168.1.136:9999/api/"" rel=""nofollow noreferrer"">http://192.168.1.136:9999/api/</a>...</p>&#xA;&#xA;<p>I believe, this will also fix your CORS issue</p>&#xA;"
39426473,39425569,5421671,2016-09-10T13:19:42,"<p>To get started with Spring Boot and JMS use this getting started guide <a href=""https://spring.io/guides/gs/messaging-jms/"" rel=""nofollow"">https://spring.io/guides/gs/messaging-jms/</a></p>&#xA;&#xA;<p>Once you have that sorted then adding the microservices is just a matter of adding the Spring MVC and Rest components which you could first experiment as a standalone project and then integrate with the JMS application.</p>&#xA;&#xA;<p>To get started with Spring Book and Microservices use the getting started guide <a href=""https://spring.io/guides/gs/rest-service/"" rel=""nofollow"">https://spring.io/guides/gs/rest-service/</a></p>&#xA;"
35566261,35555602,2028135,2016-02-22T23:53:36,"<p>If you are running the official sample application use the correct dependencies, servo-core latest dependencies are compiled with JDK8, Use an older version</p>&#xA;&#xA;<pre><code>        &lt;dependency&gt;&#xA;           &lt;groupId&gt;com.netflix.servo&lt;/groupId&gt;&#xA;           &lt;artifactId&gt;servo-core&lt;/artifactId&gt;&#xA;           &lt;version&gt;0.10.0&lt;/version&gt;&#xA;        &lt;/dependency&gt;&#xA;</code></pre>&#xA;"
35617997,35617996,2028135,2016-02-25T03:57:15,"<p>You have to explicitly define these URLs as Eureka always points to HTTP internally. Read <a href=""http://cloud.spring.io/spring-cloud-static/spring-cloud.html#_registering_a_secure_application"" rel=""nofollow"">Here</a> for more about it.</p>&#xA;&#xA;<p>You can add following into your yaml file in the microservice.</p>&#xA;&#xA;<pre><code>eureka:&#xA;   instance: &#xA;      nonSecurePortEnabled: false&#xA;      securePortEnabled: true&#xA;      statusPageUrl: 'https://${eureka.instance.hostName}:${server.port}/info'&#xA;      healthCheckUrl:'https://${eureka.instance.hostName}:${server.port}/health'&#xA;      homePageUrl: 'https://${eureka.instance.hostName}:${server.port}/'&#xA;</code></pre>&#xA;&#xA;<p>Here <em>""eureka.instance.hostName""</em> and <em>""server.port""</em> values will be taken from the environment.</p>&#xA;"
35593424,35409492,2028135,2016-02-24T04:42:45,"<p>Well this is how I did It. Basically it is a lot easier than I anticipated. The following was copied from <a href=""https://github.com/Netflix/eureka/tree/master/eureka-examples"" rel=""noreferrer"">Netflix eureka project.</a></p>&#xA;&#xA;<pre><code>  DiscoveryManager.getInstance().initComponent(new MyDataCenterInstanceConfig(), new DefaultEurekaClientConfig());&#xA;&#xA;  String vipAddress = ""MY-SERVICE"";&#xA;&#xA;    InstanceInfo nextServerInfo = null;&#xA;    try {&#xA;        nextServerInfo = DiscoveryManager.getInstance()&#xA;                .getEurekaClient()&#xA;                .getNextServerFromEureka(vipAddress, false);&#xA;    } catch (Exception e) {&#xA;        System.err.println(""Cannot get an instance of example service to talk to from eureka"");&#xA;        System.exit(-1);&#xA;    }&#xA;&#xA;    System.out.println(""Found an instance of example service to talk to from eureka: ""&#xA;            + nextServerInfo.getVIPAddress() + "":"" + nextServerInfo.getPort());&#xA;&#xA;    System.out.println(""healthCheckUrl: "" + nextServerInfo.getHealthCheckUrl());&#xA;    System.out.println(""override: "" + nextServerInfo.getOverriddenStatus());&#xA;&#xA;    System.out.println(""Server Host Name ""+ nextServerInfo.getHostName() + "" at port "" + nextServerInfo.getPort() );&#xA;</code></pre>&#xA;&#xA;<p>Also you have to add a configuration file to the class path. Eureka client uses this file to read the information about the eureka servers.</p>&#xA;&#xA;<pre><code>eureka.preferSameZone=true&#xA;eureka.shouldUseDns=false&#xA;eureka.serviceUrl.default=http://localhost:8761/eureka/&#xA;eureka.decoderName=JacksonJson&#xA;</code></pre>&#xA;&#xA;<p>Also you have to provide the eureka client as a dependency. Eureka1 supports JDK7 though some part of it has been built with JDK8. However I had to provide older versions of ""archaius-core"" and ""servo-core"" to make it run with JDK7.</p>&#xA;&#xA;<pre><code>    &lt;dependency&gt;&#xA;        &lt;groupId&gt;com.netflix.archaius&lt;/groupId&gt;&#xA;        &lt;artifactId&gt;archaius-core&lt;/artifactId&gt;&#xA;        &lt;version&gt;0.7.3&lt;/version&gt;&#xA;    &lt;/dependency&gt;&#xA;    &lt;dependency&gt;&#xA;        &lt;groupId&gt;com.netflix.servo&lt;/groupId&gt;&#xA;        &lt;artifactId&gt;servo-core&lt;/artifactId&gt;&#xA;        &lt;version&gt;0.10.0&lt;/version&gt;&#xA;    &lt;/dependency&gt;&#xA;</code></pre>&#xA;&#xA;<p>Eureka2 fully supports JDK7.</p>&#xA;"
47076110,47061556,1315176,2017-11-02T13:04:55,<p>I suspect the issue is only with your docker-compose file. I have the same directory structure and it works fine. You just need to set the build context as the repo directory and run docker-compose from within Local-Development-Repo.</p>&#xA;&#xA;<pre><code>version: '2.1'&#xA;services:&#xA;  ui:&#xA;    image: ui&#xA;    container_name: ui&#xA;    build:&#xA;      context: ../UI-Repo&#xA;  server:&#xA;    build:&#xA;      context: ../Server-Repo&#xA;    image: server&#xA;    container_name: server&#xA;</code></pre>&#xA;
42811377,42810364,1315176,2017-03-15T13:35:31,"<p>By framework, rsp meant Express, Hapy, Restify, etc. Generically speaking, the idea would be that the service receives the request id in a header (x-request-id). For each request a logger instance is initialised with the id which it will include in any logs.</p>&#xA;&#xA;<p>In your app you need a way of accessing the single logger wherever you want to log from. I would suggest logging only happens in the route handlers so attaching the logger to the request object would likely be the way to go.</p>&#xA;&#xA;<p>Restify handles a lot of this by default. Hapi would need a plugin, and for Express you would use middleware.</p>&#xA;&#xA;<p>You will also then need to ensure that any requests to your other services include the request id header. </p>&#xA;"
41989030,41903352,730472,2017-02-01T20:23:35,"<p>(disclaimer: i haven't tried, i just looked at documentation...)</p>&#xA;&#xA;<ol>&#xA;<li>implement your own <a href=""https://github.com/Netflix/conductor/blob/dev/core/src/main/java/com/netflix/conductor/core/execution/tasks/WorkflowSystemTask.java"" rel=""nofollow noreferrer"">WorkflowSystemTask</a></li>&#xA;<li>override start() / execute() method to call your microservice</li>&#xA;<li>set task type to SIMPLE according to  <a href=""https://netflix.github.io/conductor/intro/concepts/#worker-taks"" rel=""nofollow noreferrer"">https://netflix.github.io/conductor/intro/concepts/#worker-taks</a></li>&#xA;</ol>&#xA;"
32141927,31801057,1336680,2015-08-21T13:44:35,"<p>You can view process management as another microservice, which can invoke or be invoked by other microservices.</p>&#xA;&#xA;<p>Check the SpringBoot documentation on the Activi´s user guide on that topic.</p>&#xA;"
45349190,45349101,7237224,2017-07-27T11:13:19,"<p>I'd suggest RabbitMQ, it's easy to get started in C#, you can use the MQTT protocol or AMQP. </p>&#xA;&#xA;<p>If you decide to use a different broker this is quite easy to update, minimal code changes are generally needed. </p>&#xA;&#xA;<p>Both give you a lot of functionality without a lot of code.</p>&#xA;&#xA;<p>It's possible to test clients with various online brokers, though I wouldn't use them necessarily for production.</p>&#xA;&#xA;<p>For C# clients have a look at </p>&#xA;&#xA;<p><a href=""https://m2mqtt.wordpress.com/"" rel=""nofollow noreferrer"">https://m2mqtt.wordpress.com/</a></p>&#xA;&#xA;<p><a href=""http://www.rabbitmq.com/dotnet.html"" rel=""nofollow noreferrer"">http://www.rabbitmq.com/dotnet.html</a></p>&#xA;"
39557114,39555686,51560,2016-09-18T11:24:02,"<p>During 2016 Google summer of code one of the students implemented several bridges to interconnect non vertx applications with vertx event bus. There is a C# implementation here: <a href=""https://github.com/jaymine/TCP-eventbus-client-C-Sharp"" rel=""nofollow"">https://github.com/jaymine/TCP-eventbus-client-C-Sharp</a> that will allow you to use .net applications with vertx.</p>&#xA;"
37899934,37897876,51560,2016-06-18T17:46:48,<p>I did not had the chance to test your code but it seems that the publish operation is throwing an exception and vertx will try to send back an error message. However you already replied and ended the connection. </p>&#xA;&#xA;<p>Now the error might be from your codec but due to the asynchronous nature of vertx you only see it at a later stage and mangled with the internal error handler.</p>&#xA;
39978590,39888480,51560,2016-10-11T13:38:23,"<p>The dashboard was built by Eric Zhao one of the Google Summer of Code students in 2016. The dashboard is not part of any official component however the student did a great job modularizing the code and you can see it at the blueprints project:</p>&#xA;&#xA;<p><a href=""https://github.com/sczyh30/vertx-blueprint-microservice/tree/master/monitor-dashboard"" rel=""nofollow"">https://github.com/sczyh30/vertx-blueprint-microservice/tree/master/monitor-dashboard</a></p>&#xA;"
31296397,31295897,3417449,2015-07-08T14:57:25,"<p>Definitely to prevent issues in any application there are different items to consider. For instance, one of them is having unit tests with a good coverage.</p>&#xA;&#xA;<p>However, FrisbyJS is indeed a tool that will help to check that your services are working as expected in the scenarios specified.</p>&#xA;&#xA;<p>Consider that if you plan to apply FrisbyJS, you will need to have some background with NodeJS, and Jasmine-node packages.</p>&#xA;&#xA;<p>Some other alternatives:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://github.com/visionmedia/superagent"" rel=""nofollow"">SuperAgent</a>, another&#xA;JavaScript library</li>&#xA;<li><a href=""http://jmeter.apache.org/"" rel=""nofollow"">JMeter</a>, java application that besides&#xA;validating the functionality of your services it could also help on&#xA;testing their performance under different scenarios (i.e. specific&#xA;number of concurrent users)</li>&#xA;</ul>&#xA;"
41259378,41188108,6172009,2016-12-21T09:34:01,"<p>So far i got it working. My main problem was the different versions in the different microservies. Please be care about this!</p>&#xA;&#xA;<p>Another think: I think an automatic routing (without any routing definition) is not possible. I had to write at least the name of microservice in the route definitions. The address resolution is be done by the Eureka server in that case.</p>&#xA;&#xA;<p>Please correct me, if I'm wrong.</p>&#xA;"
36159330,36152538,1925481,2016-03-22T16:00:05,"<p>The only service environment variables that are populated in a pod are the services that existed before the pod created. Environment variables are not injected into running pods once they've already been started.</p>&#xA;&#xA;<p>I'm guessing that you created the <code>abc</code> replication controller / pods before you created the <code>abc</code> service. If you kill the existing pods and let them be recreated, they should have the ABC_SERVICE_HOST environment variable set.</p>&#xA;"
42572350,42487685,1080425,2017-03-03T06:44:20,"<p>There are several ways to decouple two applications. One easy way is with  <strong>pub/sub</strong> pattern (in case you don't need a response).<br/>&#xA;<em>(Now if you have an application that is very couple, it will be very difficult to decouple it unless you do some refactoring.)</em><br/>&#xA;<a href=""https://github.com/JustinTulloss/zeromq.node"" rel=""nofollow noreferrer"">zeromq</a> offers a very good implementation of pub/sub and is very fast. <br/>&#xA;e.g.<br/></p>&#xA;&#xA;<pre><code>import zmq from ""zmq"";&#xA;socket.connect('tcp://127.0.0.1:5545');&#xA;socket.subscribe('sendConfirmation');&#xA;&#xA;socket.on('message', function (topic, message) {&#xA;    // you can get the data from message.&#xA;    // something like:&#xA;    const msg = message.toString('ascii');&#xA;    const data = JSON.parse(msg);&#xA;    // do some actions.&#xA;    // .....&#xA;&#xA;});&#xA;&#xA;//don't forget to close the socket.&#xA;process.on('SIGINT', () =&gt; {&#xA;    debug(""... closing the socket ...."");&#xA;    socket.close();&#xA;    process.exit();&#xA;});&#xA;&#xA;//-----------------------------------------&#xA;import zmq from ""zmq"";&#xA;socket.bind('tcp://127.0.0.1:5545');&#xA;socket.send(['sendConfirmation', someData]);&#xA;&#xA;process.on('SIGINT', function() {&#xA;  socket.close();&#xA;});&#xA;</code></pre>&#xA;&#xA;<p>This way you could have <strong>two different containers</strong> (docker) for your modules, just be sure to open the corresponding port.<br/>&#xA;What i don't understand, is why you inject <strong>wsSocket</strong> and also you create a new Socket. Probably what I would do is just to send the&#xA;<strong>socket id</strong>, and then just use it like: <br/></p>&#xA;&#xA;<pre><code>const _socketId = ""/#"" + data.socketId;     &#xA;io.sockets.connected[socketId].send(""some message"");&#xA;</code></pre>&#xA;&#xA;<p>You could also use another solution like <strong>kafka</strong> instead of zmq, just consider that is slower but it will keep the logs.<br/>&#xA;Hope this can get you an idea of how to solve your problem.</p>&#xA;"
45024543,45023334,5280105,2017-07-11T02:47:19,<p>Simply install nginx and do a reverse proxy to your microservices.</p>&#xA;&#xA;<p><strong>nginx</strong> example:</p>&#xA;&#xA;<pre><code>server {&#xA;    listen 80 default_server;&#xA;    server_name myprod.com;&#xA;    location /example-microservice-1 {&#xA;        proxy_pass http://localhost:8001;&#xA;        proxy_set_header Host      $host;&#xA;        proxy_set_header X-Real-IP $remote_addr;&#xA;    }&#xA;    location /example-microservice-2 {&#xA;        proxy_pass http://localhost:8002;&#xA;        proxy_set_header Host      $host;&#xA;        proxy_set_header X-Real-IP $remote_addr;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;
44507037,44503321,37481,2017-06-12T19:09:52,"<p>An Ingress is probably your simplest bet.</p>&#xA;&#xA;<p>You can schedule the creation of an Nginx IngressController quite simply; here's <a href=""https://github.com/kubernetes/ingress/blob/master/examples/daemonset/nginx/nginx-ingress-daemonset.yaml"" rel=""nofollow noreferrer"">a guide</a> for that. Note that this setup uses a DaemonSet, so there is an IngressController on each node. It also uses the hostPort config option, so the IngressController will listen on the node's IP, instead of a virtual service IP that will not be stable. </p>&#xA;&#xA;<p>Now you just need to get your HTTP traffic to any one of your nodes. You'll probably want to define an external DNS entry for each Service, each pointing to the IPs of your nodes (i.e. multiple A/AAAA records). The ingress will disambiguate and route inside the cluster based on the HTTP hostname, using <a href=""https://kubernetes.io/docs/concepts/services-networking/ingress/#name-based-virtual-hosting"" rel=""nofollow noreferrer"">name-based virtual hosting</a>.</p>&#xA;&#xA;<p>If you need to expose non-HTTP services, this gets a bit more involved, but you can look in the nginx ingress docs for more examples (e.g. <a href=""https://github.com/kubernetes/ingress/tree/master/examples/udp/nginx"" rel=""nofollow noreferrer"">UDP</a>).</p>&#xA;"
50382200,50344525,8236137,2018-05-17T01:48:56,"<p>Looks like hazelcast solution sounds ok.&#xA;<a href=""https://stackoverflow.com/questions/13386495/high-availability-singleton-processor-in-tomcat?rq=1"">high availability singleton processor in Tomcat</a></p>&#xA;&#xA;<p>And still checking whether this is the best when doing in AWS.</p>&#xA;"
49794997,49793326,9485673,2018-04-12T11:09:56,"<blockquote>&#xA;  <p>Would it be enough just to deploy everything in a several replicas,&#xA;  then expose Coffea and Tea services as a NodePort and finally expose&#xA;  Nginx as a LoadBalancer?</p>&#xA;</blockquote>&#xA;&#xA;<p>Probably yes, it would be  sufficient to make your application work, but the best way is to use your services with cluster IP and expose it by Ingress. Because Ingress is good for routing but it is not good for serving static content, so you can use the Nginx as a web server only for static content. Your app will look like 3 services hidden behind the Ingress:</p>&#xA;&#xA;<ol>&#xA;<li>Coffea service with cluster IP</li>&#xA;<li>Tea service with cluster IP</li>&#xA;<li>Nginx  service with cluster IP as a web server for static content</li>&#xA;</ol>&#xA;&#xA;<p><a href=""https://stackoverflow.com/questions/47624902/serving-static-files-from-ingress-nginx"">Here is</a> a good answer why it is better to use Nginx as a service for static content.</p>&#xA;"
42834873,42816444,7721321,2017-03-16T13:01:47,"<p>So, your requirements boil down to the following:</p>&#xA;&#xA;<p>1) Environment/Infrastructure agnostic architecture that can run anywhere (local, VPS or cloud).</p>&#xA;&#xA;<p>2) Service life-cycle management (start, stop, monitor,scale).</p>&#xA;&#xA;<p>Agree with Heiko's answer that you should look for solutions built on Docker/Kubernetes. Such solutions need to extend the available primitives to provide the kind of solution you are looking for.</p>&#xA;&#xA;<p>Hasura (<em>full-disclosure: I work here</em>) is one such PaaS that allows you to deploy your services on any infrastructure in an API Gateway pattern based architecture. Monitoring, autoscaling, and back-up &amp; restore like features are available as add-ons. The platform is free to use on your own infrastructure. Check out a <a href=""https://news.ycombinator.com/item?id=13724118"" rel=""nofollow noreferrer"">HackerNews discussion on Hasura</a>.</p>&#xA;"
40146081,40036030,1662570,2016-10-20T05:25:11,"<p>as @spencergibb mentioned it did go wrong in adding dependencies, I've tried creating a new project via <a href=""http://start.spring.io/"" rel=""nofollow"">http://start.spring.io/</a> which resolved my issue.</p>&#xA;"
47935703,47935575,1685098,2017-12-22T04:04:45,"<p>Whether it is a ""correct"" way depends on your architecture and design preferences. You can call one beanstalk application from another via API gateway, especially if you are calling public API methods, and one beanstalk application can call another directly (provided your subnets and security groups are set up to allow this (the default security group used set up by VPC does allow this)).</p>&#xA;&#xA;<p>If you are intending to call private API methods in your <code>loyalty</code> app, then it would make sense to call the 'loyalty` app directly, because even assuming you have proper authentication controls on your API gateway methods it is even better for them to simply not be exposed to the public at all.</p>&#xA;&#xA;<p>More generally, if the <code>loyalty</code> and <code>wallet</code> apps should be considered to be part of the same <em>service</em>, then I would put both under the <em>same</em> API gateway instance, and would allow <code>wallet</code> to call <code>loyalty</code> directly without going through API gateway.</p>&#xA;"
41086797,41086281,7134058,2016-12-11T13:42:03,"<p>As I think that is a major question about classical SOA vs. Microservices.</p>&#xA;&#xA;<p>I guess you can find many opposite answers to that.</p>&#xA;&#xA;<p>so IMHO:&#xA; If in your architecture services communicate each other they are not microservices, since there are dependencies between them.</p>&#xA;&#xA;<p>Instead of that if each microservice has all needed functionality (or say components) and do not depend or communicate to each other then they are microservices.</p>&#xA;&#xA;<p>So in your example you have 4 components.&#xA;Clients, Reservations, ConnectionsDetails, Trains.</p>&#xA;&#xA;<p>But, microservices may not necessary match them exactly.&#xA;As you said ""if user search Connection""...&#xA;So ""Search Connection"" that is microservice which includes all needed components (Client, ConnectionDetails, Trains) and is independent. </p>&#xA;&#xA;<p>And finally, how components (not microservices) will communicate to each other is up to you. With microservices you have a luxury to use straight POJO with no transformations, protocols, transport layers at all. &#xA;Or you can make communications more formal, which push you back closer to classical SOA rather than microservices.</p>&#xA;"
41493335,41492081,256618,2017-01-05T19:40:01,"<p>I suspect the answer is ""it depends"" (Option 3)</p>&#xA;&#xA;<p>How isolated do you need each application to be? How much redundancy (tolerance of VM failure) do you need? How many developers will have access to the Kubernetes cluster? How much traffic do the apps generate?</p>&#xA;&#xA;<p>If in doubt I recommend running all your apps on a single cluster. Simpler to manage and the overhead in providing highly available infrastructure is then shared. You'll also have greater levels of VM utilization, which perhaps might result in reduced hosting costs. The latter is a subjective observation, some apps have very little or occasional traffic resulting in very bored servers :-)</p>&#xA;"
39686991,39686227,256618,2016-09-25T12:46:29,"<p>You need to use a tool that is capable of supporting database migrations. The two I'd recommend in the Java space would be:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""http://www.liquibase.org/"" rel=""nofollow noreferrer"">liquibase</a></li>&#xA;<li><a href=""https://flywaydb.org/"" rel=""nofollow noreferrer"">flyway</a></li>&#xA;</ul>&#xA;&#xA;<p>These are by no means the only tools in this category. What they do is keep a record of the schema changes that have already been applied to your database instance, ensuring that the schema matches the desired state captured in your version control system.</p>&#xA;&#xA;<hr>&#xA;&#xA;<h2>Example</h2>&#xA;&#xA;<ul>&#xA;<li><a href=""https://stackoverflow.com/questions/20733777/best-choice-to-generate-scripts-for-different-databases/20734787#20734787"">Best choice to generate scripts for different databases</a></li>&#xA;</ul>&#xA;"
45848305,45848118,3800170,2017-08-23T19:56:47,"<p>You can configure it this way:</p>&#xA;&#xA;<pre><code>@Override&#xA;public void init(AuthenticationManagerBuilder auth) throws Exception&#xA;{&#xA;    auth.userDetailsService(getUserDetailsService());&#xA;}&#xA;&#xA;@Bean&#xA;UserDetailsService getUserDetailsService() {&#xA;  return username -&gt;&#xA;  {&#xA;    JSONObject user = callUserService(username); //Here you send the UserRequest&#xA;    if(user.has(""email"")) {&#xA;       return new User(&#xA;         user.getString(""email""),&#xA;         user.getString(""password""),&#xA;         true, true, true, true,&#xA;         Collections.emptyList());&#xA;     } else {&#xA;          throw new BadCredentialsException(""BadCredentialsException"");&#xA;      }&#xA;    };&#xA; }&#xA;</code></pre>&#xA;"
47276112,47265946,3800170,2017-11-14T01:12:50,<p>By creating your modules in different projects you create a more flexible solution.</p>&#xA;&#xA;<p>You could even use different languages and technologies in a service in particular. E.g. one of your services could be NodeJS and the rest Java/Spring.</p>&#xA;
36719901,36719045,4856939,2016-04-19T13:27:43,"<p>This is very similar to the argument of monolithic vs micro-kernel design. In a monolithic kernel, there's a single address space. Storm is kind of like that -- you have to use their specific APIs to use the services within Storm. If you use Java or a supported API, you're in luck. Again this is like a kernel library -- you look at what the kernel gives you as an API, and you use that.</p>&#xA;&#xA;<p>With a microkernel, all the kernel does is pass messages around. This is like Kafka. It's just a message passing architecture. Any process can participate in the cluster as long as it can send appropriately structured messages.</p>&#xA;&#xA;<p>Just like with monolithic vs micro kernels, it comes down to an interplay between your design goals and personal philosophy. My thesis is that you can build a Storm-like architecture via microservices, but it would take more work than using Storm out of the box. And a related thesis is that you could not build a microservices architecture via Storm.</p>&#xA;"
41777259,41776814,4074859,2017-01-21T08:30:47,"<p>It is totally based on the processing requirements and frequency of processing. </p>&#xA;&#xA;<p>You can use Amazon EMR for parsing the file and run the algorithm, and based on the requirement you can terminate the cluster or keep it alive for frequent processing. <a href=""https://aws.amazon.com/emr/getting-started/"" rel=""noreferrer"">https://aws.amazon.com/emr/getting-started/</a></p>&#xA;&#xA;<p>You can try using Amazon Athena (Recently launched) service, that will help you for parsing and processing files stored in S3. The infrastructure need will be taken care by Amazon. <a href=""http://docs.aws.amazon.com/athena/latest/ug/getting-started.html"" rel=""noreferrer"">http://docs.aws.amazon.com/athena/latest/ug/getting-started.html</a></p>&#xA;&#xA;<p>For Complex Processing flow requirements, you can use combinations of AWS services like AWS DataPipeline - for managing the flow and AWS EMR or EC2 - to run the processing task.<a href=""https://aws.amazon.com/datapipeline/"" rel=""noreferrer"">https://aws.amazon.com/datapipeline/</a></p>&#xA;&#xA;<p>Hope this helps, thanks</p>&#xA;"
28809667,28607400,4613227,2015-03-02T12:21:25,"<p>You can maintain user states into table. </p>&#xA;&#xA;<p>When users login create one unique id and store it into table with current time stamp and client IP, In client side create key value pair and store it into cookies. Use it as a session.</p>&#xA;&#xA;<p>You have many things now to check user existence.   </p>&#xA;"
31449601,31449013,1075051,2015-07-16T08:51:58,"<p>Yeah, in a sense Spring Boot can be defined as just a RAD coat on Spring. But it's highly sophisticated, appreciable and saves real lot of time.</p>&#xA;&#xA;<p>Micro-services are small decoupled applications, like that HelloController application, as you say. Every micro-service would be a different project. They will talk to one another via exposed APIs.</p>&#xA;&#xA;<p>Spring Boot has means for configuring the embedded servers via code and properties. Its reference material will have more details.</p>&#xA;&#xA;<p>With my experience on Spring Boot for quite some time, and would highly recommend it in general.</p>&#xA;"
27552117,27489124,28779,2014-12-18T17:15:42,"<p>This is one of the best use cases for message queues. Your basically create a queue for data to be translated by the translate service, and a queue for data which is already translated and ready to be sent back to the user. Your REST layer and translation layer publish and subscribe to the applicable queues, and can process the data as it comes in. This has the added benefit of decoupling your REST and translation layer, meaning it becomes trivial to add multiple translation layers later to handle additional load if necessary.</p>&#xA;&#xA;<p>Take a look at <a href=""http://www.squaremobius.net/rabbit.js/"" rel=""nofollow"">RabbitMQ</a>, but there are <a href=""http://queues.io/"" rel=""nofollow"">plenty of other options</a> as well.</p>&#xA;"
47313136,47312403,6650475,2017-11-15T16:56:22,"<p>From the filters project which builds a jar to be shared, provide a class which can be scanned by Spring to create the beans with a <strong>well defined name</strong>.  For example: </p>&#xA;&#xA;<pre><code>package com.me.common.interceptors;&#xA;&#xA;public class InterceptorConfig {&#xA;    public static final String INTERCEPTOR_BEAN_1 = ""comMeCommonInterceptorsInterceptor1"";&#xA;    public static final String INTERCEPTOR_BEAN_2 = ""comMeCommonInterceptorsInterceptor2"";   &#xA;&#xA;    @Bean(name = INTERCEPTOR_BEAN_1)&#xA;    public HandlerInterceptor getInterceptor1() {&#xA;         return new Interceptor1();&#xA;    }&#xA;&#xA;    @Bean(name = INTERCEPTOR_BEAN_1)&#xA;    public HandlerInterceptor getInterceptor2() {&#xA;         return new Interceptor2();&#xA;    }&#xA;&#xA;} &#xA;&#xA;&#xA;public class Interceptor1 implements HandlerInterceptor {&#xA;  // ...&#xA;}&#xA;&#xA;public class Interceptor2 implements HandlerInterceptor {&#xA;  // ...&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Then <a href=""https://stackoverflow.com/questions/28679581/how-to-make-springs-componentscan-search-components-in-included-jars"">configure the app to scan</a> the com.me.common.interceptors package to create the beans.  It doesn't matter the code is in a jar.</p>&#xA;&#xA;<p>Within the app, those beans can then be autowired by name, and <a href=""https://stackoverflow.com/questions/19825946/how-to-add-a-filter-class-in-spring-boot"">registered as usual</a>.</p>&#xA;&#xA;<pre><code>@Autowired&#xA;@Qualifier(InterceptorConfig .FILTER_BEAN_1)&#xA;private HandlerInterceptor interceptor1;&#xA;&#xA;@Autowired&#xA;@Qualifier(InterceptorConfig .FILTER_BEAN_2)&#xA;private HandlerInterceptor interceptor2;&#xA;</code></pre>&#xA;"
50346250,50328886,9150059,2018-05-15T09:04:43,"<p>Option 1 will work but it is looks to be inefficient way to do what you have mentioned. Nginx is a highly capable server (footprint/runtime resources) and can easily be able to serve multiple applications from a separate pod. &#xA;So I think the option 2 is a better option.</p>&#xA;&#xA;<p>Running nginx separately will have following advantages:</p>&#xA;&#xA;<ul>&#xA;<li>Efficient (save on resources and money) because a single nginx will be able to serve multiple applications</li>&#xA;<li>Possibility to use other nginx capabilities in future (e.g. load balancing)</li>&#xA;<li>Maintainability - only a single pod to maintain, monitor and troubleshoot (e.g. Upgrade rollout, monitoring etc.) and many more</li>&#xA;</ul>&#xA;&#xA;<p>I have had a similar requirement. I used a single nginx on a separate pod to serve multiple (250) application deployments running on different pods. I used proxy_pass directive to get the job done.</p>&#xA;"
42179717,42178648,3199595,2017-02-11T18:37:59,"<p>This is a very broad question, so I'll try to give you a few broad thoughts on this:</p>&#xA;&#xA;<p>First of all, I think that the general idea is perfectly fine. All microservices use sockets one way or the other, you're just taking it to a lower level, without the various abstractions on top. That makes sense, especially if you have some very specific requirements.</p>&#xA;&#xA;<p>The number of sockets doesn't seem that large to me. I've seen systems maintain thousands of concurrent open sockets <strong>ten years ago</strong>. What you're suggesting seems trivial on modern hardware and operating systems.</p>&#xA;&#xA;<p>So overall, the design is <strong>totally doable</strong>. But the problem is that you'll be forced to re-invent some wheels along the way. I'll mention a few of them.</p>&#xA;&#xA;<p>First, you'd need <strong>service discovery</strong> for the microservices to find each other.</p>&#xA;&#xA;<p>Then you'd need to design a protocol between each pair of services - you'd need to specify messages and rules for when and how to send them. You'd need message builders and parsers; as well as logic for handling various message errors, and forward/backward compatibility. This is trickier than it sounds, especially if the communication has to be robust, which brings me to the next point - handling network failures.</p>&#xA;&#xA;<p>What network failures? TCP is reliable, right? Well, not entirely. First, there are the disconnections and various network errors. Then, there are some quiet failures - Try opening a socket to a remote machine, then <em>suddenly</em> pulling its network cable. How long will it take for the local socket to throw an exception? Well, it turns out that it could take <strong>minutes</strong>, minutes during which you local socket thinks that the remove machine is alive and well. Sure, if it arrives, then it arrives without error and in the right order. But that's only <strong>if</strong> it arrives.</p>&#xA;&#xA;<p>Then there's the threading. While not unique to pure sockets, threading tends to be more painful on lower abstraction levels.</p>&#xA;&#xA;<p>So while it can be done, you might want to try to avoid re-inventing some of the wheels here. For instance, how about using an <a href=""https://en.wikipedia.org/wiki/Remote_procedure_call"" rel=""nofollow noreferrer"">RPC</a> library? They exist for all modern languages, and the already solve some of the problems you'll have to deal with.</p>&#xA;"
49584841,49584617,1435755,2018-03-31T06:14:36,"<p>Actually there could be multiple solutions and not one solution is best, the best solution is the one which is appropriate for your product's requirements.</p>&#xA;&#xA;<p>I think it would be a better idea to go with separate databases for each of your client (university) to keep the data always isolated even if somethings wrong happens. Also with time, the database could go so huge that it could cause problems to configure/manage separate backups, cleanups for individual clients etc.</p>&#xA;&#xA;<p>Now with separate databases there comes a challenge for managing distributed transactions across databases as you don't know which part is going to fail among many. To manage that, you may have to implement message/event driven mechanism across all your micro-services and ensure consistency.</p>&#xA;&#xA;<p>Regarding message/event mechanism, here is a simple use case scenario, suppose there are two services ""A"" (user-registration) and ""B"" (email-service)</p>&#xA;&#xA;<ol>&#xA;<li>""A"" registers a user temporarily and publishes an event of sending confirmation email. </li>&#xA;<li>The message goes to message broker</li>&#xA;<li>The message is received by ""B"".</li>&#xA;<li>The confirmation email is sent to the user.</li>&#xA;<li>The user confirms the email to ""B""</li>&#xA;<li>The ""B"" publishes event of user confirmation to the broker</li>&#xA;<li>""A"" receives the event of confirmation and the process is completed.</li>&#xA;</ol>&#xA;&#xA;<p>The above is the best case scenario, problems still can happen in between even with broker itself.&#xA;You have to go deep into it if you think you need this.</p>&#xA;&#xA;<p>Some links that may help.</p>&#xA;&#xA;<p><a href=""http://how-to-implement-a-microservice-event-driven-architecture-with-spring-cloud-stre"" rel=""nofollow noreferrer"">http://how-to-implement-a-microservice-event-driven-architecture-with-spring-cloud-stre</a></p>&#xA;&#xA;<p><a href=""http://www.baeldung.com/transactions-across-microservices"" rel=""nofollow noreferrer"">A Guide to Transactions Across Microservices</a></p>&#xA;"
33352043,33308154,5110123,2015-10-26T17:35:03,<p>Actually it isn't supported on Bluemix.&#xA;A solution could be install and run a load balancer correctly configured as you need on a container and make it forwarding the requests to your agents following your balancing policy.&#xA;If you create a container group instead of a container you could also have a fault tolerant solution without having a single point of failure.</p>&#xA;
47793673,47793065,5580140,2017-12-13T13:00:32,"<p>The only way to integrate different Angular Apps apart from IFrames and communication via postMessage is by ""Deep-Linking"" them and exchanging information using URL-Parameter. All Angular apps are then located on their own browser tab. But for common services and components you may want to implement a ""Shared Kernel"" used by all of your Angular apps, therefore your a stuck on a certain Angular version range. When you want this shared kernel the NGModule approach is the recommended way to go. You can mix Angular 2 and Angular 5 versions in one app because they are backward compatible. There ist no urgent need for the teams to stuck on the same version, until the Angular Team introduces breaking changes. Despite of that your technical debts grow, when you are not updating. The frequency for technical-updates is definitly higher in the Angular/Javascript area.</p>&#xA;"
28932366,28930710,469356,2015-03-08T21:55:35,"<p>For this kind of a thing I use blob leases.  Basically, I create a blob with the ID of an entity in some known storage account.  When worker 1 picks up the entity, it tries to acquire a lease on the blob (and create the blob itself, if it doesn't exist).  If it is successful in doing both, then I allow the processing of the message to occur.  Always release the lease afterwards.&#xA;If I am not successfull, I dump the message back onto the queue</p>&#xA;&#xA;<p>I follow the apporach originally described by Steve Marx here <a href=""http://blog.smarx.com/posts/managing-concurrency-in-windows-azure-with-leases"" rel=""nofollow"">http://blog.smarx.com/posts/managing-concurrency-in-windows-azure-with-leases</a>  although tweaked to use new Storage Libraries</p>&#xA;&#xA;<p>Edit after comments:&#xA;If you have a potentially high rate of messages all talking to the same entity (as your commend implies), I would redesign your approach somewhere.. either entity structure, or messaging structure.  </p>&#xA;&#xA;<p>For example: consider CQRS design pattern and store changes from processing of every message independently.  Whereby, product entity is now an aggregate of all changes done to the entity by various workers, sequentially re-applied and rehydrated into a single object</p>&#xA;"
38473862,38461294,6410092,2016-07-20T06:31:24,"<p>I tried in this way to but getting access denied error</p>&#xA;&#xA;<pre><code>String       postUrl       = ""http://10.109.218.178:9090/loadJson"";&#xA;                    HttpClient   httpClient    = HttpClientBuilder.create().build();&#xA;                    HttpPost     post          = new HttpPost(postUrl);&#xA;                    StringEntity postingString = new StringEntity(completeXml);&#xA;                    post.setEntity(postingString);&#xA;                    post.setHeader(""Content-type"", ""application/json"");&#xA;&#xA;&#xA;                    HttpResponse  response = httpClient.execute(post);&#xA;</code></pre>&#xA;&#xA;<p>here completeXml is a string that i need to pass to other microservice</p>&#xA;"
43035743,43034203,5823852,2017-03-26T23:20:12,"<p>I can tell that much depends on your application requirements. Really.&#xA;I'm now past the 5 years of experience in production microservices using several languages going from medium to very large scale system.</p>&#xA;&#xA;<p>None of them shared the same requirements, and without having a deep understanding of what you need and what are your business (product) requirements it would be hard to know what's the right answer, by the way I'll try to share some experience to help you get it right.</p>&#xA;&#xA;<p>Ideally you want the security to be encapsulated in an external service, so that you can update and apply new policies faster. Also you'll be able to deprecate all existing tokens should you find a breach in your system or if someone in your team inadvertedly pushes some secret key (or cert) to an external service.</p>&#xA;&#xA;<p>You could handle authentication on each single service or using an edge newtwork tool (such as the API Gateway). Becareful choosing how to handle it because each one has it's own privileges:</p>&#xA;&#xA;<ol>&#xA;<li><p>Choosing the API Gateway your services will remain lighter and do not need to know anything about the authentication steps, but surely at some point you'll need to know who the authenticated user is and you need some plain reference to it (a JSON record, a link or ID to a ""user profile"" service). How you do it it's up to your requirements and we can even go deeper talking about different pros and cons about each possible choice applicable for your case.</p></li>&#xA;<li><p>Choosing to handle it at the service level requires you (and your teams) to understand better about the security process taking place (you can hide it with a good library) and you'll need to give them support from your security team (it's may also be yourself btw you know the more service implementing security, the more things you'll have to think about to avoid adding unnecessary features). The big problem here is that you'll often end up stopping your tasks to think about what would help you out on this particular service and you'll be tempted to extend your authentication service (and God, unless you really know what you're doing, don't add a single call not needed for authentication purposes).</p></li>&#xA;</ol>&#xA;&#xA;<p>One thing is easy to be determined: you surely need to think about tokens (jwt, jwe or, again, whatever your requirements impose).</p>&#xA;&#xA;<p>JWT has good benefits, but data is exposed to spoofing, so never put in there sensitive data or things you wouldn't publicly share about your user (e.g. an ID is probably fine, while security questions or resolution to 2FA would not). JWE is an encrypted form of the spec. A common token (with no meaning) would require a backend to get the data, but it works much like cookie-sessions and data is not leaving your servers.</p>&#xA;&#xA;<p>You need to define yourself the boundaries of your services and do yourself a favor: make each service boundaries clean, defined and standard.</p>&#xA;&#xA;<p>Try to define common policies and standardize interactions, I know it may be easier to add a queue here, a REST endpoint there, a RPC there, but you'll soon end up with a bunch of IPC you will not be able to handle anymore and it will soon catch your attention.</p>&#xA;&#xA;<p>Also if your business solution is pretty heavy to do I don't think it's a good idea to do yourself the API Gateway, Security and so on. I'd go with open source, community supported (or even company-backed if you have some budget) and production-tested solutions.</p>&#xA;&#xA;<p>By definition microservice architectures are very dynamic, you'll fight to keep it immutable between each deployment version, but unless you're a big firm you cannot effort keeping live thousands of servers. This means you'll discover bugs that only presents under certain circumstances you cannot spot in other environments (it happens often to not be able to reproduce them).</p>&#xA;&#xA;<p>By choosing to develop the whole stack yourself you agree with having to deal with maintenance and bug-discovery in your whole stack. So when you try to load a page that has 25 services interacting you know it may be failing because of a bug in: your API Gateway, your Security implementation, your token parser, your user account service, your business service A to N, your database service (if any), your database load balance (if any), your database instance.</p>&#xA;&#xA;<p>I know it's tempting to do everything, but try to keep it flat and do what you need to do. By following this path you'll think about your product, which I think is what's the most important think to do now.</p>&#xA;&#xA;<p>To complete my answer, about the scaling issues:&#xA;it doesn't matter. Whatever choice you pick it will scale seamlessly:</p>&#xA;&#xA;<ol>&#xA;<li><p>API Gateway should be able to work on a pool of backends (so from that server you should be able to redirect to N backend machines you can put live when you need to, you can even have some API to support automatic registration of new instances, or even simples put the IP of an Elastic Load Balancer or HAproxy or equivalents, and as you add backends to them it will just work -you have moved the multiple IPs issue from the API Gateway to one layer down).</p></li>&#xA;<li><p>If you handle authentication at services level (and you have an API Gateway) see #1</p></li>&#xA;<li><p>If you handle authentication at services level (without an API Gateway) then you need to look at some other level in your stack: load balancing (layer 3 or layer 7), or the DNS level, you can use several features of DNS to put different IPs to answer from, using even advanced features like Anycast if you need latency distribution.</p></li>&#xA;</ol>&#xA;&#xA;<p>I know this answer introduced a lot of other questions, but I really tried to answer your question. The fact is that you need to understand and evaluate a lot of things when planning a microservice architecture and I'd not write a SLOC without a very-written-plan printed on every wall of my office.</p>&#xA;&#xA;<p>You'll often need to go mental focus and exit from a single service to review the global vision and check everything is going fine.</p>&#xA;&#xA;<p>I don't want to scare you, I'm rather trying to make you think to succeed.&#xA;I just want you to make sure you correctly evaluated all of the possibilities before to decide to do everything from scratch.</p>&#xA;&#xA;<p>P.S. Should you choose to act using an API gateway be sure to limit services to only accept requests through it. On the same machine just start listening on localhost, on multiple machines you'll need some advanced networking rule depending on your operating system.</p>&#xA;&#xA;<p>Good Luck!</p>&#xA;"
50141801,50139640,1110636,2018-05-02T19:04:50,"<p>I like to view each microservice as self contained smaller monoliths. When you're forcing yourself to split up your legacy application to, um, smaller monoliths, you'll find:</p>&#xA;&#xA;<ol>&#xA;<li><p>60% of your code is scaffolding and will need to be repeated across multiple services.</p></li>&#xA;<li><p>It's easier to split things (and maintain them that way) if you've established a what-goes-where rule upfront.</p></li>&#xA;</ol>&#xA;&#xA;<p>The most common approach is to split the application by functionality area. So to answer your question, I'd agree more with the image at the top-right, assuming you intended to show multiple containers there.</p>&#xA;&#xA;<p>And about #1 above, there's often a whole bunch of scaffolding modules that you can avoid writing by hand after all.</p>&#xA;"
41770951,40586946,6357613,2017-01-20T19:23:27,"<p>I just ran into the same thing and Brent's answer above worked for me. Oc is up to 1.3.3 now and minishift is up to Beta2.</p>&#xA;&#xA;<p>To answer James' question, here's my fabric8 version:</p>&#xA;&#xA;<p>gofabric8, version 0.4.113 (branch: 'master', revision: '0610f51')&#xA;  build date:       '20170110-20:02:39'&#xA;  go version:       '1.7.1'</p>&#xA;"
45699299,45698451,6619757,2017-08-15T18:35:51,"<p>Typically, if this is a customer site, you would return the customer at login. then the site would use that for other api calls. I wouldn't use local storage for this just keep it in your state. If the app is complex I would use ngrx: <a href=""https://github.com/ngrx"" rel=""nofollow noreferrer"">https://github.com/ngrx</a></p>&#xA;&#xA;<p>If the app is used by someone that will see multiple customers data you would need a call to retrieve a list of customers. You would then use that ist of customers to build the links you need. Again I would just use state not local storage.</p>&#xA;"
51589476,51541318,4851565,2018-07-30T08:02:39,"<p>There are at least three ways that I can think of to solve this problem.</p>&#xA;&#xA;<ol>&#xA;<li><strong>Document Model NoSQL DB:</strong> Instead of using a SQL DB, consider using a document model NoSQL DB like Mongo DB. So instead of a relational DB with the tables like <code>Books</code> and <code>Authors</code> and an <code>AuthorBooks</code> table that are all joined on a pair of foreign keys - you can use a document NoSQL DB like Mongo where your books are stored as document type, in a BSON format that looks almost identical to the JSON in your question. A nice feature of document DBs like Mongo is that you can set up an index on JSON inside the <code>Book</code> document on <code>Author</code>, thus improving your query time. This is discussed quite well in <a href=""https://rads.stackoverflow.com/amzn/click/0321826620"" rel=""noreferrer"">NoSQL Distilled</a> by Martin Fowler (Sec. 2.2 and Chp. 9).</li>&#xA;<li><strong>Break the foreign key relationship so that referential integrity is maintained by the service instead of the DB:</strong> Instead of relying on the relational DB to enforce referential integrity for you (via the maintenance of foreign keys) restrict access of the DB to just your microservice and maintain the integrity of foreign keys by your service itself. This tactic is discussed by Sam Newman in <a href=""https://rads.stackoverflow.com/amzn/click/1491950358"" rel=""noreferrer"">Building Microservices</a> on pp. 84-85.</li>&#xA;<li><strong>Denormalize the DB:</strong> Instead of having two tables each for books and authors, just combine them into a <a href=""https://en.wikipedia.org/wiki/Denormalization"" rel=""noreferrer"">denormalized table</a>. So, create a new table where the info for your books is duplicated and author info is unique to each book per row. It's ugly. Searches now have a larger search space, but it's simple too. For example, something like the following format:</li>&#xA;</ol>&#xA;&#xA;<pre><code>&#xA;    book_id | book_name              | book_author&#xA;    =====================================================&#xA;          1 | NoSQL Distilled        | Pramod J. Sadalage&#xA;    -----------------------------------------------------&#xA;          1 | NoSQL Distilled        | Martin Fowler&#xA;    -----------------------------------------------------&#xA;          2 | Building Microservices | Sam Newman&#xA;</code></pre>&#xA;"
38633732,38633023,397677,2016-07-28T10:28:00,"<p>This is a very opinion based question but I believe that the biggest disadvantage that you might encounter is using it or Spring at all without understanding what value it gave to you or your project. It might be completely not aligned with your requirements and it is possible that you will configure everything by yourself at some point.</p>&#xA;&#xA;<p>Let me comment bullet points that you liked in the <a href=""https://stackoverflow.com/questions/28831479/advantage-of-spring-boot"">question</a>.</p>&#xA;&#xA;<ul>&#xA;<li>Create stand-alone Spring applications => You can create standalone java application? Why Spring at the first place?</li>&#xA;<li>Embed Tomcat, Jetty or Undertow directly (no need to deploy WAR files) => What if you have to build war anyway? Little config will do the trick but it's not any major advantage.</li>&#xA;<li>Provide opinionated 'starter' POMs to simplify your Maven configuration => What if you have many legacy spring modules? What if you need to patch it up?</li>&#xA;<li>Automatically configure Spring whenever possible => What if this configuration is completely  not aligned with your requirements?</li>&#xA;<li>Provide production-ready features such as metrics, health checks, and externalized configuration => Is it sufficient for you? Is it completely wrong, as you need something more sophisticated? Useless, as you don't need this at all?</li>&#xA;</ul>&#xA;"
50263768,50261052,2168941,2018-05-10T00:17:22,"<blockquote>&#xA;  <p>My question is: If my server is constantly running this orderbook&#xA;  update function, will it block all other server functionality? Will&#xA;  users ever be able to interact with my server?</p>&#xA;</blockquote>&#xA;&#xA;<p>If you are writing asynchronously, these actions will go into your eventloop and your node server would pick next event from eventloop while these actions are being performed. If you have too many events like this, your event queue would be long and user would face really slow response or may even get a timeout</p>&#xA;&#xA;<blockquote>&#xA;  <p>Do I need to create a separate service to perform the updating, or can&#xA;  Node somehow prioritize API requests and pause the caching function?</p>&#xA;</blockquote>&#xA;&#xA;<p>Node only consumes event from the event queue. There are no priorities. </p>&#xA;&#xA;<p>From the design perspective, you should look for options which can reduce this write load like bulkCreate/edit or if you are using redis for cache, consider <a href=""https://redis.io/topics/pipelining"" rel=""nofollow noreferrer"">redis pipeline</a></p>&#xA;"
45391151,45390260,2300013,2017-07-29T15:49:31,"<p>Few considerations before giving you my two cents... </p>&#xA;&#xA;<ul>&#xA;<li><p>There is no such thing as microservices in Node.js. Microservices is an architectural style based (among the others) on the principle that you should use the right language for the job. So Node.js is one possible choices among many. </p></li>&#xA;<li><p>Your question is too broad, alright, maybe you should change it in order to narrow down its scope;</p></li>&#xA;<li><p>I don't think there is a universal or right answer to this question so I can tell you where I would start if I were you. </p></li>&#xA;</ul>&#xA;&#xA;<p>Please consider these resources to start:</p>&#xA;&#xA;<ul>&#xA;<li><p><a href=""https://martinfowler.com/articles/microservices.html"" rel=""nofollow noreferrer"">Microservice definition</a> from the bunch of guys that made it up;</p></li>&#xA;<li><p><a href=""http://rads.stackoverflow.com/amzn/click/1491956259"" rel=""nofollow noreferrer"">microservices architecture</a> is a book introducing microservices. It has an interesting example explaining how to create a shipping company microservices architecture (similar to what you're asking for) </p></li>&#xA;<li><p><a href=""https://leanpub.com/the-devops-2-toolkit"" rel=""nofollow noreferrer"">DevOps 2.0</a> explains how to build and deploy microservices and related pipeline. </p></li>&#xA;</ul>&#xA;"
45416670,45413309,2300013,2017-07-31T13:00:50,"<p>The book <a href=""http://rads.stackoverflow.com/amzn/click/1484212258"" rel=""nofollow noreferrer"">Spring Boot Messaging: Messaging APIs for Enterprise and Integration Solutions</a> chapter 7 (Using WebSockets with Spring) explains how to and provides good examples. If you're looking for code examples for a particular scenario within the answer in stackoverflow, then please narrow down the scope of the question as it is too broad.</p>&#xA;"
45509098,45484451,2300013,2017-08-04T14:15:54,"<p>Yes it is. You're looking for ""Spring Cloud Config"" server: </p>&#xA;&#xA;<blockquote>&#xA;  <p>Spring Cloud’s config server acts as a single source of configuration&#xA;  data for all other services in a microservice-based application. It is&#xA;  itself a microservice whose job is to obtain configuration data from a&#xA;  Git repository and serve it via RESTful endpoints. All other services&#xA;  can either consume the config server’s API directly or, if they’re&#xA;  Spring applications, treat the configuration server as another&#xA;  property source in Spring’s Environment abstraction.</p>&#xA;</blockquote>&#xA;&#xA;<p>You can find a nice reading along with examples <a href=""https://nofluffjuststuff.com/magazine/2016/06/cloud_native_spring_configuring_microservices"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;"
45327732,45326767,2300013,2017-07-26T12:58:58,"<p>The book <a href=""https://www.amazon.co.uk/Microservice-Architecture-Aligning-Principles-Practices/dp/1491956259/ref=sr_1_1?ie=UTF8&amp;qid=1501072840&amp;sr=8-1&amp;keywords=microservice%20architecture"" rel=""nofollow noreferrer"">Microservice Architecture</a> contains good examples of event sourcing (Chapter 5) as it models a parcel-delivery company. &#xA;The idea behind event sourcing is that:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Instead of storing structures that model the state of our world, we&#xA;  can store events that lead to the current state of our world</p>&#xA;</blockquote>&#xA;&#xA;<p>That means that instead of sharing data among microservices, you store the state of an event that is the result of another async microservice call. Such state will be made available to other microservices. Please check the book for complete example and description. </p>&#xA;"
45330497,45325062,2300013,2017-07-26T14:51:50,"<p>Yes, definitely. Contract testing is particularly useful when you rely on an 'external' service, where by external I mean any service that is not under your direct control, including the case you mentioned. <a href=""https://martinfowler.com/bliki/IntegrationContractTest.html"" rel=""nofollow noreferrer"">Here</a> is an interesting article from Martin Fowler.</p>&#xA;"
47309766,47296131,2300013,2017-11-15T14:17:49,"<p>As usual when we have this kind of issues we should look into existing best practice. In this case, <a href=""http://microservices.io/patterns/data/saga.html"" rel=""nofollow noreferrer"">SAGAs</a> design pattern is the answer to your question.</p>&#xA;"
47309683,47304927,2300013,2017-11-15T14:14:25,"<p>You're correct thinking that microservices should ideally have their own data structure so they can be deployed independently. However there are several design patterns that help you, and that doesn't necessarily translates in ""No FK"". Please refer to:  </p>&#xA;&#xA;<ul>&#xA;<li><a href=""http://microservices.io/patterns/data/database-per-service.html"" rel=""nofollow noreferrer"">Database per service</a></li>&#xA;<li><a href=""http://microservices.io/patterns/data/saga.html"" rel=""nofollow noreferrer"">Sagas</a></li>&#xA;<li><a href=""http://microservices.io/patterns/data/api-composition.html"" rel=""nofollow noreferrer"">API Composition</a></li>&#xA;<li><a href=""http://microservices.io/patterns/data/cqrs.html"" rel=""nofollow noreferrer"">CQRS</a></li>&#xA;</ul>&#xA;&#xA;<p>The patterns listed above answer both your questions.</p>&#xA;"
37531851,37413626,287085,2016-05-30T18:35:07,"<p>You could try with storing current version in persistent storage. On service startup, detect current code package version from the service context and compare that with the stored version.&#xA;If it doesn't match, take necessary steps for data migration and then update the current version... rinse and repeat. </p>&#xA;&#xA;<p>I'm not aware of any'native'way to make this work... The service context had a <code>CodePackageModified</code>event... But I'm not quite sure what that's supposed to do(or when it's triggered</p>&#xA;"
44554834,44554638,7959661,2017-06-14T21:14:00,<p>The client microservice team(s) should be writing test suites for the contract and have them runnable in the build pipeline for the server microservice.  This can be automated as part of your CI-CD.   Thus the server microservice team will know the impact of their changes on other client teams right away.</p>&#xA;
44590273,44579396,7959661,2017-06-16T13:18:27,"<p>Depends on your use case and really has nothing to do with ""decoupling"".  If your call from the producing service must wait for a response from a consuming service, then synchronous messaging should be used (RPC).  Asynchronous forms of communication like pub/sub are used when the producing service sends a message intended to be consumed by another service and carries on its marry way without waiting for, or even needing a response.  </p>&#xA;&#xA;<p>Asynchronous forms of communication are preferable in the microservices world when you can get away with it because blocked threads and waiting for responses is expensive.  Good architecture will help decouple run time dependencies between services, so that each operate independently as much as possible without directly relying on other services to do the job it's been assigned.</p>&#xA;"
46941975,46936582,7959661,2017-10-25T21:00:31,"<p>In terms of availability, if a monolithic application breaks in production due to one little thing that effects all clustered nodes, all functionality associated with it goes down.  This could be an critical business function that can literally prevent an organization from operating, resulting in loss of income.</p>&#xA;&#xA;<p>On the other hand, if that ""little thing"" happens to break every instance of an event-driven microservice with asynchronous endpoints, it can go down and affect only one aspect of functionality which is temporarily unavailable.  The rest continues happily running along.  When the faulty service is fixed, it ""catches up"" with its processing and everything runs normally again.</p>&#xA;&#xA;<p>Now take this with a grain of salt. Depending on your business case and the criticality of the specific microservice that goes down, the above advantage might not be realized. But the architecture certainly stacks more odds in your favor in that regard. </p>&#xA;"
45379794,45371705,7959661,2017-07-28T18:06:25,"<p>The granularity looks fine to me (segregation-wise).  I can't see putting location management and price quotes in the same service. Just because tables point to others with foreign keys doesn't mean they should be managed by the same service.  So this looks okay to me as far as decomposition goes.</p>&#xA;&#xA;<p>I'm not sure what you mean about ""resolve"" and have no details about the problem domain, however would you not use the price quote service having known the location and/or variant already?  Remember also that we're past the ""great normalization trend"".  It's okay to replicate some little data elements for convenience sake when the trade-off is isolation vs service-to-service run time dependencies. Space is cheap.  For example, when the price quote does its initial insert, have it also add the ""name"" field as well as the ""id"" of CarData should you really need to. If the PriceQuote service doesn't have the name on hand during the insert, instead make it event based and update it asynchronously.</p>&#xA;"
51044768,51040814,7959661,2018-06-26T13:59:56,"<p>When it comes to model classes, I say duplicate code. The entire idea of using JSON or some other language independent protocol instead of serialized native Java objects (for example) is to avoid side-effects of class changes in one microservice rippling throughout your entire ecosystem. A shared library of model classes locks multiple microservices into a polygamous marriage contract where divorce precipitated by even small disagreements between a pair of members can be very expensive. Service A may require changes to its model that not only are unneeded by Service B, but might be totally incompatible with it.</p>&#xA;&#xA;<p>This doesn't mean that code cannot be shared across different microservices. There's nothing wrong with sharing libraries among different microservices as long as their functionality is limited to addressing cross-cutting concerns. Perhaps your common timeout and retry functionality you mentioned falls in the category. Model classes that do nothing but hold a data representation of a language independent protocol do not fall into this category in my opinion.</p>&#xA;"
51433928,51428155,7959661,2018-07-20T01:41:38,"<p>Even if you were using a relational database, there are approaches to achieve persisted data encapsulation other than provisioning separate databases instances for every service. Having a separate schema for each is one alternative that can still keep your architecture ""nice and shiny"". So are privately owned tables.  As long as you follow the intent to keep things separated without any cross-domain dependencies or constraints while putting up some barrier to prevent other microservice teams from cheating, you're still on the shiny side.</p>&#xA;&#xA;<p>That all goes for Elasticsearch too. I'd take advantage of its multitenancy capabilities and just use indexes to split ownership. That should be clean and easy considering you only have two users (microservices). You could consider adding the Elasticsearch Shield plugin for role-based index ownership to prevent cheating by other teams for a really shiny solution.  With Elasticsearch this gets really flexible because you can tailor the indexes to the load and data volume for each microservice. </p>&#xA;&#xA;<p>Since you can get the degree of encapsulation required plus you have service-specific configuration (via the indexes), in my opinion your second option is OK since it still conforms to the intent of the architecture. </p>&#xA;"
51443150,51433860,7959661,2018-07-20T13:01:33,"<p>There are actually three states: OPEN, CLOSED, and HALF_OPEN. Once the circuit breaker is OPEN and a certain amount of time has passed it lets a single request sneak through. This is the HALF_OPEN state. If successful the circuit breaker is closed, otherwise it returns to the CLOSED state until that amount of time has passed again, where it enters the HALF_OPEN state once again. You can specify the amount of time between the transition to OPEN to HALF_OPEN using the circuitBreaker.sleepWindowInMilliseconds property.</p>&#xA;"
44975676,44949367,7959661,2017-07-07T16:27:29,"<p>While not technically an ""answer"", I can definitely share some of my observations and experiences. Your question concerning services calling other services for database operations reminded me of a project where an architect sold senior management on the idea of ""decoupling"" persistence from the rest of the applications by implementing hundreds of REST interfaces in what essentially was a distributed DAO pattern in front of a very large enterprise database.  The project ended up exactly the way I predicted - a dismal failure.</p>&#xA;&#xA;<p>Microservices aren't about turning a monolithic application into a distributed monolithic application.  In my example project above, the monolith was turned into a stove-piped, fragile, chaotic mess, with the coupling only moved to service contracts instead of Java class method signatures, and with a performance hit so bad the application was unusable. Last I heard they are still running their original monolith.</p>&#xA;&#xA;<p>Microservices should be more of a vertical partitioning of your application and not a horizontal one.  In my opinion it's better to think in terms of business function partitioning rather than ""converting"" an existing monolith. There's no rule that determines how big a microservice must be, but it should be big enough to do one complete synchronous function without needing to directly depend on outside services (as much as possible) to complete its work. If a microservice performs a complex business function that affects 50 tables, so be it!  It owns those many tables.  Ideally if a service goes down, it should affect only that business functionality it's responsible for, and not directly affect other services.  As you can see, this thinking is the complete opposite from that which produced the distributed mess in my project example.</p>&#xA;&#xA;<p>Not only do you need to ensure that the motivation behind replacing monoliths with microservices is sound, but also you need to step outside the monolith and revisit the actual business and begin partitioning that instead.  Like everything else, baby steps are the way to go.  Start with one small complete business function, and convert that into a single microservice instead of trying to replace a monolith all at once.</p>&#xA;"
46346096,46334096,7959661,2017-09-21T14:05:08,"<p>Don't do either.  You'll drive sysops crazy if they see the health of 30 clustered microservices that are working fine turn red just because the health of some dependency (in this case a database) turns red.  There are database health monitoring tools you know. Sysops needs to put their attention on what's down, not on what's running great. Ideally a microservice shouldn't even care about the health of a dependency. Have the transaction persisted in a queue, and have the service try again (and again...) later if you can.</p>&#xA;&#xA;<p>In any case, if you have proper aggregated logging set up with real time analytics (which you should!), then sysops will see real time connectivity issues between microservices and databases (and whatever else) anyway.  Plus ideally microservices should be more isolated from a data persistence perspective.  100 microservices directly fighting over 6 databases?</p>&#xA;"
46472849,46453981,7959661,2017-09-28T15:22:09,"<p>Since it's externalized, the Authorization service should be as ""dumb"" as possible.  Sometimes ""authorization"" based on business logic and data can become very complex.  I think business logic belongs in the service responsible for managing it.  Also the API gateway will likely need to provide the client with ownership status anyway (from the service that manages these blog posts?) so that clients can know what to expose.  So keep the authorization simple, and encapsulate more complex business checks to see what can be done within the service itself.</p>&#xA;&#xA;<p>An alternative is to beef up the authorization service to take another parameter, in this case ownership status.  The API gateway or other service checking authorization (Blog Post Manager?) can first get the ownership status from the service knowing about the business of ownership, then use the authorization service providing the role and ownership status.  Permission rules would be (optionally) based on both role and a true/false indicator. The authorization service doesn't have knowledge of what true/false means, just that Permission ""Edit Post"" is granted for Role ""Reader"" + indicator=true, and for Role ""Administrator"" + indicator=false <strong>or</strong> Role ""Administrator"" + indicator=true, etc.</p>&#xA;"
51266797,51242037,7959661,2018-07-10T13:39:19,"<p>A @ManyToMany in conceptual terms is just two @OneToMany's - one from each of the two domain's perspectives:  </p>&#xA;&#xA;<ol>&#xA;<li>Users can have many products</li>&#xA;<li>Products can have many users.</li>&#xA;</ol>&#xA;&#xA;<p>To maintain separation between our domains, we'll need 4 tables instead of the usual 3 in a monolithic, 2 in each domain:</p>&#xA;&#xA;<pre><code>user (user_id, name, ...)&#xA;user_product (user_id, product_d)&#xA;product (product_id, name, ...)&#xA;product_user (product_id, user_id)&#xA;</code></pre>&#xA;&#xA;<p>Say we're in the user domain and we wish to add a couple of existing products to them that are already identified by product_id. Your user service simply adds the records to the user_products table, and for each record publishes the event (including the product_id and user_id) to a topic or queue or whatever asynchronous method is available. A listener in the product service picks up these events, and from each inserts records into the product_user table. You can publish remove events for when you want to remove a product(s) from a user too of course.</p>&#xA;&#xA;<p>You do exactly the same thing for the other direction but from the user perspective.  Now you can get all the product objects assigned to any user_id, and all the user objects assigned to any product_id.</p>&#xA;&#xA;<p>Physically speaking, you're duplicating information in regards to your many-to-many join table by putting it in two domains.  It's a bit more work because you need to keep things synchronized using event publishing, but unfortunately you don't get isolation for free. </p>&#xA;"
45683169,45682321,7959661,2017-08-14T21:16:16,"<p>For your first question, the answer is you shouldn't ever have to ""copy/paste"" anything, and elimination of ""hard"" synchronous dependencies between your services has nothing to do with it.  It's a strong sign that the functional boundaries of your monolithic decomposition are faulty.  My question to you now would be why you are doing validation on the same data with three separate services when a single microservice is supposed to own and manage its own data?  Also, it's common to have some near cross-cutting utility-type concerns that more than one microservice may share, and a common library (JAR in the case of Java for example) does not violate the intent of the architecture. No copy/past needed.</p>&#xA;&#xA;<p>Now here is why ""strong"" synchronous dependencies are generally a bad idea.  The intent of microservice architecture is to quickly deploy new and improved ""single/do-one-thing) features into production with a minimum of risk and without adversely impacting performance. It enables evolutionary improvement, and accelerates time-to-market of desired features. If your new service misbehaves, it only affects the functionality it is responsible for - nothing else. Now imagine if you decompose your monolithic application by ""code-module"" rather than by feature/function. You only end up with a distributed monolith! Not only do you loose all the advantages and intent of the architecture, but performance and complexity would also be adversely impacted to perhaps a fatal degree.</p>&#xA;"
45337551,45334075,7959661,2017-07-26T21:14:13,"<p>I wouldn't use import-idl for any of them apart from the original master back-end API application.  You don't want a full back-end stack for each BFF, otherwise you'll have to maintain several applications much of what do the same thing and plus you'll need to synchronize your data between these data sources into some sort of ""master"". If you repoint everything to a single database and share all entities between BFF components, then it doesn't fit the microservice model.</p>&#xA;&#xA;<p>The BFF pattern is supposed to be a thin facade in front of an existing service API that filters and perhaps calls multiple service APIs when necessary to aggregate stuff to suit each client type. I see this pattern more of a convenience band-aid when you have no control over the existing API, or a (temporary) step in incremental service decomposition. Ideally microservices should not have such synchronous dependencies, and I'm not a huge fan of horizontal decomposition.  </p>&#xA;&#xA;<p>In my opinion there are better ways of implementing ""BFF"" functionality if developing from scratch without the complicated architecture and added latency of adding yet another layer of indirection.  Microservice architecture is often compared to UNIX commands.  The same UNIX command is capable of supplying more detailed information when desired to suit different needs.  Compare the output of <code>ls</code> with <code>ls -l</code> for example.  Such a strategy can be applied to single microservice endpoints as well.  </p>&#xA;"
49821284,49817769,9599341,2018-04-13T16:15:45,"<p>You've got a couple of patterns going on here.  The use of an API Gateway to abstract the underlying microservices is best described by as an <a href=""https://apigee.com/about/blog/technology/api-facade-overview"" rel=""nofollow noreferrer"">API Facade</a>.  The dynamic capabilities of discovering new ""derived"" services (i.e. your Dogs, Cats, and Rabbits services) seems to fit with a <a href=""http://microservices.io/patterns/server-side-discovery.html"" rel=""nofollow noreferrer"">Service Discovery Pattern</a>.  </p>&#xA;&#xA;<p>The Service Discovery Pattern makes use of a Service Registry to make it known what services are available (which would enable your ""Animals"" service to get a complete list of animals)</p>&#xA;&#xA;<p>In terms of how the ""Animals"" microservice should communicate to the derived services, really depends on the needs of the system.  There are benefits and tradeoffs of using either.  For simplicity, I would recommend HTTP will be the most straight forward and you have the choice to process requests synchronously or asynchronously.  Using a message bus could be more complicated to process requests synchronously.  If you expect a lot of volume in requests then a message bus could benefit, but beware of the complexity involved.</p>&#xA;"
32535464,32534401,1869933,2015-09-12T05:23:47,"<p>It's hard to choose an architecture style based on the information you provided in your question, as you don't explain the kinds of business requirements or constraints you have. Also you didn't explain what will be the use of those services.</p>&#xA;&#xA;<p>I have been working with SOAP and REST for a while, I can tell you that both have adv/dis. But, instead of enumerating over those, I will try to give my reasons and scenarios in which I will choose one over the other.</p>&#xA;&#xA;<p>REST</p>&#xA;&#xA;<ol>&#xA;<li>If services will be consumed by a web browser internet application, mobile application (when processing power is 'low') I will choose REST, as the communication is being made in json format which is more friendly with javascript technologies and the processing and parsing time is less than XML.</li>&#xA;<li>When services will expose the data as part of a simple API giving access to the user to do CRUD operations, or when not much business logic is involved in the processes. I will choose REST too if I have to build a public API.</li>&#xA;<li>If time is in play, implementation of REST is simpler than SOAP. As there are no standards you don't need to have contracts. There are some best practices, but at the end you could use HTTP verbs as you want and create URI style in the way you prefer, but having this much flexibility will make you at some point start doing a kind of RPC API instead of a resource API (remeber REST is more about resources).</li>&#xA;<li>Take advantage of cache provide by http GET verb in browsers, if the purpose of your service is expose information on web page app.</li>&#xA;<li>From a developer's perspective, which is always important, it is faster to build these kind of services. You just need to know a framework that supports REST, such as JAX-RS, Spring REST, Jersey, and some notions of JSON. The learning curve is also smaller.</li>&#xA;<li>It is a lightweight form of transport information, and it is faster as it doesn't require extensive processing.</li>&#xA;</ol>&#xA;&#xA;<p>SOAP</p>&#xA;&#xA;<ol>&#xA;<li>Have contracts that defines the communication within the services, you will be tied to this contract and your clients must follow it too. Any change in this contract will impact all your clients. You must always document what contracts you must use for your operations.</li>&#xA;<li>Have interesting standards that can work in the system integration as WS-Interoperability, WS-addressing, WS-security, so basically is a technology that have several standards which makes easier the phase of agreement, not always the implementation.</li>&#xA;<li>Can be used with different transport layer smtp, http, if you want to have different kind of clients.</li>&#xA;<li>Good standard to transport binary data MTOM/XOP or SwA. If prefer this one to send the bytes in the body of a PUT request.</li>&#xA;<li>Better definition of security and integrity, a lot of options comming from the standards, normally the more used in REST is OAuth.</li>&#xA;<li>From the developer perspective this will take more time to learn and also you need to have at least very basic knowledge of XSD, XPath, WSDL and other concepts. In some situations following the standards is not always easy. You have good tools and frameworks to build them JAX/WS, Spring-ws, CXF.</li>&#xA;</ol>&#xA;&#xA;<p>Answering your question and thinking in a small/medium company trying to standardize the IT infrastructure, I would choose SOAP as it has more mature tool support in all areas. Personally, I like SOAP:fault messages, the RPC style for business operations that SOAP offers. Assuming that your microservices are not related to a specific field, I think have some kind of ESB could help you in managing the entire infrastructure and set your business rules. On occasion, people may think SOAP is over-engineering the solution. But, I go with the idea as it is a standard that has been in the market for a while.</p>&#xA;"
45398455,45398103,1549165,2017-07-30T09:44:17,"<p>AWS <a href=""https://medium.com/@middlechild/aws-serverless-stack-in-a-nutshell-aee963decdb8"" rel=""nofollow noreferrer"">serverless stack</a> will give you the lowest total cost of ownership for a Microservices project.</p>&#xA;&#xA;<p>It mainly involves AWS API Gateway and Lambda where you will pay only for the Opex rather Capex.</p>&#xA;"
45499511,45498517,1549165,2017-08-04T06:20:37,"<p>There are few considerations when you proxy your existing services through API Gateway.</p>&#xA;&#xA;<ul>&#xA;<li>If your backend is not publicly then you need to setup a VPC and a site to site VPN connection from the VPC to your backend Network and use Lambda's to proxy your services.</li>&#xA;<li>If you need do any data transformations or aggregations, you need to use Lambda's(Inside VPC is optional unless VPN connection is needed).</li>&#xA;<li>If you have complex integrations behind the API gateway for your services, you can look into having ESB or Messaging Middleware running in your on-premise or AWS then proxy to API Gateway.</li>&#xA;<li>You can move data model schema validations to API Gateway.</li>&#xA;<li>You can move service authentication to API Gateway by writing a Custom Authorizer Lambda.</li>&#xA;<li>If you happen to move your User pool and identity service to AWS, you can migrate to AWS Cognito Manage Service and use AWS Cognito Authorizer in API Gateway to authenticate.</li>&#xA;</ul>&#xA;"
47452915,47451612,1549165,2017-11-23T10:13:50,"<p>There are several best practices I follow in designing Serverless Microservices</p>&#xA;&#xA;<ul>&#xA;<li>Start with only few Microservices (Less the better up front, unless you know exactly how the service separation should be, delaying the decision to split)</li>&#xA;<li>Separate your business logic that goes to the API, and use the handler as a controller in MVC to invoke the business logic. (This will also helps to unit test logic without depending on Lambda).</li>&#xA;<li>Its not necessary to write only simple CRUD in your API. It depends on your domain and Business Logic required. (But don't build another monolith without separating the code in to different services. Several AWS Service limits will also give you some guide on how much endpoints should be there in a service &amp; etc.)</li>&#xA;<li>Apply the design patterns available for Microservices (e.g If you want to sync data bases between each Microservice, use Pub-Sub pattern using SNS, DynamoDB Streams and Lambda)</li>&#xA;<li>Use the Angular App to put most of the presentation logic.</li>&#xA;<li>Use CloudFront as a proxy and a CDN to avoid CORs.</li>&#xA;</ul>&#xA;&#xA;<p>If you need more information you can refer the following articles I have written on this.</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://codeburst.io/deploying-angular-react-apps-in-aws-ec55a4fb7446"" rel=""nofollow noreferrer"">Deploying Angular/React Apps in AWS</a></li>&#xA;<li><a href=""https://medium.com/99xtechnology/full-stack-serverless-web-apps-with-aws-189d87da024a"" rel=""nofollow noreferrer"">Full Stack Serverless Web Apps with AWS</a></li>&#xA;</ul>&#xA;&#xA;<p>Note: You can use the CloudFormation in <a href=""https://codeburst.io/deploying-angular-react-apps-in-aws-ec55a4fb7446"" rel=""nofollow noreferrer"">Deploying Angular/React Apps in AWS</a> to automate the creation of S3 and CloudFront with best practices.</p>&#xA;"
48700308,48699742,1549165,2018-02-09T06:58:01,"<p>It is important to centralize the authentication, even for a microservices approach for a single product. So I'm assuming you will be looking at having an Identity Service(Authentication Service) which will handle the authentication and issue a token. The other microservices will be acting as the service providers which will validate the token issued.</p>&#xA;&#xA;<p>Note: In standards like OpenID connect, the id_token issued is in the format of JWT which is also stateless and self-contained with singed information about the user. So individual Microservices doesn't have to communicate with the authentication service for each token validation. However, you can look at implementing or using Refresh tokens to renew tokens without requiring users to login again.</p>&#xA;&#xA;<p>Depending on the technology you choose, it will change the nature how you issue the tokens and validate. </p>&#xA;&#xA;<p>e.g:</p>&#xA;&#xA;<ul>&#xA;<li>ExpressJS framework for backend - You can verify the tokens and routes in a Node Middleware Handler using <a href=""http://www.passportjs.org/"" rel=""nofollow noreferrer"">Passport</a>.</li>&#xA;<li>If you use API Gateway in front of your Microservice endpoints you can use a Custom Authorizer Lambda to verify the tokens.</li>&#xA;</ul>&#xA;&#xA;<p>However, it is recommended to use a standard protocol like OpenID connect so that you can be compatible with Identity Federation, SSO behaviors in future.</p>&#xA;&#xA;<p>Since you have mentioned that you are hoping to have your own solution, it will come also with some challenges to address,</p>&#xA;&#xA;<ul>&#xA;<li>Password Policies</li>&#xA;<li>Supporting standards (OpenID Connect)</li>&#xA;<li>Security (Encryption at rest and transit especially for PIDs)</li>&#xA;<li>SSO, MFA &amp; Federation support etc.</li>&#xA;<li>IDS/IPS</li>&#xA;</ul>&#xA;&#xA;<p>In addition to non-functional requirements like scalability, reliability, performance. Although these requirements might not arise in the beginning, I have seen many come down the line, when products get matured, especially for compliance.</p>&#xA;&#xA;<p>That's why most people encourage to use an identity server or service like Cognito, Auth0 &amp; etc to get a better ROI.</p>&#xA;"
45637736,45637090,1549165,2017-08-11T14:32:14,"<p>We have done a successful migration from Ruby &amp; Rails to API Gateway &amp; Lambda based Microservices written in NodeJS. The same architecture you can use if you prefer NodeJS server (Without Microservices) or using Docker Containers Cluster with ECS.</p>&#xA;&#xA;<ul>&#xA;<li>Setup CloudFront as a Proxy which will be getting all the HTTP traffic to your application domain (You can map the DNS to CloudFront CName)</li>&#xA;<li>In CloudFront you can add the current Grails Application as the default origin and behavior which makes your application works as it is same as today.</li>&#xA;<li>Then you can setup your Microservices Architecture with API Gateway and Lambda or NodeJS Web Server or Docker Container Cluster with ECS seperately. (Note that if you use a Relational Database like MySQL, it also requires to do proper placement of new server code in Lambda, WebServer or Containers so that it can access the Database)</li>&#xA;<li>Afterwards, you can write the new feature logic and override one http subpath at a time from CloudFront pointing to the new application.</li>&#xA;</ul>&#xA;&#xA;<p>Following diagram shows the architecture in high-level.&#xA;<a href=""https://i.stack.imgur.com/F0p7X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F0p7X.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>Note: In the diagram, it uses DynamoDB for new Microservices and in the migration phase, you can also connect to the current Database with proper VPC, Subnet and Server placement.</p>&#xA;&#xA;<p>In Addition you gets the benefits from CloudFront CDN in caching static assets to improve application performance and also you will be able to terminate the SSL handhshake in CloudFront with free SSL Certificates issued by Amazon.</p>&#xA;"
49504651,43041563,1549165,2018-03-27T04:57:33,"<p>First of all AWS Cognito consists of two services.</p>&#xA;&#xA;<ul>&#xA;<li>AWS Cognito UserPools (Which is the Identity Provider) - This is the service where you can create the users and manage their credentials with other policies. It can also provide the login screen where we can customize the logo and look and feel so that it can become a plug and play Login service. Then it is also possible to configure the authentication flow (For example to make the service as an OpenIDConnect authentication provider so that it will return a JWT token once user logs in). It is also possible to connect Social Identities (Facebook, Google &amp; etc.) and SAML.</li>&#xA;<li>AWS Cognito Federated Identities (Identity Federation to grant users access AWS Services) - This service is capable of accepting AWS Cognito UserPool Token or direct access from other providers where we can federate the access to AWS resources. For example, AWS Cognito Federated Identities can grant temporal access to a User, Authenticated from another provider (e.g; AWS Cognito UserPools) to upload files to S3.</li>&#xA;</ul>&#xA;&#xA;<p>For more details refer the article <a href=""https://codeburst.io/the-difference-between-aws-cognito-userpools-and-federated-identities-9b47571795d4"" rel=""nofollow noreferrer"">The Difference Between AWS Cognito UserPools and Federated Identities?</a>.</p>&#xA;&#xA;<p>So coming back to your questions,</p>&#xA;&#xA;<blockquote>&#xA;  <ol>&#xA;  <li>So far I am thinking about Cognito as a replacement for my&#xA;  authentication service?</li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p>you can use AWS Cognito UserPools authentication service to issue JWT tokens and validate the token in AWS Lambda Custom Authorizer at your other service endpoints. This is also the place where you can do Authorization.</p>&#xA;&#xA;<blockquote>&#xA;  <ol start=""2"">&#xA;  <li>My static website would implement JavaScript SDK for user&#xA;  login/register. Am I right?</li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p>Not necessarily. If you use AWS Cognito UserPools Hosted UI, you will get Login, Signup, Password Change, Confirmation pages, by default along with auto redirection for Federated Identities (Based on the configurations) such as Facebook, Google or Corporate Credentials like Office365. Although the customization is limited, you should be able to add your own logo and change the background color of these screens. If you plan to implement this by your self, then you can use AWS SDKs to implement these screens.</p>&#xA;&#xA;<p>For more details on the serverless architecture refer <a href=""https://medium.com/99xtechnology/full-stack-serverless-web-apps-with-aws-189d87da024a"" rel=""nofollow noreferrer"">Full Stack Serverless Web Apps with AWS</a>.</p>&#xA;"
48980131,48980066,1549165,2018-02-26T01:03:53,"<p>The approach you are using currently reasonable since its important to keep Lamda functions stateless while storing session information outside the function.</p>&#xA;&#xA;<p>Without provisioning a MongoDB cluster, you can use Amazon DynamoDB to store the session data and retrieve for each request.</p>&#xA;&#xA;<p>Note: If you are planning to do further performance improvements, you can consider using an in-memory database or DAX (If you use DynamoDB).</p>&#xA;"
41302690,41285879,173677,2016-12-23T13:51:51,"<blockquote>&#xA;  <p>I am using CQRS where my Command Handler would consume message off&#xA;  the Queue and do some business logic. In order to call the handler, it&#xA;  will need to make a request to the API method.</p>&#xA;</blockquote>&#xA;&#xA;<p>Are you sure this is real CQRS? CQRS occures when you handle queries and commands differently in your domain logic. Receiving a message via a calss, that's called CommandHandler and just reacting to it is not yet CQRS.</p>&#xA;&#xA;<blockquote>&#xA;  <p>My aim is to create a listener or a startup task where once messages&#xA;  are in the queue it will automatically pick it up from the Queue and&#xA;  continue with command handler but not sure how to do the ""Automatic""&#xA;  way as i describe it. I was thinking to utilise Azure Webjob that will&#xA;  continuously be running and it will act as the Consumer.  Looking for&#xA;  a good architectural way of doing it.</p>&#xA;</blockquote>&#xA;&#xA;<p>The easier you do that, the better. Don't go searching for complex solutions until you tried out all the simple ones. When I was implementing something similar, I was just running a pool of message handler scripts using Linux cron. A handler poped a message off the queue, processed it and terminated. Simple.</p>&#xA;"
33034312,32831192,173677,2015-10-09T09:11:21,"<blockquote>&#xA;  <p>Having chosen a microservice architecture for your project, what&#xA;  strategies would you now employ to implement this feature?</p>&#xA;</blockquote>&#xA;&#xA;<p>Assuming your split of your application into microservices is correct, I would say, that no joins are required.</p>&#xA;&#xA;<p>In your end-user web application you get a list of review entities, satisfying the report conditions by querying <code>Review</code> microservice. No <code>Movie</code> objects are there. Only their GUIDs. Then you iterate the collection you received collecting <code>Movie</code> GUIDs and you ask <code>Movie</code> microservice to give you objects with those GUIDs.</p>&#xA;&#xA;<p>Then you just render the report to the user using two object collections (keyed by id, for example).</p>&#xA;&#xA;<p>Will that work for you?</p>&#xA;"
41163830,41155377,173677,2016-12-15T12:08:36,"<p>Well, if your services are stateless and you would like to take silent crashes into account, why not to query that information from the services, which are currently active, when you need? Each service knows how many active sessions it has. If a service doesn't answer the request, it's dead.</p>&#xA;"
41222113,41194679,173677,2016-12-19T11:58:43,"<blockquote>&#xA;  <p>Is there a better way to do this without creating a separate&#xA;  application to do the DB writes?</p>&#xA;</blockquote>&#xA;&#xA;<p>I think creating or not a new application is a minor question here. It just depends on your architecture. If you develop a microservice, you might want to isolate and remove from it everything that's possible to keep it from making a monolythic again. At least I'd do that. It may seem easier now to keep all in one application, but I will be less so once your codebase and functionality grows considerably.</p>&#xA;&#xA;<p>The question that bothers me even more in this situation is <strong>exclusive DB locking</strong>. Avoid exclusive DB locks for all costs. Forget about everything, drop all your tasks and find a way to compute your statistics without locking database. Believe me, whatever time you spend now developing a solution will be rewarded in the future.</p>&#xA;&#xA;<p>If you choose to ask this question here on Stack, I would be happy to assist, but to suggest something, we will need to know more about your logic and data you store / aggregate.</p>&#xA;"
41061949,41039545,173677,2016-12-09T13:50:07,"<blockquote>&#xA;  <p>First solution I found is to make those components shared and maintain&#xA;  from a single point like node packages and npm install when needed&#xA;  from different groups. But at this point the microservice approach is&#xA;  broken since everybody will be dependent to these components</p>&#xA;</blockquote>&#xA;&#xA;<p>I believe your understanding of a microservice is wrong. Fowler's definition of a microservice:</p>&#xA;&#xA;<blockquote>&#xA;  <p>In short, the microservice architectural style <a href=""http://semver.org/"" rel=""nofollow noreferrer"">1</a> is an approach to&#xA;  developing a single application as a suite of small services, each&#xA;  running in its own process and communicating with lightweight&#xA;  mechanisms, often an HTTP resource API.</p>&#xA;</blockquote>&#xA;&#xA;<p>It's ok for microservices in your company to have common components. Microservices are much higher level of abstraction, than a component. Do not try to make every component a microservice.</p>&#xA;&#xA;<p>You should manage your components in separate repository each. Use <a href=""http://semver.org/"" rel=""nofollow noreferrer"">semantic versioning</a> to publish updates and watch over compatibility breaks.</p>&#xA;&#xA;<blockquote>&#xA;  <p>And very hard to maintain since in the future different groups may&#xA;  different needs from the component.</p>&#xA;</blockquote>&#xA;&#xA;<p>I do not believe this is hard to maintain. The maintainability of your configuration is certainly not higher, than that of any framework around since millions of programmers depend on those frameworks and develop them together. It's a quesion of maintainer's skill.</p>&#xA;&#xA;<p>You can go with a monolithic repository approach (put all the components into a single, but standalone repo), but I think this will mess up things pretty quickly.</p>&#xA;"
51858270,51855075,10008173,2018-08-15T11:59:04,"<p>You can keep doing exactly what you're doing now.  Add the second back-end container to the <code>docker-compose.yml</code> file, and add another <code>location</code> block to the nginx configuration that routes to the specified service.  For the layout you describe, it might look like</p>&#xA;&#xA;<pre><code>location /api/users {&#xA;  proxy_pass http://web/;&#xA;}&#xA;location /api/widgets {&#xA;  proxy_pass http://widgets/;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>When you start uttering the phrase ""production"" you typically wouldn't distribute your code separately from the Docker image.  I would try to remove the <code>volumes:</code> directive that copies your local working tree over what's installed in the image, and move the <code>command:</code> into a <code>CMD</code> in the Dockerfile.  Conversely, you might want to pass in the locations of the PostgreSQL and Redis databases as environment variables, even if they're ""normally"" in the same Docker Compose setup, in case you want to run them somewhere else (run your database on a dedicated or hosted service that gets regularly backed up, for example).</p>&#xA;"
44576070,37461203,6077549,2017-06-15T20:03:31,"<p>Since this gateway pattern is becoming to be very common, the SF team at Microsoft has provided this capability readily available for us. This was based on the recent 2017 MSBuild Event with Mark Fussell and team on stage. I have not really verified/used it yet but I believe this is the documentation provided:</p>&#xA;&#xA;<p><a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-reverseproxy"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-reverseproxy</a></p>&#xA;"
40998352,40960054,6077549,2016-12-06T14:59:14,"<p>Your problem space seems to fit the stateful or stateless model. Either one is fine depending on whether you need to maintain state or not.</p>&#xA;&#xA;<p>As general guidance, consider the actor pattern to model your problem or scenario if:</p>&#xA;&#xA;<ul>&#xA;<li>Your problem space involves a large number (thousands or more) of&#xA;small, independent, and isolated units of state and logic.</li>&#xA;<li>You want to work with single-threaded objects that do not require&#xA;significant interaction from external components, including querying&#xA;state across a set of actors.</li>&#xA;<li>Your actor instances won't block callers with unpredictable delays by&#xA;issuing I/O operations.</li>&#xA;</ul>&#xA;&#xA;<p>Reference: <a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-reliable-actors-introduction"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-reliable-actors-introduction</a></p>&#xA;"
36961391,36957369,6077549,2016-04-30T23:35:26,<p>Can you just pass a reference to that data like a URL to a blob/document storage? Passing 5MB of data within the SF service/actors is quite big and SF is not designed to store that big of data or state.</p>&#xA;
41678100,41661858,6077549,2017-01-16T14:00:39,"<p>One of the key benefits and characteristics of microservices is Decentralized Data Management, and that means each service(i.e. each service fabric app) manages its own database and any other external bounded contexts must go through the service's API to get at it. I think one of the biggest challenges in the microservice architecture is identifying the bounded contexts and how it can mirror the business and team structure/technology/geolocation.</p>&#xA;"
48768764,38492639,6077549,2018-02-13T14:09:57,"<p>You have to do this at the application level. You can use the AOP frameworks for Java(eg Spring AOP or/and AspectJ) to intercept the data in/out of your microservices endpoints or your regular methods. The <code>@Around</code> annotation allows you to intercept the incoming data(method parameters/arguments) and the outgoing data(return values/responses). </p>&#xA;&#xA;<pre><code>@Aspect&#xA;public class AuditAspect {&#xA;&#xA;   @Around(""execution(* com.foobar.controllers.*.*(..))"") &#xA;   public void doAudit(ProceedingJoinPoint joinPoint) throws Throwable {&#xA;&#xA;    System.out.println(joinPoint.getSignature().getName()); //get method name&#xA;    System.out.println(Arrays.toString(joinPoint.getArgs())); //get the arguments/params&#xA;&#xA;    joinPoint.proceed(); //continue &#xA;&#xA;    System.out.println(""intercept response "");&#xA;&#xA;   }&#xA;&#xA;}&#xA;</code></pre>&#xA;"
48748468,47364834,6077549,2018-02-12T14:12:52,"<p>@kentor logging is a cross-cutting concern. If there is a company standard for logging and a module or library is made available for it, then you may use that library. It is shared but this is fine as logging is pretty isolated and should not interfere with your business domain logic or workflow. The general guideline in microservices is to not share code. Things that can be shared are libraries that don't changed very often like US States, color, etc. To answer your question <code>what about the code for the communication between the microservices</code>, I would say not to share this code. Sharing code this code/library to other applications can potentially introduce side-effect bugs.</p>&#xA;"
48788841,48788264,6077549,2018-02-14T13:51:48,"<p>If I understand your statement correctly, the back-end client is aggregating data from your multiple services/functions to build a report. So I assume this is read-only since it's for reporting. Reports itself is a different bounded context or sub-domain. It has different behavior characteristics from read-write/CRUD models. This is what CQRS pattern is about - the notion that you can use a different model to update information than the model you use to read information. </p>&#xA;&#xA;<p>So in your case, it is practical, efficient and optimal to create a different service/function that already combine data before returning it to the consuming clients. These combined data models can be straight from your data layer (via select query or stored procs). </p>&#xA;&#xA;<p>On another note, for me even working with microservices, the main rule of distributing objects still applies which is ""Do not distribute objects. If possible. (Martin Fowler, Enterprise Integration Patterns)"".</p>&#xA;"
36659672,36573857,6077549,2016-04-16T03:05:28,"<p>Since the client app in this case require a response right away that dont depend on other state from actor/service, I think stateless service will be better choice. The state that you depend on is data from the external store like database.</p>&#xA;"
36214004,36157778,6077549,2016-03-25T04:27:33,"<p>In the getting started examples, there is a class ServiceUriBuilder. Initialize this class by passing the service name in the constructor.</p>&#xA;&#xA;<pre><code>var proxyLocation = new ServiceUriBuilder(""MasterDataMService"");&#xA;var masterDataService = ServiceProxy.Create&lt;IMasterDataMService&gt;(proxyLocation.ToUri());&#xA;&#xA;var result = await masterDataService.GetMasterDataByName(interfaceName);&#xA;</code></pre>&#xA;&#xA;<p>Also in your MasterDataMService CreateServiceInstanceListeners method, make sure it looks something like this:</p>&#xA;&#xA;<pre><code>    protected override IEnumerable&lt;ServiceInstanceListener&gt; CreateServiceInstanceListeners()&#xA;    {&#xA;        return new[] { new ServiceInstanceListener(context =&gt; new FabricTransportServiceRemotingListener(context, this)) };&#xA;    }&#xA;</code></pre>&#xA;"
37463072,37461203,6077549,2016-05-26T14:00:33,"<p>By default, service fabric services do not provide a public interface to the web. To expose your application's functionality to HTTP clients, you will need to create a web project to act as an entry point and then communicate from there to your individual services.</p>&#xA;&#xA;<p>You can create a stateless service that serves as your web API gateway. This stateless service can then call your WCF endpoint.</p>&#xA;&#xA;<p><a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-add-a-web-frontend/"" rel=""nofollow"">https://azure.microsoft.com/en-us/documentation/articles/service-fabric-add-a-web-frontend/</a></p>&#xA;"
49449547,31161436,1689965,2018-03-23T12:21:27,"<p>For today (<code>in March 2018</code>) you should use <code>Zookeeper Dependencies</code>.</p>&#xA;&#xA;<ul>&#xA;<li><p>Add dependency <code>org.springframework.cloud:spring-cloud-starter-zookeeper-discovery</code>;</p></li>&#xA;<li><p>Setting up <code>Zookeeper Dependencies</code></p>&#xA;&#xA;<pre><code>spring:&#xA;    # your app name&#xA;    application.name: app&#xA;    cloud.zookeeper:&#xA;        discovery:&#xA;            root: /path/to/your/dependencies&#xA;            enabled: true&#xA;        dependencies:&#xA;            dependencyOne:&#xA;                # absolute path is /path/to/your/dependencies/dependencyOne&#xA;                path: /dependencyOne&#xA;                loadBalancerType: ROUND_ROBIN&#xA;                required: false&#xA;                stubs: org.springframework:foo:stubs&#xA;        dependency.resttemplate.enabled: false&#xA;</code></pre></li>&#xA;</ul>&#xA;&#xA;<p>See more in <a href=""https://cloud.spring.io/spring-cloud-static/spring-cloud.html#spring-cloud-zookeeper-dependencies"" rel=""nofollow noreferrer"">spring-cloud docs</a></p>&#xA;"
48423522,48419410,7131757,2018-01-24T13:21:39,"<p>I'll answer to your question by explaining how Kafka works in general and how it deals with failures.  </p>&#xA;&#xA;<p>Every <em>topic</em>, is a particular stream of data (similar to a table in a database). Topics, are split into <em>partitions</em> (as many as you like) where each message within a partition gets an incremental id, known as offset as shown below. </p>&#xA;&#xA;<p><strong>Partition 0:</strong></p>&#xA;&#xA;<pre><code>+---+---+---+-----+&#xA;| 0 | 1 | 2 | ... |&#xA;+---+---+---+-----+&#xA;</code></pre>&#xA;&#xA;<p><strong>Partition 1:</strong></p>&#xA;&#xA;<pre><code>+---+---+---+---+----+&#xA;| 0 | 1 | 2 | 3 | .. |&#xA;+---+---+---+---+----+&#xA;</code></pre>&#xA;&#xA;<p>Now a Kafka cluster is composed of multiple <em>brokers</em>. Each broker is identified with an ID and can contain certain topic partitions. </p>&#xA;&#xA;<p>Example of 2 topics (each having 3 and 2 partitions respectively): </p>&#xA;&#xA;<p><strong>Broker 1:</strong>        </p>&#xA;&#xA;<pre><code>+-------------------+&#xA;|      Topic 1      |&#xA;|    Partition 0    |&#xA;|                   |&#xA;|                   |&#xA;|     Topic 2       |&#xA;|   Partition 1     |&#xA;+-------------------+&#xA;</code></pre>&#xA;&#xA;<p><strong>Broker 2:</strong>        </p>&#xA;&#xA;<pre><code>+-------------------+&#xA;|      Topic 1      |&#xA;|    Partition 2    |&#xA;|                   |&#xA;|                   |&#xA;|     Topic 2       |&#xA;|   Partition 0     |&#xA;+-------------------+&#xA;</code></pre>&#xA;&#xA;<p><strong>Broker 3:</strong>        </p>&#xA;&#xA;<pre><code>+-------------------+&#xA;|      Topic 1      |&#xA;|    Partition 1    |&#xA;|                   |&#xA;|                   |&#xA;|                   |&#xA;|                   |&#xA;+-------------------+&#xA;</code></pre>&#xA;&#xA;<p>Note that data is distributed (and <em>Broker 3</em> doesn't hold any data of <em>topic 2</em>).</p>&#xA;&#xA;<p>Topics, should have a <code>replication-factor</code> > 1 (usually 2 or 3) so that when a broker is down, another one can serve the data of a topic. For instance, assume that we have a topic with 2 partitions with a <code>replication-factor</code> set to 2 as shown below:</p>&#xA;&#xA;<p><strong>Broker 1:</strong>        </p>&#xA;&#xA;<pre><code>+-------------------+&#xA;|      Topic 1      |&#xA;|    Partition 0    |&#xA;|                   |&#xA;|                   |&#xA;|                   |&#xA;|                   |&#xA;+-------------------+&#xA;</code></pre>&#xA;&#xA;<p><strong>Broker 2:</strong>        </p>&#xA;&#xA;<pre><code>+-------------------+&#xA;|      Topic 1      |&#xA;|    Partition 0    |&#xA;|                   |&#xA;|                   |&#xA;|     Topic 1       |&#xA;|   Partition 0     |&#xA;+-------------------+&#xA;</code></pre>&#xA;&#xA;<p><strong>Broker 3:</strong>        </p>&#xA;&#xA;<pre><code>+-------------------+&#xA;|      Topic 1      |&#xA;|    Partition 1    |&#xA;|                   |&#xA;|                   |&#xA;|                   |&#xA;|                   |&#xA;+-------------------+&#xA;</code></pre>&#xA;&#xA;<p>Now assume that <em>Broker 2</em> has failed. <em>Broker 1</em> and 3 can still serve the data for topic 1. So a <code>replication-factor</code> of 3 is always a good idea since it allows for one broker to be taken down for maintenance purposes and also for another one to be taken down unexpectedly. <strong><em>Therefore, Apache-Kafka offers strong durability and fault tolerance guarantees.</em></strong></p>&#xA;&#xA;<p><strong>Note about Leaders:</strong>&#xA;At any time, only one broker can be a leader of a partition and only that leader can receive and serve data for that partition. The remaining brokers will just synchronize the data (in-sync replicas). Also note that when the <code>replication-factor</code> is set to 1, the <em>leader</em> cannot be moved elsewhere when a broker fails. In general, when all replicas of a partition fail or go offline, the <code>leader</code> will automatically be set to <code>-1</code>.</p>&#xA;"
41152420,41036545,59501,2016-12-14T21:23:20,"<p>One more possibility: use <code>@JsonAnySetter</code> and/or <code>@JsonAnyGetter</code> to create embeded ""dynamic"" properties. One example here:</p>&#xA;&#xA;<p><a href=""http://www.cowtowncoder.com/blog/archives/2011/07/entry_458.html"" rel=""nofollow noreferrer"">http://www.cowtowncoder.com/blog/archives/2011/07/entry_458.html</a></p>&#xA;&#xA;<p>but the basic idea is that it is possible to both bind ""anything else"" from JSON via <code>@JsonAnySetter</code> annotated method (or field in newer versions); as well as to get ""extra properties"" for serialization by similar <code>@JsonAnyGetter</code> method/field with <code>Map</code> value.</p>&#xA;"
30210860,30116581,1439597,2015-05-13T09:33:04,"<p>You can maybe achieve connection through your other service by specifying the service's remote connection to <code>Accounts</code> and <code>Meteor.users</code>:</p>&#xA;&#xA;<pre><code>var RegistrationService = DDP.connect(registration_service_url);&#xA;Accounts.connection = RegistrationService;&#xA;Meteor.users = new Meteor.Collection('users',{connection: RegistrationService});&#xA;</code></pre>&#xA;&#xA;<p>Then would call <code>Meteor.loginWithFacebook</code> and it should use the other app's methods for logging in.</p>&#xA;"
51312914,51312350,2241134,2018-07-12T19:13:25,"<p>Should each integration will be different service that depends upon what type of integration. But yes micro service in general means build multiple small services that are completely independent. </p>&#xA;&#xA;<p>One of the big issue with monolithic service is that its a single point of failure. So to medicate this risk you break your service in multiple small service. So that your integration layer do not fail if couple of service went down. </p>&#xA;&#xA;<p>In the example you describe for payment, you can use hierarchy of micro service. You can dedicate one or two services to connect your external getaway and allow other service to connect thous one or two internal services.  </p>&#xA;&#xA;<p>One thing I highly recommend if you go for micro service think about implementing CI/CD (jenkins/chef), docker and kubernetes. If you do not have any automate you will soon find yourself doing lot more maintains of the services rather than developing. </p>&#xA;&#xA;<p>In the end biggest reason you want micro service is make sure your integration layer always available and highly salable. Without some automation like, docker and kubernetes, you need lot of manual work to maintain the state of your service. </p>&#xA;"
45626743,45626670,1653870,2017-08-11T03:58:17,"<p>Mongo directly won't do that, Bu you can do that by yourself. Send _id as a unique number to mongo while storing as document and it will accept it.</p>&#xA;&#xA;<pre><code>&gt; db.a.insert({_id:111})&#xA;WriteResult({ ""nInserted"" : 1 })&#xA;</code></pre>&#xA;&#xA;<p>In fact you can use auto-incremented _ids as well using <a href=""https://docs.mongodb.com/v3.0/tutorial/create-an-auto-incrementing-field/"" rel=""nofollow noreferrer"">this</a></p>&#xA;"
36576554,36571963,346478,2016-04-12T14:39:34,"<h1>Python application workflow from development to deployment</h1>&#xA;&#xA;<p>It looks like you are in search for developing Python application, using git.</p>&#xA;&#xA;<p>Following description is applicable to any kind of Python based application,&#xA;not only to <code>Pyramid</code> based web ones.</p>&#xA;&#xA;<h2>Requirements</h2>&#xA;&#xA;<p>Situation:</p>&#xA;&#xA;<ul>&#xA;<li>developing Python based solution using Pyramid web framework</li>&#xA;<li>there are multiple python packages, participating in final solution, packages might be dependent.</li>&#xA;<li>some packages come from public pypi, others might be private ones</li>&#xA;<li>source code controlled by git</li>&#xA;</ul>&#xA;&#xA;<p>Expectation:</p>&#xA;&#xA;<ul>&#xA;<li>proposed working style shall allow:&#xA;&#xA;<ul>&#xA;<li>pull requests</li>&#xA;<li>shall work for situations, where packages are dependent</li>&#xA;</ul></li>&#xA;<li>make sure, deployments are repeatable</li>&#xA;</ul>&#xA;&#xA;<h2>Proposed solution</h2>&#xA;&#xA;<p>Concepts:</p>&#xA;&#xA;<ul>&#xA;<li>even the Pyramid application released as versioned package</li>&#xA;<li>for private pypi use <code>devpi-server</code> incl. volatile and release indexes.</li>&#xA;<li>for package creation, use <code>pbr</code></li>&#xA;<li>use <code>tox</code> for package unit testing</li>&#xA;<li>test, before you release new package version</li>&#xA;<li>test, before you deploy</li>&#xA;<li>keep deployment configuration separate form application package</li>&#xA;</ul>&#xA;&#xA;<h3>Pyramid web app as a package</h3>&#xA;&#xA;<p>Pyramid allows creation of applications in form of Python package. In&#xA;fact, whole initial tutorial (containing 21 stages) is using exactly this&#xA;approach.</p>&#xA;&#xA;<p>Despite the fact, you can run the application in develop mode, you do not have&#xA;to do so in production.  Running from released package is easy.</p>&#xA;&#xA;<p>Pyramid uses nice <code>.ini</code> configuration files. Keep <code>development.ini</code> in the&#xA;package repository, as it is integral part for development.</p>&#xA;&#xA;<p>On the other hand, make sure, production <code>.ini</code> files are not present as they&#xA;should not mix with application and belong to deployment stuff.</p>&#xA;&#xA;<p>To make deployment easier, add into your package a command, which prints to&#xA;stdout typical deployment configuration. Name the script e.g. <code>myapp_gen_ini</code>.</p>&#xA;&#xA;<p>Write unittests and configure <code>tox.ini</code> to run them.</p>&#xA;&#xA;<h3>Keep deployment stuff separate from application</h3>&#xA;&#xA;<p>Mixing application code with deployment configurations will make problem at&#xA;the moment, you will have to install to second instance (as you are likely to&#xA;change at least one line of your configuration).</p>&#xA;&#xA;<p>In deployment repository:</p>&#xA;&#xA;<ul>&#xA;<li>keep here <code>requirements.txt</code>, which lists the application package and other&#xA;packages needed for production. Be sure you specify exact package version at&#xA;least for your application package.</li>&#xA;<li>keep here <code>production.ini</code> file. If you have more deployments, use one branch per deployment.</li>&#xA;<li>put here <code>tox.ini</code></li>&#xA;</ul>&#xA;&#xA;<p><code>tox.ini</code> shall have following content:</p>&#xA;&#xA;<pre><code>[tox]&#xA;envlist = py27&#xA;# use py34 or others, if your prefer&#xA;&#xA;[testenv]&#xA;commands = &#xA;deps =&#xA;    -rrequirements.txt&#xA;</code></pre>&#xA;&#xA;<p>Expected use of deployment respository is:</p>&#xA;&#xA;<ol>&#xA;<li>clone it to the server</li>&#xA;<li>run <code>tox</code>, this will create virtualenv <code>.tox/py27</code></li>&#xA;<li>activate the virtualenv by <code>$ source .tox/py27/bin/activate</code></li>&#xA;<li>if <code>production.ini</code> does not exist in the repo yet, run command&#xA;<code>$ myapp_gen_ini &gt; production.ini</code> to generate template for production&#xA;configuration</li>&#xA;<li>edit the <code>production.ini</code> as needed.</li>&#xA;<li>test, it works.</li>&#xA;<li>commit the <code>production.ini</code> changes to the repository</li>&#xA;<li>do other stuff needed to deploy the app (configure web server, supervisord etc.)</li>&#xA;</ol>&#xA;&#xA;<h3>For <code>setup.py</code> use <code>pbr</code> package</h3>&#xA;&#xA;<p>To make package creation simpler, and to keep package versioning related to git&#xA;repository tags, use <code>pbr</code>. You will end up with <code>setup.py</code> being only 3 lines&#xA;long and all relevant stuff will be specified in <code>setup.cfg</code> in form of ini&#xA;file.</p>&#xA;&#xA;<p>Before you build the first time, you have to have some files in git repository,&#xA;otherwise it will complain. As you use git, this shall be no problem.</p>&#xA;&#xA;<p>To assign new package version, set <code>$ git tag -a 0.2.0</code> and build it. This will&#xA;create the package with version <code>0.2.0</code>.</p>&#xA;&#xA;<p>As a bonus, it will create <code>AUTHORS</code> and <code>ChangeLog</code> based on your commit&#xA;messages. Keep these files in <code>.gitignore</code> and use them to create <code>AUTHORS.rst</code>&#xA;and <code>ChangeLog.rst</code> manually (based on autogenerated content).</p>&#xA;&#xA;<p>When you push your commits to another git repository, do not forget to push the tags too.</p>&#xA;&#xA;<h3>Use <code>devpi-server</code> as private pypi</h3>&#xA;&#xA;<p><code>devpi-server</code> is excellent private pypi, which will bring you following advantages:</p>&#xA;&#xA;<ul>&#xA;<li>having private pypi at all</li>&#xA;<li>cached public pypi packages</li>&#xA;<li>faster builds of virtual environments (as it will install from cached packages)</li>&#xA;<li>being able to use <code>pip</code> even without having internet connectivity</li>&#xA;<li>pushing between various types of package indexes: one for development&#xA;(published version can change here), one for deployment (released version will not change here).</li>&#xA;<li>simple unit test run for anyone having access to it, and it will even collect&#xA;the results and make them visible via web page.</li>&#xA;</ul>&#xA;&#xA;<p>For described workflow it will contribute as repository of python packages, which can be deployed.</p>&#xA;&#xA;<p>Command to use will be:</p>&#xA;&#xA;<ul>&#xA;<li><code>$ devpi upload</code> to upload developed package to the server</li>&#xA;<li><code>$ devpi test &lt;package_name&gt;</code> to download, install, run unit test,&#xA;publish test results to devpi-server and clean up temporary installation.</li>&#xA;<li><code>$ devpi push ...</code> to push released package to proper index on <code>devpi-server</code> or even on public pypi.</li>&#xA;</ul>&#xA;&#xA;<p>Note, that all the time it is easy to have <code>pip</code> command configured to consume&#xA;packages from selected index on <code>devpi</code> server for <code>$ pip install &lt;package&gt;</code>.</p>&#xA;&#xA;<p><code>devpi-server</code> is also ready for use in continuous integration testing.</p>&#xA;&#xA;<h3>How <code>git</code> fits into this workflow</h3>&#xA;&#xA;<p>Described workflow is not bound to particular style of using <code>git</code>.</p>&#xA;&#xA;<p>On the other hand, <code>git</code> can play it's role in following situations:</p>&#xA;&#xA;<ul>&#xA;<li><code>commit</code>: commit message will be part of autogenerated <code>ChangeLog</code></li>&#xA;<li><code>tag</code>: defines versions (recognized by <code>setup.py</code> based on <code>pbr</code>).</li>&#xA;</ul>&#xA;&#xA;<p>As <code>git</code> is distributed, having multiple repositories, branches etc.,&#xA;<code>devpi-server</code> allows similar distribution as each user can have it's own&#xA;working index to publish to. Anyway, finally there will be one <code>git</code> repository&#xA;with <code>master</code> branch to use. In <code>devpi-server</code> will be also one agreed&#xA;production index.</p>&#xA;&#xA;<h1>Summary</h1>&#xA;&#xA;<p>Described process is not simple, but the complexity is relevant to complexity of the task.</p>&#xA;&#xA;<p>It is based on tools:</p>&#xA;&#xA;<ul>&#xA;<li><code>tox</code></li>&#xA;<li><code>devpi-server</code></li>&#xA;<li><code>pbr</code> (Python package)</li>&#xA;<li><code>git</code></li>&#xA;</ul>&#xA;&#xA;<p>Proposed solution allows:</p>&#xA;&#xA;<ul>&#xA;<li>managing python packages incl. release management</li>&#xA;<li>unit testing and continuous integration testing</li>&#xA;<li>any style of using <code>git</code></li>&#xA;<li>deployment and development having clearly defined scopes and interactions.</li>&#xA;</ul>&#xA;&#xA;<p>Your question assumes multiple repositories. Proposed solution allows decoupling multiple repositories by means of well managed package versions, published to devpi-server.</p>&#xA;"
38571214,38565470,3174766,2016-07-25T15:01:54,"<p>Yes, you can have multiple versions of your application running at the same time as long as they have unique application names.  See <a href=""https://stackoverflow.com/questions/38104616/keep-application-old-version-running-side-by-side-with-the-newer-version-in-azur"">Keep application old version running side-by-side with the newer version in Azure Service Fabric</a>.</p>&#xA;"
34093690,34077752,3174766,2015-12-04T17:05:57,"<p>For a workaround add the following to your application project (.sfproj) file:</p>&#xA;&#xA;<pre><code>&lt;Target Name=""CopyExtraPackageFiles"" AfterTargets=""Package""&gt;&#xA;    &lt;Copy SourceFiles=""..\AppName.Core\bin\$(Configuration)\AppName.Core.dll""&#xA;          DestinationFolder=""pkg\$(Configuration)\AppName.SF.StatefulService\Code"" /&gt;&#xA;&lt;/Target&gt;&#xA;</code></pre>&#xA;&#xA;<p>This makes some assumptions about the location of your AppName.Core project.  Adjust the path if necessary.</p>&#xA;&#xA;<p>This will manually copy AppName.Core.dll to the appropriate location in the package where it is missing.</p>&#xA;&#xA;<p><strong>EDIT:</strong>&#xA;Or try this for a general purpose workaround instead of the above code snippet.  Let me know if it works.</p>&#xA;&#xA;<pre><code>&lt;Target Name=""EnsureProjectReferencesAreConfigured"" BeforeTargets=""GetCopyToOutputDirectoryItems""&gt;&#xA;  &lt;MSBuild&#xA;      Condition="" '@(ServiceProjectReference)' != '' ""&#xA;      Projects=""@(ServiceProjectReference)""&#xA;      Targets=""AssignProjectConfiguration""&#xA;      Properties=""Configuration=$(Configuration);Platform=$(Platform)"" /&gt;&#xA;&lt;/Target&gt;&#xA;</code></pre>&#xA;"
37524899,37523631,3834478,2016-05-30T11:45:57,"<p>HTTP and JSON or XML are generally used because they're platform and language independent. The HTTP API allows for a ReSTful architecture, which has proven to be a scalable model for developing distributed systems.</p>&#xA;&#xA;<p>Historically, RPC-based approaches to distributed systems have shown a number of weak points:</p>&#xA;&#xA;<ul>&#xA;<li><p>often they're language dependent. Thrift and Protobuf are more interoperable but they're still dependent on fairly specific 3rd party libraries. In comparison, there are many implementations of HTTP clients and XML or JSON data bindings / processors.</p></li>&#xA;<li><p>by tying together the client and server upgrades can become difficult - the client often must be upgraded at the same time as the server. In a truly distributed network this can be impossible.</p></li>&#xA;<li><p>RPC is often not a great metaphor in a distributed system. By abstracting the network to an implementation concern they often encourage low-level 'chatty' interfaces which either involve too much network traffic or are not resilient to unreliable networks.</p></li>&#xA;<li><p>binary transfer formats are more difficult to analyse / debug when something goes wrong.</p></li>&#xA;</ul>&#xA;&#xA;<p>For these kinds of reasons people tend to choose Rest-with-HTTP-based APIs over proprietary RPC APIs.</p>&#xA;"
46669941,46668418,4775534,2017-10-10T14:59:36,"<p>This can be achieved in several ways.</p>&#xA;&#xA;<p>In order to return a result from the current thread (a controller in this case) while still doing some long-running operation, you will need another thread.</p>&#xA;&#xA;<ul>&#xA;<li>Use <code>Executor</code> directly.</li>&#xA;</ul>&#xA;&#xA;<p>A controller:</p>&#xA;&#xA;<pre><code>@Controller&#xA;public class AsyncController {&#xA;&#xA;    private AsyncService asyncService;&#xA;&#xA;    @Autowired&#xA;    public void setAsyncService(AsyncService asyncService) {&#xA;        this.asyncService = asyncService;&#xA;    }&#xA;&#xA;    private ResponseEntity asyncMethod(@RequestBody Object request) {&#xA;        asyncService.process(new MyLongRunningRunnable());&#xA;&#xA;        // returns immediately&#xA;        return ResponseEntity.ok(""ok"");&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>And a service:</p>&#xA;&#xA;<pre><code>@Service&#xA;public class AsyncService {&#xA;    private ExecutorService executorService;&#xA;&#xA;    @PostConstruct&#xA;    private void create() {&#xA;        executorService = Executors.newSingleThreadExecutor();&#xA;    }&#xA;&#xA;    public void process(Runnable operation) {&#xA;        // no result operation&#xA;        executorService.submit(operation);&#xA;    }&#xA;&#xA;&#xA;    @PreDestroy&#xA;    private void destroy() {&#xA;        executorService.shutdown();&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>More details can be found here <a href=""https://docs.oracle.com/javase/tutorial/essential/concurrency/runthread.html"" rel=""nofollow noreferrer"">https://docs.oracle.com/javase/tutorial/essential/concurrency/runthread.html</a></p>&#xA;&#xA;<ul>&#xA;<li>Another way is to use Spring built-in async capabilities</li>&#xA;</ul>&#xA;&#xA;<p>You can simply annotate a method with <code>@Async</code>, having <code>void</code> or <code>Future</code> return type. </p>&#xA;&#xA;<p>If you still want to supply your own executor, you may implement <code>AsyncConfigurer</code> interface in your spring configuration bean.&#xA;This approach also requires <code>@EnableAsync</code> annotation.</p>&#xA;&#xA;<pre><code>@Configuration&#xA;@EnableAsync&#xA;public class AsyncConfiguration implements AsyncConfigurer {&#xA;&#xA;    @Override&#xA;    public Executor getAsyncExecutor() {&#xA;        return Executors.newSingleThreadExecutor();&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>More on this topic <a href=""https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/scheduling/annotation/Async.html"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/scheduling/annotation/Async.html</a></p>&#xA;"
45777570,45776238,4648046,2017-08-20T00:58:31,"<p>The other answer here advocates putting each microservice into its own repository. There may be valid reasons for splitting things up that way, but there may be equally valid reasons from wanting to keep everything in one repository as well (it really depends on your project / circumstances)</p>&#xA;&#xA;<p>If you want all the code in one repository, you can- you just need to follow Go's package rules. (this is a good read: <a href=""https://golang.org/doc/code.html#Workspaces"" rel=""noreferrer"">https://golang.org/doc/code.html#Workspaces</a>)</p>&#xA;&#xA;<p>If you have a mix of commands and libraries, the directory structure you proposed in your question comes close, but you probably don't need the <code>src</code> directories in there. Here's an example of how a directory structure within a repo with libraries and commands might look:</p>&#xA;&#xA;<pre><code>lib1/&#xA;-- some.go&#xA;-- source.go&#xA;lib2/&#xA;-- more.go&#xA;-- source.go&#xA;cmd/&#xA;-- microservice1/&#xA;   -- main.go&#xA;-- microservice2/&#xA;   -- anothermain.go&#xA;</code></pre>&#xA;&#xA;<p>To use this repository, you would clone it inside a Go workspace on your system (see the link I shared above). Assuming your repository lives in github.com/mybiz/project, and your <code>GOPATH</code> was <code>~/go</code>, the workspace would look as follows:</p>&#xA;&#xA;<pre><code>~/go/src/github.com/mybiz/&#xA;  -- project/&#xA;     &lt;clone repo in here&gt;&#xA;</code></pre>&#xA;&#xA;<p>The file <code>cmd/microservice1/main.go</code> would include the library <code>lib1</code> via a path it expects it in relative to <code>$GOPATH/src</code> as follows:</p>&#xA;&#xA;<pre><code>import ""github.com/mybiz/project/lib1""&#xA;</code></pre>&#xA;&#xA;<p>Now, your code has access to the exported symbols in that package using the package name declared in the files under <code>lib1</code>... usually just:</p>&#xA;&#xA;<pre><code>package lib1&#xA;</code></pre>&#xA;&#xA;<p>In <code>cmd/microservice1/main.go</code>, with the import above, you could use <code>lib1</code> symbols as follows:</p>&#xA;&#xA;<pre><code>lib1.CallMe()&#xA;</code></pre>&#xA;&#xA;<p>I hope that helps clear up how Go's directory structure works.</p>&#xA;"
40007817,39982623,325742,2016-10-12T20:31:10,"<p>You could use a combination of groovy and the <a href=""https://docs.oracle.com/javase/7/docs/api/java/awt/Desktop.html"" rel=""nofollow"">java.awt.Desktop</a> class (around since Java 1.6) to open whatever URL you wish:</p>&#xA;&#xA;<pre><code>&lt;build&gt;&#xA;    &lt;plugins&gt;&#xA;        &lt;plugin&gt;&#xA;            &lt;groupId&gt;org.codehaus.gmavenplus&lt;/groupId&gt;&#xA;            &lt;artifactId&gt;gmavenplus-plugin&lt;/artifactId&gt;&#xA;            &lt;version&gt;1.5&lt;/version&gt;&#xA;            &lt;executions&gt;&#xA;                &lt;execution&gt;&#xA;                    &lt;phase&gt;install&lt;/phase&gt;&#xA;                    &lt;goals&gt;&#xA;                        &lt;goal&gt;execute&lt;/goal&gt;&#xA;                    &lt;/goals&gt;&#xA;                &lt;/execution&gt;&#xA;            &lt;/executions&gt;&#xA;            &lt;configuration&gt;&#xA;                &lt;scripts&gt;&#xA;                    &lt;script&gt;&#xA;                    &lt;![CDATA[&#xA;                        import java.awt.Desktop&#xA;                        import java.net.URI&#xA;                        Desktop.getDesktop().browse(new URI(""http://www.example.com""))&#xA;                    ]]&gt;&#xA;                    &lt;/script&gt;&#xA;                &lt;/scripts&gt;&#xA;            &lt;/configuration&gt;&#xA;            &lt;dependencies&gt;&#xA;                &lt;dependency&gt;&#xA;                    &lt;groupId&gt;org.codehaus.groovy&lt;/groupId&gt;&#xA;                    &lt;artifactId&gt;groovy-all&lt;/artifactId&gt;&#xA;                    &lt;!-- any version of Groovy \&gt;= 1.5.0 should work here --&gt;&#xA;                    &lt;version&gt;2.4.7&lt;/version&gt;&#xA;                    &lt;scope&gt;runtime&lt;/scope&gt;&#xA;                &lt;/dependency&gt;&#xA;            &lt;/dependencies&gt;&#xA;        &lt;/plugin&gt;&#xA;    &lt;/plugins&gt;&#xA;&lt;/build&gt;&#xA;</code></pre>&#xA;&#xA;<p>(This was tested on OSX, but since the java Desktop class is being used, should be cross-platform)</p>&#xA;"
42676425,42674145,4473711,2017-03-08T16:15:14,"<p>Answer from spencergibb @github:</p>&#xA;&#xA;<p>This is a documented issue here <a href=""http://cloud.spring.io/spring-cloud-static/Camden.SR5/#_customizing_the_ribbon_client"" rel=""nofollow noreferrer"">http://cloud.spring.io/spring-cloud-static/Camden.SR5/#_customizing_the_ribbon_client</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>The FooConfiguration has to be @Configuration but take care that it&#xA;  is not in a @ComponentScan for the main application context, otherwise&#xA;  it will be shared by all the @RibbonClients. If you use @ComponentScan&#xA;  (or @SpringBootApplication) you need to take steps to avoid it being&#xA;  included (for instance put it in a separate, non-overlapping package,&#xA;  or specify the packages to scan explicitly in the @ComponentScan).</p>&#xA;</blockquote>&#xA;&#xA;<p>In your case RibbonConfigAvailability is picked up by the component scan and shared between all ribbon clients (which is bad). Either remove the @Configuration (it doesn't have to be annotated as such, this is how I tested) or exclude it from component scanning.</p>&#xA;&#xA;<p>The wrong Code:</p>&#xA;&#xA;<pre><code>@Configuration //THIS ANNOTATION IS THE ERROR    &#xA;public class RibbonConfigAvailability {&#xA;&#xA;    @Bean&#xA;    public IRule ribbonRule() {&#xA;        return new AvailabilityFilteringRule();&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;"
52023864,52023726,4554863,2018-08-26T06:50:39,"<p>As always... it depends. If your backend is purely proxying the calls, it is kind of useless. But you could also utilize it for logging, load balancing etc. Javascript apps for example cannot run in parallel and utilize all your cores. So this could be a reason to process requests in the backend as well. There is really no one-fits-all solution.</p>&#xA;"
43752634,40856925,7508847,2017-05-03T06:29:10,"<p>The answer is based on the which kind of data should be shared from 3 B instances. Some occasions:</p>&#xA;&#xA;<ol>&#xA;<li><p>The B is just read data without write anything, the DB can use replicate methodology, and three B instance just read data from different DB instance, and DB was replicated.</p></li>&#xA;<li><p>The B instance can read/write data without interrupt other B instance, that mean every B instance can have designated data, and no data sharing between instances, the database was changed to three databases with same schema but totally different data;</p></li>&#xA;<li><p>The B instances should share the most of data, and every instance can occasion write the data back to the DB. So B instance should use one DB and some DB lock to avoid conflict between the instances. </p></li>&#xA;</ol>&#xA;&#xA;<p>In other some different situation, there will be many other approaches to solve the issue such as using memory DB like redis, queue service like rabbitMQ for B instance. </p>&#xA;"
48503552,42691892,45935,2018-01-29T14:28:11,"<p>Each microservice should have its own set of SQL tables that no other microservice can access.  But having one microservice per SQL table, and having each microservice just support CRUD operations is generally an anti-pattern: it turns a powerful DBMS and query language into a simple record manager: no cross-table transactions, joins, filtering, sorting, pagination, etc.  </p>&#xA;"
49163144,29644916,8042095,2018-03-07T23:48:01,"<p>Short answer : Use Oauth2.0 kind token based authentication, which can be used in any type of applications like a webapp or mobile app. The sequence of steps involved for a web application would be then to </p>&#xA;&#xA;<ol>&#xA;<li>authenticate against ID provider</li>&#xA;<li>keep the access token in cookie</li>&#xA;<li>access the pages in webapp</li>&#xA;<li>call the services</li>&#xA;</ol>&#xA;&#xA;<p>Diagram below depicts the components which would be needed. Such an architecture separating the web and data apis will give a good scalability, resilience and stability</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/zNbPk.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zNbPk.jpg"" alt=""enter image description here""></a></p>&#xA;"
40170924,40170212,50543,2016-10-21T07:37:02,"<p>Well, you aren't setting Content-Length.</p>&#xA;&#xA;<p>(FWIW, your Content-Type is bogus as well).</p>&#xA;"
42295020,42073576,3731823,2017-02-17T10:06:28,"<p>By ""frontend"" do you mean Javascript running on a web browser or a piece of software you've got running within the same datacenter? I'll assume we are not talking about web browser scenario here.</p>&#xA;&#xA;<p>I think client-side discovery with smart caching and round-robin load balancing scales the best as there is no single point of failure and it reacts very fast to any disruptions within the cluster. But it pushes more logic to client side and makes logging more difficult than the trivial access log of Nginx.</p>&#xA;&#xA;<p>2nd option is very standard and well understood, and Nginx and Haproxy were designed for this workload. Note that you should have a few of them available not to have a single point of failure, and upgrading their binaries (especially if you run them on Docker) will cause a short period of downtime. Clients need to discover these load balancers somehow anyway, DNS is the most common option. DNS works well when the situation is quite static and everything is running on default ports so you don't need to tinker too much with TTLs and SRV records.</p>&#xA;&#xA;<p>3rd option makes client logic simpler because the API Gateway can act as a ""view"" to the services you've got internally available. But you still need service discovery for clients to find these so they don't really solve the original problem.</p>&#xA;&#xA;<p>Any feedback is welcome, this is a very broad topic and your mileage may vary.</p>&#xA;&#xA;<p>Update: Also if you are using HTTP protocol you might want to secure it by HTTPS. With a load balancer you have the chance of terminating HTTPS there and have simpler non-encrypted traffic within your VPC or whatever behind a firewall.</p>&#xA;"
46334349,46283367,8456816,2017-09-21T02:13:11,"<p>As you said, it can be simply wrapped under <code>try-catch</code> block then why choose Hystrix or some other library?&#xA;What i experienced:</p>&#xA;&#xA;<ul>&#xA;<li>Already <code>test proven</code> library.</li>&#xA;<li>Ability to <code>skip original intended calls and fallback</code>. Note that if you wrap it under try-catch, there will be still be an attempt to connect and send command which will eventually timeout due to degraded dependency. Knowing this information prior to call will enable to skip the calls for sometime (as per configuration) and you can save those resources</li>&#xA;<li>Provides circuit breaking using <code>Sliding Time Window</code> as well</li>&#xA;<li><code>Metrics and Dashboarding</code> provided Out of the Box which can help you peek into your system and dependent connection </li>&#xA;<li>Implements <code>BulkHead</code> by using different <code>Thread Pools</code></li>&#xA;<li>Lower maintenance cost</li>&#xA;<li>Health check ability. It provides a health check class which plugins with Health monitoring APIs </li>&#xA;</ul>&#xA;"
45672384,39622142,8456816,2017-08-14T10:21:17,"<p>Other than BulkHead and CBs, few other things that can be implemented:</p>&#xA;&#xA;<ul>&#xA;<li>Retry Pattern on Idempotent Ops. This requires the Operation to be retried is Idempotent and will produce the same results on repeated execution.</li>&#xA;<li>Proper Timeout Configurations like Connection, Command Timeouts in case of network dependency</li>&#xA;<li>Bounded Request Queues at Virtual Host/Listener level</li>&#xA;<li>Failover Strategy like Caching</li>&#xA;</ul>&#xA;&#xA;<p>Redundancy, Failover Systems can be incorporated to achieve resiliency against system failures as well</p>&#xA;"
45779733,45779616,8456816,2017-08-20T07:46:59,"<p>Assuming that you came up with Product and location micro services based on some factors. Since you need correlation between these 2 datasets, this has to be done in another micro service which is responsible for this. This new service will interact with these 2 services to get things done</p>&#xA;"
38738076,38737317,4334448,2016-08-03T08:15:10,<p><strong>Solution:</strong> Moved @EnableBinding to main SpringBootApplication class.</p>&#xA;
27439325,26331854,2711152,2014-12-12T08:06:15,"<p>Please have a look at. Please note that this is a hot topic and things are changing daily. Right now the core movement on micro-services is happening mostly in Java world.</p>&#xA;&#xA;<p><a href=""http://blog.pivotal.io/pivotal/case-studies-2/tech-start-up-lessons-micro-services-architecture-with-net-and-rabbitmq"" rel=""nofollow"">http://blog.pivotal.io/pivotal/case-studies-2/tech-start-up-lessons-micro-services-architecture-with-net-and-rabbitmq</a></p>&#xA;&#xA;<p><a href=""http://abdullin.com/post/how-micro-services-approach-worked-out-in-production/"" rel=""nofollow"">http://abdullin.com/post/how-micro-services-approach-worked-out-in-production/</a></p>&#xA;"
49580791,49580394,32880,2018-03-30T20:09:44,"<p>Syscalls are expensive, and writing data over loopback is very fast, so this is essentially just timing how fast you can make 1 million calls to <code>write</code>.</p>&#xA;&#xA;<p>Making lots of small reads and writes is the primary use case for using buffered io. Wrapping the normal <code>net.TCPConn</code> in a <a href=""https://golang.org/pkg/bufio/#NewWriter"" rel=""nofollow noreferrer""><code>bufio.Writer</code></a> will provide the expected performance increase.</p>&#xA;"
51269214,51189616,9210480,2018-07-10T15:32:44,"<p>I finally found the problem. it was bad handling in my event class, I was using <strong>Preconditions.checkNotNull</strong> on my variable which is apparently wrong. I removed this xpression from my code and everything works fine</p>&#xA;"
51775873,49612709,1043824,2018-08-09T21:14:35,"<p>I have worked with the following setup, and it has worked out pretty well:</p>&#xA;&#xA;<h2>Login flow:</h2>&#xA;&#xA;<ol>&#xA;<li>Fresh browser makes a request for a specific resource</li>&#xA;<li>Gateway service detects the absence of the jwt cookie and redirects to login form</li>&#xA;<li>Login form talks to auth-service via gateway. Gateway allows non-jwt calls only to auth-service</li>&#xA;<li>Auth service gives a login-form a newly created jwt cookie and redirects to original URL</li>&#xA;</ol>&#xA;&#xA;<h2>Normal operation:</h2>&#xA;&#xA;<ol>&#xA;<li>Browser makes a request for a resource along with jwt cookie</li>&#xA;<li>Gateway service intercepts the request and forwards jwt to auth-service for validation</li>&#xA;<li>Auth service checks signature then timestamp then blacklist and returns a positive or negative result</li>&#xA;<li>If positive, Gateway service forwards the request to respective backend service, otherwise redirects to login</li>&#xA;<li>Backend service does not do jwt validation - it just trusts the gateway to send only valid requests.</li>&#xA;<li>Backend service does check for roles/permissions/entitlements defined in jwt</li>&#xA;</ol>&#xA;&#xA;<h2>Logout flow:</h2>&#xA;&#xA;<ol>&#xA;<li>Browser makes a request to the auth-service/logout</li>&#xA;<li>Auth service puts the jwt in blacklist and redirects to login form</li>&#xA;</ol>&#xA;&#xA;<p>Now, this is a simple workflow we implemented without <s>any</s> (much) 3rd party  help. At some point we did had to use session cookies but that is for other reasons. Note that the system is almost stateless except the blacklist at auth service. <a href=""https://medium.com/devgorilla/how-to-log-out-when-using-jwt-a8c7823e8a6"" rel=""nofollow noreferrer"">One does not simply log out with jwt!</a> We had a REDIS to manage the blacklists. You can implement logout with session cookies at gateway or auth service.</p>&#xA;&#xA;<p>Most of the backend services expected their own set of roles/privileges/entitlements in the jwt. The roles were granted to user by the auth service and were written in the granted jwt. If a new role was grated to a user, the user had to logoff/logon to reflect that privilege. If some privilege is removed, then the user had to force logged off - that is where REDIS played.</p>&#xA;"
43321125,39467200,5112433,2017-04-10T10:44:56,"<p>First of all I recommend to use <a href=""https://github.com/PlayFab/consuldotnet"" rel=""noreferrer"">Consul.NET</a>&#xA; to interact with Consul. Using it, a service registration may look like:</p>&#xA;&#xA;<pre><code>var registration = new AgentServiceRegistration&#xA;{&#xA;    Name = ""foo"",&#xA;    Port = 4242,&#xA;    Address = ""http://bar""&#xA;};&#xA;&#xA;await client.Agent.ServiceRegister(registration);&#xA;</code></pre>&#xA;&#xA;<p>Now let's integrate this code into ASP.NET Core startup process with help of DI and loose coupling. Read your json file into <code>ConsulOptions</code> instance (DTO without any logic):</p>&#xA;&#xA;<pre><code>public void ConfigureServices(IServiceCollection services)&#xA;{&#xA;    services.AddOptions();&#xA;    services.Configure&lt;ConsulOptions&gt;(Configuration);&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Incapsulate all Consul-related logic in class <code>ConsulService</code> accepting <code>ConsulOptions</code> as dependence:</p>&#xA;&#xA;<pre><code>public class ConsulService : IDisposable&#xA;{&#xA;    public ConsulService(IOptions&lt;ConsulOptions&gt; optAccessor) { }&#xA;&#xA;    public void Register() &#xA;    {&#xA;        //possible implementation of synchronous API&#xA;        client.Agent.ServiceRegister(registration).GetAwaiter().GetResult();&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Add the class itself to DI container:</p>&#xA;&#xA;<pre><code>services.AddTransient&lt;ConsulService&gt;();&#xA;</code></pre>&#xA;&#xA;<p>Then create an extention method of <code>IApplicationBuilder</code> and call it:</p>&#xA;&#xA;<pre><code>public void Configure(IApplicationBuilder app)&#xA;{&#xA;    app.ConsulRegister();&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>In <code>ConsulRegister</code> implementation we add our hooks on application start/stop:</p>&#xA;&#xA;<pre><code>public static class ApplicationBuilderExtensions&#xA;{&#xA;    public static ConsulService Service { get; set; }&#xA;&#xA;    public static IApplicationBuilder ConsulRegister(this IApplicationBuilder app)&#xA;    {&#xA;        //design ConsulService class as long-lived or store ApplicationServices instead&#xA;        Service = app.ApplicationServices.GetService&lt;ConsulService&gt;();&#xA;&#xA;        var life = app.ApplicationServices.GetService&lt;IApplicationLifetime&gt;();&#xA;&#xA;        life.ApplicationStarted.Register(OnStarted);&#xA;        life.ApplicationStopping.Register(OnStopping);&#xA;&#xA;        return app;&#xA;    }&#xA;&#xA;    private static void OnStarted()&#xA;    {&#xA;        Service.Register(); //finally, register the API in Consul&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Locking absence and static fields are OK because of <code>Startup</code> class is executed once on application start. Don't forget to de-register the API in <code>OnStopping</code> method!</p>&#xA;"
48330862,48271493,9113457,2018-01-18T21:47:16,"<p>Eventhough I agree with the answer from @rad Microsoft recently published an ebook about microservices and asp.net core in which they combine microservices and DDD successfully.</p>&#xA;&#xA;<p>See <a href=""https://www.microsoft.com/net/download/thank-you/microservices-architecture-ebook"" rel=""nofollow noreferrer"">https://www.microsoft.com/net/download/thank-you/microservices-architecture-ebook</a> </p>&#xA;"
41041171,40988204,1646449,2016-12-08T13:53:49,<p>Both approaches have advantages and disadvantages. The answer changes according to your server. The client side rendering lightens the server side job while saves some time from clients rendering time. If you have server power then it is best practice to follow but as users increase the load on the server will be increased too. At that point you may decide according to your server.</p>&#xA;&#xA;<p>On both you can make enterprise entegrations.</p>&#xA;&#xA;<p>On both you can handle different client types.&#xA;on csr or ssr you can use different bffs(backend for frontend) to create the required data but only difference you decide is that it will be rendered or not?</p>&#xA;&#xA;<p>The thirt one is one of the disadvantages of ssr.</p>&#xA;
36353333,36318795,870769,2016-04-01T09:58:03,"<p>Do you mean if the Spring Cloud Config Server itself is what the Spring Cloud documentation labels as Sidecar? Then no, as far as I know it is just a plain, regular Spring Boot app.</p>&#xA;&#xA;<p>A Sidecar as referred to in <a href=""http://cloud.spring.io/spring-cloud-static/spring-cloud.html#_polyglot_support_with_sidecar"" rel=""nofollow"">Polyglot support with Sidecar</a> is a Spring Boot application that acts as a bridge between your service infrastructure and a service that is not written in a JVM language. Apps written in Python, Go, Ruby, C#, NodeJS, Erlang or really any other language that can bind <em>something</em> to a port come to mind.</p>&#xA;&#xA;<p>The benefits of the Sidecar are, that your Non-JVM apps </p>&#xA;&#xA;<ul>&#xA;<li><code>service discovery</code> become automatically discoverable through <a href=""http://cloud.spring.io/spring-cloud-static/spring-cloud.html#_service_discovery_eureka_clients"" rel=""nofollow"">Eureka</a>, which means that JVM services can resolve the <code>host:port/&lt;service-id&gt;</code> of the Non-JVM apps as well as the other way around,</li>&#xA;<li><code>monitoring</code> are <em>monitorable</em> through the same health-endpoints-infrastructure that is available in Spring Boot (Actuator), i.e. by manually providing the health endpoint in the Non-JVM app Eureka knows when the Non-JVM service is down</li>&#xA;<li><code>routing/proxying</code> query the services by either manually looking up their hosts/ports or proxying these requests through <a href=""http://cloud.spring.io/spring-cloud-static/spring-cloud.html#_router_and_filter_zuul"" rel=""nofollow"">Zuul</a>, which in turn resolves their current addresses through Eureka</li>&#xA;<li><code>balancing</code> be load balanced by <a href=""http://cloud.spring.io/spring-cloud-static/spring-cloud.html#spring-cloud-ribbon"" rel=""nofollow"">Ribbon</a> and </li>&#xA;<li><code>configuration</code> may consume configuration properties provided via <a href=""http://cloud.spring.io/spring-cloud-static/spring-cloud.html#_spring_cloud_config"" rel=""nofollow"">Spring Cloud Config</a>.</li>&#xA;</ul>&#xA;&#xA;<blockquote>&#xA;  <p>I hope this answer addresses your question, if not (or someone finds it to be inaccurate or misleading) just let me know and I delete it to make room for something more suitable. ;-)</p>&#xA;</blockquote>&#xA;"
45273188,45270983,5655665,2017-07-24T05:28:46,"<p>At first, creating a Swarm cluster compose of different regions is wiered and at some points, it is not even possible as TCP ports of different regions (or cloud vendors) may not allowed to connect with each other via the Internet which is an essential requirement for setting up a Swarm.</p>&#xA;&#xA;<p>As far as sharing data is concerned, you have different options which you can choose from. For example you can use Azure File Storage or AWS EFS\EBS in your Docker containers via volume plugin mounts and storage, eventually, is shared across managers and workers in a cluster (distributed file-systems), you can use a volume orchestrator like Flocker volume plugin or setup your own NFS.</p>&#xA;"
42933607,36705199,171121,2017-03-21T16:57:29,"<p>There are several built-in ways of doing this.  If you only have a single worker, you can set <code>MessageOptions.MaxConcurrentCalls = 1</code>.  </p>&#xA;&#xA;<p>If you have multiple, you can use the <a href=""https://github.com/Azure/azure-webjobs-sdk/wiki/Singleton"" rel=""nofollow noreferrer"">Singleton</a> attribute.  This gives you the option of setting it in Listener mode or Function mode.  The former gives the behavior you're asking for, a serially-processed FIFO queue.  The latter lets you lock more granularly, so you can specifically lock around critical sections, ensuring consistency while allowing greater throughput, but doesn't necessarily preserve <em>order</em>.</p>&#xA;&#xA;<p><em>My Guess</em> is they'd have implemented the singleton attribute similarly to your Redis approach, so performance should be equivalent.  I've done no testing with that though.</p>&#xA;"
48277611,46247154,3197297,2018-01-16T09:10:35,"<p>I found and fixed this issue.</p>&#xA;&#xA;<p>In my <strong>start.sh</strong> script last line I have this below line.</p>&#xA;&#xA;<pre><code>java -cp $CLASS_PATH $JVM_OPTIONS $CLASS_NAME ${ARGUMENTS[@]} &amp; echo $! &gt; $PID_FILE&#xA;</code></pre>&#xA;&#xA;<p>In that line, I did remove this <strong>&amp; echo $! > $PID_FILE</strong>, </p>&#xA;&#xA;<p><strong>Working:</strong></p>&#xA;&#xA;<pre><code>  java -cp $CLASS_PATH $JVM_OPTIONS $CLASS_NAME ${ARGUMENTS[@]} &#xA;</code></pre>&#xA;"
44085030,44084744,6115442,2017-05-20T10:55:40,"<p>You might be interested in using <code>Command Query Responsibility Segregation</code></p>&#xA;&#xA;<p>See this <a href=""http://microservices.io/patterns/data/cqrs.html"" rel=""nofollow noreferrer"">implementing queries that need to retrieve data owned by multiple services</a>.</p>&#xA;&#xA;<p>You can draw much inspirations from <a href=""https://github.com/eventuate-examples/eventuate-examples-java-customers-and-orders"" rel=""nofollow noreferrer"">this example</a> (Customers and Orders example of <a href=""http://eventuate.io/"" rel=""nofollow noreferrer"">http://eventuate.io/</a>).</p>&#xA;"
50017078,35772518,6183282,2018-04-25T08:06:52,"<p>If you are running multiple instances of the config server, you can have them all register themselves in Eureka, and maybe do a lookup to the config server with it's application name via Eureka in all the other microservices. This way, Zuul (and Ribbon) will take care of the load balancing.</p>&#xA;&#xA;<p><strong>Edit:</strong>&#xA;I guess spencergibb is right. It's best to use a load balancer, for eg: ELB, if you're going to deploy on AWS.</p>&#xA;&#xA;<p><a href=""https://github.com/spring-cloud/spring-cloud-config/issues/87"" rel=""nofollow noreferrer"">Consider multiple spring-cloud-config-uris for high availability</a></p>&#xA;"
27846674,27839789,956786,2015-01-08T18:04:12,"<p>Seems to me that the issue is with parallelizing this implementation is you are thinking that each thread will call:</p>&#xA;&#xA;<p><code>&lt;GET provider/items&gt;</code></p>&#xA;&#xA;<p>One solution would be to get all the items first then do the async processing.</p>&#xA;&#xA;<p>My Ruby is non-existent but it might look something like this:</p>&#xA;&#xA;<pre><code>class HardWorker&#xA;    include Sidekiq::Worker&#xA;    def perform(item)&#xA;        process item&#xA;        &lt;DELETE provider/items/#{item.id}&gt;&#xA;    end&#xA;end&#xA;&#xA;items = &lt;GET provider/items&gt;&#xA;&#xA;items.each do |item|&#xA;    HardWorker.perform_async(item)&#xA;end&#xA;</code></pre>&#xA;&#xA;<p>This way your ""producer"" is the loop and the consumer is the async <code>HardWorker</code>.</p>&#xA;"
51384401,51250307,2992754,2018-07-17T14:49:13,"<p>One approach is to use an environment variable in order to set the host machine IP.</p>&#xA;&#xA;<p>Assuming that you're using a *.yml file as your service configuration file where you will be able to set something like this:</p>&#xA;&#xA;<pre><code>eureka:&#xA;  instance:&#xA;    preferIpAddress: true&#xA;    ipAddress: ""${HOST}""&#xA;  client:&#xA;    fetch-registry: true&#xA;    register-with-eureka: true&#xA;    serviceUrl:&#xA;      defaultZone: http://&lt;eureka-host&gt;:&lt;eureka-port&gt;/eureka/&#xA;</code></pre>&#xA;&#xA;<p>So then you can inject the host IP as en environment variable while ruining the docker container with your service:</p>&#xA;&#xA;<pre><code>docker container run -e HOST=host_ip IMAGE_NAME&#xA;</code></pre>&#xA;"
50157492,50154865,2992754,2018-05-03T14:27:11,"<p>According to the <a href=""https://cloud.spring.io/spring-cloud-netflix/multi/multi__service_discovery_eureka_clients.html#_registering_with_eureka"" rel=""nofollow noreferrer"">documentation</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p>When a client registers with Eureka, it provides meta-data about&#xA;  itself — such as host, port, health indicator URL, home page, and&#xA;  other details. Eureka receives heartbeat messages from each instance&#xA;  belonging to a service. If the heartbeat fails over a configurable&#xA;  timetable, the instance is normally removed from the registry.</p>&#xA;</blockquote>&#xA;&#xA;<p><a href=""https://cloud.spring.io/spring-cloud-netflix/multi/multi__service_discovery_eureka_clients.html#_eureka_s_health_checks"" rel=""nofollow noreferrer"">Eureka</a> uses the client heartbeat to determine if a client is up.</p>&#xA;&#xA;<p>There are multiple health indicators such as:</p>&#xA;&#xA;<ul>&#xA;<li>DiskSpaceHealthIndicator </li>&#xA;<li>RefreshScopeHealthIndicator</li>&#xA;<li>HystrixHealthIndicator</li>&#xA;</ul>&#xA;&#xA;<p>HealthCheckHandler maps that status into one of the Eureka-supported statuses. This status will then be propagated to the Eureka server through heartbeats.</p>&#xA;"
47767970,47766808,2992754,2017-12-12T08:14:40,"<p>Try returning a ResponseEntity:</p>&#xA;&#xA;<pre><code>@CrossOrigin(origins = ""http://localhost:4200"")&#xA;@RequestMapping(value = ""/checkAuthentication"", method = RequestMethod.POST)&#xA;public ResponseEntity checkLoginByName(@RequestBody Users user) throws Exception{&#xA;&#xA;  ObjectMapper mapper = new ObjectMapper();&#xA;  Users useObj1 = userRepo.findByUsernameAndPassword(user.username,user.password);&#xA;  return ResponseEntity&lt;&gt;(mapper.writeValueAsString(useObj1), HttpStatus.OK);&#xA;&#xA;}&#xA;</code></pre>&#xA;"
51726797,51726515,636018,2018-08-07T12:26:44,"<p>All three examples would be considered asynchronous as they prompt a response due to cause and effect - call and respond. While all three of these could happen concurrently, each in and of themselves is not synchronous.</p>&#xA;&#xA;<p>Synchronous communication happens simultaneously, like two people editing the same document online. Each editor reads and writes at the same time, but does not interrupt the other in any way.</p>&#xA;&#xA;<p>The best example of synchronous communication is a telephone conversation. All connected parties <strong>can hear (receive) &amp; speak (transmit) at the same time</strong>, and although humans have difficulting performing both actions simultaneously, the telephone connection itself has no trouble providing both concurrently.</p>&#xA;&#xA;<p>Asynchronous acts like a two-way radio. You must stop transmitting in order to receive.</p>&#xA;"
33578647,33399988,4174962,2015-11-07T02:53:20,"<p>Having one database instance per microservice instance is a very unusual architecture.  If you are concerned about load on the database, you could cluster it for higher throughput, however, inserts don't cause much load.  </p>&#xA;&#xA;<p>I'd suggest you look into a NoSQL database if you are concerned about the database being a bottleneck.  NoSQL databases are designed to scale better for high throughput and handle large amounts of data well.  Of course the downside is that they don't handle complex data models well.</p>&#xA;"
33578905,33556707,4174962,2015-11-07T03:45:32,"<p>I am involved in a large project using Docker-based microservices and Consul.  (We are using a different queueing service-- RabbitMQ, so I can't speak in details to that aspect, but in general, a queue is a queue.)</p>&#xA;&#xA;<p>Neither Docker, Consul, nor any queue technology that I am aware of provide autoscaling functionality.  Docker provides an easy way to spin up multiple instances of the service, and Consul provides service discovery (as you said) and a key/value persistence store.  The queue is just a way of passing messages between the service instances.  There's nothing you've mentioned that handles autoscaling.</p>&#xA;&#xA;<p>To add autoscaling functionality, you'll need to look at something like Kubernetes.</p>&#xA;&#xA;<p>You could look at something like CloudFoundry or Mesos.  However, both of those require a virtualization layer, such as OpenStack or VMWare vSphere.  With those products, comes significant value, but also price and complexity.</p>&#xA;&#xA;<p>Instead of going that route, I recommend you look into Amazon Web Services.  Using AWS, you can easily run docker containers and set up autoscaling based on CPU load, messages in the queue, time of day (or day of week), etc.  I know using AWS carries a price tag, but when well designed and managed, it'll cost you far less than attempting to design, implement and maintain yourself.  You can also use the smallest (ie, free) machines and or spot instances to minimize costs. </p>&#xA;"
46215730,41335071,6798706,2017-09-14T09:43:51,"<p>ELK approach : </p>&#xA;&#xA;<p>•<strong>Elastic Search</strong>: Search and Analyse data .</p>&#xA;&#xA;<p>•<strong>LogStash</strong>: Collect, parse and enrich data from each system</p>&#xA;&#xA;<p><strong>•Kibana</strong>: Explore and visualise your data graphically</p>&#xA;"
48520630,48515460,1556338,2018-01-30T11:43:28,"<p>We containerise our db in production (on-premises enterprise application). Many do. It's perfectly stable and the deployment is much simplified. Of course our db is not under stress; we're dealing with hundreds of concurrent users, not tens of thousands. We just make sure that the container has enough RAM and is monitored well.</p>&#xA;&#xA;<p>If we did need to dedicate an entire VM to the db alone, then yes I would skip docker.</p>&#xA;"
39344369,39343694,1556338,2016-09-06T08:38:54,"<p>You can specify a different project for your compose file.</p>&#xA;&#xA;<pre><code>docker-compose -p namespace2 up -d&#xA;</code></pre>&#xA;&#xA;<p>By default, the project name is the name of the directory where your docker-compose is. You could also create a new dir and copy your compose file into it. It's the same as specifying <code>-p</code>. It basically specifies the namespace for the objects in your compose file.</p>&#xA;"
51297862,51280734,1037682,2018-07-12T05:22:42,"<p>Is your region name correct ?</p>&#xA;&#xA;<p><a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html</a></p>&#xA;"
42934595,42934172,3528562,2017-03-21T17:48:24,"<p>In your docker-compose file, you can make your containers join an existing network, which can be the default network. So, all you need to do is make sure that your containers join a network in which your angularjs container already is. Then, you can refer to your API container simply by the name you gave it in your docker-compose file (if you named it <strong>api</strong>, for example, <code>ping api</code> will work). See <a href=""https://docs.docker.com/compose/networking/#configuring-the-default-network"" rel=""nofollow noreferrer"">Networking in Compose</a>.</p>&#xA;"
41917911,41914911,11635,2017-01-29T05:27:50,"<p>The key thing is to have an unambiguous single owner - i.e. if you share a store, that's fine, as long as only one service ever uses a given set of streams.</p>&#xA;&#xA;<p>In NEventStore v5+ for example, this is codified in having a ""bucket"" be a subdivision within a store - each service gets an isolated set of state that way. Or one might do the same via multiple SCHEMAs in a SQL SB.</p>&#xA;&#xA;<p>There are of course lots of good reasons to separate to the max too</p>&#xA;&#xA;<ul>&#xA;<li>you don't want to leave people open to temptation to read cross service</li>&#xA;<li>you want to enable the services to go Separate Ways - you don't want to have any infrastructure change for Service B to require a deploy of Service A</li>&#xA;<li>having a shared lib, which can go hand in hand with this view, is also a slipperly slope</li>&#xA;</ul>&#xA;&#xA;<p>It should be said that this concern is a general constraint in line with the autonomy tenet of microservices (and SOA before it)</p>&#xA;"
50715276,50702676,11635,2018-06-06T08:07:03,"<p>In general you want to avoid relying on a two-phase commit of the kind you describe.</p>&#xA;&#xA;<p>In general, (presuming an event-sourced system; not sure if that's implicit in your question/an option for you - perhaps <a href=""https://github.com/SQLStreamStore/SQLStreamStore"" rel=""nofollow noreferrer""><code>SqlStreamStore</code></a> might be relevant in your context?), this is typically managed by having something <code>project</code> from from a single authoritative set of events on a pull basis - each event being written that requires an associated action against some downstream maintains a pointer to how far it has got projecting events from the base stream, and restarts from there if interrupted.</p>&#xA;"
43250940,28500066,800883,2017-04-06T09:21:59,"<p>The <code>nohup</code> and the <code>at now + 1 minutes</code> didn't work for me.&#xA;Since Jenkins was killing the process spun in the background, I ensured the process to not be killed by setting a fake BUILD_ID to that Jenkins task. This is what the Jenkins Execute shell task looks like:</p>&#xA;&#xA;<pre><code>BUILD_ID=do_not_kill_me&#xA;java -jar -Dserver.port=8053 /root/Deployments/my_application.war &amp;&#xA;exit&#xA;</code></pre>&#xA;&#xA;<p>As discussed <a href=""http://jenkins-ci.361315.n4.nabble.com/Managing-a-Spring-Boot-self-executable-application-td4720491.html"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;"
44466351,44464660,3234163,2017-06-09T20:39:08,"<p>It's up to you how much you want to group together vs be separate on their own. You can also have all your functions in 1 project in Visual Studio then deploy it to N number of function apps, then you can specify which functions a given function app should run in your <code>host.json</code>. Check the functions property in here <a href=""https://github.com/Azure/azure-webjobs-sdk-script/wiki/host.json"" rel=""nofollow noreferrer"">https://github.com/Azure/azure-webjobs-sdk-script/wiki/host.json</a> </p>&#xA;&#xA;<p>Note that the Portal UI doesn't handle that correctly now, so you may get odd errors if you go in the portal. Tracked here <a href=""https://github.com/Azure/azure-functions-ux/issues/1428"" rel=""nofollow noreferrer"">https://github.com/Azure/azure-functions-ux/issues/1428</a></p>&#xA;"
28828273,28767707,4481395,2015-03-03T09:30:58,"<p>From my perspective only the objects uses the services should have the state. Let's consider your example: service X responsible for the user Id, service A responsible for the profile information, etc. Lets assume the user Y that has some security token (that may be created for example by using it's user name and password - should be unique) entries to the system. Then the client, contains the user information, sends the security token to the service X. The service X contains info about user ID linked to such token. In case the new user, the service X creates the new ID and stores it's token. Then the service X returns ID to the user object. The user object asks service A about the user profile by providing user ID. Service A takes the ID and asks service X if that ID exists. Service X sends the positive answer then service A may search the profile information by user ID or ask the user to provide such information in order to create it. The same logic should work with the B and C services. They have to talk each with other but they don't need to know about the user state. </p>&#xA;&#xA;<p>Few words about the environments. I would suggest to use <a href=""https://puppetlabs.com/"" rel=""nofollow"">puppets</a>. This is the way to automate the service deploy process. We are using the puppets to deploy the services on the different environments. The puppet script is reach and allows flexible configuration.  </p>&#xA;"
48830059,48803038,2721139,2018-02-16T15:34:15,"<p>If you're calling your asynchronous operation with <code>PipeTo</code>, it won't block your current code execution path. This means that the try/catch statement will never be executed (as success or failure of the task representing asynchronous operation will be redirected to <code>Self</code>).</p>&#xA;&#xA;<p>There are 2 ways to handle exceptions that may happen during async operation execution:</p>&#xA;&#xA;<ol>&#xA;<li>Use <code>ReceiveAsync&lt;&gt;</code> instead of <code>Receive&lt;&gt;</code> in combination with async lambda - this will allow you to simply <code>await</code> for asynchronous operation to complete. You're free to use try/catch in this context, as it will work as expected. Keep in mind that this will keep your actor non-reentrant - it means, that the actor will not process any messages until current asynchronous operation completes.</li>&#xA;<li>Keep <code>PipeTo</code> and add another <code>Receive&lt;&gt;</code> handler in your logic - in case when a task wrapping asynchronous operation will fail, the exception will be wrapped into a message and redirected back to <code>PipeTo</code>'s target (<code>Self</code> in your case). By default exceptions produced this way are wrapped in <code>Status.Failure</code> message - in this case you'd need to add <code>Receive&lt;Status.Failure&gt;</code> handler in your actor. You can also specify a custom message constructor used to handle <code>PipeTo</code> failures i.e.: <code>_repoComponent.InsertJobAsync(input).PipeTo(Context.Self, failure: exception =&gt; new MyErrorMessage(exception))</code></li>&#xA;</ol>&#xA;"
45825460,45820257,3641203,2017-08-22T19:24:17,"<p>This sounds like a perfect candidate for auto-configuration.</p>&#xA;&#xA;<p>Take a look here <a href=""https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-developing-auto-configuration.html"" rel=""nofollow noreferrer"">Creating your own auto-configuration</a></p>&#xA;"
51126600,30027545,8295283,2018-07-01T19:57:53,"<p>Vert.x currently has a number of official modules for creating microservices architectures, that - I believe - did not exist at the time your question was phrased.</p>&#xA;&#xA;<p>These are the official <a href=""https://vertx.io/docs/#microservices"" rel=""nofollow noreferrer"">Microservices modules</a>:</p>&#xA;&#xA;<ul>&#xA;<li><strong>Vert.x Service Discovery</strong> - publish, lookup and bind to any type of services.</li>&#xA;<li><strong>Vert.x Circuit Breaker</strong> - provides an implementation of the circuit breaker pattern for Vert.x</li>&#xA;<li><strong>Vert.x Config</strong> - provides an extensible way to configure Vert.x applications.</li>&#xA;</ul>&#xA;&#xA;<p>And there are more official modules that come in handy:</p>&#xA;&#xA;<ul>&#xA;<li><strong>Metrics using Dropwizard</strong> - gets metrics from core components and sends to Dropwizard.</li>&#xA;<li><strong>Metrics using Micrometer</strong> - gets metrics from core components and sends to Micrometer.</li>&#xA;<li><strong>Vert.x Health Check</strong> - provides a simple way to expose health checks.</li>&#xA;<li><a href=""https://vertx.io/docs/#clustering"" rel=""nofollow noreferrer""><strong>Vert.x Clustering</strong></a> - support for Hazelcast, Zookeeper, Apache Ignite and Infinicast</li>&#xA;<li><a href=""https://vertx.io/docs/#services"" rel=""nofollow noreferrer""><strong>Vert.x Services</strong></a> - simple and effective way to encapsulate reusable functionality for use elsewhere.</li>&#xA;</ul>&#xA;&#xA;<p>There are many more official modules you can incorporate in your microservices design.</p>&#xA;&#xA;<p>And also there are some great articles on how to apply them in practice:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""http://escoffier.me/vertx-hol/"" rel=""nofollow noreferrer""><strong>Vert.x - From zero to (micro)-hero</strong></a> by Clement Escoffier (Vert.x core developer)</li>&#xA;<li><a href=""https://piotrminkowski.wordpress.com/2017/08/24/asynchronous-microservices-with-vert-x/"" rel=""nofollow noreferrer""><strong>Asynchronous Microservices with Vert.x</strong></a> by Piotr Mińkowski</li>&#xA;<li><a href=""https://dzone.com/articles/running-vertx-microservices-on-kubernetesopenshift"" rel=""nofollow noreferrer""><strong>Running Vert.x Microservices on Kubernetes/OpenShift</strong></a> by Piotr Mińkowski on DZone</li>&#xA;<li><a href=""https://dzone.com/articles/event-driven-microservices-with-vertx-and-kubernet"" rel=""nofollow noreferrer""><strong>Event-Driven Microservices With Vert.x and Kubernetes (Part 1 of 3)</strong></a> by Andy Moncsek</li>&#xA;</ul>&#xA;&#xA;<p>And finally if you want to see code that goes much further than <code>HelloWorld</code>, then look at:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://github.com/sczyh30/vertx-blueprint-microservice"" rel=""nofollow noreferrer""><strong>Vert.x Microservices Blueprint</strong></a> by Eric Zhao on Github</li>&#xA;</ul>&#xA;&#xA;<p>And there is much more code to be had (like an <a href=""https://github.com/DTeam-Top/dgate"" rel=""nofollow noreferrer"">API Gateway</a>, though Readme in Chinese).</p>&#xA;"
47898626,45489456,181363,2017-12-20T04:05:17,"<p>I agree with Grimmy's answer, but just to add some color, I thought I'd describe the system we've been using for the last year to great effect.  Basically, we try to mirror our prod environments as closely as possible on the dev machines.  We run some services in prod on instances directly (mongo, haproxy, etcd) and everything else runs in Docker.  So on dev machines, we do the same, except in Vagrant.  And actually, all our AWS environments are basically like the prod environment, just plus or minus scale/redundancy.  All disposable ""cattle"" that can be rebuilt trivially.</p>&#xA;&#xA;<p>We have our own wacky little docker orchestration system built internally (pre-k8, pre-swarm) that essentially takes a declarative manifest of ""stuff that should be running"" and sticks it into etcd.  Then, an agent running in docker on each system (for vagrant, just one) checks images and configuration signatures to see what is actually running, and if anything is missing runs it.  (And if anything is running that isn't supposed to be, kills it).  For network routing, there is some horrific but amazing haproxy manipulation via confd templating from etcd that I don't want to ever touch again :-) that routes requests to the correct docker backends, supporting blue-green deploys.</p>&#xA;&#xA;<p>For dev work, its all exactly the same.  For rapid dev cycling though, you don't want to have to rebuild containers all the time, its annoying.  So, in the cluster manifest, we have a magic little flag that you can specify that marks one service as ""on the host"".  This goes through the same etcd/confd/haproxy stuff but the backend is your dev system!  Big benefit here is that you get https routing to your service under test via the exact same haproxy setup that runs in prod.</p>&#xA;&#xA;<p>It's a little old school, but we <em>love</em> haproxy.  :-)</p>&#xA;&#xA;<p>So basically, strive to get as close as you can to a totally isolated prod environment on your local system.  Oh, and strive for prod to not be a pet.  </p>&#xA;"
49499481,49482516,840563,2018-03-26T19:48:25,"<p>Just use whatever is best works for you.&#xA;I prefer:&#xA;<a href=""https://www.draw.io/"" rel=""nofollow noreferrer"">https://www.draw.io/</a>&#xA;<a href=""http://plantuml.com/"" rel=""nofollow noreferrer"">http://plantuml.com/</a> - in the wiki (confluence)</p>&#xA;"
35726606,34094882,4447859,2016-03-01T15:16:20,"<p>I had to solve a similar problem at work (although we don't use Docker; instead we have a <code>Vagrantfile</code> which provisions a VM for each of our microservices).</p>&#xA;&#xA;<p>Create a <code>develop</code> repository containing your <code>docker-compose.yml</code>. Each developer should clone the <code>develop</code> repository, as well as his or her forks of the microservices, to a common directory (e.g. <code>~/workspace</code>):</p>&#xA;&#xA;<pre><code>/Users/conar/workspace&#xA;  /develop&#xA;    /docker-compose.yml&#xA;  /service1&#xA;    /Dockerfile&#xA;    /app.js&#xA;  /service2&#xA;    /Dockerfile&#xA;    /app.js&#xA;  /service3&#xA;    /Dockerfile&#xA;    /app.js&#xA;</code></pre>&#xA;&#xA;<p>Your <code>docker-compose.yml</code> might look something like this:</p>&#xA;&#xA;<pre><code>service1:&#xA;  build: ../service1/Dockerfile&#xA;  volumes:&#xA;    - ../service1:/app&#xA;service2:&#xA;  build: ../service2/Dockerfile&#xA;  volumes:&#xA;    - ../service2:/app&#xA;service3:&#xA;  build: ../service3/Dockerfile&#xA;  volumes:&#xA;    - ../service3:/app&#xA;</code></pre>&#xA;&#xA;<p>Now each service's <code>app.js</code> (and all other code) is available under <code>/app</code>.</p>&#xA;"
49302816,49302010,9428851,2018-03-15T15:00:14,"<p>Most of applications using 10% of code during 90% of the time.</p>&#xA;&#xA;<p>The core idea of the micro-services which is modern SOA. You are able to elastically scale the critical part of your application in the micro-service specific special cloud. Cloud is an elastic cluster, where each node is a virtual server (XEN or VMware etc.). Cloud can extend or shrink nodes count automatically according to the load factor, without manual attention.</p>&#xA;&#xA;<p>With the classic monolithic applications, you need to scale a whole monolithic application. Usually such application use a large amount of RAM, as well as require strong hardware or strong and expensive virtual server. Another disadvantage of monolithic - if you need to release a new business feature, release cycle will be really long, since you've done a modification to the huge and complicated code-base with a code entropy. It will require time/budget expensive regression testing. But you have a benefit - different application parts (subsystems and modules) can be integrated much easier comparative to the SOA approach, off cause if you have good application design.</p>&#xA;&#xA;<p>On another side - you can split your application logic to the set of small applications, such application called a micro-service. For instance you have one micro-service responsible for UI only - i.e. only Spring MVC with Angluar.js, another micro-service for business logic and persistence, and one more micro-service for obtaining data from social networks. All those services are interconnected with each other using some web-services usually RestFull, but can be SOAP or something like Google Protocol Buffers RPC etc. So now you are able to auto-scale only UI micro-service, which is expected to be performance critical, without touching business logic and social network micro-services. And you are able to update UI micro-service even once a weak, since UI only testing is not so expensive like business logic testing. But there is a disadvantage - cluster structure became more complicated, and require stronger team to maintain it (usually automate with some Chef or Docker based scripts). It is also hardly to implement subsystem integrations and you need to think about your system design more carefully. </p>&#xA;&#xA;<p>So, if you have a huge and complicated system which is hi-loaded (like Amazon.com, Ebay, Facebook or stackoverflow). SOA approach gives you an opportunity to save some money on infrastructure and hardware. But it will be more expensive in development. And if you system is very simple, i.e. internet cafe site with a few pages - monolithic approach is preferred.</p>&#xA;"
37857198,37854185,5903220,2016-06-16T10:57:31,"<p>In OAuth2, there is a flow dedicated to server-to-server authorization (Client Credentials Grant Flow). The calling service is a regular client for the second (the resource server), so it must get a token and use it.</p>&#xA;&#xA;<p>In a nutshell, the client tells the authorization server who is he (using its client id / app id), the authorization server gives it a token, which can be used to query the resource server.</p>&#xA;&#xA;<p>I have a resource in french <a href=""http://www.bubblecode.net/fr/2016/01/22/comprendre-oauth2/#Lrsquoautorisation_serveur_serveur_Client_Credentials_Grant"" rel=""nofollow"">here</a>, the sequence diagram is in english and should be helpful. You can find more information about this flow easily.</p>&#xA;&#xA;<p>For the Spring Security specific stuff, see the <a href=""http://projects.spring.io/spring-security-oauth/docs/oauth2.html"" rel=""nofollow"">spring-security-oauth2 doc</a>.</p>&#xA;"
35614007,35613841,141172,2016-02-24T21:57:29,"<p><a href=""https://en.wikipedia.org/wiki/Apache_Thrift"" rel=""nofollow"">Apache Thrift</a> was designed to communicate with high efficiency. It was developed by Facebook (now open source), and an implementation is <a href=""https://thrift.apache.org/lib/csharp"" rel=""nofollow"">available for C#</a>.</p>&#xA;&#xA;<p><a href=""https://developers.google.com/protocol-buffers/"" rel=""nofollow"">Google developed</a> the highly efficient <a href=""https://en.wikipedia.org/wiki/Protocol_Buffers"" rel=""nofollow"">protocol buffers</a> serialization mechanism. Protocol buffers does not include RPC (remote procedure call) but can be used to send data over a variety of transport mechanisms. It is also <a href=""https://www.nuget.org/packages/protobuf-net/"" rel=""nofollow"">available for C#</a> (project by SO's own Marc Gravell).</p>&#xA;"
45057076,45052239,1720180,2017-07-12T11:57:43,"<p>The API gateway definitely adds value - what you split is your implementation into smaller units (the microservices) - it allows you to manage, monitor and govern the API interfaces centrally as a single unit, separately from the implementation(s). </p>&#xA;&#xA;<p>Exposing individual microservice endpoints might not be easy to manage - for instance, you would need to apply access control policies separately to each. </p>&#xA;"
45623462,45622414,1720180,2017-08-10T21:06:06,"<p>Whether a microservice that you design and develop requires authentication is up to your functional requirements and the way you design it. </p>&#xA;&#xA;<p>A common technique used is to not have authentication on each individual microservice but to group them together behind a common fascade (such as an API Manager). You can then apply authentication and other policies at one place - the <em>policy enforcement point/API Manager</em> - for ""external"" consumers while ""internally"", behind your common security boundary, your microservices remain lightweight and can call each other without authentication (if that makes sense for your <strong>usecase/requirements/architecture</strong> etc. etc.) </p>&#xA;&#xA;<p>To sum up - it's a <strong>design decision</strong> that involves <strong>multiple tradeoffs</strong>. &#xA;Clearly, if you have a critical business service fetching or updating sensitive data, you might want only authorised callers to access it. But you might not want many internal callers (could be other microservices) running within your organisation's ""trusted"" network to be burdened with unnecessary policy enforcement. &#xA;But then, there might be situations where even internal callers need to authenticate properly (e.g. if it is a payment service) </p>&#xA;"
47750854,47748434,1006964,2017-12-11T10:28:24,"<p>Since you are using the spring boot, it supports UBER JAR format, which is the most recommended way for microservices these days since it has the embedded support for server tomcat / jetty / etc. Check <a href=""https://12factor.net/"" rel=""nofollow noreferrer"">12-factor apps</a> </p>&#xA;&#xA;<p>Once you have the jar simply pushing it in the EC2 and running the <code>java -jar</code> command would start running the server. And in spring boot <code>application.properties</code> you can define the server listen port and the same can be allowed from security groups. </p>&#xA;&#xA;<p>Further you can have Ec2 attached to the ELB.</p>&#xA;"
46979111,46975781,6785908,2017-10-27T15:57:50,"<blockquote>&#xA;  <p>Should I have a separate packages.json for each Dockerfile?&#xA;  From what I've read, I understand the Dockerfile will actually run NPM&#xA;  and download the actual packages. If I have a single packages file I&#xA;  would have a lot of bloat in each of my images, with unused modules.</p>&#xA;</blockquote>&#xA;&#xA;<p>Sure you will have bloat, that's the beauty of Microservice, and <a href=""http://www.markhneedham.com/blog/2012/11/28/micro-services-the-curse-of-code-duplication/"" rel=""nofollow noreferrer"">That's the curse of Microservice</a>.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Should I have a separate packages.json?</p>&#xA;</blockquote>&#xA;&#xA;<p>Okay, let me try to address this question specifically. Let's just say your moduleA uses lodash.x.x.x. And you would like to use lodash.x+1.x.x in moduleB. You are sure that lodash.x+1.x.x is not backward compatible with lodash.x.x.x. So, now you are forced to make ModuleA's code compatible with lodash.x+1.x.x. If the above said sounds true, we call those applications as Monoliths, not microservices. To answer your question, yes, you might need separate packages.json unless you can have a parent package.json with common dependencies and submodules with it's own package.json (I'm not from node-land, so not sure whether package.json have such capabilities) </p>&#xA;&#xA;<blockquote>&#xA;  <p>What I can't find is information on how I should organize and deploy&#xA;  my code. Should I have a different Node project (with separate&#xA;  packages.json) for each of my modules? Or should I have a single Node&#xA;  project for all my modules and deploy each of them individually?</p>&#xA;</blockquote>&#xA;&#xA;<p>I have seen both the patterns (separate projects as well as submodules in same project). My personal opinion (for what it's worth) is to have separate project and separate code base, because your ""modules"" are talking over a language-agnostic protocol-AMQP already. Tomorrow, you might want to use golang/kotlin for ""ModuleB"" (microserviceB, depending on how you look at it :)), who knows.</p>&#xA;&#xA;<blockquote>&#xA;  <p>After I have my project organized, is there a script tool that will&#xA;  generate (build* and deploy) each of my modules to its own container?</p>&#xA;</blockquote>&#xA;&#xA;<p>One thing to clarify here is, development and deployment of one microservice should not force you to change/deploy other microservices as long as the contract between them is kept intact, otherwise it is a distributed-monolith (whoa..did I just invent a new term? fancy!!). When there is a contract change you should bump the version and have both the version run for a while until all the concerned parties (your other microservices) get a chance to upgrade.</p>&#xA;"
40381178,40377076,6785908,2016-11-02T13:28:02,"<p>The functionality you look for can be achieved using netflix zuul + netflix eureka. ""name-service"" is the name of your service with which you registered to eureka server. You should be hitting ZUUL microproxy address; not the eureka address</p>&#xA;&#xA;<p>I suggest you start from here. <a href=""https://spring.io/blog/2015/01/20/microservice-registration-and-discovery-with-spring-cloud-and-netflix-s-eureka"" rel=""nofollow noreferrer"">https://spring.io/blog/2015/01/20/microservice-registration-and-discovery-with-spring-cloud-and-netflix-s-eureka</a> . This is a nice article by Josh Long</p>&#xA;"
51250035,51248930,6785908,2018-07-09T16:22:04,"<blockquote>&#xA;  <p>1) Do we need to consider creating different micro service project.</p>&#xA;</blockquote>&#xA;&#xA;<p>If your data makes sense to move to micro-service architecture, Yes, you should consider micro-services architecture.</p>&#xA;&#xA;<blockquote>&#xA;  <p>3) Assume, we keep Admin or Customer Service APIS in respective Micro&#xA;  services, how good it is. my point is about exposing or potential to&#xA;  expose Admin API to the world</p>&#xA;</blockquote>&#xA;&#xA;<p>I am afraid that you are looking at this from a wrong angle. User's role should not be the one which drives the micro-service design. Instead it should be your data/entities/domains. Rule of thump is datastore should be different for different micro-services. </p>&#xA;&#xA;<blockquote>&#xA;  <p>2) Assume we consider different micro services, do we need to share&#xA;  all the domain models to this micro service.</p>&#xA;</blockquote>&#xA;&#xA;<p>I wanted to answer this after the above one. If you design your micro-services right, your domain overlap will be minimal, in turn the code (domain POJOs) that need to be shared will be minimal, but still it's not totally avoidable. There are some creative ways to share the code</p>&#xA;&#xA;<ol>&#xA;<li><p>publish a library jar file with all the shared code to your internal repo and refer it in your main projects</p></li>&#xA;<li><p>Have a parent project which will have all your micro-services, so that you can share the code. ( Congrats, you just created a monolithic codebase :) ) In all seriousness, IMHO, this is against the spirit of micro-services. Even though you still can deploy each of your micro-services as separate application/jar files, the codebase is still monolithic)</p></li>&#xA;</ol>&#xA;&#xA;<p>A small piece of advice is, just don't get hung up on code share-ability. What if you decide to use a non-JVM language like go/rust to write your other micro-services. After all, that is one of the main selling point of a micro service architecture.</p>&#xA;"
40669959,40669943,6785908,2016-11-18T05:29:34,"<p>Short answer: mainly to <em>Stop cascading failures in a complex distributed system.</em></p>&#xA;&#xA;<blockquote>&#xA;  <p>I don't have any meaningful alternative response to provide, why would&#xA;  I need a circuit breaker at all?</p>&#xA;</blockquote>&#xA;&#xA;<p>This question is relevant only if your server serves just a single REST end point (and a single HTTP verb). But almost always, that is not the case. Even the 'micro'services will have combination of multiple endpoints + multiple http verbs. You don't want one endpoint to hung up on a slow down-stream microservice and pileup threads after threads keep waiting there and eventually bringdown your entire application.</p>&#xA;&#xA;<p>Read the official <a href=""https://github.com/Netflix/Hystrix/wiki#what-is-hystrix-for"" rel=""nofollow noreferrer"">documentation</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>What Is Hystrix For?&#xA;  --- Hystrix is designed to do the following:</p>&#xA;  &#xA;  <ul>&#xA;  <li>Give protection from and control over latency and failure from    dependencies accessed (typically over the network) via third-party<br>&#xA;  client libraries.</li>&#xA;  <li>Stop cascading failures in a complex distributed system.</li>&#xA;  <li>Fail fast and rapidly recover.</li>&#xA;  <li>Fallback and gracefully degrade when possible.</li>&#xA;  <li>Enable near real-time monitoring, alerting, and operational control.</li>&#xA;  </ul>&#xA;</blockquote>&#xA;&#xA;<p><em>""Fallback and gracefully degrade when possible""</em> is just one among the features that hystrix offers.</p>&#xA;"
45623046,45547556,6785908,2017-08-10T20:37:20,"<blockquote>&#xA;  <p>Ideally, the approach should be, creating the entire server structure including ELB's and HA Proxies for each micro service right?</p>&#xA;</blockquote>&#xA;&#xA;<p>This is not necessarily true. The deployment (blue green or canary, no matter what your deployment strategy is) should be <a href=""https://ell.stackexchange.com/a/17699"">transparent</a> to it's consumers (in your case other 7 microservices). That means, your services DNS name (Or IP) to which other services interacts should stay the same. IMHO, in the event of a microservice deployment, you shouldnt have to think about other services in the ecosystem as long as you are keeping your part of the contract; after all that's the whole point of ""micro""services. As other SOer pointed out, if you can't deploy your one microservice without making changes to other services, that is not a microservice, it's just a monolith talking over http.</p>&#xA;&#xA;<p>I would suggest you to read this article&#xA;<a href=""https://www.thoughtworks.com/insights/blog/implementing-blue-green-deployments-aws"" rel=""nofollow noreferrer"">https://www.thoughtworks.com/insights/blog/implementing-blue-green-deployments-aws</a></p>&#xA;&#xA;<p>I am quoting relevant parts here </p>&#xA;&#xA;<h1>Multiple EC2 instances behind an ELB</h1>&#xA;&#xA;<blockquote>&#xA;  <p>If you are serving content through a load balancer, then the same&#xA;  technique would not work because you cannot associate Elastic IPs to&#xA;  ELBs. In this scenario, the current blue environment is a pool of EC2&#xA;  instances and the load balancer will route requests to any healthy&#xA;  instance in the pool. To perform the blue-green switch behind the same&#xA;  load balancer you need to replace the entire pool with a new set of&#xA;  EC2 instances containing the new version of the software. There are&#xA;  two ways to do this -- automating a series of API calls or using&#xA;  AutoScaling groups.</p>&#xA;</blockquote>&#xA;&#xA;<p>There are other creatives ways like this too</p>&#xA;&#xA;<h1>DNS redirection using Route53</h1>&#xA;&#xA;<blockquote>&#xA;  <p>Instead of exposing Elastic IP addresses or long ELB hostnames to your&#xA;  users, you can have a domain name for all your public-facing URLs.&#xA;  Outside of AWS, you could perform the blue-green switch by changing&#xA;  CNAME records in DNS. In AWS, you can use Route53 to achieve the same&#xA;  result. With Route53, you create a hosted zone and define resource&#xA;  record sets to tell the Domain Name System how traffic is routed for&#xA;  that domain.</p>&#xA;</blockquote>&#xA;&#xA;<p>To answer other question.</p>&#xA;&#xA;<blockquote>&#xA;  <p>But then how about the external micro service dns's which hard coded&#xA;  in side the application.properties file?</p>&#xA;</blockquote>&#xA;&#xA;<p>If you are doing this, I would suggest you to read about <a href=""https://12factor.net/"" rel=""nofollow noreferrer"">12factor</a> app; especially the <a href=""https://12factor.net/config"" rel=""nofollow noreferrer"">config</a> part. You should take a look at service discovery options too, if you haven't already done so.</p>&#xA;&#xA;<p>I have a feeling that, what you have here is a spaghetti of not-so-micro-services. If it is a greenfield project and if your timeline-budget allows, I would suggest you to look in to containerizing your application along with it's infrastructure (a single word: Dockerizing) and use any container orchestration technology like kubernetes, Docker swarm or AWS ECS (easiest of all, provided you are already on AWS-land), I know this is out of scope of this question, just a suggestion.</p>&#xA;"
43034026,43032883,60020,2017-03-26T20:15:43,"<p>Another bit of detail that the others haven't touched on:</p>&#xA;&#xA;<p>If your clients are using web browsers, and if you want the user to be authenticated with both services without having to re-login (single sign-on)</p>&#xA;&#xA;<p>E.g. if your domain is example.com, service A is at example.com and service B is at b.example.com.</p>&#xA;&#xA;<ol>&#xA;<li>Send login request to authentication service at example.com</li>&#xA;<li>Auth service verifies credentials and if they match, sends a cookie back to client with domain set to <code>example.com</code></li>&#xA;<li>Client receives and stores cookie</li>&#xA;<li>On all future requests the browser will send the cookie to service A and service B</li>&#xA;<li>In service A and service B verify the  JWT received in the cookie (see other peoples answers)</li>&#xA;</ol>&#xA;&#xA;<p>This method ONLY works when you're running on same root domain and your authentication service MUST be on the root domain. If you have a different setup I would recommend looking at a single sign-on product (e.g. <a href=""https://www.apereo.org/projects/cas"" rel=""nofollow noreferrer"">Central Authentication Service</a>)</p>&#xA;"
44940512,44939302,2619508,2017-07-06T05:35:04,"<p>The above warning is not coming from your app. It is actually coming from one of the chrome extension (Grammarly for Chrome). Disabling the extension solved problem!!</p>&#xA;&#xA;<p>check this <a href=""https://github.com/facebookincubator/create-react-app/issues/2722"" rel=""noreferrer"">https://github.com/facebookincubator/create-react-app/issues/2722</a></p>&#xA;"
37754440,37750638,68725,2016-06-10T17:53:41,"<p>I thought about this for a couple of minutes and this is what I currently think is the best method, based on experience.</p>&#xA;&#xA;<p>There are a couple of problems you will face with availability. First is always having at least one endpoint up. This is easy enough to do by installing on multiple servers. In the enterprise space, you would use a name for the endpoint and then have it resolve to multiple servers (virtual or hardware). You would also load balance it.</p>&#xA;&#xA;<p>The second is registry. This is a very easy problem with API management software. The really good software in this space is not cheap, so this is not a weekend hobbyist type of software. But there are open source API Management solutions out there. As I work in the Enterprise space, I am very familiar with options like Apigee, CA, Mashery, etc. so I cannot recommend an open source option and feel good about myself. </p>&#xA;&#xA;<p>You could build your own registry, if you desire. Just be careful how you design it, as a ""registry of all interface points"" leads to a service that becomes more tightly coupled.</p>&#xA;"
37754798,37749087,68725,2016-06-10T18:15:10,"<p>I am going to give an architects answer rather than drill down into details. I hope you don't mind.</p>&#xA;&#xA;<p>The first suggestion is decouple all of the concepts: events, messages, bus and/or queue and asynch. This opens up possibilities, provided you have not already decided on the software you are using to implement your bus.</p>&#xA;&#xA;<p>From an architecture standpoint, if you require a ""must deliver"" type of scenario, you will persist the messages when the service fails. Yes, you will likely need some sort of clean up in the system, as stuff happens, but focus on the guaranteed delivery problem first. I see two basic options off hand which can be expanded on (there are likely more, but these are sufficient to start thinking about the problem).</p>&#xA;&#xA;<ul>&#xA;<li>The inventory service handles pulling the message from a queue. In this method, the service spins back up and finds any messages.</li>&#xA;<li>The ""bus"" guarantees delivery. When there is a failure, it waits until the service is back up (could ping to see if up again or the service can re-register as a subscriber (Enterprise Service Bus type of scenario).</li>&#xA;</ul>&#xA;&#xA;<p>Just because the system is asynch and event based does not mean you cannot implement some type of guaranteed delivery. A queue is an option (you seem to discard this idea?), but a bus that persists on failure and retries once subscribers are up again is another. And you can persist without blocking.</p>&#xA;&#xA;<p>One other issue is what tokens the messages use to get them synched back to the business function at hand, but I assume you have this handled in the system somehow. The only concept you may not have is having systems all respect the token and respect the other systems in returning messages in cases of failures.</p>&#xA;&#xA;<p>Note that asynchronous communication, from the business standpoint, does not mean fire and forget at the point of contact. You can return messages back without using the asynch method on every single bit of information. What I mean here is the inventory system starting up may process a message and send to the application on the UI end and it can return ""forget about it, you were too slow"" so the transaction is returned to its original state (nonexistent?).</p>&#xA;&#xA;<p>I don't have enough information (or time?) to suggest which method is best for your architecture, as the details are still a bit too high level, but hopefully this stirs some thought. </p>&#xA;&#xA;<p>Hope this makes sense, as I basically did a brain to keyboard maneuver in my ADHD state. ;-)</p>&#xA;"
46591898,44645743,4215925,2017-10-05T17:49:23,"<p>In my case the solution was to tell Zuul not to strip the prefix.  I was hitting the wrong url on the auth server and getting a 401 forbidden and Zuul was swallowing the error and returning 200.</p>&#xA;&#xA;<pre><code>zuul:&#xA;  routes:&#xA;    auth:&#xA;      path: /oauth/**&#xA;      serviceId: saapi-auth-server&#xA;      stripPrefix: false&#xA;      sensitiveHeaders: true&#xA;</code></pre>&#xA;&#xA;<p>I have a ResourceServerConfigurerAdapter like this below on the Zuul client.  I had to disable CSRF, but all of our calls are system to system so we should be good.</p>&#xA;&#xA;<pre><code>@Configuration&#xA;@EnableResourceServer&#xA;public class JwtSecurityConfig extends ResourceServerConfigurerAdapter {&#xA;&#xA;@Override&#xA;public void configure(HttpSecurity http) throws Exception {&#xA;    http.authorizeRequests()&#xA;            .antMatchers(""/oauth/**"").permitAll()&#xA;            .antMatchers(""/api/**"").hasAuthority(""ROLE_API_ACCESS"")&#xA;            .and()&#xA;            .csrf().disable();&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
36981085,36913462,2083957,2016-05-02T11:13:37,"<p><strong>OSGI > Java Component Deployment</strong></p>&#xA;&#xA;<p>Converting Java Modules into Components and Deploying them independently.</p>&#xA;&#xA;<blockquote>&#xA;  <p>units of resources called bundles</p>&#xA;  &#xA;  <p>Bundles can export services or run processes</p>&#xA;  &#xA;  <p>bundle can be expected to have its requirements managed by the container</p>&#xA;  &#xA;  <p>any valid OSGi bundle can theoretically be installed in any valid OSGi container</p>&#xA;  &#xA;  <p>is lightweight in the sense that it offers very few services</p>&#xA;  &#xA;  <p>has little runtime overhead</p>&#xA;</blockquote>&#xA;&#xA;<pre><code>OSGi Container &gt; Implementations &gt; &#xA;&#xA;OSGi Release 4 Core Framework&#xA;|   |   |&#xA;|   |   |&#xA;|   |   |Core Open Source Implementations&#xA;|   |               |&#xA;|   |               |&#xA;|   |               |&#xA;|   |               |Apache Felix&#xA;|   |               |Eclipse Equinox (Eclipse IDE is built on this.)&#xA;|   |               |Knopflerfish&#xA;|   |&#xA;|   |&#xA;|   |OSGi runtime environment (wraps an OSGi Core and provide developers with a simple interface to manage their OSGi applications) &#xA;|               |&#xA;|               |&#xA;|               |&#xA;|               |Apache Karaf (Felix Core)&#xA;|               |Eclipse Virgo (Equinox Core)&#xA;|&#xA;|&#xA;|&#xA;|Dependency Injection Framework for OSGI&#xA;            |&#xA;            |&#xA;            |&#xA;            |Blueprint (Blueprint support the dynamic nature of OSGi services by describing how the components get instantiated and wired together)&#xA;</code></pre>&#xA;&#xA;<p><strong>Usage Wise (Highest to Lowest) :</strong> </p>&#xA;&#xA;<ol>&#xA;<li><p>OSGi R4 Core Bundle521 > Apache Felix&#xA;org.apache.felix » org.osgi.core under OSGI Containers&#xA;OSGi Service Platform Release 4 Core Interfaces and Classes.</p></li>&#xA;<li><p>Apache Karaf :> Assemblies :> Default Distribution107 &#xA;org.apache.karaf » apache-karaf under OSGI Containers&#xA;Apache Karaf :> Assemblies :> Default Distribution</p></li>&#xA;<li><p>Apache Aries Blueprint Bundle84 &#xA;org.apache.aries.blueprint » org.apache.aries.blueprint under OSGI Containers&#xA;This bundle contains the standalone implementation along with the API. It's composed of the blueprint-api, blueprint-core and blueprint-cm modules.</p></li>&#xA;<li><p>Osgi4 > Apache Equinox&#xA;org.eclipse.equinox » osgi under OSGI Containers</p></li>&#xA;</ol>&#xA;&#xA;<p><strong>OSGI > Uses</strong> </p>&#xA;&#xA;<blockquote>&#xA;  <p>Modular architecture reduces complexity</p>&#xA;  &#xA;  <p>allows for better changeability</p>&#xA;  &#xA;  <p>parallel development</p>&#xA;  &#xA;  <p>reuse and flexibility</p>&#xA;  &#xA;  <p>reducing time and expenses for development and maintenance</p>&#xA;</blockquote>&#xA;&#xA;<p><strong>OSGI Container > Functions</strong> </p>&#xA;&#xA;<blockquote>&#xA;  <p>install new bundles</p>&#xA;  &#xA;  <p>start them</p>&#xA;  &#xA;  <p>stop them</p>&#xA;  &#xA;  <p>uninstall them</p>&#xA;  &#xA;  <p>check their dependencies</p>&#xA;  &#xA;  <p>check registered services </p>&#xA;  &#xA;  <p>number of other things</p>&#xA;  &#xA;  <p>executes the container's boot process</p>&#xA;  &#xA;  <p>each container has a different startup environment</p>&#xA;  &#xA;  <p>each container has slightly different capabilities</p>&#xA;</blockquote>&#xA;&#xA;<p><strong>OSGI Container > Bundles</strong></p>&#xA;&#xA;<blockquote>&#xA;  <p>container is running</p>&#xA;  &#xA;  <p>each bundle consists of one or more Java packages</p>&#xA;  &#xA;  <p>each bundle has a MANIGEST file</p>&#xA;  &#xA;  <p>the Packages and Manifest file are bundled into a JAR</p>&#xA;  &#xA;  <p>First bundle has an id of ""0""</p>&#xA;  &#xA;  <p>ID is used to control the bundle's lifecycle.</p>&#xA;  &#xA;  <p>bundle provides a lifecycle and exported service</p>&#xA;  &#xA;  <blockquote>&#xA;    <p>OSGi bundle requires an ""activator""</p>&#xA;    &#xA;    <p>""Activator"" manages the lifecycle of the bundle</p>&#xA;    &#xA;    <p>at least one of bundle contains an Activator</p>&#xA;    &#xA;    <p>a feature is a set of bundles that provide a particular facility.</p>&#xA;  </blockquote>&#xA;</blockquote>&#xA;&#xA;<p><strong>OSGI Container > Life Cycles</strong></p>&#xA;&#xA;<blockquote>&#xA;  <p>starting</p>&#xA;  &#xA;  <p>stopping</p>&#xA;  &#xA;  <p>updating</p>&#xA;  &#xA;  <p>or removing bundles and downloading of management policies</p>&#xA;  &#xA;  <p>remotely and via an API</p>&#xA;</blockquote>&#xA;&#xA;<p><strong>OSGI Container > Services</strong></p>&#xA;&#xA;<blockquote>&#xA;  <p>services can be provided by one bundle to another bundle dynamically</p>&#xA;  &#xA;  <p>service registry allows bundles to detect the addition of new services, or the removal of services, and adapt accordingly.</p>&#xA;</blockquote>&#xA;&#xA;<p><strong>Apache Karaf</strong></p>&#xA;&#xA;<blockquote>&#xA;  <p>Karaf bundles are platform-specific because Karaf uses native-code libraries</p>&#xA;  &#xA;  <p>Starts simply by unpacking the distribution bundle into any convenient directory</p>&#xA;</blockquote>&#xA;&#xA;<p><strong>Apache Karaf > Commands</strong></p>&#xA;&#xA;<p>osgi:list</p>&#xA;&#xA;<p>osgi:install file:/path/to/my/bundle.jar</p>&#xA;&#xA;<p>osgi:start [id]</p>&#xA;&#xA;<p>osgi:stop [id]</p>&#xA;&#xA;<p>osgi:uninstall [id]</p>&#xA;&#xA;<p>find-class [class_name]</p>&#xA;"
32838800,32838312,710005,2015-09-29T08:03:02,"<p>What i would do is, </p>&#xA;&#xA;<ol>&#xA;<li>Create separate modules for each microservice. Depending on what microservice you want to run, just have a route for that in express.</li>&#xA;<li>Inject the modules you want into an instance of <code>express()</code>.</li>&#xA;</ol>&#xA;&#xA;<p>Example + shameless plug - <a href=""https://github.com/swarajgiri/express-bootstrap/blob/master/core/index.js"" rel=""nofollow"">https://github.com/swarajgiri/express-bootstrap/blob/master/core/index.js</a></p>&#xA;&#xA;<p><strong>Disclaimer</strong> - The above solution is a highly opinionated way of solving your problem. </p>&#xA;"
47015645,47007159,7011701,2017-10-30T12:45:32,"<p><strong>If you have a single client application then you can do following steps</strong></p>&#xA;&#xA;<ul>&#xA;<li><p>Make one microservice for authentication that generates jwt token.</p></li>&#xA;<li><p>The jwt contains all essential user information in its payload, ie Role, UserId etc.</p></li>&#xA;<li><p>The jwt token will be sent in Authorization header for every authorised request.</p></li>&#xA;<li><p>Before processing any request you can validate and decode the jwt token using middlewares. Now you can set the user's info in req object easliy and can easily access users role and its id in your controller. </p></li>&#xA;<li><p>if the token is not valid then you can throw error in middlewares and it will provide json response of unauthorised.</p></li>&#xA;<li><p>You can call the authentication api to validate and decode your token or you can write 3 to 4 line of code in every microservice in middleware.</p></li>&#xA;</ul>&#xA;&#xA;<p>Here are some links for sample implementation of jwt, you should customize these sample code according to above steps.</p>&#xA;&#xA;<p><a href=""https://www.codementor.io/olatundegaruba/5-steps-to-authenticating-node-js-with-jwt-7ahb5dmyr"" rel=""noreferrer"">5-steps-to-authenticating-node-js</a></p>&#xA;&#xA;<p><a href=""https://scotch.io/tutorials/authenticate-a-node-js-api-with-json-web-tokens"" rel=""noreferrer"">authenticate a nodejs api with json web tokens</a></p>&#xA;&#xA;<p><strong>If you have multiple client applications</strong> </p>&#xA;&#xA;<ul>&#xA;<li><p>You should use openid connect standard that provides single sign on solution to authenticate multiple application with same username and password.</p></li>&#xA;<li><p><a href=""https://openidconnect.net/"" rel=""noreferrer"">here</a> is a openid connect playground to understand the authorization flow.</p></li>&#xA;</ul>&#xA;"
36717494,36698706,630,2016-04-19T11:49:00,"<p>I think the six-step approach you describe above is on the right track. Service Fabric doesn't support cross-service transactions today:</p>&#xA;&#xA;<p><a href=""https://feedback.azure.com/forums/293901-service-fabric/suggestions/8548390-cross-actors-services-transaction"" rel=""nofollow"">UserVoice request to consider that feature</a></p>&#xA;&#xA;<p>I'd speculate that either this will never be supported, or perhaps supported in some limited fashion. Transactional consistency in distributed systems is a known difficult problem. This is the reason why eventual consistency is the current preferred pattern in modern cloud architectures:</p>&#xA;&#xA;<p><a href=""https://en.wikipedia.org/wiki/Eventual_consistency"" rel=""nofollow"">eventual consistency description</a></p>&#xA;&#xA;<p>A bit more background on the problem:</p>&#xA;&#xA;<p><a href=""https://en.wikipedia.org/wiki/CAP_theorem"" rel=""nofollow"">CAP theorem</a></p>&#xA;&#xA;<p><a href=""https://www.google.com/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=transactional%20consistency%20in%20distributed%20systems"" rel=""nofollow"">links to related research</a></p>&#xA;&#xA;<p>As for your solution, I'd say your approach, while not exactly simple, is the right one. As written, UpdateTheThing() has an implicit notion of two states... ""nothing is updated"" and ""everything is updated"". You'd need to introduce a few other states that the caller is explicitly aware of, and handles accordingly:</p>&#xA;&#xA;<ul>&#xA;<li>nothing is updated</li>&#xA;<li>local state is updated</li>&#xA;<li>local state is updated + remote state update request is sent</li>&#xA;<li>local state is updated + caller asynchronously receives ack of remote update success/failure</li>&#xA;</ul>&#xA;&#xA;<p>You'd also likely want an error condition after not receiving remote update ack beyond a certain timeout period, etc. You may also want formal states for retry behaviors, etc. too.</p>&#xA;&#xA;<p>Depending on your exact scenario there is obviously plenty of complexity beyond this. The key point is that you probably don't want UpdateTheThing() to try to hide the complexity of this from the caller... the caller needs to be aware of the possible states and handle/respond to them appropriately.</p>&#xA;&#xA;<p>As you say its complex, but that's the nature of distributed work (in the cloud, or otherwise).</p>&#xA;&#xA;<p>Best of luck!</p>&#xA;"
44816362,44803729,5965430,2017-06-29T04:29:40,"<p>I'll try to clarify few bits.</p>&#xA;&#xA;<blockquote>&#xA;  <p>upload images -> if it really is new -> queues it up for processing</p>&#xA;</blockquote>&#xA;&#xA;<p>Upon a new upload event, you'd want to process the image. Here's a <a href=""https://github.com/sabbyanandan/thumbnail-generator"" rel=""nofollow noreferrer"">similar use-case</a>, but more of a real-time streaming style solution. This is not what you're looking to do, but I thought it might be useful.</p>&#xA;&#xA;<p>Porting the image processing code to a Spring Cloud Stream application is as simple as adding <code>@EnableBinding(Processor.class)</code>. It is the same business logic - whether you're running it separately or orchestrating it via SCDF, it is <em>still</em> a standalone microservice. However, SCDF expects it to be either a Source, Processor, Sink, or Task application types. We will be opening this up to support any arbitrary ""functions"" (lambdas) in the future release.</p>&#xA;&#xA;<blockquote>&#xA;  <p>We can create a Queue endpoint and send the data directly a Queue source that receives the outside event and forwards it to an SCDF queue.</p>&#xA;</blockquote>&#xA;&#xA;<p>This is one of the standard solutions. You can directly consume new events (images) from a queue/topic and process it in the image-processor that we created in previous step. The <a href=""http://docs.spring.io/spring-cloud-dataflow/docs/1.2.2.RELEASE/reference/htmlsingle/#spring-cloud-dataflow-stream-explicit-destination-names"" rel=""nofollow noreferrer"">named-channel support</a> in DSL facilitates just that.</p>&#xA;&#xA;<blockquote>&#xA;  <p>What would be awesome is if DataFlow could connect the start of the queue for me, without repackaging the microservice as a Source.</p>&#xA;</blockquote>&#xA;&#xA;<p>I'm not sure I understand this. If I were to assume, you're looking for ""named-channel"" as source and that is supported. </p>&#xA;&#xA;<blockquote>&#xA;  <p>The major issue with Spring Data Flow is that it does not automatically start up deployed streams when the server starts up, and we need to be reasonably sure that microservice is always up.</p>&#xA;</blockquote>&#xA;&#xA;<p>The moment you deploy a Stream in SCDF, all the individual steps included in the DSL (i.e., stream definition) are resolved and deployed as standalone apps in the target runtime (cloud foundry, kubernetes, etc.,). Once deployed, it is left to the platform where the apps run for lifecycle management. SCDF does not retain or track the app states.</p>&#xA;"
51603630,51557001,5965430,2018-07-30T23:42:19,"<p>Given the limited information in the post, it is hard to convince on all the matters pertaining to this type of architecture, but I'll attempt to share some specifics, and point to samples. Also for the same reasons, it is hard to solve for your needs end-to-end. From the surface, it appears you're attempting to build event-driven applications and wondering whether Spring Cloud Stream (SCSt) and Spring Cloud Data Flow (SCDF) could help. </p>&#xA;&#xA;<p>They can, yes. </p>&#xA;&#xA;<p>The Order, User, and Catalog seem like domain objects and it would all come together to solve for a use-case. For instance, querying for a number of orders for a particular product, and group by the user. There are a few samples that articulate the data flow between the entities to solve similar problems. Here's a <a href=""https://www.youtube.com/watch?v=r7AGQsM7ncA"" rel=""nofollow noreferrer"">live code-walkthrough</a> of event-driven systems in action. There's another example of <a href=""https://github.com/sabbyanandan/eda"" rel=""nofollow noreferrer"">social-graph</a> application, too. </p>&#xA;&#xA;<p>Though these event-driven applications can run standalone as individual services with the help of of message broker (eg: Kafka or RabbitMQ), you could of course also register them in SCDF and use them in the SCDF DSL to build a coherent data pipeline. We are expanding on more direct capabilities in SCDF for these types of use-cases, but there are ways to orchestrate them today with current abilities, too. Follow <a href=""https://github.com/spring-cloud/spring-cloud-dataflow/issues/2331#issuecomment-406444350"" rel=""nofollow noreferrer"">spring-cloud/spring-cloud-#2331#issuecomment-406444350</a> for more details.</p>&#xA;&#xA;<p>I hope this gives an idea. Try to build something small using SCSt/SCDF, prove it out, and expand to more complex use-cases.</p>&#xA;"
50877252,50845313,5965430,2018-06-15T14:07:34,"<p>Depending on the platform (eg: cf, k8s..) that you're orchestrating the batch-jobs in SCDF, you could write a simple Quartz based Boot Application that can interact with SCDF's REST endpoints to schedule the Task definitions defined in SCDF.</p>&#xA;&#xA;<p>There are several online literatures on Quartz + Boot solution.</p>&#xA;&#xA;<p>We are also working on a native scheduler integration for <a href=""https://github.com/spring-cloud/spring-cloud-scheduler-cloudfoundry"" rel=""nofollow noreferrer"">Cloud Foundry</a> (via <a href=""https://docs.pivotal.io/pcf-scheduler/1-1/"" rel=""nofollow noreferrer"">PCF Scheduler</a>). Once it ready, you'd be able to schedule (i.e., cron-expressions) for Tasks from SCDF's Dashboard natively.</p>&#xA;"
49928390,49925171,5965430,2018-04-19T19:00:06,"<p>The SI based ""RSS crawler"" service can be packaged as a Spring Cloud Stream application. Once you have done that, your ""RSS crawler"" turns into a standalone event-driven microservice that automatically can either talk to Kafka, Rabbit, or other brokers (depending on the binder implementation in the classpath). The same app is a portable workload that can run on any public or private cloud platforms.</p>&#xA;&#xA;<p>Once you have an ""n"" number of such standalone applications, you'd need a higher-level orchestration layer to compose the applications into a coherent data pipeline. Spring Cloud Data Flow helps with it. You can build a pipeline like the following with SCDF's DSL.</p>&#xA;&#xA;<blockquote>&#xA;  <p>stream create foo --definition ""rss-crawler | filter | transform | cassandra""</p>&#xA;</blockquote>&#xA;&#xA;<p>Here, four applications come together to form a data pipeline. Each one of them is a Spring Cloud Stream application that can be independently developed and tested in isolation. Finally, SCDF would deploy the applications onto target platforms like Cloud Foundry or Kubernetes as native applications.</p>&#xA;&#xA;<p>Hope this further clarifies. </p>&#xA;"
43660268,43617787,5965430,2017-04-27T14:10:00,"<p>Activiti and I believe Camunda, too, is a BPM tool. Spring Cloud Data Flow (SCDF) is <em>not</em> one! </p>&#xA;&#xA;<p>SCDF's core premise is to solve data integration challenges. Whether it is pure ingest, data processing, or persistence, you'd be able to build a coherent data pipeline made of these operations rapidly. </p>&#xA;&#xA;<p>To get there, you can leverage any of the out-of-the-box <a href=""http://cloud.spring.io/spring-cloud-stream-app-starters/#available-applications"" rel=""nofollow noreferrer"">streaming/task applications</a>, or you could build your custom application with the help of <a href=""http://cloud.spring.io/spring-cloud-stream/"" rel=""nofollow noreferrer"">Spring Cloud Stream</a> and <a href=""http://cloud.spring.io/spring-cloud-task/"" rel=""nofollow noreferrer"">Spring Cloud Task</a> microservice frameworks. These applications can benefit from standalone development, testing in isolation, and native CI/CD practices. Once the application is ready, you can <a href=""http://docs.spring.io/spring-cloud-dataflow/docs/1.2.0.RC1/reference/htmlsingle/#spring-cloud-dataflow-register-apps"" rel=""nofollow noreferrer"">register and use</a> them directly in SCDF. </p>&#xA;&#xA;<p>As well, SCDF comes with a <a href=""http://docs.spring.io/spring-cloud-dataflow/docs/1.2.0.RC1/reference/htmlsingle/#arch-intro"" rel=""nofollow noreferrer"">runtime abstraction</a> and with that, you have the option to orchestrate data pipelines made of microservice applications on a variety of cloud-native runtime platforms - one could avoid vendor specific lock-ins. </p>&#xA;"
39766451,39764239,2108278,2016-09-29T09:24:54,"<p>Create a new Conmuter operator type. This is like a Source. It is made of several Sources representing Event and Command topics. It starts in ""recovering"" state. In this state, it reads from the events topics up to their latest. Meanwhile, for the commands, it stores or drops them. Once up to date, it considers recovered and ""opens"" the way to commands. It could be implemented separately as a source plus an operator.</p>&#xA;&#xA;<p>FlinkKafkaProducerXX is not enough to do this, but it would be the base to implement it.</p>&#xA;"
46977615,45757001,8533110,2017-10-27T14:38:19,"<p>You can use Spring profiles to control which version of implementation should be used via spring properties. </p>&#xA;&#xA;<p>In spring properties add below entry</p>&#xA;&#xA;<pre><code>spring.profiles.active=NAME_OF_ACTIVE_PROFILE&#xA;</code></pre>&#xA;&#xA;<p>Every service implementation needs profile annotation. That's how your services implementation should look like:</p>&#xA;&#xA;<pre><code>@Component&#xA;@Profile(""local"")&#xA;public class LocalExampleService implements ExampleService{}&#xA;&#xA;@Component&#xA;@Profile(""remote"")&#xA;public class RemoteExampleService implements ExampleService{}&#xA;</code></pre>&#xA;&#xA;<p>If your project needs to use local implementation of a service then in properties instead of <code>NAME_OF_ACTIVE_PROFILE</code> insert local otherwise remote.</p>&#xA;&#xA;<p>For fully automatic auto-wiring you need to add method running at the startup that checks whether local implementation class exists and then set profile properly. To do this you need to modify code in spring boot main method:</p>&#xA;&#xA;<pre><code>public static void main(String[] args){&#xA;    String profile = checkCurrentProfile(); //Method that decides which profile should be used&#xA;    System.setProperty(AbstractEnvironment.ACTIVE_PROFILES_PROPERTY_NAME, profile);&#xA;    SpringApplication.run(MyApplication.class, args);&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>If you choose this approach then you don't need previous entry in properties file.</p>&#xA;&#xA;&#xA;"
51382269,51382040,8074614,2018-07-17T13:11:08,"<p>If you're using token based authentication, you can pass token between your systems.</p>&#xA;"
28632796,26854986,621686,2015-02-20T15:41:37,"<p>Maybe this is a bit more involved than you were hoping, but as mentioned <a href=""https://stackoverflow.com/a/27815639/621686"">here</a>, this is exactly what <a href=""https://github.com/Netflix/eureka"" rel=""nofollow noreferrer"">Eureka</a> is for.  It also has really nice integration with the new <a href=""http://projects.spring.io/spring-cloud/"" rel=""nofollow noreferrer"">Spring Cloud</a> project.</p>&#xA;"
48705015,48687955,3327350,2018-02-09T11:39:45,"<p>This is a bit generic question, and I'm not a Kafka expert; but I will try to tell about Hazelcast's messaging features.</p>&#xA;&#xA;<p>Hazelcast contains two types of topics; one is regular <code>ITopic</code> and the other one is Reliable Topic, which again implements <code>ITopic</code> interface. The usages are mostly the same, but they differ in what they guarantee. Regular <code>ITopic</code> is based on Hazelcast's eventing mechanism, and does not guarantee message delivery. Reliable Topic is backed up by the <code>Ringbuffer</code>, and the events are not lost since the Ringbuffer is configured with one synchronous backup by default. Also, each Reliable <code>ITopic</code> gets its own Ringbuffer; if a topic has a very fast producer, it will not lead to problems at topics that run at a slower pace. Lastly, since the event system behind a regular <code>ITopic</code> is shared with other data structures, e.g., collection listeners, you can run into isolation problems. This does not happen with the Reliable <code>ITopic</code>. But with all these disadvantages, regular <code>ITopic</code> can run a bit faster since it uses fire&amp;forget eventing mechanism.</p>&#xA;&#xA;<p>Apache Kafka has its own great advantages; such as it is all built as a message streaming platform with a temporal persistent log, and therefore  has ability to store data to disk for fault-tolerance. </p>&#xA;&#xA;<p>In summary, if you need message persistence with all features needed for messaging on your application, go with Kafka since it is more specialized. But if you need a in-memory data platform including support for messaging, go with Hazelcast.  </p>&#xA;"
48059594,48055121,3327350,2018-01-02T10:50:44,"<p>The Connection Attempt Limit and Connection Attempt Period configuration elements help to configure clients' reconnection behaviour. The client will retry as many as <code>ClientNetworkConfig.connectionAttemptLimit</code> times to reconnect to the cluster. Connection Attempt Period is the duration in milliseconds between the connection attempts defined by <code>ClientNetworkConfig.connectionAttemptLimit</code>. Here is an example of how you configure them:</p>&#xA;&#xA;<pre><code>ClientConfig clientConfig = new ClientConfig();&#xA;clientConfig.getNetworkConfig().setConnectionAttemptLimit(5);&#xA;clientConfig.getNetworkConfig().setConnectionAttemptPeriod(5000);&#xA;</code></pre>&#xA;&#xA;<p>Starting with Hazelcast 3.9, you can use configuration element <code>reconnect-mode</code> to configure how the client will reconnect to the cluster after a disconnection. It has three options (<code>OFF</code>, <code>ON</code> or <code>ASYNC</code>). The option <code>OFF</code> disables the reconnection. <code>ON</code> enables reconnection in a blocking manner where all the waiting invocations will be blocked until a cluster connection is established or failed. The option <code>ASYNC</code> enables reconnection in a non-blocking manner where all the waiting invocations will receive a <code>HazelcastClientOfflineException</code>. Its default value is <code>ON</code>. You can see a configuration example below:</p>&#xA;&#xA;<pre><code>ClientConfig clientConfig = new ClientConfig();&#xA;clientConfig.getConnectionStrategyConfig()&#xA;               .setReconnectMode(ClientConnectionStrategyConfig.ReconnectMode.ON);&#xA;</code></pre>&#xA;"
42484425,42265210,3545719,2017-02-27T11:16:32,<p>The issue causing this is that Eureka intellisense gives the shutdown instance more time to come up.  The intellisense works this way for multiple microservices with the same name running on the same box.</p>&#xA;&#xA;<p>When the services are deployed on different boxes then this issue doesn't come up.</p>&#xA;
36780850,36137802,6237559,2016-04-21T21:31:47,<p>You could also create a flag for each entry inside of the event store which tells if this event was already published. Another process could poll the event store for those unpublished events and put them into a message queue or topic. The disadvantage of this approach is that consumers of this queue or topic must be designed to de-duplicate incoming messages because this pattern does only guarantee at-least-once delivery. Another disadvantage could be latency because of the polling frequency. But since we have already entered the eventually consistent area here this might not be such a big concern. </p>&#xA;
39099191,39096701,2650743,2016-08-23T10:53:40,"<p>It depends. There are trade-offs either way. If you store all the information inside the identity context then it needs to have knowledge of all the other contexts and needs to change whenever some permission or access rule changes in any context. If each context manages its own permission rules then they only need to know about themselves. </p>&#xA;&#xA;<p>You also need to consider how things are managed. Is there a concept of centrally managing roles and permissions? </p>&#xA;&#xA;<p>It also depends on how course or fine-grained the roles need to be and how complex the domain is in terms of identity / roles / permissions etc. </p>&#xA;&#xA;<p>If you have very course-grained roles (I.e. 'Administrator', 'User') then I'd probably do something along the lines of having the identity context manage user accounts and roles, but leave the permissions side of things to each individual context. I.e 'here's an authenticated user with roles X and Y' then each individual context determines what this allows.</p>&#xA;"
38712542,38711908,2650743,2016-08-02T05:42:27,"<p>Most decisions are about trade-offs, and this is no exception. In the usual definition of microservices, each service would have it's own database for the reasons @thiago-custodio mentions. You get complete separation and encapsulation of your service. This, however, puts more burden on the operational aspects of the business, as you now have N databases to administrate. You also have a new ""how do all the things talk to each other"" problem to solve. This may or may not be an issue for you.</p>&#xA;&#xA;<p>You could get some of the benefits of service separation and encapsulation from sharing a database but, for example, having separate schemas for each service. You still get some level of isolation at the database level, but reduce your operational complexity. Again, trade-offs.</p>&#xA;&#xA;<p>I view the use of a single event store / message queue as an acceptable trade off to make as a way to enable communication between lots of services. </p>&#xA;&#xA;<blockquote>&#xA;  <p>It's clear there are four microservice, </p>&#xA;</blockquote>&#xA;&#xA;<p>Not necessarily from just viewing the diagram out of context. There are two controllers that write events to an event store, and a controller that returns a query. This looks like a diagram to explain CQRS rather than microservices. CQRS is not a top-level architecture - you would apply it <em>within</em> a microservice.</p>&#xA;&#xA;<blockquote>&#xA;  <p>but I don't understand becouse the ""command side"" microservices don't have own database.</p>&#xA;</blockquote>&#xA;&#xA;<p>The event store <em>is</em> the database. The query side has a separate database that will be optimised for queries.</p>&#xA;"
38859368,38858550,2650743,2016-08-09T19:55:43,"<p>This is one way to handle it yes and usually the preferred method. You keep a cache locally in your service that holds copies of the data from another service. In an event-driven system, this would involve listening to events of interest and using them to update your local cache. The cache could be in-memory, or persisted. An example for your use case would be when raising an invoice, the Accounting context would look in it's local cache for a user/agentid before creating the Invoice. </p>&#xA;&#xA;<p>Other options:</p>&#xA;&#xA;<p><strong>Shared database</strong></p>&#xA;&#xA;<p>I know it is frowned upon (for good reason) but you can always share a database schema. For example, the Identity context can write to a user table and the Accounting context can read from it when it needs an AgentId to put in an invoice. The trade-off is you are coupling at the database level, and introducing a single point of failure. </p>&#xA;&#xA;<p><strong>RPC</strong></p>&#xA;&#xA;<p>You can make a RPC call to another service when you need information. In your example, the Accounting context would call the Identity Management context for the AgentId/User information before raising an invoice. Trade-off with this approach is again a coupling to the other service. What do you do when it is not available? You cannot raise an Invoice.</p>&#xA;&#xA;<p><strong>Reporting domain</strong></p>&#xA;&#xA;<p>Another option is to have a completely separate service that listens for data from other services and maintains view models for UIs. This would keep your other services ignorant of other service's concerns. When using an event-driven system, you'd be listening for events form other services that allow you to build a view model for the UI. This is usually a good option if all you are doing is viewing the data</p>&#xA;"
38049471,38049329,2650743,2016-06-27T08:46:21,"<p>How do people change their names?</p>&#xA;&#xA;<p>I would try and have a single place where this is allowed, and have that context own the data. Other context can then subscribe to changes and act accordingly.</p>&#xA;"
41954999,27054162,2914935,2017-01-31T10:20:50,"<p>Let me introduce you a taxonomic view of those term:</p>&#xA;&#xA;<h3>Microservices</h3>&#xA;&#xA;<p>are a subtype of services specialized by minimal responsibility.</p>&#xA;&#xA;<h3>Webservices</h3>&#xA;&#xA;<p>are a subtype of services as well, specialized by the type of service they provide wich fall under web requirements and needs.</p>&#xA;&#xA;<h3>SOA</h3>&#xA;&#xA;<p>is a subtype of architectures and hence a structural view of some components and their relationships, where happens to be services and communications between those services respectively.</p>&#xA;&#xA;<h3>rest</h3>&#xA;&#xA;<p>is a subtype of communication, underlying http protocol.</p>&#xA;&#xA;<h3>restful</h3>&#xA;&#xA;<p>is a subtype of architectures (a structural view of some components and their relationships) where specialized by relationships among components restricted to be rest communications.</p>&#xA;"
39868267,39866282,1739895,2016-10-05T07:50:52,"<p>Snippet in Python:</p>&#xA;&#xA;<pre class=""lang-py prettyprint-override""><code>import os&#xA;&#xA;if os.getenv('VCAP_APPLICATION'):&#xA;  # cf environment&#xA;  my_env_var = os.getenv('MY_ENV_VAR')&#xA;else:&#xA;  # local&#xA;  my_env_var = some_default_value&#xA;</code></pre>&#xA;"
39745310,39741506,437879,2016-09-28T10:47:40,"<p>If you run your example in multiple JVM's on the same host, then these JVM's will be clustered.</p>&#xA;&#xA;<p>If you want to run the JVM's on different hosts, then it is important to specify the ip and not localhost.</p>&#xA;&#xA;<p>Here is the Troubelshoot section of the vert.x doc on this topic: <a href=""http://vertx.io/docs/vertx-hazelcast/java/#_using_wrong_network_interface"" rel=""nofollow"">http://vertx.io/docs/vertx-hazelcast/java/#_using_wrong_network_interface</a> </p>&#xA;&#xA;<p>If you are interested in playing around with the clustering on one host / different hosts you can use this application <a href=""https://github.com/swisspush/vertx-cluster-watchdog"" rel=""nofollow"">https://github.com/swisspush/vertx-cluster-watchdog</a></p>&#xA;"
46829993,46829594,1891959,2017-10-19T12:33:33,"<p>I think you should do it with a authentication header. Check out these threads: </p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://stackoverflow.com/questions/6068113/do-sessions-really-violate-restfulness"">Do sessions really violate RESTfulness?</a></li>&#xA;<li><a href=""https://stackoverflow.com/questions/3105296/if-rest-applications-are-supposed-to-be-stateless-how-do-you-manage-sessions?noredirect=1&amp;lq=1"">If REST applications are supposed to be stateless, how do you manage sessions?</a></li>&#xA;</ul>&#xA;"
44899210,36461493,983928,2017-07-04T07:08:45,"<p>I had the same problem and was able to solve it in simpler way</p>&#xA;&#xA;<p>Just put this into you Filter <code>run()</code> method</p>&#xA;&#xA;<pre><code>    if (&lt;your condition&gt;) {&#xA;        ZuulException zuulException = new ZuulException(""User message"", statusCode, ""Error Details message"");&#xA;        throw new ZuulRuntimeException(zuulException);&#xA;    }&#xA;</code></pre>&#xA;&#xA;<p>and <code>SendErrorFilter</code> will deliver to the user the message with the desired <code>statusCode</code>.</p>&#xA;&#xA;<p>This Exception in an Exception pattern does not look exactly nice, but it works here.</p>&#xA;"
35362513,35324974,5902456,2016-02-12T12:19:22,"<p>With this property it works for me: </p>&#xA;&#xA;<pre><code>eureka.client.register-with-eureka=false&#xA;</code></pre>&#xA;&#xA;<p>But I also found an example in the eureka documentation:&#xA;<a href=""https://github.com/Netflix/eureka/blob/master/eureka-examples/conf/sample-eureka-client.properties"" rel=""nofollow"">https://github.com/Netflix/eureka/blob/master/eureka-examples/conf/sample-eureka-client.properties</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>note that for a purely client usage (e.g. only used to get information about other services, there is no need for registration. [...]</p>&#xA;</blockquote>&#xA;&#xA;<pre><code>eureka.registration.enabled=false&#xA;</code></pre>&#xA;"
42333139,42332938,932728,2017-02-19T21:31:51,"<p>An application is a required top level container for services. You deploy applications, not services. So you cannot really speak about differences between the two since you cannot have services without an application.</p>&#xA;&#xA;<p>From <a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-application-model"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-application-model</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p><strong>An application is a collection of constituent services</strong> that perform a certain function or functions. A service performs a complete and standalone function (it can start and run independently of other services) and is composed of code, configuration, and data. For each service, code consists of the executable binaries, configuration consists of service settings that can be loaded at run time, and data consists of arbitrary static data to be consumed by the service. Each component in this hierarchical application model can be versioned and upgraded independently.</p>&#xA;</blockquote>&#xA;&#xA;<p>Take a look at the link provided and you will see the hierarchical relationship.</p>&#xA;"
50067734,50067214,932728,2018-04-27T17:48:18,"<p>Can we safely assume that both the web api project and the microservices project are Stateless Services?</p>&#xA;&#xA;<p>Can't you create DTO's in the interface project and have the microservices project return them?</p>&#xA;&#xA;<p>But I do wonder, if the api is just a gateway to a stateless service, why bother with the stateless service at all? I would put that logic in the api. I mean, what is the benefit otherwise?</p>&#xA;&#xA;<p>Or put an http endpoint on the microservices service and remove the remoting communication listener. That way your frontend can directly talk to the service and you can remove the api controllers (the microservice effectively becomes your api). </p>&#xA;"
44243798,44241320,932728,2017-05-29T13:32:47,"<p>Short answer: no, only local development clusters can.</p>&#xA;&#xA;<p>When running on Azure it comes with Windows Server by default. (I think it is 2012 R2)</p>&#xA;&#xA;<p>When running on-premises these are the requirements (<a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-cluster-standalone-deployment-preparation"" rel=""nofollow noreferrer"">source</a>)</p>&#xA;&#xA;<blockquote>&#xA;  <p>Step 2: Prepare the machines to meet the prerequisites&#xA;  Prerequisites for each machine that you want to add to the cluster:</p>&#xA;  &#xA;  <ul>&#xA;  <li>A minimum of 16 GB of RAM is recommended.</li>&#xA;  <li>A minimum of 40 of GB available disk space is recommended.</li>&#xA;  <li>A 4 core or greater CPU is recommended.</li>&#xA;  <li>Connectivity to a secure network or networks for all machines.</li>&#xA;  <li><strong>Windows Server 2012 R2 or Windows Server 2016.</strong></li>&#xA;  <li>.NET Framework 4.5.1 or higher, full install.</li>&#xA;  <li>Windows PowerShell 3.0.</li>&#xA;  <li>The RemoteRegistry service should be running on all the machines.</li>&#xA;  </ul>&#xA;</blockquote>&#xA;"
40139417,39615381,932728,2016-10-19T18:54:56,"<p>I think this page compares it well: <a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-cloud-services-migration-differences/"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-cloud-services-migration-differences/</a></p>&#xA;&#xA;<p>I can't tell it better than this.</p>&#xA;&#xA;<p>There is not really a rule of thumb. Service Fabric might seem more complex but offers some things that Cloud Services / Web Apps don't.</p>&#xA;&#xA;<p>A quick summary (taken from the link provided):</p>&#xA;&#xA;<p>Service Fabric itself is an application platform layer that runs on Windows or Linux, whereas Cloud Services is a system for deploying Azure-managed VMs with workloads attached. The Service Fabric application model has a number of advantages:</p>&#xA;&#xA;<ul>&#xA;<li>Fast deployment times. Creating VM instances can be time consuming. In Service Fabric, VMs are only deployed once to form a cluster that hosts the Service Fabric application platform. From that point on, application packages can be deployed to the cluster very quickly.</li>&#xA;<li>High-density hosting. In Cloud Services, a Worker Role VM hosts one workload. In Service Fabric, applications are separate from the VMs that run them, meaning you can deploy a large number of applications to a small number of VMs, which can lower overall cost for larger deployments.</li>&#xA;<li>The Service Fabric platform can run anywhere that has Windows Server or Linux machines, whether it's Azure or on-premises. The platform provides an abstraction layer over the underlying infrastructure so your application can run on different environments. </li>&#xA;<li>Distributed application management. Service Fabric is a platform that not only hosts distributed applications, but also helps manage their lifecycle independently of the hosting VM or machine lifecycle.</li>&#xA;</ul>&#xA;"
44667929,44655441,2575224,2017-06-21T06:00:55,"<p>You could try the <a href=""http://cqrs.nu/"" rel=""nofollow noreferrer"">CQRS</a> architecture.</p>&#xA;&#xA;<p>In CQRS every command has exactly one handler; as components you need to have a <code>CommandDispatcher</code> that uses a <code>CommandSubscriber</code> to dispatch every command to the right handler. After each command is executed, one or more events are generated; every event is also dispatched to one or more event handlers.</p>&#xA;&#xA;<p>If every message has the same class, <code>Message</code>, then your <code>CommandSubscriber</code> should register command handlers based on the <code>message.EventCode</code>. If every message has a different class (i.e. <code>Message</code> is an <code>interface</code>), then <code>CommandSubscriber</code> should register command handlers based on the message class. This architecture style respects the <code>Open-close principle</code> whereas your switch-based style does not.</p>&#xA;"
47107482,47089196,2575224,2017-11-04T04:47:07,"<p>What you need are <a href=""http://basho.com/posts/technical/vector-clocks-revisited/"" rel=""nofollow noreferrer"">Version clocks</a>, an old idea that can be used to merge paralel data modifications and to detect conflicts.</p>&#xA;&#xA;<blockquote>&#xA;  <p>It's possible to solve this problem by storing the whole history of variable modifications, but it's a huge data overhead which I'd like to avoid.</p>&#xA;</blockquote>&#xA;&#xA;<p>With vector clocks you don't keep the entire history, but a counter for each <code>variable</code> and node (so each <code>variable</code> has a vector of counters).</p>&#xA;"
47653516,47650139,2575224,2017-12-05T12:21:06,"<blockquote>&#xA;  <p>FindandModify() along with progress field. So we will call this function with query of progress field to be ""0"". We will update the value to 1 so other services can not access it. On getting the success from Rest call we can delete the record. On getting the failure we will again call the FindandModify() with update value to be ""0"" so other service can access at later time.</p>&#xA;</blockquote>&#xA;&#xA;<p>This solution is OK and is simpler and faster than the second.</p>&#xA;&#xA;<blockquote>&#xA;  <p>We we call Find() function which will give us one document. We get the ""_id"" of the document and store it into another collection. If another service also gets the same document and that document ""_id"" is already present. Then it will not be insert again and that service again call Find() function.</p>&#xA;</blockquote>&#xA;&#xA;<p>This solution (as you presented) would not work unless the document is removed from the first collection after it is inserted in the second, otherwise the processes will be caught in an infinite loop. The drawback is that this solution implies two collections and the document moving process.</p>&#xA;&#xA;<p>In any case, you must take into account the failure of the processes in any of the phases and you must be able to detect failure and recover from it.</p>&#xA;"
47671588,47656406,2575224,2017-12-06T10:05:43,"<blockquote>&#xA;  <p>Now, my question is, do these individual classes still qualify/classify as domain objects or should they be referred to as services</p>&#xA;</blockquote>&#xA;&#xA;<p>From the DDD point of view, in the Domain layer, there are the following terms that could be implemented using classes: <code>Domain entities</code>, <code>Aggregate roots</code> (a type of <code>Domain entity</code>), <code>Value objects</code> and <code>Domain services</code>.</p>&#xA;&#xA;<p>Because your <em>things</em> don't have an <code>Identity</code> they cannot be <code>Domain entities</code> or <code>Aggregate roots</code>. Calculations could be done inside <code>Value objects</code> or <code>Domain services</code>. <code>Value objects</code> contain specific behavior related to values so most probable <strong>your calculations are implemented using <code>Domain services</code></strong>.</p>&#xA;&#xA;<p>From my experience, not every domain class must be a DDD building block, they could be just classes extracted for better maintainability, Single responsibility principle (SOLID in general) etc.</p>&#xA;"
47556096,47554214,2575224,2017-11-29T15:17:12,"<p>Your solutions seem OK but there are some things that need to be clarified:</p>&#xA;&#xA;<p>In DDD, aggregates are consistencies boundaries. An Aggregate is always in a consistent state, no matter what command it receives and if that command succeeds or not. But this does not mean that the whole system is in a permitted permanent state from the business point of view. There are moments when the system as whole is in a not-permitted state. This is OK as long as <em>eventually</em> it will transition in a permitted state. Here comes the <strong><a href=""https://msdn.microsoft.com/en-us/library/jj591569.aspx"" rel=""nofollow noreferrer"">Saga/Process managers</a></strong>. This is exactly their role: to bring the system in a valid state. They could be deployed as separate microservices.</p>&#xA;&#xA;<p>One other type of component/pattern that I used in my CQRS projects are <strong>Eventually-consistent command validators</strong>. They validate a command (and reject it if it is not <em>valid</em>) before it reaches the Aggregate using a private read-model. These components minimize the situations when the system enters an invalid state and they complement the Sagas. They should be deployed inside the microservice that contains the Aggregate, as a layer on top of the domain layer (aggregate).</p>&#xA;&#xA;<p>Now, back to Earth. Your solutions are a combination of Aggregates, Sagas and Eventually-consistent command validations.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Solution 1</p>&#xA;  &#xA;  <ul>&#xA;  <li>Microservice A only allows linking A to B if it has previously received a ""B created"" event and no ""B deleted"" event.</li>&#xA;  <li>Microservice A listens to ""B deleted"" events and, upon receiving such an event, unlinks A from B.</li>&#xA;  </ul>&#xA;</blockquote>&#xA;&#xA;<p>In this architecture, Microservice A contains <code>Aggregate A</code> and a <code>Command validator</code> and Microservice B contains <code>Aggregate B</code> and a <code>Saga</code>. Here is important to understand that the validator would not prevent the system's invalid state but only would reduce the probability.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Solution 2:</p>&#xA;  &#xA;  <ul>&#xA;  <li>Microservice A always allows linking A to B.</li>&#xA;  <li>Microservice B listens for ""A linked to B"" events and, upon receiving such an event, verifies that B exists. If it doesn't, it&#xA;  emits a ""link to B refused"" event.</li>&#xA;  <li>Microservice A listens for ""B deleted"" and ""link to B refused"" events and, upon receiving such an event, unlinks A from B.</li>&#xA;  </ul>&#xA;</blockquote>&#xA;&#xA;<p>In this architecture, Microservice A contains <code>Aggregate A</code> and a Saga and <code>Microservice B</code> contains <code>Aggregate B</code> and also a Saga. This solution could be simplified if the Saga on B would verify the <em>existence</em> of B and send <code>Unlink B from A</code> command to A instead of yielding an event.</p>&#xA;&#xA;<p>In any case, in order to apply the <a href=""https://en.wikipedia.org/wiki/Single_responsibility_principle"" rel=""nofollow noreferrer"">SRP</a>, you could extract the Sagas to their own microservices. In this case you would have a microservice per Aggregate and per Saga.</p>&#xA;"
48485325,48483491,2575224,2018-01-28T10:28:13,"<p>If this is not provided by your environment (i.e. AWS Lambda) then you probably have to do it yourself.</p>&#xA;&#xA;<p>For this you need a method of programatically scaling up/down the microservices (i.e. <code>docker service scale xyz=2</code>) and a meaning of determining that a service needs scaling up/down. For this you need to be able to <em>read the relevant metrics</em> from the microservice and a <em>scaling controller</em> that use those metrics to compute the scaling reguirements. For example, if the CPU usage is at least 90% for at least 5 seconds then scale up, if the CPU is less than 10% for at least 5 seconds then scale down.</p>&#xA;&#xA;<p>You can even design the microservice to report its own metrics to the controller for more business specific metrics.</p>&#xA;"
48549725,48540114,2575224,2018-01-31T19:10:16,"<blockquote>&#xA;  <p>Ps: keep in mind that i might face a large trafic .</p>&#xA;</blockquote>&#xA;&#xA;<p>I will base my answer on this.</p>&#xA;&#xA;<p>First of all, you should move <code>Favorite Restaurants</code> to a dedicated microservice and let the API Gateway do routing and cross concerns only (authentication, authorization, SSL termination etc).</p>&#xA;&#xA;<p>Second, you could split the concerns in Read and Write concerns in the Favorite Restaurants bounded context - use CQRS. </p>&#xA;&#xA;<p>So, adding/removing a favorite restaurant could be handled by a microservice that publishes the <code>FavoriteRestaurantAdded(UserId, RestaurantId)</code> and <code>FavoriteRestaurantRemoved(UserId, RestaurantId)</code> domain events.</p>&#xA;&#xA;<p>Another microservice could be responsible with maintaining the list of favorite restaurants for each user in a fully denormalized way: the list contains the ID and the title of the Restaurant (and other needed properties) - a Read model in CQRS. The microservice is subscribed to Restaurant related domain events like <code>RestaurantRenamed</code> or <code>RestaurantRemoved</code> and updates the Favorite restaurants accordingly (for example, it removes the Favorite restaurant when the <code>RestaurantRemoved</code> event is received). This microservice can be made as fast as possible (i.e. by using sharding or indexes and by not using any <em>joins</em>) and can be scaled independently.</p>&#xA;&#xA;<p>On important note, the Read microservice is eventually consistent with the other microservices (the Favorite restaurant adding/removing microservice and Restaurant management microservice). You should keep this in mind when you design the UI; or, you could make the API Gateway to wait for the Read model to be updated.</p>&#xA;&#xA;<p>This architecture is more complex but it gives you better separation of concerns and as fast-as-light, linear-scalable queries.</p>&#xA;"
48474073,48473966,2575224,2018-01-27T08:39:13,"<p>You said that this responsibility belongs to a microservice. So, the other microservices don't check permissions, they delegate. </p>&#xA;&#xA;<p>If you use an API Gateway and the other microservices are not accessible from the outside then it calls the authentication/authorisation microservice before forwarding the request to the upstream microservice.</p>&#xA;&#xA;<p>If you don't use an API Gateway then each microservice call the authentication/authorisation microservice before actually performing the action.</p>&#xA;"
48459864,48458627,2575224,2018-01-26T10:48:17,"<p>You could use <a href=""http://cqrs.nu/"" rel=""nofollow noreferrer"">CQRS</a>. In this low level architecture, the model use for writing data is split from the model use to read/query data. The write model is the canonical source of information, is the source of truth. </p>&#xA;&#xA;<p>The write model publishes events that are interpreted/projected by one or more read models, in an eventually consistent manner. Those events could be even published in a message queue and consumed by external read models (other microservices). There is no 1:1 mapping from write to read. You can have 1 model for write and 3 models for read. Each read model is optimized for its use-case. This is the part that interests you: an speed-optimized read model.</p>&#xA;&#xA;<p>An optimized read model has every thing it needs when it answers the queries. The data is fully denormalized (this means it needs no joins) and already indexed. </p>&#xA;&#xA;<p>A read model can have its data sharded. You do this in order to minimize the collection size (a small collection is faster than a bigger one). In your case, you could shard by user: each user would have its own collection of statistics (i.e. a table in SQL or a document collection in NoSQL). You can use the build-in sharding of the database or you could shard it manually, by splitting in separate collections (tables).</p>&#xA;&#xA;<blockquote>&#xA;  <p>Services doesn't have all the data. </p>&#xA;</blockquote>&#xA;&#xA;<p>A read model could subscribe to many sources of truth (i.e. microservices or event streams).</p>&#xA;&#xA;<p>One particular case that works very well with CQRS is Event sourcing; it has the advantage that you have the events from the begging of time, without the need to store them in a persistent message queue.</p>&#xA;&#xA;<p><em>P.S. I could not think about a use-case when a read model could not be made fast enough, given enough hardware resources.</em></p>&#xA;"
49423978,49421760,2575224,2018-03-22T08:39:29,<p><code>PHP</code> is not very good at long running services. The <em>best</em> way that I found so far is to use a <code>cron</code> job that checks if there are new messages to be processed and process them if they are.</p>&#xA;
43376051,43374386,2575224,2017-04-12T17:31:16,"<blockquote>&#xA;  <p>Is this a reasonable way to process HTTP requests that are dependent upon a second level of HTTP requests? Are there situations in which one would be preferred over the other?</p>&#xA;</blockquote>&#xA;&#xA;<p>It really depends on the system requirements, it depends on how you want to behave in case of failure of some of its components or under varying workload.</p>&#xA;&#xA;<p>If you want your system to be reactive or scalable you should use asynchronous requests whenever possible. For this your system should be message driven. You could read more about <a href=""http://www.reactivemanifesto.org/"" rel=""nofollow noreferrer"">reactive system here</a>. This seems like your first option.</p>&#xA;&#xA;<p>If you want a simpler system then use synchronous/blocking requests, like your option no. 2</p>&#xA;"
43384670,43383125,2575224,2017-04-13T06:06:26,"<p>Splitting a monolith into multiple microservices should result in multiple applications that communicate over the network. So, the microservice's internals are opaque from the other microservices point of view. In this case, the shared/common code should be copy/pasted to any microservice that uses it. This is necessary as it <strong>decouples</strong> the microservices from each other so they can <strong>evolve independently</strong>, one of the desired benefits of using a microservices architecture. As an example, in the future you could change the programming language from <code>Java</code> to <code>JavaScript</code> or <code>PHP</code> and <em>nobody</em> will notice.</p>&#xA;&#xA;<p>Read more about this <a href=""https://www.nginx.com/wp-content/uploads/2015/01/Building_Microservices_Nginx.pdf"" rel=""nofollow noreferrer"">here</a>, at page 33, ""DRY and the Perils of Code Reuse in a Microservice World"".</p>&#xA;"
43427397,43426699,2575224,2017-04-15T14:46:39,"<p>If you share the same database then you loose two of the most important advantages of microservices: <a href=""https://www.nginx.com/wp-content/uploads/2015/01/Building_Microservices_Nginx.pdf"" rel=""noreferrer"">strong cohesion and loose coupling (page 25)</a>.</p>&#xA;&#xA;<p>You can share the same database if you don't share the tables in it. For example, <code>microservice1</code> uses <code>table1_1</code> and <code>table_1_2</code> and <code>microservice2</code> uses <code>table2_1</code> and <code>table2_2</code>. When I say uses I mean read and write. One microservice don't read and don't write on the other's tables.</p>&#xA;"
43493883,43492974,2575224,2017-04-19T11:02:06,"<p>Try not to use IP addresses inside <code>nginx</code> config file. &#xA;Also, you should use the same port number for both services: 8080 (if this is the port that nodejs application is listening).</p>&#xA;&#xA;<p>Then you should properly define your routes to each service using <code>location</code> in each <code>server</code> context.</p>&#xA;&#xA;<p>So, you should modify <code>/etc/nginx/conf.d/default.conf</code> inside <code>nginx</code> container like this:</p>&#xA;&#xA;<pre><code># service1.local&#xA;upstream service1.local {&#xA;            ## Can be connect with ""nginxproxy_default"" network&#xA;            # nginxproxy_service1_1&#xA;            server service1:8080;&#xA;}&#xA;&#xA;server {&#xA;    server_name service1.local;&#xA;    listen 80 ;&#xA;    access_log /var/log/nginx/access.log vhost;&#xA;    location /service1 { #note this line&#xA;        proxy_pass http://service1.local;&#xA;    }&#xA;}&#xA;&#xA;# service2.local&#xA;upstream service2.local {&#xA;            ## Can be connect with ""nginxproxy_default"" network&#xA;            # nginxproxy_service2_1&#xA;            server service2:8080; #same port&#xA;}&#xA;&#xA;server {&#xA;    server_name service2.local;&#xA;    listen 80 ;&#xA;    access_log /var/log/nginx/access.log vhost;&#xA;    location /service2 { #note this line&#xA;        proxy_pass http://service2.local;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
43398704,43396744,2575224,2017-04-13T17:49:42,"<p>So, a <code>React component</code> needs two things: the <code>JavaScript</code> source code and the data.</p>&#xA;&#xA;<p>The JavaScript source code can be provided by a CDN.</p>&#xA;&#xA;<p>The data must be provided by the Microservices.</p>&#xA;&#xA;<p>If you don't want server side rendering, then the skeleton index.html file along with the <code>JS</code> files are provided by a CDN.</p>&#xA;&#xA;<p>If you need server side rendering (for SEO purposes, for example) then the API gateway (or another Web server) will render the components using <code>NodeJS</code> by requesting their source code from the CDN and their data from the microservices then return the full <code>HTML</code> to the browser.</p>&#xA;&#xA;<p>On the client side, <code>React</code> will continue to load other data from the right microservice as <code>JSON</code> using the <code>API gateway</code>.</p>&#xA;"
49064012,49062631,2575224,2018-03-02T06:43:40,"<p>In order to be posibile that the response from any of the service B instances to be handled by any of the service A instances then the response from B must contain all the information necessary to identify the request from A and resume the process, i.e. an Entity  ID, Processs ID or something that most probable is opaque to B.</p>&#xA;&#xA;<p>Also, this means that A1 (or whatever instance sends the request message to B) should not block or wait for the response from B.</p>&#xA;"
49176567,49165457,2575224,2018-03-08T15:20:25,"<p>The idea with Serverless is that you reduce server operational costs by <strong>not having servers</strong>. </p>&#xA;&#xA;<p>If you provide your own servers then Serverless is pointless, you have the worst of both worlds.</p>&#xA;"
46583333,46581254,2575224,2017-10-05T10:23:49,"<p>It depends on how granular you want your architecture to be. Theoretically, for maximum granularity and according to Single Responsibility Principle you should make a microservice for each Aggregate, in each bounded context. This means that every command should have an endpoint (I assume that each endpoint is reached at a single URI, i.e. <code>https://server/place/order</code>).</p>&#xA;&#xA;<p><strong>If you use a CQRS architecture</strong> then for the read/query side, you could also have a microservice for each read model; in this way you can scale independently each read model (using DB replication or entire microservice instance replication). </p>&#xA;"
46616980,46615008,2575224,2017-10-07T05:28:17,"<blockquote>&#xA;  <p>From what I understand, these patterns are useful if microservices communicate via HTTP.</p>&#xA;</blockquote>&#xA;&#xA;<p>It is irrelevant that the communication is HTTP. The circuit breaker is useful in prevention of cascade failures that are more probable to occur in the architectures that use a <strong>synchronous</strong> communication style.&#xA;Event-driven architectures are in general <strong>asynchronous</strong> so cascade failure is less probable to occur.</p>&#xA;&#xA;<p>Service discovery is used in order for the microservices to discover each other but in Event-driven architectures microservices communicate only to the messaging infrastructure (i.e. the Event store in Event sourcing) so discoverability could be used only at the infrastructure level.</p>&#xA;"
44804114,44802594,2575224,2017-06-28T13:36:55,"<p>There are two problems that need to be fixed:</p>&#xA;&#xA;<ol>&#xA;<li><p>the two services do not belong to the same network so you need to add the <code>rabbitmq</code> service to the <code>webnet</code> network or create a new network for the two services</p></li>&#xA;<li><p><code>rabbitmq</code> may take some time to become fully available (i.e. to listen to the <code>5672</code> port) so you need to make the <code>service1</code> service to wait for the <code>rabbitmq</code> service; see <a href=""https://stackoverflow.com/questions/31746182/docker-compose-wait-for-container-x-before-starting-y"">this question</a> about this issue.</p></li>&#xA;</ol>&#xA;"
44840111,44839692,2575224,2017-06-30T06:38:12,"<blockquote>&#xA;  <p>My understanding is that individual microservices don't interact with each other directly if there is an orchestrator. Is my understanding correct?</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes, you are correct.</p>&#xA;&#xA;<p>In orchestration, by definition, there is a central brain that do the all the communication between microservices. The idea is that individual microservices do not know of each other, so how can they interact with each other?</p>&#xA;&#xA;<p>For more information you can read <a href=""https://www.amazon.co.uk/d/Books/Building-Microservices-Sam-Newman/1491950358"" rel=""nofollow noreferrer"">this book</a>, page 43.</p>&#xA;"
44692113,44690557,2575224,2017-06-22T06:49:27,"<blockquote>&#xA;  <p>Front end server takes these requests and then makes a GET, POST, PUT request to the backend API.</p>&#xA;</blockquote>&#xA;&#xA;<p>The pattern that you describe is called <a href=""http://microservices.io/patterns/apigateway.html"" rel=""nofollow noreferrer"">API Gateway</a> and it has the following characteristics:</p>&#xA;&#xA;<p><strong>Benefits:</strong></p>&#xA;&#xA;<ul>&#xA;<li>Insulates the clients from how the application is partitioned into microservices</li>&#xA;<li>Insulates the clients from the problem of determining the locations of service instances</li>&#xA;<li>Provides the optimal API for each client</li>&#xA;<li>Reduces the number of requests/roundtrips. For example, the API gateway enables clients to retrieve data from multiple services with a single round-trip. Fewer requests also means less overhead and improves the user experience. An API gateway is essential for mobile applications.</li>&#xA;<li>Simplifies the client by moving logic for calling multiple services from the client to API gateway</li>&#xA;<li>Translates from a “standard” public web-friendly API protocol to whatever protocols are used internally</li>&#xA;</ul>&#xA;&#xA;<p><strong>Drawbacks:</strong></p>&#xA;&#xA;<ul>&#xA;<li>Increased complexity - the API gateway is yet another moving part that must be developed, deployed and managed</li>&#xA;<li>Increased response time due to the additional network hop through the API gateway - however, for most applications the cost of an extra roundtrip is insignificant.</li>&#xA;</ul>&#xA;&#xA;<p><strong>Conclusion:</strong> if you don't need the advantages that the API Gateway provides then you should not use it.</p>&#xA;"
45005940,45002471,2575224,2017-07-10T07:19:32,"<p>Docker (17.03) is a great tool to secure isolate processes. It uses <strong>Kernel namespaces</strong>, <strong>Control groups</strong> and some <strong>kernel capabilities</strong> in order to isolate processes that run in different containers.</p>&#xA;&#xA;<p>But, those processes are not 100% isolated from each other because they <strong>use the same kernel</strong> resources. Every dockerize process that make an IO call will leave for that period of time its isolated environment and will enter a shared environment, the kernel. Although you can set limits per container, like how much processor or how much RAM it may use you cannot set limits on <em>all</em> kernel resources.</p>&#xA;&#xA;<p>You can read <a href=""https://www.oreilly.com/ideas/docker-security"" rel=""nofollow noreferrer"">this article</a> for more information.</p>&#xA;"
45072914,45072041,2575224,2017-07-13T06:26:47,"<p>A hacker cannot create a valid <code>JWT token</code> if he does not know the the signing key. If he somehow manages to get that signing key it is reasonable to assume that he is able to get your ""secret phrase"" also.</p>&#xA;&#xA;<p>About the checking: <code>JWT tokens</code> can be checked by the <code>API service</code> as they contain all the information needed (except the signing key that must be known by the API service). The expiration can be checked here also. Anyway, you also need the information stored inside the token, like user ID. You should do this if you want better scalability.</p>&#xA;&#xA;<p>The only reason why you would need to check a <code>JWT token</code> against a third <code>Auth service</code> is to see if it has been invalidated; for this you need a central service although you could replicate the list of invalid tokens to all the <code>API services</code> for better resilience.</p>&#xA;"
45096226,45095183,2575224,2017-07-14T06:34:40,"<p>I think the term ""Orchestration"" is not good for what you are asking. From what I've encountered so far in microservices world the term ""Orchestration"" is used when a complex business process is involved and not for service discovery. What you need is a <a href=""http://microservices.io/patterns/service-registry.html"" rel=""nofollow noreferrer"">Service registry</a> combined with a <code>Load balancer</code>. You can find <a href=""https://www.nginx.com/blog/service-discovery-in-a-microservices-architecture/"" rel=""nofollow noreferrer"">here</a> all the information you need. Here are some relevant extras that great article:</p>&#xA;&#xA;<p>There are two main service discovery patterns: client‑side discovery and server‑side discovery. Let’s first look at client‑side discovery.</p>&#xA;&#xA;<p><strong>The Client‑Side Discovery Pattern</strong></p>&#xA;&#xA;<p>When using client‑side discovery, the client is responsible for determining the network locations of available service instances and load balancing requests across them. The client queries a service registry, which is a database of available service instances. The client then uses a load‑balancing algorithm to select one of the available service instances and makes a request.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/C0JhN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/C0JhN.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>The network location of a service instance is registered with the service registry when it starts up. It is removed from the service registry when the instance terminates. The service instance’s registration is typically refreshed periodically using a heartbeat mechanism.</p>&#xA;&#xA;<p><a href=""https://netflix.github.io/"" rel=""nofollow noreferrer"">Netflix OSS</a> provides a great example of the client‑side discovery pattern. <a href=""https://github.com/Netflix/eureka"" rel=""nofollow noreferrer"">Netflix Eureka</a> is a service registry. It provides a REST API for managing service‑instance registration and for querying available instances. <a href=""https://github.com/Netflix/ribbon"" rel=""nofollow noreferrer"">Netflix Ribbon</a> is an IPC client that works with Eureka to load balance requests across the available service instances. We will discuss Eureka in more depth later in this article.</p>&#xA;&#xA;<p>The client‑side discovery pattern has a variety of benefits and drawbacks. This pattern is relatively straightforward and, except for the service registry, there are no other moving parts. Also, since the client knows about the available services instances, it can make intelligent, application‑specific load‑balancing decisions such as using hashing consistently. One significant drawback of this pattern is that it couples the client with the service registry. You must implement client‑side service discovery logic for each programming language and framework used by your service clients.</p>&#xA;&#xA;<p><strong>The Server‑Side Discovery Pattern</strong></p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/rQsut.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rQsut.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>The client makes a request to a service via a load balancer. The load balancer queries the service registry and routes each request to an available service instance. As with client‑side discovery, service instances are registered and deregistered with the service registry.</p>&#xA;&#xA;<p>The <a href=""https://aws.amazon.com/elasticloadbalancing/"" rel=""nofollow noreferrer"">AWS Elastic Load Balancer</a> (ELB) is an example of a server-side discovery router. An ELB is commonly used to load balance external traffic from the Internet. However, you can also use an ELB to load balance traffic that is internal to a virtual private cloud (VPC). A client makes requests (HTTP or TCP) via the ELB using its DNS name. The ELB load balances the traffic among a set of registered Elastic Compute Cloud (EC2) instances or EC2 Container Service (ECS) containers. There isn’t a separate service registry. Instead, EC2 instances and ECS containers are registered with the ELB itself.</p>&#xA;&#xA;<p>HTTP servers and load balancers such as <a href=""https://www.nginx.com/products/"" rel=""nofollow noreferrer"">NGINX Plus</a> and NGINX can also be used as a server-side discovery load balancer. For example, <a href=""https://www.airpair.com/scalable-architecture-with-docker-consul-and-nginx"" rel=""nofollow noreferrer"">this blog</a> post describes using <a href=""https://github.com/hashicorp/consul-template"" rel=""nofollow noreferrer"">Consul Template</a> to dynamically reconfigure NGINX reverse proxying. Consul Template is a tool that periodically regenerates arbitrary configuration files from configuration data stored in the <a href=""https://www.consul.io/"" rel=""nofollow noreferrer"">Consul service registry</a>. It runs an arbitrary shell command whenever the files change. In the example described by the blog post, Consul Template generates an nginx.conf file, which configures the reverse proxying, and then runs a command that tells NGINX to reload the configuration. A more sophisticated implementation could dynamically reconfigure NGINX Plus using either <a href=""https://www.nginx.com/products/on-the-fly-reconfiguration/"" rel=""nofollow noreferrer"">its HTTP API or DNS</a>.</p>&#xA;&#xA;<p>Some deployment environments such as Kubernetes and Marathon run a proxy on each host in the cluster. The proxy plays the role of a server‑side discovery load balancer. In order to make a request to a service, a client routes the request via the proxy using the host’s IP address and the service’s assigned port. The proxy then transparently forwards the request to an available service instance running somewhere in the cluster.</p>&#xA;&#xA;<p>The server‑side discovery pattern has several benefits and drawbacks. One great benefit of this pattern is that details of discovery are abstracted away from the client. Clients simply make requests to the load balancer. This eliminates the need to implement discovery logic for each programming language and framework used by your service clients. Also, as mentioned above, some deployment environments provide this functionality for free. This pattern also has some drawbacks, however. Unless the load balancer is provided by the deployment environment, it is yet another highly available system component that you need to set up and manage.</p>&#xA;"
44941512,44936115,2575224,2017-07-06T06:41:40,"<p>So, you have three domains:</p>&#xA;&#xA;<ol>&#xA;<li>Authentication: responsible for identifying the user</li>&#xA;<li>Authorization: responsible for restricting access to resources</li>&#xA;<li>Todos: your core domain</li>&#xA;</ol>&#xA;&#xA;<p>You have done well identifying three <a href=""https://martinfowler.com/bliki/BoundedContext.html"" rel=""nofollow noreferrer"">bounded contexts</a>, one for each domain and implemented in three microservices (MS). You are conforming to the best practices regarding <a href=""https://en.wikipedia.org/wiki/Domain-driven_design"" rel=""nofollow noreferrer"">DDD</a>.</p>&#xA;&#xA;<p>No, your question is how could you integrate those three microservices in such a way that the system is <a href=""http://www.reactivemanifesto.org/"" rel=""nofollow noreferrer"">resilient</a>, i.e. its microservices continue to work even if some of the other microservices fail.</p>&#xA;&#xA;<p>You have two options regarding integration (communication between microservices):</p>&#xA;&#xA;<ol>&#xA;<li><p>Synchronous communication: every time the Todos MS receive a request, it queries the Authorization MS to check if the user is permitted to do what it wants. This has the advantage that is simple to implement but it has the disadvantage that is susceptible to cascade failure: if the Authorization MS fails then this MS also fails. So, this option is not good for you.</p></li>&#xA;<li><p>Asynchronous communication: somehow in the background, there is some data that is replicated from the Authorization MS to the Todos MS. You have at least two options for this replication to be done: a) in <a href=""https://en.wikipedia.org/wiki/Cron"" rel=""nofollow noreferrer"">cron</a> or <a href=""https://en.wikipedia.org/wiki/Windows_Task_Scheduler"" rel=""nofollow noreferrer"">scheduled tasks</a> or similar and b) using a <a href=""https://en.wikipedia.org/wiki/Event-driven_architecture"" rel=""nofollow noreferrer"">event driven architecture</a>. This has the advantage that provides more resilience but it has the disadvantage that is more complex, harder to implement. <strong>This option seems to fit your need</strong>.</p></li>&#xA;</ol>&#xA;&#xA;<p>Further reading:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://www.nginx.com/wp-content/uploads/2015/01/Building_Microservices_Nginx.pdf"" rel=""nofollow noreferrer"">Building microservices</a></li>&#xA;</ul>&#xA;"
45114036,45111662,2575224,2017-07-15T02:59:46,"<p>It is not recommended to share a database or a table between microservices. They should have distinct, well defined responsibilities and should communicate only using the network; the protocol must hide the technology used inside a microservice: for example you can use JSON for request/responses.</p>&#xA;&#xA;<p>The reason you do this is that a <strong>microservice should not depend on the tehnology of another microservice</strong> as microservices should be easily replaced with other microservices that use other technology stack but fulfill the same purpose.</p>&#xA;&#xA;<p>If you need data from one microservice in another you can make a:</p>&#xA;&#xA;<ul>&#xA;<li><p>synchronous call: this is easier to implement but is susceptible to cascade failure</p></li>&#xA;<li><p>asynchronous call: harder to implement but leads to a more resilient system</p></li>&#xA;</ul>&#xA;"
44929611,44923182,2575224,2017-07-05T14:51:56,"<p>You can create a new service for each <code>account ID</code> and pass that information as an environment variable. Here is an example:</p>&#xA;&#xA;<pre><code>docker service create --name service-for-account-123 --env accountId=123 imagename:latest &#xA;</code></pre>&#xA;&#xA;<p>To stop the containers of this service you can scale it to zero, like this:</p>&#xA;&#xA;<pre><code>docker service scale service-for-account-123=0&#xA;</code></pre>&#xA;&#xA;<p>To remove all containers you must remove the service:</p>&#xA;&#xA;<pre><code>docker service rm service-for-account-123&#xA;</code></pre>&#xA;"
45073247,45058457,2575224,2017-07-13T06:46:34,"<p>I use <a href=""https://docs.docker.com/engine/swarm/"" rel=""nofollow noreferrer"">Docker swarm</a> and this gives me a cloud-like environment where I can upscale or downscale a service very easily.</p>&#xA;&#xA;<p>You can read here about using <a href=""https://spring.io/guides/gs/spring-boot-docker/"" rel=""nofollow noreferrer"">Docker with Spring Boot</a>. There is also <a href=""https://www.3pillarglobal.com/insights/building-a-microservice-architecture-with-spring-boot-and-docker-part-i"" rel=""nofollow noreferrer"">this article</a>. </p>&#xA;&#xA;<p>P.S. Docker and Docker swarm are free or charge (the community editions).</p>&#xA;"
43246936,43246560,2575224,2017-04-06T06:05:04,"<blockquote>&#xA;  <p>TL;DR Should a service opt for saving data in its local database that it needs occasionally, or request the data every time from the service that the data originated from?</p>&#xA;</blockquote>&#xA;&#xA;<p>I hate to say this but it depends. It depends on your business requirements. It depends on if you want to have a rezilient system or not. How do you want <code>service A</code> to behave if <code>service B</code> is not available? You have two choises:</p>&#xA;&#xA;<ol>&#xA;<li><p>You want <code>Service A</code> to refuse working as it can't get the fresh data from <code>service B</code>. You do this if the data changes a lot or the data used in <code>service A</code> must be super-fresh all the time.</p></li>&#xA;<li><p>You want <code>Service A</code> to keep working, possibly by notifying the user that the data might not be fresh. In this case you should duplicate the data from <code>service B</code> to <code>service A</code>, by listening to events or by caching.</p></li>&#xA;</ol>&#xA;"
43255037,43252404,2575224,2017-04-06T12:19:15,"<blockquote>&#xA;  <p>should these apps communicate each other by REST API or different way?</p>&#xA;</blockquote>&#xA;&#xA;<p>Microservices should communicate over network always. If they have a REST API then use that.</p>&#xA;&#xA;<blockquote>&#xA;  <p>if so, is there a way to save some time, by not serializing/ deserializing transfer objects?</p>&#xA;</blockquote>&#xA;&#xA;<p>If they are communicating over network the serialization is a must. Anyway, serialization help the decoupling. Microservices should share data but not schema/classes. The serialization must be done by loosing the schema, i.e. you could use JSON. If you share the schema (classes) you break the microservice's encapsulation. You won't be able to change a microservice implementation with other implementation (that is using a different technology stack, <code>PHP</code> with <code>Nginx</code> for example).</p>&#xA;"
51948150,51944296,2575224,2018-08-21T11:57:25,"<p>You should avoid chaining calls from one microservice to another in order to fulfill a client's request. It doesn't matter if the calls are synchronous or asynchronous.  This can lead to cascade failures so the availability of the system is affected.</p>&#xA;&#xA;<p>Instead, you should gather all the needed data in background (i.e. using <code>cron</code> or events) <em>before</em> the clients' requests. In this way, if service A is down, service B continues to work.</p>&#xA;"
52050305,52041354,2575224,2018-08-28T04:50:09,"<p>If you need resilience and scalability then the <em>best practices</em> says to use asynchronous message based communication between microservices. In your case, one microservice asynchronously sends a <code>RequestValidationOrSomething</code> message to the other one (async means it does not block while waiting for the response). The validating microservice receive the message, perform the validation and sends another message back (success or failure).</p>&#xA;&#xA;<p>If you need a simple solution then one microservice make synchronous calls to the other, similar to local in-process calls.</p>&#xA;"
52070862,52070247,2575224,2018-08-29T06:29:47,"<p>Eureka does not handle communication between the two servers, only the registration. This means that a service uses Eureka only to find out the address of the other service. After that the communication is done directly between the two services, without Eureka.</p>&#xA;&#xA;<p>If Eureka is down, the client uses a cached copy of the remote service's address, which is good for resilience and temporary network failures. </p>&#xA;&#xA;<p>If you think about it a little, why wouldn't it work? If Eureka is down it doesn't mean that the remote service is also down or that it's physical address has changed.</p>&#xA;"
52091387,52090842,2575224,2018-08-30T07:25:39,"<p>In general, the success of a project is dictated not by the efficiency of how the application's components communicate but by how fast the application adapts to new business and technologic requests. This is easier to view in large applications, with a large code base and with possible many teams. Big monoliths are harder to change.</p>&#xA;&#xA;<p>Microservices help by splitting a monolith in components that can evolve separately, with different technologies/frameworks and with different teams and at different speeds. </p>&#xA;&#xA;<p>Microservices could also improve compute efficiency by using the right tool (programming language, framework) for the right job. For example JavaScript is good at asynchronous jobs while it sucks at 3D renderings or human face detection. Or when a new programming language is invented that would work best for some job, one could use it in a microservice without other microservices to know or to care.</p>&#xA;"
52013227,52011686,2575224,2018-08-25T00:57:12,"<p>You should <code>listen</code>  on 80 and 443 inside the container, i.e. <code>options.Listen(IPAddress.Any, 80);</code> because this docker declaration </p>&#xA;&#xA;<pre><code>ports: &#xA;   - ""5000:80""&#xA;</code></pre>&#xA;&#xA;<p>means that the local port 80 (the port from your source code) is exported to external port 5000, and not the other way around.</p>&#xA;"
43841971,43835206,2575224,2017-05-08T07:23:50,"<p>As a general rule, validation should be put as near to client as possible in order to <a href=""https://en.wikipedia.org/wiki/Fail-fast"" rel=""nofollow noreferrer"">fail fast</a> but without compromising the architecture and responsibilities of each component/microservice. </p>&#xA;&#xA;<p>That being said, you could put authentication in the API Gateway but <em>input validation is specific to each microservice</em> and I don't think that you can centralize that into the API Gateway. Also, there are too many types of validation that can exists to give an accurate answer about them but you could try to apply the rule in the first paragraph as much as you can.</p>&#xA;"
43923849,43910795,2575224,2017-05-11T19:13:30,"<p>Considering that the IDs of the newly created <code>project</code> entity is not known at the request time (i.e. it is generated after the insertion into the database) you indeed cannot generate the url to the <code>project</code> resource.</p>&#xA;&#xA;<p>Instead, you could assign an ID (i.e. <code>1234-abcd-5678-efgh</code>) to the command before sending to the bus and keep track of its execution status on the API gateway itself. Then you can respond to the client with an command execution status endpoint like <code>/commands/1234-abcd-5678-efgh</code> where it can query by polling.</p>&#xA;&#xA;<p>The alternative would be to use another service that would reserve&amp;deliver unique IDs but you must make a blocking call to it and this hurts scalability. Or you can host this service inside the API gateway itself (onto the same node) to minimize latency. Also, there is a risk of loosing some IDs in case of project creation failures but this can be compensated by releasing those IDs in those situations (thus increasing the architecture complexity).</p>&#xA;&#xA;<p>A third solution could be the use of a <code>project</code> surogate ID, like a GUID, assigned as a property of the <code>project</code>, included in the command, having the purpose of an alternate  identity that can be used only in the pre-creation phase of the process. Then, the response to the client could be like this: <code>/projects/by-guid/1234-abcd-5678-efgh</code> and after the <code>project</code> is created a <code>GET</code> to this <code>url</code> would permanently redirect to the final project url.</p>&#xA;"
43816774,43814764,2575224,2017-05-06T04:38:48,"<p>An alternative approach to end-to-end testing is <a href=""https://www.martinfowler.com/articles/consumerDrivenContracts.html"" rel=""nofollow noreferrer"">Consumer-Driven Contract</a> (CDC).</p>&#xA;&#xA;<p>Although is useful to have some end-to-end tests, they have some disadvantages like:</p>&#xA;&#xA;<ul>&#xA;<li><p>the consumer service must know how to start the provider service. This sounds like unnecessary information, likely difficult to maintain when the number of services start ramping up;</p></li>&#xA;<li><p>starting up a service can be slow. Even if we’re only talking a few seconds, this is adding overhead to build times. If a consumer depends on multiple services, this all starts adding up;</p></li>&#xA;<li><p>the provider service might depend on a data store or other services to work as expected. It means that now not only the Provider needs to be started but also a few other services, maybe a database.</p></li>&#xA;</ul>&#xA;&#xA;<p>The idea of CDC is described shortly as:</p>&#xA;&#xA;<ol>&#xA;<li>The consumer defines what it expects from a specific request to a service</li>&#xA;<li>The provider and the consumer agree on this contract</li>&#xA;<li>The provider continuously verifies that the contract is fulfilled</li>&#xA;</ol>&#xA;&#xA;<p>This information is taken from <a href=""http://techblog.newsweaver.com/why-should-you-use-consumer-driven-contracts-for-microservices-integration-tests/"" rel=""nofollow noreferrer"">here</a>. Read more on this <a href=""https://specto.io/blog/2016/11/16/spring-cloud-contract/"" rel=""nofollow noreferrer"">article</a>, it can be useful even if it is specific to Java.</p>&#xA;"
48277198,48271960,2575224,2018-01-16T08:47:22,"<p>I will give you a partial answer based on my experience in Event sourcing.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Event ordering (only workaround single topic/partition?)</p>&#xA;  &#xA;  <ol>&#xA;  <li><p>AddItem before AddCategory, invalid category reference.</p></li>&#xA;  <li><p>UpdateItem before AddCategory, used to be a valid reference, now invalid.</p></li>&#xA;  <li>RemoveCategory before AddItem, category reference invalid.</li>&#xA;  <li>....infinite list of other concurrency issues.</li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p>All scalable Event stores that I know of guaranty events ordering inside a partition only. In <a href=""https://en.wikipedia.org/wiki/Domain-driven_design"" rel=""nofollow noreferrer"">DDD</a> terms, the Event store ensure that the Aggregate is rehydrated correctly by replaying the events in the order they were generated. The Apache-kafka topic seems to be a good choice for that. While this is sufficient for the Write side of an application, it is harder for the Read side to use it. Harder but not impossible.</p>&#xA;&#xA;<p>Given that the events are already validated by the Write side (because they represent facts that already happened) we can be sure that any inconsistency that appears in the system is due to the wrong ordering of events. Also, given that the Read side is eventually consistent with the Write side, the missing events will eventually reach our Read models. </p>&#xA;&#xA;<p>So, first thing, in your case <code>AddItem before AddCategory, invalid category reference</code>, should be in fact <code>ItemAdded before CategoryAdded</code> (terms are in the past). </p>&#xA;&#xA;<p>Second, when <code>ItemAdded</code> arrives, you try to load the Category by ID and if it fails (because of the <em>delayed</em> <code>CategoryAdded</code> event) then you can create a <code>NotYetAvailableCategory</code> having the ID equal to the referenced ID in the <code>ItemAdded</code> event and a title of ""Not Yet Available Please Wait a few miliseconds"". Then, when the <code>CategoryAdded</code> event arrives, you just update all the <code>Items</code> that reference that category ID. <strong>So, the main idea is that you create temporary entities that will be finalized when their events eventually arrive</strong>.</p>&#xA;&#xA;<p>In the case of <code>CategoryRemoved before ItemAdded, category reference invalid</code>, when the <code>ItemAdded</code> event arrives, you could check that the category was deleted (by havind a <code>ListOfCategoriesThatWereDeleted</code> read model) and then take the appropriate actions in your <code>Item</code> entity - what depends on you business.</p>&#xA;"
48296715,48294450,2575224,2018-01-17T08:33:51,"<p>Strong consystency is hard in distributed services and even harder with microservices because they own their data. This means that you can have strong consystency only inside a microservice. </p>&#xA;&#xA;<p>However, you could model the critical operations as a complex process using a <a href=""https://msdn.microsoft.com/en-us/library/jj591569.aspx"" rel=""nofollow noreferrer"">Saga/Process manager</a>. This means that you use a Saga to orchestrate the completion of the operation in a manner that is acceptable by your business. For example you could use something like the <a href=""https://www.infoq.com/news/2009/09/reservations"" rel=""nofollow noreferrer"">Reservation pattern</a> </p>&#xA;&#xA;<blockquote>&#xA;  <p>This pattern enables managing the resource allocation process in an&#xA;  orderly manner by implementing a two pass protocol - somewhat similar&#xA;  to a two phase commit. During the first pass, the initiator asks each&#xA;  participant to reserve itself. If the initiator gets an OK from all&#xA;  the involved services - within a timeout - it will start the second&#xA;  pass, confirming reservation to all the participants.</p>&#xA;</blockquote>&#xA;"
48313998,48308464,2575224,2018-01-18T04:45:13,"<p>The simplest solution is to use <code>multipart/form-data</code>. This is supported (as in builin) by all web application servers and almost web clients. It supports multiple arbitrary binary files and other data in the same request.</p>&#xA;&#xA;<p>So my answer is <a href=""https://en.m.wikipedia.org/wiki/KISS_principle"" rel=""nofollow noreferrer"">KISS</a>.</p>&#xA;&#xA;<p><em>P.S. It is not only for forms.</em></p>&#xA;"
48339766,48338676,2575224,2018-01-19T11:09:51,"<p>You could use the following workflow:</p>&#xA;&#xA;<ul>&#xA;<li>prepare the microservice B to push the events to the queue or stop it if it is already pushing to the queue; instead, it <strong>pushes to a circular buffer</strong> (a buffer that is rewritten when full) and waits for a signal from microservice A</li>&#xA;<li>deploy the microservice A into production servers but you don't reference it from nowhere; it just runs, waiting for events in the queue</li>&#xA;<li>run a script that get all product names from microservice B and push them into the queue as a simulated event; when it finishes the product names it signals the microservice B (optionally telling the date or sequence number or whatever de-duplication technique you have to detect duplicate events)</li>&#xA;<li>microservice B then copy the events from the buffer newer that the last pushed by microservice A (or it finds out itself from the queue what is the last one) into the queue and then ignores the buffer and continue to work as normally.</li>&#xA;</ul>&#xA;"
48216116,48209566,2575224,2018-01-11T21:31:19,"<blockquote>&#xA;  <p>I have 2 microservices, A and B. When A receives a request from a user, it gets processed (store some things in the database) and a message is sent to a queue that is read by B.</p>&#xA;</blockquote>&#xA;&#xA;<p>Ok. Let's suppose that something is processed and saved to the database = some side effects were produced.</p>&#xA;&#xA;<blockquote>&#xA;  <p>If the queue is down, my initial thought is to make the entire request fall over and show an error to the user asking to try again later. Is it a bad practice?</p>&#xA;</blockquote>&#xA;&#xA;<p>But you need to undo the first step (the one discussed above) and only then fail the client's request! You have two options:</p>&#xA;&#xA;<ul>&#xA;<li><p>wrap the side effects and the queue adding in a transaction; this works only with undoable side effects (i.e. deleting a file is not undoable unless it is just moved to trash). This solution should be used when the first step must be process synchronously.</p></li>&#xA;<li><p>add also the request to the queue and process it later, first asynchronous step being  microservice A. If the adding to the queue fails then fail the request. This solution is more scalable than the first.</p></li>&#xA;</ul>&#xA;"
48283791,48279479,2575224,2018-01-16T14:42:59,"<p><em>In addition to @VoiceOfUnreason's answer,</em></p>&#xA;&#xA;<p>If the two microservices are RESTFul, the <code>CommandManager</code> could return a <code>202 Accepted</code> with a link pointing to the resource that will be created in the future. The client could then poll that resource until the server responds with a <code>200 OK</code>.</p>&#xA;&#xA;<p>Another solution would be that the <code>CommandManager</code> would return a <code>202 Accepted</code> with a link pointing to a <code>command/status</code> endpoint. The client would poll that endpoint until the status is <code>command-processed</code> (including the URL to the the actual resource) or <code>command-failed</code> (including a descriptive message for the failure).</p>&#xA;&#xA;<p>These solutions could be augmented by sending the status of all processed commands using <a href=""https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events"" rel=""nofollow noreferrer"">Server Sent Events</a>. In this way, the client gets notified without polling.</p>&#xA;&#xA;<p>If the client is not aware that the architecture is  asynchronous, a solution is to use an API gateway that blocks the client's request until the upstream microservice processes the command and then to respond with the complete resource's data.</p>&#xA;"
48387677,48370641,2575224,2018-01-22T18:04:07,"<p>One of the <a href=""http://microservices.io/patterns/microservices.html"" rel=""nofollow noreferrer"">benefits of using microservices</a> (if not the biggest) is that it eliminates any long-term commitment to a technology stack. When developing a new service you can pick a new technology stack. Similarly, when making major changes to an existing service you can rewrite it using a new technology stack.</p>&#xA;&#xA;<p>So, you can <strong>choose whatever communication mechanism you want as long as it does not prevent you changing the technology stack</strong>. REST over HTTP is a good choice because it hides the technologies used by the microservice to generate the responses in a request-response synchronous style. Message-based communications are also a good fit because the messages could also hide the technologies used to produce them (as long as you don't use the builtin serialization of the producer programming language), i.e. <a href=""https://www.rabbitmq.com/"" rel=""nofollow noreferrer"">RabbitMQ</a> or <a href=""https://kafka.apache.org/"" rel=""nofollow noreferrer"">Apache Kafka</a>.</p>&#xA;"
48203197,48201224,2575224,2018-01-11T09:04:23,"<p>I believe that this is a message-driven and a data-driven architecture, but this should not be important. What it is more important is that the microservices use Choreography (as opposed to Orchestration). This <a href=""https://stackoverflow.com/questions/47918407/microservice-architecture-carry-message-through-services-when-order-doesnt-ma/47920156#47920156"">question</a> could help.</p>&#xA;&#xA;<p>The cleanest architecture would be to put all the data in the messages, in this way the number dependencies is limited to 2. Also, the resilience of the system is increased: if the microservice A is down, the other downstream microservices could continue to work.</p>&#xA;&#xA;<p>Every microservice consume only the part of the message that interests it and ignores the other. This creates a nice and extendable pipeline of stream-like processing. If, however, the message is too big, you should use the Microservice A (or any other microservice) as the reference for more data.</p>&#xA;"
48343262,48341375,2575224,2018-01-19T14:26:18,"<p>User registration is a separate <a href=""https://martinfowler.com/bliki/BoundedContext.html"" rel=""nofollow noreferrer"">bounded context</a> so a separate microservice is the best approach. There is a blog post about this <a href=""http://richardwellum.com/2017/04/authentication-authorization-and-bounded-contexts/"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;&#xA;<p>Authentication is a cross cutting concern that could be handled by the <a href=""http://microservices.io/patterns/apigateway.html"" rel=""nofollow noreferrer"">API Gateway</a>. This would free the upstream microservices of this responsibility - their main job is not verifying that a user is who he claims to be.</p>&#xA;&#xA;<p>Authorization should be the responsibility of a dedicated microservice. It could be called by the API Gateway or by individual microservices.</p>&#xA;&#xA;<p><strong>P.S. My answer assumes that the microservices are not directly accessible from the outside, they can be reached only by the API Gateway.</strong></p>&#xA;"
51929500,51926575,2575224,2018-08-20T11:11:19,"<p>If you want to build a scalable and resilient application your microservices should not make synchronous calls from one to another (you can read <code>The Art of Scalability</code> book). </p>&#xA;&#xA;<p>This means that when a microservice receive a request from its clients it should have all the data already gathered in its local storage. In your case, you have two possibilities:</p>&#xA;&#xA;<ol>&#xA;<li>add the <code>firstName</code>, <code>lastName</code> columns to the <code>Orders table</code></li>&#xA;<li>create another table with users having <code>id</code>, <code>firstName</code>, <code>lastName</code> columns and make a <code>join</code> when returning data to the clients.</li>&#xA;</ol>&#xA;&#xA;<p>To keep the replicated information eventually consistent with the source (the Users service) you can use one of the following technics:</p>&#xA;&#xA;<ol>&#xA;<li>have a <code>cron</code> job that fetches all the needed user information and replaces all the <code>firstName</code>, <code>lastName</code> columns.</li>&#xA;<li>use integration events; in CQRS/Event sourcing architectures you already have the Domain events - you can subscribe to those. If you don't have CQRS but a plain architecture, then you can add triggers to your database that pushes low level mutation events (row created/updated/deleted) to the subscribed services. For more options you can read <code>Migrating to Microservice Databases</code> book by Edson Yanaga</li>&#xA;</ol>&#xA;"
51588964,51566509,2575224,2018-07-30T07:28:18,"<p>The main idea when you design your microservices is that <strong>one microservice should not make any synchronous calls to any other microservice</strong>. This implies that every microservice should gather any required external state in an asynchronous mode, before it will answer to queries or execute commands. One way to do that is (1) by subscribing to events; the other way (2) is by periodically querying some exposed Read-model (see <a href=""http://cqrs.nu"" rel=""nofollow noreferrer"">CQRS</a>), i.e. in a <code>cron</code> job.</p>&#xA;&#xA;<p>In any case, you should not expose the entire Aggregate, otherwise you risk to break its encapsulation by depending on its internals. Instead, you should publish its domain events (1) or create a specially designed canonical Readmodel that present <em>the most probable model</em> to the other microservices; something like a canonical Read-model; I would avoid this unless the domain is too simple, too CRUD.</p>&#xA;"
46442684,46432262,2575224,2017-09-27T08:12:27,"<p>You make an stateful microservice by attaching a shared resource. In your case, you attach or use a database that stores all the products that customers have buyed. A shared resource means that the database should be accessible my multiple instances of the microservice. In case one microservice fails and you must start another instance then the data is not lost. This helps also if you want to scale the microservice by running multiple instances at the same time.</p>&#xA;&#xA;<blockquote>&#xA;  <p>it can be like Asyncronous with event source with Kafka/RabbitMQ</p>&#xA;</blockquote>&#xA;&#xA;<p>You use events to synchronize between multiple microservices. For example, the <code>Product microservice</code> publishes an event (<code>ProductTitleChanged</code> event) that is picked up by <code>Checkout microservice</code> to update its state (a command is issued to <code>UpdateProductTitle</code>). This is needed because microservices duplicate some data from one to another in order to achieve greater resilience (i.e. one can function even if another is down).</p>&#xA;&#xA;<blockquote>&#xA;  <p>But please suggest architecture, code, example in detail how to get product details in checkout service</p>&#xA;</blockquote>&#xA;&#xA;<p>There are just too many architectures on too many levels. One that I particularly like is <a href=""http://cqrs.nu/"" rel=""nofollow noreferrer"">CQRS</a> with <a href=""http://microservices.io/patterns/data/event-sourcing.html"" rel=""nofollow noreferrer"">Event sourcing</a>. In this architecture one microservice persist the events to an Event store. Then, the other microservices poll the Event store and get the new published events. In this way, the Checkout microservice could get all product related events (like <code>ProductAddedToInventory</code>, <code>ProductChangedTitle</code>, <code>ProductChangedDescription</code> etc) and maintain a local list of all products but with only the properties that are relevant to it. So, when the Checkout microservice needs to display the product's title it won't make a remote call to the Inventory microservice but query its local database; this increases the <a href=""https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/architect-microservice-container-applications/resilient-high-availability-microservices"" rel=""nofollow noreferrer"">resilience</a> and also the speed.</p>&#xA;"
46472963,46471625,2575224,2017-09-28T15:27:14,<p>You should not share anything between microservices. This is because you would loose the main benefit of microservices: <strong>you can deploy every microservice independently</strong>. </p>&#xA;&#xA;<p>They should also be loosely coupled. They should be agnostic of the other technologies. This means that if you change from PHP to Java in one microservice then the others would not be affected. In fact they should not even know or care. The same with the database.</p>&#xA;
46461962,46451544,2575224,2017-09-28T05:59:42,"<p>Desktop application and microservices <strong>are mutually exclusive</strong> because Desktop means one machine and microservices implies multiple machines (physical or virtual) that communicate through network using a technology agnostic protocol. </p>&#xA;&#xA;<p>What you can do is to use <a href=""https://www.oreilly.com/ideas/modules-vs-microservices"" rel=""nofollow noreferrer"">modularization</a>. This is an alternative to microservices that can work in some scenarios like yours.</p>&#xA;"
46294086,46291406,2575224,2017-09-19T07:08:54,"<p>Although POST is not idempotent you could implement this by assigning each command a unique ID, for example a GUID. Then, on the microservice, before you execute a command, you check that the command was not processed already. If it was, then you ignore it otherwise you send it to the real component that do the real update and mark the command as executed.</p>&#xA;&#xA;<p>This solution is elegant as it can be implemented as an additional component that can <strong>decorate the core logic</strong>, following the <a href=""https://en.wikipedia.org/wiki/Open/closed_principle"" rel=""nofollow noreferrer"">Open/close principle</a> and <a href=""https://en.wikipedia.org/wiki/Single_responsibility_principle"" rel=""nofollow noreferrer"">Single responsibility principle</a>. Additionally, you can use the <a href=""https://stackoverflow.com/questions/6277771/what-is-a-composition-root-in-the-context-of-dependency-injection"">Dependency injection root configuration</a> to use or not this idem-potency protection.</p>&#xA;"
50755444,50734807,2575224,2018-06-08T07:23:11,"<p>It's dangerous to share schemas across microservices because they could become very coupled, or at least not like that. It's normal that microservices use data from each other, but models should not be fully imported in another microservice. Instead, the dependent microservices should use a subset, a local representation of the remote model. For this you should use an <a href=""https://docs.microsoft.com/en-us/azure/architecture/patterns/anti-corruption-layer"" rel=""nofollow noreferrer"">Anti-corruption layer</a>. This ACL would receive as input remote models and produce as output a local, immutable/readonly representation of that model. The ACL lives at the outer boundary of the microservice, i.e. where the remote calls are made.</p>&#xA;&#xA;<p>Also, sharing <code>*schema.js</code> files across microservices would force you to use JavaScript/NodeJS in all the other microservices, which is not good. Each microservice should use whatever programming language is best suited for it.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Should i make a database service exposed over http</p>&#xA;</blockquote>&#xA;&#xA;<p>The database is private to the owning microservice. It should not be exposed.</p>&#xA;"
50755937,50702676,2575224,2018-06-08T07:53:46,"<p>First of all, an Event store is a type of Persistence, which stores the applications state as a series of events as opposed to a <em>flat</em> persistence that stores the last projected state.</p>&#xA;&#xA;<blockquote>&#xA;  <p>If a microservice 1 persists object X into Database A. In the same time, for micro-service 2 to feed on the data from micro-service 1, micro-service 1 writes the same object X to an event store B.</p>&#xA;</blockquote>&#xA;&#xA;<p>You are trying to have two sources of truth that must be kept in sync by some sort of distributed transaction which is not very scalable.</p>&#xA;&#xA;<p>This is an unusual mode of using an Event store. In general an Event store is the canonical source of information, the single source of truth. You are trying to use it as an communication channel. The Event store is the persistence of an event-sourced Aggregate (see Domain Driven Design).</p>&#xA;&#xA;<p>I see to options:</p>&#xA;&#xA;<ol>&#xA;<li><p>you could refactor your architecture and make the <code>object X</code> and event-sourced entity having as persistence the Event store. Then have a Read-model subscribe to the Event store and build a flat representation of the <code>object X</code> that is persisted in the database A. In other words, write first to the Event store and then in the Database A (but in an eventually consistent manner!). This is a big jump and you should really think if you want to go event-sourced.</p></li>&#xA;<li><p>you could use CQRS without Event sourcing. This means that after every modification, the <code>object X</code> emits one or more Domain events, that are persisted in the Database A in the same local transaction as the <code>object X</code> itself. The microservice 2 could subscribe to the Database A to get the  emitted events. The actual subscribing depends on the type of database.</p></li>&#xA;</ol>&#xA;"
46151027,46117792,2575224,2017-09-11T08:13:23,"<p>The responsibility of restarting a failed service or scaling up/down is that of an orchestrator. For example, in my latest project, I used Docker Swarm. </p>&#xA;&#xA;<p>Currently, Docker's <a href=""https://docs.docker.com/engine/reference/run/#restart-policies-restart"" rel=""nofollow noreferrer"">restart policies</a> are:</p>&#xA;&#xA;<ul>&#xA;<li><strong>no</strong>: Do not automatically restart the container when it exits. This is the default.</li>&#xA;<li><strong>on-failure</strong>[:max-retries]: Restart only if the container exits with a non-zero exit status. Optionally, limit the number of restart retries the Docker daemon attempts.</li>&#xA;<li><strong>unless-stopped</strong>: Always restart the container regardless of the exit status. When you specify always, the Docker daemon will try to restart the container indefinitely. The container will also always start on daemon startup, regardless of the current state of the container.</li>&#xA;<li><strong>always</strong>: Always restart the container regardless of the exit status, but do not start it on daemon startup if the container has been put to a stopped state before.</li>&#xA;</ul>&#xA;"
46175088,46171533,2575224,2017-09-12T11:20:46,"<p>It depends on how <a href=""http://www.reactivemanifesto.org/"" rel=""nofollow noreferrer"">resilient</a> you want your system to be but it boils down to two type of communication: synchronous or asynchronous.</p>&#xA;&#xA;<p>For <strong>lesser resilience but a simpler architecture</strong> you could use synchronous communication. In this architecture when a microservice fails then all the dependent microservices would also fail. You should use the <a href=""http://microservices.io/patterns/reliability/circuit-breaker.html"" rel=""nofollow noreferrer"">Circuit breaker</a> or <a href=""https://www.linkedin.com/pulse/designing-bulkheads-microservices-architecture-subhash-chandran/"" rel=""nofollow noreferrer"">Bulkheads</a> patterns.</p>&#xA;&#xA;<p>For a <strong>greater resilience but a more complex architecture</strong> you should use an asynchronous communication style. In this style the microservices could communicate using <a href=""http://microservices.io/patterns/communication-style/messaging.html"" rel=""nofollow noreferrer"">messages</a>.</p>&#xA;"
46211908,46206775,2575224,2017-09-14T06:21:09,"<blockquote>&#xA;  <p>Should these events be called: SEND_RESERVATION_CONFIRMATION_EMAIL</p>&#xA;</blockquote>&#xA;&#xA;<p>No. Events should be named as sentences in the past.</p>&#xA;&#xA;<blockquote>&#xA;  <p>making the reservation service aware of the email communication</p>&#xA;</blockquote>&#xA;&#xA;<p>I would not make that coupling. The reservation service is responsible with reservations, not with methods of notifying the customers.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Or should there be a more generic event RESERVATION_CONFIRMED, resulting in a confirmation email?</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes, <code>RESERVATION_CONFIRMED</code> seems a good choice; it represent <strong>what really had happened</strong> and it does not contain indication of what should be done next. The workflow/process of notifying the customer should be managed by another component, i.e. a Saga/Process manager. This Saga would receive the <code>RESERVATION_CONFIRMED</code> event and then would send <code>SEND_RESERVATION_CONFIRMATION_EMAIL</code> <strong>command</strong> to the responsible microservice.</p>&#xA;"
46239247,46236744,2575224,2017-09-15T12:09:36,"<p>If you have strongly typed events, you could use reflection to publish the structure of the events and that should be sufficient for a client of your microservice.</p>&#xA;&#xA;<p>If you have some event descriptors (xml or similar) used to re-hydrate the events from the event store/event log then you can publish those.</p>&#xA;&#xA;<p>Otherwise I don't know of any tools that would work like Swagger but for events.</p>&#xA;"
46174890,46171136,2575224,2017-09-12T11:10:17,"<p>You would break the microservices encapsulation by assuming the type of persistence that each microservice is using. </p>&#xA;&#xA;<p>Each microservice should be <strong>free to use whatever persistence type it wants</strong> or to <strong>change it when it wants</strong> transparently, without notifying or requesting permission from the other microservices about this.</p>&#xA;"
46111062,46092604,2575224,2017-09-08T07:34:21,"<p>You have two choose between synchronous calls and asynchronous calls, the latter permitting a more resilient architecture so if this is what you want then go with it.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Maybe I'm making a mistake with the design but I expect that after a client calls the API gateway the request proceeds asynchronously and the data consistency won't be guaranteed.</p>&#xA;</blockquote>&#xA;&#xA;<p>As the call is asynchronous, you will have <a href=""https://en.wikipedia.org/wiki/Consistency_model"" rel=""nofollow noreferrer"">eventual consistency</a>.</p>&#xA;&#xA;<blockquote>&#xA;  <p>So how the client knows if the resource has been created and its id?</p>&#xA;</blockquote>&#xA;&#xA;<p>It doesn't know. I see two choices: </p>&#xA;&#xA;<ol>&#xA;<li>the client generates the IDs, preferable <code>GUIDs</code> (or any <em>stateless</em> unique ID) - the preferred way for high scalability. Then, the client polls the server to check the status of the resource by using that <code>GUID</code> or <a href=""https://en.wikipedia.org/wiki/HATEOAS"" rel=""nofollow noreferrer"">HATEOAS</a> URLs returned by the server.</li>&#xA;<li>the client sends the requests without a preexisting ID but the server returns an endpoint URL where the client can poll for the command status, probably using an unique command ID (ex: <code>/commands/1234-abcd-5678-efgh/status</code>); after the command is executed, the server returns the created resource ID or the resource URL (in case of having a RESTFUL service/<a href=""https://en.wikipedia.org/wiki/HATEOAS"" rel=""nofollow noreferrer"">HATEOAS</a>)</li>&#xA;</ol>&#xA;"
46111361,46110222,2575224,2017-09-08T07:50:49,"<p>I don't know what is the <em>recommended</em> way, I know how this is done in <a href=""https://en.wikipedia.org/wiki/Domain-driven_design"" rel=""nofollow noreferrer"">DDD</a> and maybe this can help you as DDD and <code>microservices</code> are friends.</p>&#xA;&#xA;<p>What you have is a long-running/multi-step process that involves information from multiple microservices. In DDD this can be implemented using a <a href=""https://msdn.microsoft.com/en-us/library/jj591569.aspx"" rel=""nofollow noreferrer"">Saga/Process manager</a>. The Saga maintains a local state by subscribing to events from both the <code>registration service</code> and the <code>activation service</code>. As the events come, the Saga check to see if it has all the information it needs to generate secure keys by submitting a <code>CreateSecureKey</code> command. The events may come in any order and even can be duplicated but this is not a problem as the Saga can compensate for this.</p>&#xA;&#xA;<p>In case of bugs or new features, you could create special scripts or other processes that search for a particular situation and handle it by submitting specific compensating commands, without reprocessing all the past events. </p>&#xA;&#xA;<p>In case of new features you may even have to process old events that now are interesting for your business process. You do this in the same way, by querying the events source for the newly interesting old events and send them to the newly updated Saga. After that <em>import process</em>, you subscribe the Saga to these newly interesting events and the Saga continues to function as usual.</p>&#xA;"
50016178,50010287,2575224,2018-04-25T07:14:18,"<p><em>This questions is about a very low level view of Event sourcing and depends a lot on the actual implementation of an Event store. All-in-all, I can give you an answer, hoping to shed some light on your understanding of Event stores.</em></p>&#xA;&#xA;<blockquote>&#xA;  <p>Is the sole purpose of event stream to give ability to iterate over list of events that does not sound so useful maybe I am missing something but why can't I just get rid of event stream and call it a day?</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes, an Event stream provides a mean to iterate over a possible large list of events, without retrieving all of them from the Event store in a blocking way. In general it is used only to read events, so its interface does not contain methods to append events to the Event store. </p>&#xA;&#xA;<p>So, the client code needs only the events from the stream.</p>&#xA;&#xA;<p>When adding events to an Event store, in order to protect from concurrent writes, one needs to pass the expected version of the Event stream. One can do this by using a <code>version</code> parameter to the method <code>EventStore.appendEvents(expectedVersion, newEvents)</code> or it can pass the previously loaded event stream and let the Event store retrieve the last seen <code>version</code>, thus reducing the coupling of the client code to the actual implementation of event stream locking mechanism. So, the signature of the appending method could be like this:</p>&#xA;&#xA;<pre><code>EventStore.appendEvents(previousEventStream, newEvents)&#xA;</code></pre>&#xA;&#xA;<p>So, the  client code doesn't know/care what locking mechanism (optimistic or pesimistic) is the Event store using to protect from concurrent writes.</p>&#xA;&#xA;<p>One example of this can be found <a href=""https://github.com/xprt64/dudulina/blob/master/src/Dudulina/EventStore.php"" rel=""nofollow noreferrer"">here</a> <em>(disclaimer: it's mine)</em>:</p>&#xA;&#xA;<pre><code>public function appendEventsForAggregate(AggregateDescriptor $aggregateDescriptor, $eventsWithMetaData, AggregateEventStream $expectedEventStream): void;&#xA;</code></pre>&#xA;"
50016440,50006750,2575224,2018-04-25T07:30:04,"<blockquote>&#xA;  <p>I wanted to see if there is a way for event store to handle scenario where it does not need to query the relation table to retrieve the invoice/invoices and then apply events to it.</p>&#xA;</blockquote>&#xA;&#xA;<p>The Event store should not query any other table except its own. And also, it should not be the responsibility of the Event store to load an Aggregate and to apply the events to it (to rehydrate an Aggregate). This would be the responsibility of a Repository that uses an Event store. </p>&#xA;&#xA;<p>The Event store is at a lower level, and it should have these two methods:</p>&#xA;&#xA;<pre><code>interface EventStore&#xA;{&#xA;    EventStream loadEvents(aggregateId);&#xA;    void appendEvents(aggregateId, previousEventStream, eventsToBeAppended);&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>It <em>may</em> have other methods, but at the same level of abstraction like the above.</p>&#xA;&#xA;<p>On the other side, an Event sourcing aware Invoice repository would have this interface:</p>&#xA;&#xA;<pre><code>interface InvoiceRepository {&#xA;   Invoice loadInvoice(invoiceId);&#xA;   void persistInvoice(invoiceId, invoiceNewEvents);&#xA;}&#xA;</code></pre>&#xA;&#xA;<p><a href=""https://github.com/xprt64/dudulina/blob/master/src/Dudulina/Aggregate/AggregateRepository.php"" rel=""nofollow noreferrer"">Here</a> is an example of such a (althought generic) repository.</p>&#xA;"
42780042,42778151,2575224,2017-03-14T07:27:12,"<p>You have two Bounded Contexts: <code>User management</code> and  <code>Authentication</code>. </p>&#xA;&#xA;<p><code>User management</code> BC deals with the life-cicle of a user (creation, mutating and deletion).&#xA;<code>Authentication</code> BC deals with how the users identify themselves in the system.</p>&#xA;&#xA;<p>So, it is a valid assumption that a user can exists even if it has (yet) no possibility to identify himself in the system. </p>&#xA;&#xA;<p>That being said, you should emit the <code>AUserWasCreatedEvent</code> immediately after the User management BC processes the <code>CreateUserCommand</code> because in that moment the user is born. It has an ID, let's name it <code>UserID</code>, so it exists.</p>&#xA;&#xA;<p>Then, this user needs a mean to identify himself and a <code>Saga</code> (or <code>Process manager</code> or whatever you want to call it) catch the event and create a <code>CreateAuth0UserCommand</code> that it is sent to the <code>Authentication</code> BC by calling the Auth0 API. The API respond with some data, possibly including a <code>token</code>; that token is handled by the <code>Authentication</code> BC and it is associated with the <code>UserID</code>.</p>&#xA;"
48758338,48731168,2575224,2018-02-13T01:53:59,<p>It looks like you have already done a good job spliting the application in microservices. Every one of the has its own persistence and they should communicate with a technology agnostic protocol or by asynchronous events.</p>&#xA;&#xA;<p>I would do it pretty much like you did. Maybe <code>Auth</code> should be splitted into <code>Authentication</code> (i.e. using a stateless jwt)  and <code>Authorization</code> (+its own database).</p>&#xA;&#xA;<p>The <code>Authentication</code> would ensure that the User is who he says it is.</p>&#xA;&#xA;<p>The <code>Authorization</code> would verify that a User may modify only his own polls. </p>&#xA;
50675120,50672490,2575224,2018-06-04T07:13:07,"<p>It all depends on the resilience requirements that you have. Do you want your microservice to function when the other microservices are down or not? </p>&#xA;&#xA;<p>The first solution that you presented is the less resilient: if any of the Users or Products microservices goes down, the Invoice microservice would also go down. Is this what you want? On the other hand, this architecture is the simplest. A variation of this architecture is to let the client make the join requests; this leads to a chatty conversation but it has the advantage that the client could replace the missing information with default information when the other microservices are down.</p>&#xA;&#xA;<p>The second solution offers the biggest possible resilience but it's more complex. Having an event-driven architecture helps a lot in this case. In this architecture the microservices act as <a href=""http://theartofscalability.com/"" rel=""nofollow noreferrer"">swimming lanes</a>. A failure in one of the microservices does not propagate to other microservices.</p>&#xA;"
50694195,50686734,2575224,2018-06-05T07:14:03,"<p><strong>You should use the Circuit breaker pattern whenever you have remote calls</strong>.</p>&#xA;&#xA;<p>If you don't use it, then in some circumstances (i.e. when some microservices are down) your system would act as it is under a self DOS attack. This situation manifests itself when you have chained synchronous calls. For example, if you have the following: A -> B -> C (A calls B which calls C). If C is not responding and A keeps calling then B could be overwhelmed with managing waiting calls from A and could not respond to legitimate calls from other services that would normally succeed.</p>&#xA;&#xA;<p>The most common place to use the Circuit breaker is in the API Gateway, where most of the remote calls are made (this is it's primary responsibility). You could use the pattern also in clients, to force them stop continuously and repeatedly calling a dead microservice.</p>&#xA;&#xA;<p>Although microservices are independent with regards to resilience (they could function even when other fail), this does not mean that they don't communicate with one another. They may communicate but in an asynchronous manner, i.e. when one microservices wants to update its own local cache with data from another microservice in a background process.</p>&#xA;"
43568821,43568325,2575224,2017-04-23T08:09:43,"<p>You should <strong>not go with DRY</strong> in microservices. Think about one of the advantages of using them: you can replace one microservice with other stack at any time (<code>PHP</code> + <code>Apache</code>) and the system would not care, as long as it respects the contract.</p>&#xA;&#xA;<p>You can read more <a href=""http://rads.stackoverflow.com/amzn/click/1491950358"" rel=""nofollow noreferrer"">here</a>, page 59, <code>DRY and the Perils of Code Reuse in Microservices World</code>.</p>&#xA;"
43683096,43682155,2575224,2017-04-28T14:51:52,"<p>You cannot assume the structure of the url but you can return an entity id.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Should I use ""/users/1"" (""self"" link) as user ID in the pics&#xA;  microservice?</p>&#xA;</blockquote>&#xA;&#xA;<p>If you do this you are assuming that the second microservice is using the same url scheme which is bad. You couple the two microservices.</p>&#xA;&#xA;<p>Also, the urls included in the response of a request have a meaning only to that (micro)services, you cannot just assume that a corresponding <code>resource</code> in another system has the same <code>id</code></p>&#xA;"
43637941,43633659,2575224,2017-04-26T15:04:08,"<blockquote>&#xA;  <p>... if I should even be communicating between services?</p>&#xA;</blockquote>&#xA;&#xA;<p>They communicate if they need and in most cases they do need. So, there is communication between microservices and between an <code>API Gateway</code> or a Client (web browser, mobile application, desktop application) and microservices. In any case, <em>the communication is done using the network</em>.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Should each micro-service be a web service that only serves Http or should I be using a service bus to pass work requests around?</p>&#xA;</blockquote>&#xA;&#xA;<p>You are asking how should they communicate.</p>&#xA;&#xA;<p>There are two types of communication: </p>&#xA;&#xA;<ol>&#xA;<li><p>synchronous, i.e. request-response; the most typical usage in between the Client and the <code>API gateway</code> or a microservice; in this type the client send a request and waits for the response; its the simplest but less scalable or resilient; <em>be aware of cascading failures</em> in nested calls between multiple microservices;</p></li>&#xA;<li><p>asynchronous, i.e. event-based; the Client does not wait for the response and it is notified when the operation succeeds or fails; this is more complex but it is more scalable and it is used by <a href=""http://www.reactivemanifesto.org/"" rel=""nofollow noreferrer"">resilient systems</a>.</p></li>&#xA;</ol>&#xA;&#xA;<p>Then, depending on the protocol, there are at least the following types of communication:</p>&#xA;&#xA;<ol>&#xA;<li><a href=""https://en.wikipedia.org/wiki/Remote_procedure_call"" rel=""nofollow noreferrer"">RPC</a></li>&#xA;<li><a href=""https://en.wikipedia.org/wiki/Representational_state_transfer"" rel=""nofollow noreferrer"">REST</a> - recommended specially with HATEOAS</li>&#xA;<li><a href=""https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern"" rel=""nofollow noreferrer"">Publish-subscribe</a></li>&#xA;</ol>&#xA;&#xA;<p>You can read more <a href=""http://shop.oreilly.com/product/0636920033158.do"" rel=""nofollow noreferrer"">here</a> and <a href=""https://www.nginx.com/wp-content/uploads/2015/01/Building_Microservices_Nginx.pdf"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;"
44349939,44346827,2575224,2017-06-04T01:20:45,"<p>From <code>Domain driven design</code> perspective, the authorization should be a separate bounded context. Thus, the authorization checks should be done outside the <code>Users administration  bounded context</code>. So, in the simplest implementation, you could use some Api services that do the actual checking, called from the Application layer. If the current authenticated user has the required permission (for example <code>CanCreateNewUsers</code>) then the call to the <code>Users administration bounded context</code> is allowed, otherwise is rejected with an error.</p>&#xA;&#xA;<p>A more complex/DDD solution would be to use an Anti-coruption layer between the two bounded contexts.</p>&#xA;&#xA;<p>Btw, I suggest you to use permissions and not roles when you do the actual checking. You can use roles in the <code>Authorization bounded context</code>.</p>&#xA;"
44384147,44360649,2575224,2017-06-06T07:44:29,"<p>As there are more than one bounded contexts that need to be queried for the validation to pass you need to consider eventual consistency. That being said, there is always a chance that the process as a whole can be in an invalid state for a ""small"" amount of time. For example, the user could be deactivated after the command is accepted and before the order is shipped. An online shop is a complex system and exceptions could appear in any of its subsystems. However, being implemented as an event-driven system helps; every time the ordering process enters an invalid state you can take compensatory actions/commands. For example, if the user is deactivated in the meantime you can cancel all its standing orders, release the reserved products, announce the potential customers that have those products in the wishlist that they are not available and so on.</p>&#xA;&#xA;<p>There are many kinds of validation in DDD but I follow the general rule that the validation should be done as early as possible but without compromising data consistency. So, in order to be early you could query the readmodel to reject the commands that couldn't possible be valid and in order for the system to be consistent you need to make another check just before the order is shipped.</p>&#xA;&#xA;<p>Now let's talk about your specific questions:</p>&#xA;&#xA;<blockquote>&#xA;  <p>How to know that the customer is really exists in database (query-side customer service) and still active?</p>&#xA;</blockquote>&#xA;&#xA;<p>You can query the readmodel to verify that the user exists and it is still active. You should do this as a command that comes from an invalid user is a strong indication of some kind of attack and you don't want those kind of commands passing through your system. However, even if a command passes this check, it does not necessarily mean that the order will be shipped as other exceptions could be raised in between.</p>&#xA;&#xA;<blockquote>&#xA;  <p>How to know that the product is exists in database and the status of the product is published?</p>&#xA;</blockquote>&#xA;&#xA;<p>Again, you can query the readmodel in order to notify the user that the product is not available at the moment. Or, depending on your business, you could allow the command to pass if you know that those products will be available in less than 24 hours based on some previous statistics (for example you know that TV sets arrive daily in your stock). Or you could let the customer choose whether it waits or not. In this case, if the products are not in stock at the final phase of the ordering (the shipping) you notify the customer that the products are not in stock anymore.</p>&#xA;&#xA;<blockquote>&#xA;  <p>How to know whether the customer eligible to get the promo price from the related product?</p>&#xA;</blockquote>&#xA;&#xA;<p>You will probably have to query another bounded context like <code>Promotions BC</code> to check this. This depends on how promotions are validated/used.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Is it ok to call API directly (like point-to-point / ajax / request promise) to validate this payload in order command-side service? But I think, the performance will get worse if the API called directly just for validation.</p>&#xA;</blockquote>&#xA;&#xA;<p>This depends on how resilient you want your system to be and how fast you want to reject invalid commands. </p>&#xA;&#xA;<p>Synchronous call are simpler to implement but they lead to a less resilient system (you should be aware of cascade failures and use technics like <a href=""https://martinfowler.com/bliki/CircuitBreaker.html"" rel=""nofollow noreferrer"">circuit breaker</a> to stop them). </p>&#xA;&#xA;<p>Asynchronous (i.e. using events) calls are harder to implement but make you system more resilient. In order to have async calls, the <code>ordering</code> system can subscribe to other systems for events and maintain a private state that can be queried for validation purposes as the commands arrive. In this way, the ordering system continues to work even of the link to <code>inventory</code> or <code>customer management</code> systems are down.</p>&#xA;&#xA;<p><em>In any case, it really depends on your business and none of us can tell you exaclty what to do.</em></p>&#xA;"
47919908,47917777,2575224,2017-12-21T07:24:58,"<p>Microservices architecture is a nice thing but it comes with a big price: you will have one more distributed moving part in your system. </p>&#xA;&#xA;<p>If you need to:</p>&#xA;&#xA;<ul>&#xA;<li>deploy separately this new microservice,</li>&#xA;<li>you want to be able to use whatever programming language you want for this,</li>&#xA;<li>you need to scale separately this functionality (you can still scale the <code>Elasticache</code>, separately, without a microservice) or </li>&#xA;<li>you need to be able to change from <code>Elasticache</code> to another cache, </li>&#xA;</ul>&#xA;&#xA;<p>then YES, do extract it to a microservice otherwise NO, don't, just extract it to a module inside the monolith.</p>&#xA;"
47841642,47680711,2575224,2017-12-16T01:05:47,"<p>There are some errors that should not be retried because they seem permanent:</p>&#xA;&#xA;<ul>&#xA;<li>400 Bad Request </li>&#xA;<li>401 Unauthorized </li>&#xA;<li>402 Payment Required </li>&#xA;<li>403 Forbidden</li>&#xA;<li>405 Method Not Allowed </li>&#xA;<li>406 Not Acceptable </li>&#xA;<li>407 Proxy Authentication Required </li>&#xA;<li>409 Conflict - <em>it depends</em> </li>&#xA;<li>410 Gone </li>&#xA;<li>411 Length Required </li>&#xA;<li>412 Precondition Failed </li>&#xA;<li>413 Payload Too Large  </li>&#xA;<li>414 URI Too Long </li>&#xA;<li>415 Unsupported Media Type </li>&#xA;<li>416 Range Not Satisfiable </li>&#xA;<li>417 Expectation Failed </li>&#xA;<li>418 I'm a teapot - <em>not sure about this one</em> </li>&#xA;<li>421 Misdirected Request </li>&#xA;<li>422 Unprocessable Entity </li>&#xA;<li>423 Locked - <em>it depends on how long a resource is locked in average (?)</em>  </li>&#xA;<li>424 Failed Dependency </li>&#xA;<li>426 Upgrade Required - <em>can the client be upgraded automatically?</em> </li>&#xA;<li>428 Precondition Required - <em>I don't thing that the precondition can be&#xA;fulfiled the second time without retring from the beginning of the&#xA;whole process but it depends</em>  </li>&#xA;<li>429 Too Many Requests - <em>it depends but it should not be retried to fast</em> </li>&#xA;<li>431 Request Header Fields TooLarge </li>&#xA;<li>451 Unavailable For Legal Reasons</li>&#xA;</ul>&#xA;&#xA;<p>So, most of the 4** Client errors should not be retried.</p>&#xA;&#xA;<p>The 5** Servers errors that should not be retried:</p>&#xA;&#xA;<ul>&#xA;<li>500 Internal Server Error</li>&#xA;<li>501 Not Implemented</li>&#xA;<li>502 Bad Gateway - <em>I saw used for  temporary errors so it depends</em></li>&#xA;<li>505 HTTP Version Not Supported</li>&#xA;<li>506 Variant Also Negotiates</li>&#xA;<li>507 Insufficient Storage</li>&#xA;<li>508 Loop Detected</li>&#xA;<li>510 Not Extended</li>&#xA;<li>511 Network Authentication Required</li>&#xA;</ul>&#xA;&#xA;<p>However, in order to make the microservices more resilient you should use the <a href=""https://github.com/Netflix/Hystrix/wiki/How-it-Works#CircuitBreaker"" rel=""nofollow noreferrer"">Circuit breaker</a> pattern and fail fast when the upstream is down.</p>&#xA;"
47920156,47918407,2575224,2017-12-21T07:42:09,"<p>There are two methods of managing a long running process (or a processing involving multiple microservices): Orchestration and choreography. There are a lot of articles describing them. </p>&#xA;&#xA;<p><em>Long story short</em>: In <strong>Orchestration</strong> you have a microservice that keeps track of the process status and in <strong>Choreography</strong> all the microservices know where to send next the message and/or when the process is done.</p>&#xA;&#xA;<p>This <a href=""https://medium.com/capital-one-developers/microservices-when-to-react-vs-orchestrate-c6b18308a14c"" rel=""noreferrer"">article</a> explains the benefits and tradeofs of the two styles. </p>&#xA;&#xA;<p><strong>Orchestration</strong>&#xA;<a href=""https://i.stack.imgur.com/iIHkU.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/iIHkU.jpg"" alt=""Orchestration""></a></p>&#xA;&#xA;<p><strong>Orchestration Benefits</strong></p>&#xA;&#xA;<ul>&#xA;<li>Provides a good way for controlling the flow of the application when there is synchronous processing. For example, if Service A needs to complete successfully before Service B is invoked.</li>&#xA;</ul>&#xA;&#xA;<p><strong>Orchestration Tradeoffs</strong></p>&#xA;&#xA;<ul>&#xA;<li><p>Couples the services together creating dependencies. If service A is down, service B and C will never be called.</p></li>&#xA;<li><p>If there is a central shared instance of the orchestrator for all requests, then the orchestrator is a single point of failure. If it goes down, all processing stops.</p></li>&#xA;<li><p>Leverages synchronous processing that blocks requests. In this example, the total end-to-end processing time is the sum of time it takes for Service A + Service B + Service C to be called.</p></li>&#xA;</ul>&#xA;&#xA;<p><strong>Choreography</strong>&#xA;<a href=""https://i.stack.imgur.com/ewmRu.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ewmRu.jpg"" alt=""Choreography""></a></p>&#xA;&#xA;<p><strong>Choreography Benefits</strong></p>&#xA;&#xA;<ul>&#xA;<li><p>Enables faster end-to-end processing as services can be executed in parallel/asynchronously.</p></li>&#xA;<li><p>Easier to add/update services as they can be plugged in/out of the event stream easily.</p></li>&#xA;<li><p>Aligns well with an agile delivery model as teams can focus on particular services instead of the entire application.</p></li>&#xA;<li><p>Control is distributed, so there is no longer a single orchestrator serving as a central point of failure.</p></li>&#xA;<li><p>Several patterns can be used with a reactive architecture to provide additional benefits. For example, Event Sourcing is when the Event Stream stores all of the events and enables event replay. This way, if a service went down while events were still being produced, when it came back online it could replay those events to catch back up. Also, Command Query Responsibility Segregation (CQRS) can be applied to separate out the read and write activities. This enables each of these to be scaled independently. This comes in handy if you have an application that is read-heavy and light on writes or vice versa.</p></li>&#xA;</ul>&#xA;&#xA;<p><strong>Choreography Tradeoffs</strong></p>&#xA;&#xA;<ul>&#xA;<li><p>Async programming is often a significant mindshift for developers. I tend to think of it as similar to recursion, where you can’t figure out how code will execute by just looking at it, you have to think through all of the possibilities that could be true at a particular point in time.</p></li>&#xA;<li><p>Complexity is shifted. Instead of having the flow control centralized in the orchestrator, the flow control is now broken up and distributed across the individual services. Each service would have its own flow logic, and this logic would identify when and how it should react based on specific data in the event stream.</p></li>&#xA;</ul>&#xA;"
45790785,45779616,2575224,2017-08-21T06:45:32,"<p>Every microservice should have <em>one</em> responsibility. This keeps them <em>micro</em>. So, <code>Product MS</code> is responsible for enforcing business rules on products, <code>Location MS</code> is responsible for enforcing business rules on locations. </p>&#xA;&#xA;<p>On the other hand you have a third responsibility: to list all/some of the products along with their locations in a human readable way. For this you need another microservice who will have that responsibility. This third microservice should listen to events from both the other microservices and keep an eventually consistent list of products joined with their locations.</p>&#xA;&#xA;<p>BTW: none of the microservices should know/care that the others have a <code>sql</code> or <code>nosql</code> or whatever database used internally, that's against the microservices architecture.</p>&#xA;"
45790928,45789168,2575224,2017-08-21T06:54:06,"<p>A microservice exposes it's interface, what it can do, by means of an API. The API is the list of all endpoints that a microservice respond when it receives a command/query. The microservice contains the API <em>and</em> other internal+hidden things that it uses to respond to client's requests.</p>&#xA;&#xA;<p>An API is all that the clients see when they look at the microservice, although the microservice is bigger than that. A microservice hides its internal structure, it's technology stack, it's database type (<code>sql</code>, <code>nosql</code> - it could be anything); a microservice could move from <code>sql</code> to <code>nosql</code>, from <code>python</code> to <code>php</code>, but keep it's API unchanged.</p>&#xA;"
50444540,50435696,2575224,2018-05-21T08:11:02,"<blockquote>&#xA;  <p>Currently we're using Django admin extensively and I wonder if it's possible to continue using it once the monolith is broken. It means reading and manipulating data from all the microservices in a ""used to work on"" UI. It would also be helpful for this process to be done more smoothly.</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes, you can but it may not access the other microservices databases (neither write nor read). This means that if the Admin microservice update some Article (or whatever entity types you have, this is just an example) then this is not reflected immediately in the microservice that displays that Article. You need to have some mechanism to transfer the updates from the Admin to the other microservices. So shared databases/tables is not an option.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Authentication and authorization - Would we still be able to use this built in ""app"" in a microservice architecture? Is it possible to take this pare only to another service and communicate with it over HTTP?</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes, but you need to split it into two sides. One side is responsible for managing users and roles/permissions and the other is responsible to authenticate the users and to check if a user may perform some action.</p>&#xA;&#xA;<p>The first side should be a microservices (the creation/administration or users and the managing of roles/permissions). </p>&#xA;&#xA;<p>The checking part can be a microservice but those responsibilities are in general taken by the API gateway or by a module (+ local, replicated data) in every microservice that need authentication or authorisation. These are cross-cutting concerns. If they reside in a separate microservice then there is the problem of resilience: if that microservice fails then it brings down you entire system.</p>&#xA;"
50444179,50435154,2575224,2018-05-21T07:45:52,"<blockquote>&#xA;  <p>So the problem is this Should the air-miles microservice take decisions based on its own view model which is being updated from events coming from the current-account, and similarly, on picking which reward it should give out to the Customer?</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes. In fact, you should revise your architecture and even create more microservices. What I mean is that, being a event-driven architecture (also an Event-sourced one), your microservices have two responsibilities: they need to keep two different models: the write model and the read model. </p>&#xA;&#xA;<p>So, for each Aggregate should be a microservice that keeps only the write model, that is, it only processes Commands, without building also a read model.</p>&#xA;&#xA;<p>Then, for each read/query use case you should have a microservice that build the <em>perfect</em> read model. This is required if you need to keep the Aggregate microservice clean (as you should) because in general, the read models needs data from multiple Aggregate types/bounded contexts. Read models may cross bounded context boundaries, Aggregates may not. So you see, you don't really have a choice if you need to fully respect DDD.</p>&#xA;&#xA;<p>Some says that domain events should be hidden, only local to the owning microservice. I disagree. In an event-driven architecture the domain events are first class citizens, they are allowed to reach other microservices. This gives the other microservices the chance to build their own interpretation of the system state. Otherwise, the emitting microservice would have the impossible <em>additional</em> responsibility/task of building a state that must match every possible need that all the microservices would ever want(!); i.e. maybe a microservices would want to lookup a <em>deleted</em> remote entity's <code>title</code>, how could it do that if the emitting microservice keeps only the list of <em>non-deleted-yet</em> entities? You may say: but then it will keep all the entities, deleted or not. But maybe someone needs the date that an entity was deleted; you may say: but then I keep also the <code>deletedDate</code>. You see what you do? You break the Open/closed principle. Every time you create a microservice you need to modify the emitting microservice.</p>&#xA;&#xA;<p>There is also the resilience of the microservices. In the <a href=""http://theartofscalability.com/"" rel=""nofollow noreferrer"">Art of scalability</a>, the authors speak about swimming lanes. They are a strategy to separate the components of a system into lanes of failures. A failure in a lane does not propagate to other lanes. Our microservices are lanes. Components in a lane are not allowed to access any component from other lane. One down microservice should not bring the others down. It's not a matter of speed/optimisation, it's a matter of resilience. The domain events are the perfect modality of keeping two remote systems synchronized. They also emphasize the fact that the data is eventually consistent; the events travel at a limited speed (from nanoseconds to even days). When a system is designed with that in mind then no other microservice can bring it down.</p>&#xA;&#xA;<p>Yes, there will be some code duplication. And yes, although I said that you don't have a choice, you have. In order to reduce the code duplication at the cost of a lower resilience, you can have some Canonical read models that build a <em>normal</em> flat state and other microservices could query that. This is dangerous in most cases as it breaks the swimming lanes concept. Should the Canonical microservices go down, go down all dependent microservices. Canonical microservices works best for CRUD-like bounded context.</p>&#xA;&#xA;<p>There are however valid cases when you may have some internal events that you don't want to expose. In other words, you are not required to publish all domain events.</p>&#xA;"
49535462,49534740,2575224,2018-03-28T13:07:32,"<p>The solution  is very simple, instead of using IPs or Hostnames you can use the service's name.</p>&#xA;&#xA;<p>In your example, in the <code>streamapp</code> service you can access the other by using <code>http://storeapp:8080</code>.</p>&#xA;&#xA;<p>Similar, in the <code>storeapp</code> service you can access the other at <code>http://streamapp:8080</code>.</p>&#xA;&#xA;<p>Please not that you must use the internal ports, not the exported ones.</p>&#xA;&#xA;<p>This does not apply when you access the service from the other machines, i.e. from the internet. In that case you must use the form <code>http://{IP_OF_THE_MACHINE}:8090</code></p>&#xA;"
42563610,42562820,2575224,2017-03-02T18:52:33,"<p>I recommend that you choose a event-driven solution, but not necessarily to use microservices. You could build an event-driven monolith in order to spend much less time on synchronizing the two models. When the application grows to big then you split the monolith into microservices. You could use CQRS to split event more the models into write and read. If you use event-sourcing things get even more interesting.</p>&#xA;&#xA;<p>In my experience, with shared kernel, the models become god objects, one-size-fits-all kind of objects.</p>&#xA;"
42529947,42528718,2575224,2017-03-01T11:00:32,"<p>I think that the answer depends on how resilient you want the system to be, that is, how to handle the situation in wich the <code>Contacts Microservice</code> is down (not responding or very slow).</p>&#xA;&#xA;<p><strong>1. You want to be very resilient</strong></p>&#xA;&#xA;<p>If the <code>Contacts Microservice</code> is down, you want to be able to emit invoices for some (maybe most) of the contacts. In this case you listen to the <code>ContactCreated</code> and <code>ContactDeleted</code> and maintain a (eventually consistent) local list of <em>valid</em> contacts; they should be named accordingly to the Ubiquitous language in this bounded context, like <code>Payers</code> (or something like that). Then, in the Application layer, when building the <code>CreateInvoiceCommand</code> you check that <code>Payer</code> is valid and create the command.</p>&#xA;&#xA;<p><strong>2. You don't need to be resilient</strong></p>&#xA;&#xA;<p>If the <code>Contacts Microservice</code> is down, you refuse to generate invoices. In this case, when building the command you make a request to the <code>Invoices Microservice</code> API endpoint and verify that the <code>Payer</code> is valid.</p>&#xA;&#xA;<p>In any case, you check for contact's validity before the command is dispatched.</p>&#xA;"
48862394,48861926,2575224,2018-02-19T08:53:01,"<p>There are some things that you can do to minimize the impact of the client-server out-of-sync situation.</p>&#xA;&#xA;<p>The first measure that you can take is to let the <strong>client generate the entity IDs</strong>, for example by using GUIDs. This prevents the server to generate a new entity every time the client retries a CreateEntityCommand.</p>&#xA;&#xA;<p>In addition, you can make the <strong>command handing idempotent</strong>. This means that if the server receives a second CreateEntityCommand, it just silently ignores it (i.e. it does not throw an exception). This depends on every use case; some commands cannot be made idempotent (like <code>updateEntity</code>).</p>&#xA;&#xA;<p>Another thing that you can do is to <strong>de-duplicate commands</strong>. This means that every command that you send to a server must be tagged with an unique ID. This can also be a GUID. When the server receives a command with an ID that it already had processed then it ignores it and gives a positive response (i.e. <code>200</code>), maybe including some meta-information about the fact that the command was already processed. The command de-duplication can be placed on top of the stack, as a separate layer, independent of the domain (i.e. in front of the <code>Application layer</code>).</p>&#xA;"
48849080,48838101,2575224,2018-02-18T06:27:49,"<p><em>I will not try to convince you to not do this checking before placing an order and to rely on Sagas as it is usually done; I will consider that this is a business requirement that you must implement.</em></p>&#xA;&#xA;<p>This seems like a new sub-domain to me: bad-behavior-prevention (or how do you want to call it) that comes with a new responsibility: to prevent abusers. You could add this responsibility to the Order microservice but you would break the SRP. So, <strong>it should be done in another microservice</strong>. </p>&#xA;&#xA;<p>This new microservice is called from your API Gateway (if you have one) or from the Orders microservice.</p>&#xA;&#xA;<p>If you do not to add a new microservice (from different reasons) then you could implement this new functionality as a module inside of the Orders microservice but I strongly recommend to make it highly decoupled from its host (separate and private persistence/database/table).</p>&#xA;"
44172078,44170365,2575224,2017-05-25T04:03:16,"<p>As a PHP programmer you <em>should</em> be aware of and use the <a href=""http://www.php-fig.org/"" rel=""nofollow noreferrer"">PSR</a>. In this case you should use dependency injection and the <a href=""https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-3-logger-interface.md"" rel=""nofollow noreferrer"">LoggerInterfaces</a>.</p>&#xA;&#xA;<p>For web application you should configure your composition root to use a logger implementation that logs to a file. For console application you should log to the terminal.</p>&#xA;&#xA;<p>The composition root is the place where you configure your Dependency Injection Container (DIC). See more about <a href=""http://www.yiiframework.com/doc-2.0/guide-concept-di-container.html"" rel=""nofollow noreferrer"">Yii DIC here</a>.</p>&#xA;&#xA;<p>In order to do that you should be able to switch between these two composition roots by an environment variable or by <a href=""https://stackoverflow.com/q/173851/2575224"">php_sapi_name</a>.</p>&#xA;"
44127237,44124914,2575224,2017-05-23T06:24:18,"<blockquote>&#xA;  <p>Is it possible to have multiple versions of service(s) deployed in production at the same time</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes, it is possible. The idea is to keep all <em>used</em> microservices in production (v1, v2 ...) at the same time and to bring down the versions that are not used anymore. For this, you <em>should</em> somehow know when a version is not used anymore.</p>&#xA;&#xA;<p>AFAIK, you have to options:</p>&#xA;&#xA;<ol>&#xA;<li><p>For every new version you make a new endpoint (like /v2/someApiCall) that is connected to the same (now upgraded) microservice and gradually instruct clients to use the new endpoind; when the old endpoint is not used anymore you deleted it; this is the preferred way.</p></li>&#xA;<li><p>For every new version you make a new microservice that share the same persistence with the old microservice; you should avoid the use of this solution; Netflix uses this strategy in <em>rare</em> occasions when the cost of changing old consumers is too high.</p></li>&#xA;</ol>&#xA;&#xA;<p>You can read more at page 62 from <a href=""http://rads.stackoverflow.com/amzn/click/1491950358"" rel=""nofollow noreferrer"">Building microservices by Sam Newman</a>.</p>&#xA;"
45870706,45853546,2575224,2017-08-24T20:53:50,"<p>Because we speak of microservices you are <em>not allowed</em> to share a database between them. One microservice should not depend/know/care about the other's technology stack. The state is owned by each microservice, it should not be accesed otherwise but through one's microservice API. </p>&#xA;&#xA;<p>That being said you need a technology agnostic way of sharing that boolean flag between the two microservices. You can synchronously or asynchronously update the flag, that depends on your system's resilience requirements. You can use REST with JSON to hide the technology stack.</p>&#xA;&#xA;<p>If you share a database you would loose the main benefits of the microservices architecture.</p>&#xA;"
46036970,46021905,2575224,2017-09-04T12:22:07,"<p>I think that you should use an API Gateway. As always, it depends, but if you need to stop/start/scale-up/down the microservices, you will have to have such a gateway to hide the internal MS architecture from the web clients and keep them connected during devops.</p>&#xA;"
45870528,45866422,2575224,2017-08-24T20:41:23,"<p>In CQRS+Event sourcing you won't necessarily need any joins. Why? See below.</p>&#xA;&#xA;<p>On the write side you have the aggregates that are rehydrated from the event store by replaying the previous events so no joins.</p>&#xA;&#xA;<p>On the read side you have the readmodels/projection and sagas. Here, you denormalize the data. You design your models to contain all the data that is needed by the clients. For example, on the list of comments of a post in a blog you put the author's username also, along with its ID. In this way you don't need to fetch data from the users table because the data is already there (you won't need a general purpose users table either). </p>&#xA;&#xA;<p>There is the question of data freshness. You keep the related/denormalized data fresh by listening to the relevant events. For our example, your readmodel listens to the <code>UsernameWasChanged</code> event and change the username for all its posts.</p>&#xA;&#xA;<p>CQRS makes the joins obsolete.</p>&#xA;"
46034510,46031939,2575224,2017-09-04T09:52:40,"<p>You can query the event store. The actual method of querying is specific to every implementation but in general you can poll for events or subscribe and be notified when a new event is persisted.</p>&#xA;&#xA;<p>The event store is just a persistence for the write side that guaranties a strong consistency for the write operations and an eventual consistency for the read operations. In order to ""understand"" something from the events you need to project those events to a read-model then query the read-model. For example you can have a read-model that contain the current balance for every account as a MongoDB collection.</p>&#xA;"
48143328,48141031,2575224,2018-01-08T01:53:28,"<blockquote>&#xA;  <p>According to micro-service architecture I need separate database for each replica. How can I do that?</p>&#xA;</blockquote>&#xA;&#xA;<p>This ""rule"" refers to the microservice types, not to the instances of the  same microservice.  So, you can scale separately the <code>seat_reservation_service</code> and <code>seat_reservation_sql</code>. For example, you could have 4 instances of <code>seat_reservation_service</code> and 3 instances of <code>seat_reservation_sql</code> (1 master and 2 slaves or a <a href=""https://github.com/colinmollenhour/mariadb-galera-swarm/blob/master/README.md"" rel=""nofollow noreferrer"">Galera</a> cluster).</p>&#xA;"
48168572,48167778,2575224,2018-01-09T12:48:26,"<blockquote>&#xA;  <p>What if I have to call service B from service A, should I go all the way and use the cluster's external IP/domain and use HTTPS (<a href=""https://example.com/api/user/1"" rel=""nofollow noreferrer"">https://example.com/api/user/1</a>) to call the service or could I use the internal IP of the service and use HTTP (<a href=""http://serviceb/api/user/1"" rel=""nofollow noreferrer"">http://serviceb/api/user/1</a>)?</p>&#xA;</blockquote>&#xA;&#xA;<p>If you need to use the features that you API Gateway is offering (authentication, cache, high availability, load balancing) then YES, otherwise DON'T. The External facing API should contain only endpoints that are used by external clients (from outside the cluster).</p>&#xA;&#xA;<blockquote>&#xA;  <p>Do I have to encrypt the data or is it ""safe"" as long as it isn't leaving the private k8s network?</p>&#xA;</blockquote>&#xA;&#xA;<p>""safe"" is a very relative word and I believe that there are no 100% safe networks. You should put in the balance <strong>the probability</strong> of ""somebody"" or ""something"" sniffing data from the network and <strong>the impact</strong> that it has on your business if that happens.</p>&#xA;&#xA;<p>If this helps you: for any project that I've worked for (or I heard from somebody I know), the private network between containers/services was more than sufficient.</p>&#xA;&#xA;<blockquote>&#xA;  <p>What if I want to have ""internal"" endpoints that should only be accessible from within the cluster - when I'm always using the external https-url those endpoints would be reachable for everyone. </p>&#xA;</blockquote>&#xA;&#xA;<p>Exactly what I was saying on top of the answer. Keeping those endpoints inside the cluster makes them inaccessible by design from outside.</p>&#xA;&#xA;<p>One last thing, managing a lot of <code>SSL</code> certificates for a lot of internal services is a pain that one should avoid if not necessary.</p>&#xA;"
48005770,48004725,2575224,2017-12-28T10:28:50,"<p>What you are describing could be an <strong>API Gateway</strong>. <a href=""http://microservices.io/patterns/apigateway"" rel=""nofollow noreferrer"">Here</a> is a great tutorial explaining this pattern.</p>&#xA;&#xA;<p>Implement an API gateway that is the single entry point for all clients. The API gateway handles requests in one of two ways. Some requests are simply proxied/routed to the appropriate service. It handles other requests by fanning out to multiple services.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/edRD6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/edRD6.jpg"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>A variation of this pattern is the Backend for Front-End pattern. It defines a separate API gateway for each kind of client.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/vkzud.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vkzud.jpg"" alt=""enter image description here""></a></p>&#xA;&#xA;<p><strong>Using an API gateway has the following benefits:</strong></p>&#xA;&#xA;<ul>&#xA;<li><p>Insulates the clients from how the application is partitioned into microservices</p></li>&#xA;<li><p>Insulates the clients from the problem of determining the locations of service instances</p></li>&#xA;<li><p>Provides the optimal API for each client</p></li>&#xA;<li><p>Reduces the number of requests/roundtrips. For example, the API gateway enables clients to retrieve data from multiple services with a single round-trip. Fewer requests also means less overhead and improves the user experience. An API gateway is essential for mobile applications.</p></li>&#xA;<li><p>Simplifies the client by moving logic for calling multiple services from the client to API gateway</p></li>&#xA;<li><p>Translates from a “standard” public web-friendly API protocol to whatever protocols are used internally</p></li>&#xA;</ul>&#xA;&#xA;<p><strong>The API gateway pattern has some drawbacks:</strong></p>&#xA;&#xA;<ul>&#xA;<li><p>Increased complexity - the API gateway is yet another moving part that must be developed, deployed and managed</p></li>&#xA;<li><p>Increased response time due to the additional network hop through the API gateway - however, for most applications the cost of an extra roundtrip is insignificant.</p></li>&#xA;</ul>&#xA;&#xA;<p><strong>How implement the API gateway?</strong> </p>&#xA;&#xA;<p>An event-driven/reactive approach is best if it must scale to scale to handle high loads. On the JVM, NIO-based libraries such as Netty, Spring Reactor, etc. make sense. NodeJS is another option.</p>&#xA;"
48145061,48144298,2575224,2018-01-08T06:12:12,"<p>It is the bigest zone/area/place where a term has a consistent meaning (it means the same thing for the business specialists and the developers). I intentionally avoid to use the word ""context"". Ideally, a Bounded context is exactly a Domain from the real world.</p>&#xA;&#xA;<p>When it comes to microservices, a microservice should not be larger than a Bounded context.</p>&#xA;&#xA;<p>UPDATE</p>&#xA;&#xA;<p>A <code>Bounded context</code> <em>should be</em> an independent domain, if the system is correctly designed; in reality, when things are not done correctly, a <code>Bounded context</code> is larger than a domain. In large enterprises, some developers create objects (models) that try to capture all the behavior related to some term. For example, <code>Product</code> in a Shop. This term is very broad. The <code>Product</code> from the online-shop and the <code>Product</code> from the inventory system are not one and the same thing, although they may seem that way. In this case, the online-shop should be bounded context and the inventory should be a different one.</p>&#xA;&#xA;<p><strong>Implementation</strong></p>&#xA;&#xA;<p>Each ""product"" should have a different class in each bounded context, for example. A bounded context can be <em>implemented</em> as (a better expression would be ""can be seen as"") a <code>namespace</code> or <code>package</code> in monoliths, or as a <strong>microservice</strong> in distributed systems.</p>&#xA;"
48182099,48181477,2575224,2018-01-10T07:14:05,"<blockquote>&#xA;  <p>Which means that they will get a request and based on that request they will consume a stream of events and generate their read model then return it to the caller.</p>&#xA;</blockquote>&#xA;&#xA;<p>This is a strange type of Read model, at least for me. It does not seem very fast and speed is one of the Read model's strength.</p>&#xA;&#xA;<p>In general, Read models process events in the background, as early as possible (i.e. milliseconds after they are emitted); the results are persisted in a fast database (on disk or memory), with all the indexes applied so when the request comes the response is fast.</p>&#xA;&#xA;<blockquote>&#xA;  <ol>&#xA;  <li><p>We need to provide historical representations of data with each change and the state of the aggregate at that change.</p></li>&#xA;  <li><p>We need to be able to get a snapshot of an aggregate at any point in time per event basis, for example, changing a name, then we need the state at that name changed event</p></li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p>The state of the Aggregate should be hidden, private - the Aggregate needs a high level of encapsulation. Maybe you need an interpretation of the Events generated up until that point: this is a Read model's responsibility. The state is used <strong>only</strong> by the Aggregate to decide if and what events it will generate on the next command. </p>&#xA;&#xA;<p>So, I suggest that you design a Read model that does exactly that: it maintains another state for each Aggregate, in a flat (non-event-sourced) persistence.</p>&#xA;&#xA;<blockquote>&#xA;  <ol start=""3"">&#xA;  <li>We need to represent the diff of the state of the aggregate between points in time</li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p>Again, this should be done by a Read model.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Since both consumer 1 and consumer 2 are going to execute basically the same logic to replay the events, then where should the code for replaying the events be? Will we implement a common library code? Does this mean that we will have duplicate replay code across consumers?</p>&#xA;</blockquote>&#xA;&#xA;<p>But then you said: <code>Consumer 2 --&gt; does exactly the same thing but presents a different read model</code>. This means they don't basically do the same thing. If you are referring to the code that fetches the events from the Event store and feeds the Consumers, then yes, you can put that in a common library.</p>&#xA;&#xA;<blockquote>&#xA;  <p>I am worried that we when update our event schema we need to update multiple consumers</p>&#xA;</blockquote>&#xA;&#xA;<p>This is a <em>problem</em> but one that has <a href=""https://leanpub.com/esversioning/read"" rel=""nofollow noreferrer"">multiple solutions</a>.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Is this a good case of event sourcing?</p>&#xA;</blockquote>&#xA;&#xA;<p>It seems that YES, it <em>may</em> be a case of event sourcing.</p>&#xA;"
45242921,45241581,2575224,2017-07-21T16:54:48,"<p><strong>Yes</strong>, as long as the other microservices would not know or care that the Faas microservice is using this kind of technology. That's the beauty of the microservices architecture: you could have anything inside a microservice as long as it communicates to the outside world using a technology agnostic protocol that hides the underlying technology stack (<strong>yes</strong> to JSON, <strong>no</strong> to Sql protocol - database/table sharing).</p>&#xA;"
45275167,45255905,2575224,2017-07-24T08:01:35,"<p>You are describing a <em>process</em> of sending an <em>order summary email</em> to the customer after the <em>order is completed</em>.</p>&#xA;&#xA;<p><strong>In CQRS this is implemented with a Saga/Process manager</strong>. </p>&#xA;&#xA;<p>The idea is that <code>OrderSummaryEmailSaga</code> subscribe to the <code>OrderWasCompleted</code> event; when such event is fired, the saga queries the <code>Pickup service</code> for the information it needs (most probable from a read-model) and then:</p>&#xA;&#xA;<ol>&#xA;<li>it builds+sends a complete <code>SendOrderSummaryEmail</code> command to the relevant aggregate from the <code>orders service</code> or</li>&#xA;<li>it calls an infrastructure service that, having all the data, it builds an email and send it to the customer</li>&#xA;<li>or a combination of the previous points, depending on how you want to manage this process</li>&#xA;</ol>&#xA;&#xA;<p>The details are specific to you case, like what domain services (building and formatting the email) or infrastructure services (actual sending of the email using <code>sendmail</code> or <code>postfix</code> or whatever)  you need to build.</p>&#xA;"
45275256,45270940,2575224,2017-07-24T08:06:09,"<p>You can expose the IP address of <strong>any node in the cluster</strong> as Docker has a <code>swarm load balancer</code> running on any of the nodes.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/ng8WR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ng8WR.png"" alt=""enter image description here""></a></p>&#xA;"
47324387,47319604,2575224,2017-11-16T07:59:34,"<blockquote>&#xA;  <p>One of the disadvantages of Monolithic architecture is that one faulty functionality will bring the entire system down. If one of these functions stopped working as expected, how will it bring the entire system down?</p>&#xA;</blockquote>&#xA;&#xA;<p>No, not necessarily. You could have a very resilient monolith, with low coupled <strong>modules</strong> that communicate asynchronous for example. If one module goes down then the others will not go down but wait for it to become available or retry and/or use a circuit breaker or whatever mechanism you use to avoid cascade failure. </p>&#xA;&#xA;<p>Or, you could have a microservice architecture in which a single microservice brings down the entire application (i.e. cascade failure).</p>&#xA;&#xA;<blockquote>&#xA;  <p>If it does then how would this problem be solved in a Microservices architecture? The Microservice conversion would mean splitting the single file into multiple files each doing one specific thing. The requests are made to these many different model files. Since the dependencies between that specific service and others would still be there, would it not bring the system down in Microservices architecture as well?</p>&#xA;</blockquote>&#xA;&#xA;<p>It doesn't. Microservices would not resolve your bad (as in non-resilient) architecture. </p>&#xA;&#xA;<p>The main advantage of using microservices is that you can deploy and evolve one microservice independently of the others because they are like black boxes from the others point of view. As long as a microservice expose the same API (or a compatible one) then you could replace it or roll back with another implementation at any given moment.</p>&#xA;"
47306089,47304927,2575224,2017-11-15T11:21:41,"<blockquote>&#xA;  <p>Does it mean no foreign keys referring to tables in another microservices?</p>&#xA;</blockquote>&#xA;&#xA;<p>Not in the database sense. One microservice may hold <code>IDs</code> of remote entities but should not assume anything about the remote microservice persistence (i.e. the database type, it could be anything from SQL to NoSQL).</p>&#xA;&#xA;<blockquote>&#xA;  <p>Which of the above two has to be followed and why?</p>&#xA;</blockquote>&#xA;&#xA;<p>This really depends. There are two types of architectures: choreography and orchestration. Both of them are good. Which one to use? Only you can decide. Here are a few blog posts about them:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://medium.com/capital-one-developers/microservices-when-to-react-vs-orchestrate-c6b18308a14c"" rel=""nofollow noreferrer"">Microservices — When to React Vs. Orchestrate</a></li>&#xA;<li><a href=""https://specify.io/concepts/microservices"" rel=""nofollow noreferrer"">Benefits of Microservices - Choreography over Orchestration, Low Coupling and High Cohesion</a></li>&#xA;</ul>&#xA;&#xA;<p>Also, the solution to this <a href=""https://stackoverflow.com/questions/29117570/orchestrating-microservices"">SO question</a> might be useful.</p>&#xA;"
43308147,43306842,1277048,2017-04-09T14:50:45,"<p>A microservice should not share <em>functionality</em> with other microservices. its a functional unit. If many of your services utilize the same code, there's nothing wrong with extending some common codebase into various different microservices. Even better - write all the common code in some library and have all of them include/import it.</p>&#xA;&#xA;<p>I think you might be confusing code-level with microservices (a functional level).</p>&#xA;&#xA;<p>Regarding the auth issue: As stated in the comment to the question, its not clear which authorization is discussed there. If you're referring to users of the microservices, it might make sense to have one auth-gateway to handle all incoming requests and then (once authorized, re-route the request to the actual service). In this sense, <em>inside</em> your cloud, no authorization would be required allowing easier inter-service communication. Only requests coming from the outside would need to authorize.</p>&#xA;&#xA;<p>Or you could authorize in every service. this makes less sense to me, though. If you go with this approach, you could have the common code (authorization or anything else) as a library.</p>&#xA;"
43246902,43246560,1277048,2017-04-06T06:03:08,"<p>I must commend you for the finely-written question, but of course the answer would depend greatly on the business logic you're dealing with. This question is related to that of eventual-consistency (a property offered by some no-sql databases - like Couchbase). </p>&#xA;&#xA;<p>Ultimately, its a question of tradeoffs: the 'cost' of retrieving the freshest data vs. the cost of using somewhat stale data that's readily available.</p>&#xA;&#xA;<p>a few things factor in:</p>&#xA;&#xA;<ul>&#xA;<li><p>how often the data updates?</p></li>&#xA;<li><p>and more importantly, what happens (business-logic-wise) when you use stale data. Is it even acceptable to your users/apps?</p></li>&#xA;<li><p>what would be the impact on your system to fetch fresh data every time? what's the infrastructure cost of doing that (in machines/money) and what's the latency it would incur?</p></li>&#xA;</ul>&#xA;"
43247378,43238799,1277048,2017-04-06T06:29:40,"<p>We're using <a href=""https://github.com/prometheus/prometheus"" rel=""nofollow noreferrer"">Prometheus</a> + <a href=""https://github.com/influxdata/telegraf/blob/master/docs/WINDOWS_SERVICE.md"" rel=""nofollow noreferrer"">Telegraf</a> + <a href=""https://grafana.com/"" rel=""nofollow noreferrer"">Grafana</a>: a solution designed to collect metrics from applications and offers alerting and graphs (with Grafana). Very happy with it: its open-source, resilient and built for scale.</p>&#xA;&#xA;<p>I see there's an <a href=""https://github.com/andrasm/prometheus-net"" rel=""nofollow noreferrer"">open-source Prometheus client for .net</a>, although you could just use Telegraf to expose your metrics if that was not available.</p>&#xA;"
43255640,43252404,1277048,2017-04-06T12:45:53,"<p>If efficiency is paramount, you could use <a href=""https://developers.google.com/protocol-buffers/"" rel=""nofollow noreferrer"">Google's Protobuf</a>. Its a bit of a pain (when compared to json) but very efficient. Its also language-agnostic (or to be more precise: it has implementations in most common languages). </p>&#xA;&#xA;<p>You basically define a message according to the proto spec and then a special compiler generates the relevant get/set code. you use that in your code to send and receive super-efficient messages.</p>&#xA;"
43281762,43264727,1277048,2017-04-07T15:21:46,"<p>Splitting your app into multiple servers, as you've suggested, carries some trade-offs. </p>&#xA;&#xA;<p>On the plus side, splitting it up provides you with more flexibility in terms of load balancing. In other words, if your flask servers are overburdened, you can always spin a few more and scale horizontally with a load-balancer. Of course this assumes that whatever it is you're doing on those flask server can be done in parallel (depends on your actual business logic).</p>&#xA;&#xA;<p>It also offers high-availability: you eliminate one potential single-point-of-failure.</p>&#xA;&#xA;<p>However, this 'microservice' approach does incur some overheads</p>&#xA;&#xA;<ul>&#xA;<li>more code to write, since now you're writing 2 kinds of servers</li>&#xA;<li>some network overhead, since now you're communicating over the network as opposed to function calls.</li>&#xA;<li>more machines to spin (although you could run everything in containers and they could all be on the same machine, if you dont need the extra processing power)</li>&#xA;</ul>&#xA;&#xA;<p>You could consider using <a href=""https://github.com/google/protobuf"" rel=""nofollow noreferrer"">google-protobuff</a> to serialize/de serialize the messages. its language-agnostic and saves some of the network overhead. its not as easy as sending json, but if efficiency is paramount, it might be worth the trouble. Plus it's supported in both python and go.</p>&#xA;"
43199411,43179951,1277048,2017-04-04T06:03:23,"<p>So, I'm not sure this would answer your needs - but i'll describe our overall approach to BI:</p>&#xA;&#xA;<ol>&#xA;<li>Everything in our system generates an event: actions in the backend, actions in the mobile apps - everything we want to track produces event with the relevant data (ids, time, name etc).</li>&#xA;<li>All the events are sent to some common funnel for collection - its a backend app that takes events - makes sure they're valid - and stores them.</li>&#xA;<li>You can store the events in some no-sql storage (like Elasticsearch) or on a cloud (like google's BigQuery). </li>&#xA;<li>Once they're in, its just a matter of querying and cross-referencing to get the overall picture you want. That's what our BI people do: they generate a picture from the heaps of events we collect.</li>&#xA;</ol>&#xA;"
42695657,42691892,1277048,2017-03-09T12:40:33,"<p>You're mixing up different, unrelated things.</p>&#xA;&#xA;<p>(micro)services are logical entities that do some specific task. they communicate with other services to perform a larger-scope task.</p>&#xA;&#xA;<p>Tables/CRUD/SQL/NO-SQL come from an entirety different level. its where data is saved and how its accessed.</p>&#xA;&#xA;<p>Its true that services use SQL and have tables. Its also probably a good idea to have separate tables for each service. I would even go as far as saying that if 2 services directly use the same table you're probably looking at a design problem.</p>&#xA;&#xA;<p>but you can't equate services with tables, conceptually, they belong in different worlds.</p>&#xA;"
42606089,42582463,1277048,2017-03-05T08:21:16,"<p>TL;DR: Prometheus (usually) works by pulling metrics off a server, so I dont see how you could apply it directly onto S3, unless you generate a dynamic page with the number of png's on S3.</p>&#xA;&#xA;<p>in details:&#xA;The way Prometheus works is by pulling metrics, available as HTTP pages, from servers. Your server will need to publish this special page called /metrics and Prometheus will go there and get its contents.</p>&#xA;&#xA;<p>If you can generate a <em>dynamic</em> <em>public</em> page on S3 that would export the current number of .pngs in your bucket, that this should work. just point Prometheus to it.</p>&#xA;"
45737104,45735971,1213560,2017-08-17T13:55:36,"<p>The memory and disk specified are on a per application instance basis [1]. </p>&#xA;&#xA;<p>In your example:</p>&#xA;&#xA;<ul>&#xA;<li>App Instance 1: 32Gb memory </li>&#xA;<li>App Instance 2: 32Gb memory </li>&#xA;<li>App Instance 3: 32Gb memory </li>&#xA;<li>App Instance 4: 32Gb memory </li>&#xA;<li>App Instance 5: 32Gb memory</li>&#xA;</ul>&#xA;&#xA;<p>[1] <a href=""https://docs.cloudfoundry.org/devguide/deploy-apps/cf-scale.html"" rel=""nofollow noreferrer"">https://docs.cloudfoundry.org/devguide/deploy-apps/cf-scale.html</a></p>&#xA;"
49536601,49536335,2525304,2018-03-28T14:00:53,"<p>A microservice is a just a web service that is part of a larger system of microservices. Each microservice is doing a specific job and it's the collaborative work between all your microservices that makes a complete product.</p>&#xA;&#xA;<p>What you made is one service that is offering an API (JSON being the response type). In a microservice system, other services would call this API and your services would also call APIs of other services.</p>&#xA;&#xA;<p>If you want to add a microservice to your system, create a second ASP.NET Core Web API that will run parallelly to your other system. They then need to communicate together to make some useful work. For example, if you were making a e-commerce platform, there could be a product service that keeps track of the available products, a billing service that takes care of the checkout and a user service that handles everything related to users.</p>&#xA;&#xA;<p>This might help you: <a href=""https://smartbear.com/learn/api-design/what-are-microservices/"" rel=""nofollow noreferrer"">https://smartbear.com/learn/api-design/what-are-microservices/</a></p>&#xA;"
27127631,26866479,766328,2014-11-25T13:08:23,"<p>As usual in Software Engineering, the answer is it depends. I can't imagine a reason right now but the option 1 could be useful in some particular scenarios.</p>&#xA;&#xA;<p>However, considering the formal definition of microservices, the option 2 illustrates it better. One of the main advantages of having microservices is being able to reuse it. Different applications have different requirements and needs on presenting the information. Making your microservices return a JSON representation of your data will give you more flexibility on how to format this information.</p>&#xA;"
36418526,36400599,421753,2016-04-05T06:18:23,"<p>I have found the issue. The system which it is running is already have some other server running. So, the jetty container could not use the port allocated for it and throwing the exception ""Address already in use: bind"". Thank you @Lawrence Choy for pointing out the issues. </p>&#xA;"
47714865,37180556,633721,2017-12-08T12:54:10,"<p>PactNet v2 already supports <a href=""https://github.com/bethesque/pact-wiki/blob/master/Regular%20expressions%20and%20type%20matching%20with%20Pact.md"" rel=""nofollow noreferrer"">type and regex matching</a>:</p>&#xA;&#xA;<p>Type matching for whole body:</p>&#xA;&#xA;<pre><code>// IMockProviderService&#xA;.WillRespondWith(new ProviderServiceResponse&#xA;{&#xA;    Body = Match.Type(new { Id = 123, FirstName = ""John"" })&#xA;});&#xA;</code></pre>&#xA;&#xA;<p>or for a property:</p>&#xA;&#xA;<pre><code>.WillRespondWith(new ProviderServiceResponse&#xA;{&#xA;    Body = new { Id = 123, FirstName = Match.Type(""John"") }&#xA;});&#xA;</code></pre>&#xA;&#xA;<p>Regex matching:</p>&#xA;&#xA;<pre><code>.WillRespondWith(new ProviderServiceResponse&#xA;{&#xA;    Body = new { FirstName = Match.Regex(""Jan"", @""\A\w+\z"") }&#xA;});&#xA;</code></pre>&#xA;&#xA;<p>There's also <a href=""https://stackoverflow.com/a/47384753/633721"">Match.MinType</a> for arrays. </p>&#xA;"
30498763,30496218,1420921,2015-05-28T06:27:57,"<p><strong>Another observation:</strong></p>&#xA;&#xA;<pre><code>Undertow.Builder builder = Undertow.builder()&#xA;.setSocketOption(Options.BACKLOG, 100000) // &lt;&lt; Does impact the REST&#xA;</code></pre>&#xA;&#xA;<p><strong>and also MAY related to:</strong></p>&#xA;&#xA;<pre><code>$ sysctl -a | grep -i sync&#xA;...&#xA;net.ipv4.tcp_max_syn_backlog = 100000&#xA;</code></pre>&#xA;"
30496924,30496218,1420921,2015-05-28T03:51:33,<p><strong>I just FOUND OUT similar issue from:</strong> &#xA;blog.scene.ro/posts/apache-benchmark-apr_socket_recv </p>&#xA;&#xA;<pre><code>sudo sysctl -w net.ipv4.tcp_syncookies=0 &#xA;</code></pre>&#xA;&#xA;<p>This did the job. NO MORE Connection reset by peer (104). &#xA;Guess this might not be a undertow or xnio-api issue.</p>&#xA;
46845684,46828175,7458937,2017-10-20T08:59:10,<p>Thanks @ g00glen00b </p>&#xA;&#xA;<p>I am changed config service bootstrap yml as follows :</p>&#xA;&#xA;<pre><code> spring:&#xA;  application: &#xA;    name: config &#xA;  cloud:&#xA;    config: &#xA;      server:&#xA;        native: &#xA;          searchLocations: classpath:/config/DEVELOP/&#xA;  profiles:&#xA;     active: native&#xA;&#xA;server:&#xA;  port: 8888&#xA;</code></pre>&#xA;&#xA;<p>Its working fine.</p>&#xA;
49937367,48824086,6814073,2018-04-20T08:31:15,<p>Using version <code>3.3.0.Final</code> for the starter and adapter works for me. When I change the version to <code>3.4.3.Final</code> I get the same exception.</p>&#xA;&#xA;<p>I'm running Keycloak server 3.4.3 (standalone)...</p>&#xA;&#xA;<pre><code>&lt;dependency&gt;&#xA;    &lt;groupId&gt;org.keycloak&lt;/groupId&gt;&#xA;    &lt;artifactId&gt;keycloak-spring-security-adapter&lt;/artifactId&gt;&#xA;    &lt;version&gt;3.3.0.Final&lt;/version&gt;&#xA;&lt;/dependency&gt;&#xA;&#xA;&lt;dependency&gt;&#xA;    &lt;groupId&gt;org.keycloak&lt;/groupId&gt;&#xA;    &lt;artifactId&gt;keycloak-spring-boot-starter&lt;/artifactId&gt;&#xA;    &lt;version&gt;3.3.0.Final&lt;/version&gt;&#xA;&lt;/dependency&gt;&#xA;</code></pre>&#xA;&#xA;<p>Anyone knows a solution to use the current version of the dependencies?</p>&#xA;
35114283,35113957,1055866,2016-01-31T13:19:06,"<p>Am not 100% sure on your question so this will be a wide answer.  </p>&#xA;&#xA;<p>1) <strong>Everything can be in the same compose file</strong> if it's running on the same machine or server cluster.</p>&#xA;&#xA;<pre><code>#proxy&#xA;haproxy:&#xA;  image: haproxy:latest&#xA;  ports:&#xA;    - 80:80&#xA;&#xA;&#xA;#setup 1&#xA;ubuntu_1:&#xA;  image: ubuntu&#xA;  links:&#xA;    - db_1:mysql&#xA;  ports:&#xA;    - 80&#xA;&#xA;db1:&#xA;  image: ubuntu&#xA;  environment:&#xA;    MYSQL_ROOT_PASSWORD: 123&#xA;&#xA;&#xA;#setup 2&#xA;ubuntu_2:&#xA;   image: ubuntu&#xA;   links:&#xA;     - db_2:mysql&#xA;   ports:&#xA;    - 80&#xA;&#xA;db2:&#xA;  image: ubuntu&#xA;  environment:&#xA;    MYSQL_ROOT_PASSWORD: 123&#xA;</code></pre>&#xA;&#xA;<p>It's also possible to <strong>combine several yml files</strong> like<br>&#xA;<code>$docker-compose -f [File A].yml -f [File B].yml up -d</code></p>&#xA;&#xA;<p>2) Every container in the build can be <strong>controlled separately with compose.</strong><br>&#xA;<code>$docker-compose stop/start/build/ ubuntu_1</code></p>&#xA;&#xA;<p>3) Using <code>$docker-compose build</code> it <strong>will only rebuild where changes have been done.</strong></p>&#xA;&#xA;<p>Here is more information that could be useful &#xA;<a href=""https://docs.docker.com/compose/extends/#extending-services"" rel=""nofollow"">https://docs.docker.com/compose/extends/#extending-services</a></p>&#xA;&#xA;<p>If none of above is correct please example of build.</p>&#xA;"
39914839,39913816,284538,2016-10-07T10:15:37,"<p>From that article:</p>&#xA;&#xA;<blockquote>&#xA;  <ul>&#xA;  <li>maintain two copies of your production environment (“blue” and “green”) </li>&#xA;  <li>route all traffic to the the blue environment by mapping production URLs to it; </li>&#xA;  <li>deploy and test any changes to the application in the green environment; </li>&#xA;  <li>“flip the switch” by mapping URLs onto green and unmapping them from blue.</li>&#xA;  </ul>&#xA;</blockquote>&#xA;&#xA;<p>It's actually not that difficult if you have the equipment and a good deployment process set up. If you've set up your security mechanisms so that the user doesn't need to start a new session in blue environment, the pain is almost entirely in the provisioning and ensuring the blue environment is in as perfect of a state as you want it to be. After that, it's just a matter of getting into your load balancer's configuration and flipping the settings to point to the blue environment.</p>&#xA;&#xA;<p>But once you have ""green"" and ""blue"" up and running, you can flip back and forth. Ideally, once blue is verified, immediately upgrade green. Also, make sure you are sharing database operations between the two environments so green can also be a fallback for blue if blue fails.</p>&#xA;&#xA;<p>Granted, my experience planning and helping with this was on Hadoop where you often have a formal data ingestion pipeline that can easily be configured to feed all new data to two completely separate environments.</p>&#xA;"
45300627,45300410,8225146,2017-07-25T10:29:30,"<p>It have many way to desiged microservice, you can write file restfull method like </p>&#xA;&#xA;<pre><code>class RestFulClient:&#xA;   @classmethod&#xA;   def get(cls, url, loggers, headers):&#xA;       return is_success, status_code, data&#xA;&#xA;   @classmethod&#xA;   def post(cls, url, headers, loggers, params={}):&#xA;       return is_success, status_code, status_message, data&#xA;&#xA;   @classmethod&#xA;   def put(cls, url, headers, loggers, params={}):&#xA;       return is_success, status_code, status_message, data&#xA;&#xA;   @classmethod&#xA;   def delete(cls, url, headers, loggers, params={}):&#xA;       return is_success, status_code, status_message&#xA;</code></pre>&#xA;&#xA;<p>Any question?</p>&#xA;"
31694513,31546631,306042,2015-07-29T07:56:54,"<p>I was looking for something similar, including support for rate limiting, UI console, etc. It boils down to a few freemium tools like:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""http://apigee.com"" rel=""noreferrer"">apigee</a></li>&#xA;<li><a href=""http://mashape.com"" rel=""noreferrer"">mashape</a></li>&#xA;<li><a href=""http://apiary.io"" rel=""noreferrer"">apiary</a></li>&#xA;<li><a href=""http://3scale.net"" rel=""noreferrer"">3scale.net</a></li>&#xA;</ul>&#xA;&#xA;<p>and a few open source ones:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""http://tyk.io"" rel=""noreferrer"">tyk</a></li>&#xA;<li><a href=""http://getkong.org"" rel=""noreferrer"">kong</a></li>&#xA;<li><a href=""http://apiaxle.com/"" rel=""noreferrer"">ApiAxle</a></li>&#xA;<li><a href=""http://wso2.com/api-management/"" rel=""noreferrer"">WSO2</a></li>&#xA;<li><a href=""https://apiumbrella.io/"" rel=""noreferrer"">API Umbrella</a></li>&#xA;</ul>&#xA;&#xA;<p>I've decided on tyk since it has a nice UI console and solid docs. All of them were mentioned on Quora, which is nice when you want to go shopping :) </p>&#xA;"
47379489,47379427,242042,2017-11-19T16:59:36,"<p>Use an HTTP Client that would pass the message.  In JAX-RS you'd have the JAX-RS Clients.</p>&#xA;&#xA;<p>If you use <a href=""https://github.com/wso2/msf4j/tree/master/client/src/main/java/org/wso2/msf4j/client"" rel=""nofollow noreferrer"">MS4J from WSO2</a> they provide &#xA;* Feign which goes on top of Apache HTTP Client</p>&#xA;&#xA;<p>For my own framework <a href=""https://github.com/trajano/app-ms"" rel=""nofollow noreferrer"">app-ms</a> I use RestEasy's JAX-RS implementation but utilize vert.x as my HTTP client so I can get better performance.</p>&#xA;"
35318721,35286730,5726587,2016-02-10T15:06:26,"<p>In order to reduce the number of requests made you might create a facade that encapsulates incoming API calls. This enables you to get rid of expensive network roundtrips.</p>&#xA;&#xA;<p>Netflix blogged about a very similar challenge</p>&#xA;&#xA;<p><a href=""http://techblog.netflix.com/2013/01/optimizing-netflix-api.html"" rel=""nofollow"">http://techblog.netflix.com/2013/01/optimizing-netflix-api.html</a></p>&#xA;&#xA;<p><a href=""http://techblog.netflix.com/2012/07/embracing-differences-inside-netflix.html"" rel=""nofollow"">http://techblog.netflix.com/2012/07/embracing-differences-inside-netflix.html</a></p>&#xA;"
37744577,37632735,563323,2016-06-10T09:23:40,"<p>You should never store an Infinispan CacheManager (or a Cache) in the session. Make it a singleton.</p>&#xA;&#xA;<p>Infinispan is able to handle replication without the need of Redis, just make sure you enable the right Cache Mode (for example 'replicated' or 'distributed') and your user data will be available from all other nodes without needing to store the cache in Redis.</p>&#xA;"
37707506,37706697,5154683,2016-06-08T16:01:15,"<p>You're running into the biggest challenge of distributed systems -- <a href=""https://en.wikipedia.org/wiki/CAP_theorem"" rel=""nofollow"">the CAP theorem</a>. To summarize, a distributed system (like you have) can never guarantee all three of the below:</p>&#xA;&#xA;<ul>&#xA;<li><strong>C</strong>onsistency - all services see the same data at the same time</li>&#xA;<li><strong>A</strong>vailability - all services' requests to each other receive a success or failure response</li>&#xA;<li><strong>P</strong>artition tolerance - the system continues to function even when services cannot reach each other</li>&#xA;</ul>&#xA;&#xA;<p>In your specific problem, the lack of consistency is hurting you. You can fix it, but you'll have to give up one of the other points. For example, before sending each SMS, you could have service B perform a synchronous HTTP request to service A to verify that the intended recipient still has permissions to receive SMSes. This will fix your consistency issue, but creates a dependency that A must be up and running for B to function (meaning, you've lost the <strong>P</strong> from CAP).</p>&#xA;&#xA;<p>You could also mitigate the issue by sending SMS requests as events through the same queue that the permission updates go through. It's still possible for an SMS to send after the permission was disabled, but that would only happen if the permission update came in after the SMS was meant to be sent. This is still lacking consistency, but the impact is less severe (at a cost of SMS latency).</p>&#xA;"
51801828,51726683,2863995,2018-08-11T16:53:29,"<p>I don't think your question is strictly related to AWS but more like a general way of communication between the services.</p>&#xA;&#xA;<p>API Gateway is used as an edge service which is a service at your backend boundary and accessible by external parties. For communication behind the API Gateway, between your microservices, you don't necessary have to go through the API Gateway again.</p>&#xA;&#xA;<p>There are 2 ways of communication which I'd mention for your case:</p>&#xA;&#xA;<ul>&#xA;<li>HTTP</li>&#xA;<li>Messaging</li>&#xA;</ul>&#xA;&#xA;<p>HTTP is the most simplistic way of communication as it's naturally easier to understand and there are tons of libraries which makes it easy to use. </p>&#xA;&#xA;<p>Despite the fact of the advantages, there are a couple of things to look out for.</p>&#xA;&#xA;<ul>&#xA;<li>Failure handling</li>&#xA;<li>Circuit breaking in case a service is unavailable to respond</li>&#xA;<li>Consistency</li>&#xA;<li>Retries</li>&#xA;<li>Using service discovery (e.g. Eureka) to make the system more flexible when calling another service</li>&#xA;</ul>&#xA;&#xA;<p>On the messaging side, you have to deal with asynchronous processing, infrastructure problems like setting up the message broker and maintaining it, it's not as easy to use as pure HTTP, but you can solve consistency problems with just being eventually consistent.</p>&#xA;&#xA;<p>Overall, there are tons of things which you have to consider and everything is about trade-offs. If you are just starting with microservices, I think it's best to start with using HTTP for communication and then slowly going to the messaging alternative.</p>&#xA;&#xA;<p>For example in the Java + Spring Cloud Netflix world, you can have Eureka with Feign and with that it's really easy to use logical address to the services which is translated by Eureka to actual IP and ports. Also, if you wanna use Swagger for your REST APIs, you can even <a href=""https://blog.arnoldgalovics.com/2018/06/11/generating-feign-clients-with-swagger-codegen-and-gradle/"" rel=""nofollow noreferrer"">generate Feign client stubs</a> from it.</p>&#xA;"
51801461,51799330,2863995,2018-08-11T16:05:06,"<p>I don't think you wanna use API gateway for communication between your services. API Gateway is used for providing a service where all the external API calls are going through.</p>&#xA;&#xA;<p>If you don't wanna use messaging, they you can go straight with <code>RestTemplate</code> as you mentioned but keep in mind that if you are referencing the services directly with IP and port, it might be painful to run it in different environments in the future.</p>&#xA;&#xA;<p>I guess you are using the Spring Cloud Netflix stack and if that's the case I'd say go with using Eureka for storing the service metadata and go straight with Feign. It has an integration with Eureka where you can basically resolve logical names into actual IP and port numbers like the following:</p>&#xA;&#xA;<pre><code>http://some-service/endpoint -&gt; http://12.34.56.78:9101/endpoint&#xA;</code></pre>&#xA;&#xA;<p>In this case you can save time on the long run when you wanna deploy your application into different kind of environments with different network setups and port configurations.</p>&#xA;"
51907935,51907715,2863995,2018-08-18T11:06:23,"<p>The problem is your <code>TokenRelayRequestInterceptor</code> that it tries to resolve authentication information from the current thread-bound security context.</p>&#xA;&#xA;<p>Obviously, when you are in a consumer thread, you don't have such information (by default at least) thus the resolution fails.</p>&#xA;&#xA;<p>The thing you can do is the following:</p>&#xA;&#xA;<ul>&#xA;<li>Pass the necessary information in the event (might be problematic for Event sourced systems if it's the access token as it might expire)</li>&#xA;<li>Somehow try to fake the access token which is accepted by the third service</li>&#xA;</ul>&#xA;&#xA;<p>On the consumer side, you can either manually set up the SecurityContext with the information needed for the interceptor or you forget about the interceptor on the consumer side and manually provide the data the third service needs (I guess it's just an <code>Authorization</code> header).</p>&#xA;&#xA;<p><strong>UPDATE</strong>&#xA;You can also create an internal endpoint in the third service which doesn't require any authentication at all, it's not accessible by externals.</p>&#xA;"
43062261,43056426,2863995,2017-03-28T06:37:06,"<p>If you really wanna follow the microservices way then I'd say don't use the shared library.</p>&#xA;&#xA;<p>The whole point of this concept is decoupling the services. If you make a shared library which contains some classes which are ""the same"" for multiple services then each time when a service want to enhance these classes, it has to be done within the shared library, forcing the other services to adapt.</p>&#xA;&#xA;<p>Decoupling the services makes possible for each of them to evolve individually.</p>&#xA;"
48853591,48791411,2863995,2018-02-18T15:56:15,"<p>I'm not familiar with RMQ but I'll try to help.</p>&#xA;&#xA;<p>For me it seems that you want to scale your consumers in a way that each consumer is dedicated for a single resource. This is necessary as you have dependency between the sequences for a single resource, thus there is no point distributing the work between multiple consumers.</p>&#xA;&#xA;<p>I have experience with Kafka and there you can use so called ""topics"" to send messages to and have a consumer dedicated for a single topic which grabs the work items from there.</p>&#xA;&#xA;<p>Not sure if this is possible on RMQ though.</p>&#xA;&#xA;<p>If this is not an option on your architecture, I'd try the following:&#xA;Dedicate a single consumer for a resource by checking the message payload first whether it is relevant for that resource. If yes then execute the work, if not then requeue it. </p>&#xA;"
48098155,48097374,2863995,2018-01-04T15:10:51,"<p>In Zuul routing config, use <code>stripPrefix: false</code> as the default is true.</p>&#xA;"
51855227,51855075,7588475,2018-08-15T08:19:23,"<p>Note: I'd go with Kubernetes instead Docker-Compose.</p>&#xA;&#xA;<ol>&#xA;<li>Try adding to compose:</li>&#xA;</ol>&#xA;&#xA;<p><code>webAuth:&#xA;    build: DockerfileAuth&#xA;    image: dockerdjangoexample&#xA;    command: bash -c ""gunicorn demosite.wsgi:application -b 0.0.0.0:8000""&#xA;    volumes:&#xA;      - .:/code&#xA;    depends_on:&#xA;      - db&#xA;      - cache&#xA;   webAnotherService:&#xA;    build: DockerfileAnotherService&#xA;    image: dockerdjangoexample&#xA;    command: bash -c ""gunicorn demosite.wsgi:application -b 0.0.0.0:8010""&#xA;    volumes:&#xA;      - .:/code&#xA;    depends_on:&#xA;      - db&#xA;      - cache&#xA;</code></p>&#xA;&#xA;<ol start=""2"">&#xA;<li><p>Rename your Dockerfile accordingly. </p></li>&#xA;<li><p>Add your WSGI AnotherService to Nginx.</p></li>&#xA;</ol>&#xA;&#xA;<p>Another Note: Take a look at Nginx Unit, which is design for (Micro-)Service Architecture - <a href=""https://www.nginx.com/products/nginx-unit/"" rel=""nofollow noreferrer"">Nginx Unit</a></p>&#xA;"
46188800,46185813,707451,2017-09-13T04:21:00,"<p>To log internal HTTP request sent from a Node.js server, you can create a Proxy Node.js server and log all requests there using <a href=""https://www.npmjs.com/package/morgan"" rel=""nofollow noreferrer"">Morgan</a>.</p>&#xA;&#xA;<p>First, define 3 constants (or read from your project config file):</p>&#xA;&#xA;<pre><code>// The real API endpoint, such as ""another micro-service"" in your network&#xA;const API = http://&lt;real_server&gt;&#xA;// Proxy Node.js server running on localhost&#xA;const LOGGER_ENDPOINT=http://localhost:3010&#xA;// Flag, decide whether logger is enabled.&#xA;const ENABLE_LOGGER=true&#xA;</code></pre>&#xA;&#xA;<p>Second, When your Node.js server is launched, start the logger server at the same time if <code>ENABLE_LOGGER</code> is true. The logger server only do one thing: log the request and forward it to the real API server using <code>request</code> module. You can use <a href=""https://www.npmjs.com/package/morgan"" rel=""nofollow noreferrer"">Morgan</a> to provide more readable format.</p>&#xA;&#xA;<pre><code>const request = require('request');&#xA;const morgan = require('morgan')(':method :url :status Cookie: :req[Cookie] :res[content-length] - :response-time ms');&#xA;...&#xA;if (ENABLE_LOGGER &amp;&amp; LOGGER_ENDPOINT) {&#xA;  let loggerPort = 3010;&#xA;  const logger = http.createServer((req, res) =&gt; {&#xA;    morgan(req, res, () =&gt; {&#xA;      req.pipe(request(API + req.url)).pipe(res);&#xA;    });&#xA;  });&#xA;  logger.listen(loggerPort);&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Third, in your Node.js server, send API request to logger server when <code>ENABLE_LOGGER</code> is true, and send API directly to the real server when <code>ENABLE_LOGGER</code> is false.</p>&#xA;&#xA;<pre><code>let app = express(); // assume Express is used, but this strategy can be easily applied to other Node.js web framework.&#xA;...&#xA;let API_Endpoint = ENABLE_LOGGER ? LOGGER_ENDPOINT : API;&#xA;app.set('API', API_Endpoint);&#xA;...&#xA;// When HTTP request is sent internally&#xA;request(app.get('API') + '/some-url')... &#xA;</code></pre>&#xA;"
48577436,44305351,7588987,2018-02-02T07:09:13,"<p>I posted with the requested data in XML format and the code look like this. You should add the request property Accept and Content-Type also.</p>&#xA;&#xA;<pre><code>URL url = new URL(""...."");&#xA;HttpURLConnection httpConnection = (HttpURLConnection) url.openConnection();&#xA;&#xA;httpConnection.setRequestMethod(""POST"");&#xA;httpConnection.setRequestProperty(""Accept"", ""application/xml"");&#xA;httpConnection.setRequestProperty(""Content-Type"", ""application/xml"");&#xA;&#xA;httpConnection.setDoOutput(true);&#xA;OutputStream outStream = httpConnection.getOutputStream();&#xA;OutputStreamWriter outStreamWriter = new OutputStreamWriter(outStream, ""UTF-8"");&#xA;outStreamWriter.write(requestedXml);&#xA;outStreamWriter.flush();&#xA;outStreamWriter.close();&#xA;outStream.close();&#xA;&#xA;System.out.println(httpConnection.getResponseCode());&#xA;System.out.println(httpConnection.getResponseMessage());&#xA;&#xA;InputStream xml = httpConnection.getInputStream();&#xA;</code></pre>&#xA;"
45645641,45644454,185723,2017-08-12T01:19:09,"<p>You are probably getting 404 because your controller thinks it should bind a View with the data it fetches.</p>&#xA;&#xA;<p>Tell it not to bind a View essentially making it a REST endpoint and directly write into the HTTP Response Body by annotating your method with <strong>@ResponseBody</strong>.</p>&#xA;&#xA;<p><a href=""http://docs.spring.io/spring/docs/current/spring-framework-reference/htmlsingle/#mvc-ann-responsebody"" rel=""nofollow noreferrer"">Some further info on @ResponseBody from the Documentation</a></p>&#xA;"
35642027,35639882,18044,2016-02-26T01:58:19,"<p>It's not really clear why you would want to store tokens in Redis. The security token typically contains information about the user (claims data) already. If you need information about the user that is not stored in the token, you should be able to look that up by a simple database query on the user id claim.</p>&#xA;&#xA;<p>Each service can validate the incoming token by checking its <a href=""https://en.wikipedia.org/wiki/Digital_signature"" rel=""nofollow"">digital signature</a> (only needs the public key of the signing certificate for this), lifetime (when does the token expire), audience (who is the token for) etc. If the caller presents a valid token, the user is authenticated.</p>&#xA;"
32578061,32574103,18044,2015-09-15T05:01:42,"<p>Your question is about two independent issues. </p>&#xA;&#xA;<p>Making your service accessible from another origin is easily solved by implementing <a href=""http://www.html5rocks.com/en/tutorials/cors/"" rel=""nofollow"">CORS</a>. For non-browser clients, cross-origin is not an issue at all.</p>&#xA;&#xA;<p>The second problem about service authentication is typically solved using <a href=""https://scotch.io/tutorials/the-ins-and-outs-of-token-based-authentication"" rel=""nofollow"">token based authentication</a>. </p>&#xA;&#xA;<p>Any caller of one of your microservices would get an access token from the authorization server or STS for that specific service. </p>&#xA;&#xA;<p>Your client authenticates with the authorization server or STS either through an established session (cookies) or by sending a valid token along with the request.</p>&#xA;"
38864136,38863827,18044,2016-08-10T04:17:10,"<p>There's no right or wrong answer for this question. IMO, it depends on whether messages are stand-alone resources or are part of a user resource in your domain logic.</p>&#xA;&#xA;<p>If messages always belong to a single user, then you could view the user's messages as a sub-resource or a hierarchical division in the collection of messages and I would probably prefer the first URI scheme. In this case, I would probably go with a path like <code>/user/:id/messages</code> instead of the plural 'users'. Or put the user id behind the messages like: <code>/messages/user/:id</code></p>&#xA;&#xA;<p>If messages are an entity by themselves within your domain or can belong to multiple users (like email messages) it would make more sense to filter messages using a query string scheme.</p>&#xA;"
33128889,33118913,18044,2015-10-14T15:00:46,"<p>Yeah, that's a bit of a chicken and egg problem. Not sure how much control you have over the authorization server, but one way of solving this issue is to secure the call to the user info service using <a href=""https://en.wikipedia.org/wiki/Client_certificate"" rel=""nofollow"">client certificates</a>.</p>&#xA;&#xA;<p>Another is to combine the user info service and authorization server into one service and eliminate the need for the call all together.</p>&#xA;"
49071077,46120795,761468,2018-03-02T14:24:08,"<p>In order to register your service with a non-automatically configured hostname, place the host name in <code>eureka:instance:hostName</code> AND set <code>eureka:instance:registrationMethod</code> to <code>hostname</code></p>&#xA;"
50591790,46131196,6931836,2018-05-29T19:41:06,"<p>This is happening because it is trying to connect to any known server, so to stop that error, a known server is eureka server on port 8761 which is its default port, you will have to update the application.properties with following</p>&#xA;&#xA;<pre><code>server.port=8761&#xA;</code></pre>&#xA;&#xA;<p>To avoid eureka from registering itself, you add this to the application.properties</p>&#xA;&#xA;<pre><code>eureka.client.register-with-eureka=false&#xA;</code></pre>&#xA;&#xA;<p>Ensure that EurekaServer is enabled, for example using spring boot you write the below on the main class.</p>&#xA;&#xA;<pre><code>@EnableEurekaServer&#xA;</code></pre>&#xA;&#xA;<p>Please pardon me providing solution using .properties file but that is what i work with but .yml configurations shouldn't be too different.</p>&#xA;"
46618862,46532286,5639549,2017-10-07T09:51:48,"<p><strong>How can I dynamically handle the interactions</strong> -</p>&#xA;&#xA;<p>See if using an existing EIP pattern solves your problem to implement the logistics</p>&#xA;&#xA;<p>Depending on how your design shapes up, you may need to use Distributed Lock Management</p>&#xA;&#xA;<p>Or maybe your application is simple enough to use a Consul K/V store as a semaphore &amp; a simple mosquitto topic based bus. </p>&#xA;&#xA;<p><strong>Provide scalability</strong></p>&#xA;&#xA;<p>What is the solution you are trying to scale? AMQP, Consul, ""microservices"" in themselves are very scalable &amp; distributed&#xA;However, to scale your thought process &amp; devops, you need to find a way to see things as patterns that help you split the problem &amp; tackle the complexity</p>&#xA;&#xA;<p><strong>Do you know something maybe like an ESB where I can couple two services together with a message transform adapter in the middle of it and I can change dependenciesat runtime?</strong></p>&#xA;&#xA;<p>Read up on EIP. ESBs are just one of the many ways you can solve your problem. RTFM, &amp; get some perspective.</p>&#xA;&#xA;<p><strong>But I don't know a good tool/program for managing the data flow.</strong> </p>&#xA;&#xA;<p>Ask yourself if your problem is related to distributed workflow management, or if a data pipeline is what you are really looking for</p>&#xA;&#xA;<p>Look at Spark, Storm, Luigi, Airflow - they all have a different purpose - but you will know what to do with them if you manage to read up on everything else in this post ;)</p>&#xA;"
43384694,43383125,140925,2017-04-13T06:09:13,"<p>How about an evolutionary design approach instead of refactoring using your old EJBs?</p>&#xA;&#xA;<blockquote>&#xA;  <p>Indeed many microservice groups take this further by explicitly expecting many services to be scrapped rather than evolved in the longer term.</p>&#xA;  &#xA;  <p>The Guardian website is a good example of an application that was designed and built as a monolith, but has been evolving in a microservice direction. The monolith still is the core of the website, but they prefer to add new features by building microservices that use the monolith's API.</p>&#xA;</blockquote>&#xA;&#xA;<p>(<a href=""https://martinfowler.com/articles/microservices.html#EvolutionaryDesign"" rel=""nofollow noreferrer"">From Martin Fowler</a>)</p>&#xA;"
44626314,42484087,2830461,2017-06-19T09:03:56,"<p>I've been building similar distributed applications since a while using the second approach you suggested, in which microservices run in an Akka Cluster, with few predefined seeder nodes. Each microservice identifies itself using Roles, and has a list of roles that it's interested in.</p>&#xA;&#xA;<p>Each node/microservice has an actor listening to Member Events which is responsible for keeping path address' for nodes interested in, and also it acts as a router for inter-node messages.</p>&#xA;&#xA;<p>I'm fine with this approach, it provides scalability in and out easily.</p>&#xA;"
44631822,44625192,6654077,2017-06-19T13:24:00,"<p><strong>About Kafka:</strong></p>&#xA;&#xA;<p>First of all, consider if you really want to run Kafka as your message broker.</p>&#xA;&#xA;<p>Kafka is fast, but there are a lot of subtleties behind it that make it harder to use than you might expect. If you are running only web services maybe other pub/sub technologies might be more appropriate.</p>&#xA;&#xA;<p>Kafka is a topic on its own, so I will make it short on what you should regard more closely. This is opinionated, and from my own experience with the techno:</p>&#xA;&#xA;<ul>&#xA;<li>you cannot easily read, browse or delete messages in your topics, good luck finding that one single buggy message if you do not perform proper logging</li>&#xA;<li>the partitioning system requires extra-administration efforts because they use a different server API than the one used for topics by common libraries. If you run only a few consumers you might be fine with a single partition per topic.</li>&#xA;<li>the offset commit feature: it is of big importance when you want to ensure that not a single message is missed, so you might want to disable autocommit and perform manual validation of offsets</li>&#xA;<li>unless you guarantee idempotence, you will have to implement an 'exactly-once' feature by yourself, because Kafka is 'at least once': you do not want to debit that customer twice, or ship your products again, or spam-flood the customers because you mishandled Kafka's replayability features.</li>&#xA;<li>kafka consumers classes usually 'block' to read until they have events, which might not be ideal if you run single-threaded processes and require to monitor other stuff... You might end up doing polls() but those implementations may vary according to your client library.</li>&#xA;<li>the best monitoring tool out there for Kafka was made by Yahoo and is community-maintained. The official Kafka administration and monitoring tools are poor and laughable at.</li>&#xA;<li>Kafka is still immature and I would not say it is production-safe but since 0.10 things are better.</li>&#xA;<li>I don't know right now, but, months ago, most of the client JS libraries were either obsolete (Kafka &lt;= 0.8), badly documented or painful to use.</li>&#xA;</ul>&#xA;&#xA;<p><strong>About topic architecture:</strong></p>&#xA;&#xA;<p>First of all, @Michal Borowiecki gave some good points which are worth considering.</p>&#xA;&#xA;<p>From my own experience, you will find convenient to have a topic per event type: whenever you open the topic you know what you will find in it.</p>&#xA;&#xA;<p>If you need to consume different event types, you can do it by consuming multiple topics at the same time (beware, Kafka consumers are not really good at balancing, if you consume 5 topics and one is flooded you might get stuck into consuming all that is incoming from there without being given any data from other topics until things calm down... Reactivity may suffer).</p>&#xA;&#xA;<p>One topic per event type should not prevent you to have different event names for a single event type, and it is fine if your service drops/filters events. </p>&#xA;&#xA;<p>Eg: you can have an event type 'User Connexion' and have 'Logged in' and 'Logged out' as event names for that event type, all within the same topic (user-connexion) - the event name is embedded into the metadata of your event. This make sense because you want to ensure that those messages are handled in order: log-ins must precede log-outs and a logged out user cannot be allowed to perform privileged operations.</p>&#xA;&#xA;<p>If you want to have a more global overview on how events are correlated, or replay events based on a cluster-wide order, you will probably have to implement <strong>correlation IDS</strong>, <strong>vector clocks</strong> or <strong>interval-tree clocks</strong>  and proper storage of your messages for easier manipulation and inspection (eg: dumping the topics onto MongoDB...).</p>&#xA;&#xA;<p>Also, if you use MongoDB, take a look at the findAndModify() feature of MongoDB which will allow you to consistently flag each message with a unique cluster-wide ID.</p>&#xA;"
44633389,44619634,6654077,2017-06-19T14:34:59,"<p>Depending of how your project goes, it might or might not be a good idea to build microservices communication over Socket.io</p>&#xA;&#xA;<p>The first thing that comes to mind are the poor guarantees of the socket.io messaging system, especially the fact that your messages are not stored on disk. While this might be convenient, this is also a real problem when it comes to building audit trails, debugging or replayability.</p>&#xA;&#xA;<p>Another issue I see is scalability/cluster-ability: yes you can have multiple services communicating over one nodeJS broker, but what happens when you have to add more? How will you pass the message from one broker to the other? This will lead you to building an entire message system and you will find safer to use already existing solutions (RabbitMQ, Kafka...)</p>&#xA;"
44632177,44626450,6654077,2017-06-19T13:38:39,"<p>I highly recommend that you proxify all your API calls. </p>&#xA;&#xA;<p>Calling a 3rd party API is ok for some use cases, but not if you start having to deal with a lot of them.</p>&#xA;&#xA;<p>Here are my key points:</p>&#xA;&#xA;<ul>&#xA;<li>Interfacing APIs permits to concentrate the listing, organization and updates of the 3rd party APIs. It also makes easier to build your own tracking, stats and monitoring.</li>&#xA;<li>You can reroute any API if it is down, and provide adequate error handling so that you avoid frustrating timeouts / ugly error messages for your customers due to that 3rd party API being down</li>&#xA;<li>You isolate and secure your customers from outside service: yes if the 3rd party API is exploited by a malicious user (eg: returns 'bad' pictures', redirects the navigation...), you can filter it.</li>&#xA;<li>It is easy to change ONE hostname for your API proxy, it is harder to change 20. If you want to migrate your application into closed environments (private networks) the API proxy will come as a real helper when it comes to all the issues with DNS, proxies, gateways, etc.</li>&#xA;<li>You can offer your own standardized API which interfaces all the others: this will be an accelerator for developments. Take a look at <strong>GraphQL</strong> if that could help you perform both multi-API calls and result sizes optimizations.</li>&#xA;</ul>&#xA;"
44632796,44610219,6654077,2017-06-19T14:06:42,"<p>Advantages of microservices over in-process is not really in the change it represents for message consumption.</p>&#xA;&#xA;<p>Microservices allow you to execute portion of your code on specific nodes within a cluster, permitting to allocate the heavy calculations on powerful computers and secondary or light resources on less powerful resources. Overall it allows you to balance the performances better and scale your resources on the portions of code that require it.</p>&#xA;&#xA;<p>Also, whenever you update the code of a micro-service you do not impact the other services, so that your changes (and errors) are isolated. If everything runs within the same process any wrong update might actually render the entire solution unusable.</p>&#xA;&#xA;<p>In the end, getting the communication out of your process (3rd party broker) allows you to share it with more people, agents, processes, etc. Otherwise people have to become part of your process (a module?) and this is really not efficient.</p>&#xA;&#xA;<p>Honestly, the only good reason you have for intra-process communication within your monolithic is for speed (in-memory communication rather than on-the-wire communication).</p>&#xA;"
44633149,44602525,6654077,2017-06-19T14:23:17,"<p>Welcome in the schemas restaurants, where a huge collection of soups await you!</p>&#xA;&#xA;<p>From experience, it is not a bad thing at all to interface each data provider (service) with our own normalized and internal schema.</p>&#xA;&#xA;<p>Basically: you have one service which provides its own soup, and you build a microservice (or update an existing one) for the sole purpose of representing the schema in a new format.</p>&#xA;&#xA;<p>No easy solution here, but take a look at pipeline solutions such as <strong>Apache Nifi</strong> or <strong>node-red JS</strong> which could help you to quickly build transformation pipelines. Solutiopns like <strong>JOLT</strong> <a href=""https://github.com/bazaarvoice/jolt"" rel=""nofollow noreferrer"">https://github.com/bazaarvoice/jolt</a> can help you transform JSON and perform the effort with a comprehensive set of scripts and tools.</p>&#xA;"
33855579,33798965,2002557,2015-11-22T13:57:53,"<p>I think that option #2 on your list is the correct way of doing it. You haven't said anything about latency numbers in your question. </p>&#xA;&#xA;<p>Generally MS architecture implies that you run on many 'machines/dockers' across multiple regions, any instance can born or die at any given moment, and if you are not running in the same data center then no matter what you do you'll get latency. </p>&#xA;&#xA;<p>We (coming from big bloated enterprise apps) sometimes have a hard time to vertically break down application into logical functional pieces. Because of that, I think it's worth to consider taking out parts from UserService and OtherService and make yet another MS. Don't think that it will be too many MS's, companies have literally in mid-1000s of them and it works fine. Don't be afraid to do it outside the box.</p>&#xA;"
33861844,32106454,2002557,2015-11-23T00:05:23,"<p>What you probably want is an edge/proxy server. I don't know what technology stack you are using, but something like Zuul (Java/Spring) or haproxy/nginx. In other words you have an ""umbrella"" above your MS that does the routing/rewriting/etc for you based on the rules you specify.</p>&#xA;&#xA;<p>Have a look at this post:&#xA;<a href=""http://techblog.netflix.com/2013/06/announcing-zuul-edge-service-in-cloud.html"" rel=""nofollow"">http://techblog.netflix.com/2013/06/announcing-zuul-edge-service-in-cloud.html</a></p>&#xA;"
47089553,47089196,2152061,2017-11-03T06:14:08,"<p>Storing the whole history of variable modifications doesn't sound too awful, actually. For example, you can put modification information onto a queue, then, have a service that will process that queue by batch of elements at a time and put the result into one single place.</p>&#xA;&#xA;<p>This is a common approach, for example, in situations when there is huge parallel workload and you can't synchronize access to only one place with a lock.</p>&#xA;&#xA;<p>Later you can even scale out workers that process the queue.</p>&#xA;"
42377347,42375333,2152061,2017-02-21T20:30:12,"<p>Normally you would have a service that would do all the work related to authentication. Basically that means that X1 and X2 will call that service to authenticate user or verify existing authentication. The only thing that should be performed on X1 and X2 is validation of the token. Token could be valid, valid and expired or not valid. In case of it is valid, you just perform necessary work, regardless of particular server. If it is not valid, you reject request and if it is expired, you redirect user to reauthentication.</p>&#xA;&#xA;<p>But if you are asking about particular environment-specific details and your problem is that X2 does not have some specific cryptography keys that X1 has or something like this, then you forgot to mention what frameworks you are using to obtain JWT.</p>&#xA;"
43237663,43232660,2152061,2017-04-05T17:17:00,"<blockquote>&#xA;  <p>I'm planning to create separate microservice and move all the database operations into it</p>&#xA;</blockquote>&#xA;&#xA;<p>That's how you will lose all benefits from microservice architecture. One service is down — the whole application is down. Unless you have replication on several nodes.</p>&#xA;&#xA;<p>If your app does not work if one service went down(not implying that it's that service that connects to database), then it's still bad architecture and you are not using benefits of microservice architecture.</p>&#xA;&#xA;<p>Correct for of communication would be if service would have their own databases. Or at least that every service that wants, for example, entity User, will not fetch it from DB, but will fetch it from appropriate service. And that appropriate service could fetch it from common DB at the beginning.</p>&#xA;&#xA;<p>Next step (improvement) in the process of accommodation to microservice architecture would be creation of separate databases for each service. And by “separate” I mean that temporal fault of one service or temporal fault of one database will allow the rest of the app to be alive and functioning.</p>&#xA;&#xA;<p>Generally, there are no hidden challenges in your approach. It just does not give any benefits, as an intermediate form between monolith application and microservice-based. </p>&#xA;"
42118643,42115903,2152061,2017-02-08T16:33:05,"<p>It depends on your goals.</p>&#xA;&#xA;<p>From my experience, such “host” or “proxy” services tend to grow bigger and bigger without any real responsibility. Coupling many classes, functionalities and responsibilities in one place is definitely a bad idea — it will be difficult to maintain such service with time.</p>&#xA;&#xA;<p>Moreover, it could be very difficult and resources ineffective to scale out instances of such service if, for example, only Authors service would need to be scaled out. So, if you are going to scale out different services and activities separately, then, sure, you don't need to proxy requests with only one service. You can have several proxy services, for instance, to scale out them separately or call Authors service directly and scale out them instead of proxy.</p>&#xA;&#xA;<p>But if you don't see mentioned opportunities, then keep it as simple as possible — you will always be able to separate services and their proxies when it's time to do this, just keep everything separate inside.</p>&#xA;"
39958707,39668149,2152061,2016-10-10T12:49:47,"<p>1) You should be able to retrieve an ItemContract directly from corresponding actor, by its ID. And perform any changes on these items directly through their actors, otherwise I don't understand the decision to use actors here for ItemContract.</p>&#xA;&#xA;<p>2) It is possible, but looks like a Reliable Service is a better collection here. Actor will be single-threaded even for events, so you would not handle updates simultaneously. That will cause poor performance.</p>&#xA;&#xA;<p>3) Just let any single actor for ItemContract be called and used to serve updates on ItemContracts and let any single actor responsible for ItemGroup be called and used for work with groups only.</p>&#xA;&#xA;<p>Think about Tasks — you don't have to use loops, you can work with actors(at least retrieve their state) in parallel.</p>&#xA;&#xA;<p>Think about third service that will map Group actors to Item actors. Just mapping. No state updates, no event tracking. However, want to warn you that storing of lists in any kind of Reliable dictionary — is a bad idea.</p>&#xA;"
45751682,45751211,2152061,2017-08-18T08:09:33,"<p>To avoid breaking integration, use public contracts. For example, <a href=""https://raml.org/"" rel=""nofollow noreferrer"">RAML</a>. Describe models that clients of particular services would get and pass, describe actions and parameters. In this case, nobody depends on particular service or implementation, both sides need to just test their communication with contracts.</p>&#xA;&#xA;<p>Additional safety measure would be publishing your contract as dll. In this case, consumers of a service would notice changes in a minute. And if they have unit tests, they will probably show any problems more earlier.</p>&#xA;&#xA;<p>As a main rule to stay decoupled: don't reference implementation, reference contracts instead.</p>&#xA;"
42573981,42539505,2152061,2017-03-03T08:28:55,"<p>I agree with victor — POST request could solve potential issue. If you are concerned about performance, for example, in case of thousands of ids, you should consider merging these services together. Or duplicate prices data in Categories service.</p>&#xA;&#xA;<p>Data duplication and merging several services into one for performance reasons are normal situations in microservices architecture. But if your Products service only stores information and doesn't have any business logic, then I definitely would consider merging.</p>&#xA;&#xA;<p>Also, consider having all ids already sorted in Categories service. For me it would be the best solution in this case if Products service will not do anything business useful and only sorting</p>&#xA;"
40851218,40829959,5037437,2016-11-28T18:37:52,"<p>For the first time you can write generic service that is used by two or more consumers, but it will bring you problems if business logic changes for one of the consumers. </p>&#xA;&#xA;<p>My advice would be, if you think that system will not change frequently and does not grow much, make shared service, otherwise decoupling would be much better approach.</p>&#xA;"
40852041,40737349,5037437,2016-11-28T19:31:37,"<ol>&#xA;<li>as many microservice practitioners suggets , you can use api gateway as entry point for clients and make service composition there. more information&#xA;<a href=""http://microservices.io/patterns/apigateway.html"" rel=""nofollow noreferrer"">here</a></li>&#xA;<li>yes http has overhead but sometimes it is overweighted by decomposition</li>&#xA;</ol>&#xA;"
40874136,40856925,5037437,2016-11-29T19:23:06,<p>using one database by mutliple service instances is ok when you are using data partitioning.</p>&#xA;
43341941,43340819,5300093,2017-04-11T09:28:10,"<p>Consider using this library by @thejameskyle  <a href=""https://github.com/thejameskyle/react-loadable"" rel=""nofollow noreferrer"">https://github.com/thejameskyle/react-loadable</a> It acts as a lazy loader for your react components including remote ones</p>&#xA;"
37267816,37219072,6285437,2016-05-17T05:27:25,"<p>Model your domain problem and the boundaries will become apparent. You're making the mistake of designing by technology, pattern, and framework first; instead of modeling your problem domain first and then considering consistency, concurrency, performance etc.</p>&#xA;&#xA;<p>If you're not driving your solution by first considering the domain, you're not doing DDD.</p>&#xA;"
35039332,34841789,4194942,2016-01-27T13:56:32,"<p>There are a few options:</p>&#xA;&#xA;<ul>&#xA;<li>Load balance on client as you suggest for which you'll either need to find a ready-build service discovery library that works with SRV records and handles load balancing and circuit breaking. Another answer suggested Netflix' <a href=""https://github.com/Netflix/ribbon"" rel=""nofollow"">ribbon</a> which I have not used and will only be interesting if you are on JVM. Note that if you are building your own, you might find it simpler to just use Consul's HTTP API for discovering services than DNS SRV records. That way you can ""watch"" for changes too rather than caching the list and letting it get stale.</li>&#xA;<li>If you don't want to reinvent that particular wheel, another popular and simple option is to use a HAProxy instance as the load balancer. You can integrate it with consul via <a href=""https://github.com/hashicorp/consul-template#examples"" rel=""nofollow"">consul-template</a> which will automatically watch for new/failed instances of your services and update LB config. HAProxy then provides robust load balancing and health checking with a lot of options (http/tcp, different balancing algorithms, etc). One possible setup is to have a local HAProxy instance on each docker host and a fixed port assigned statically to each logical service (can store it in Consul KV) so you connect to <code>localhost:1234</code> for service A for example and <code>localhost:2345</code> for service B. Local instance means you don't pay for extra round trip to loadbalancer instance then to the actual service instance but this might not be an issue for you.</li>&#xA;</ul>&#xA;"
51532114,51531230,66686,2018-07-26T06:14:12,"<p>You seem to try to follow the (good) advice that a microservice should maintain its own data and be as independent as possible from other services. Taking this as the premise Domain Driven Design offers helpful advice.</p>&#xA;&#xA;<p>Your microservices manage one (or a group of) Aggregate each. An Aggregate is a cluster of instances that </p>&#xA;&#xA;<ul>&#xA;<li>stay consistent at all times (except for within a method call to the Aggregate Root)</li>&#xA;<li>get stored and loaded atomically from a persistence store.</li>&#xA;</ul>&#xA;&#xA;<p>Each Aggregate has exactly one Aggregate Root. It's component only get accessed through methods of the Aggregate Root. This allows the Aggregate Root to ensure consistency.</p>&#xA;&#xA;<p>Combination of Aggregates are only eventually consistent, i.e. they might be inconsistent at a given point of time but eventually will become consistent. This allows keeping the Aggregates simple since they don't need to bother themselves about the state of other Aggregates. In order to allow that Aggregates refer to each other only by id.</p>&#xA;&#xA;<p>So here is how you apply that to your situation.</p>&#xA;&#xA;<p>If you design the <code>Person</code> entity for your microservices you will probably discover that they don't have the same attributes. For example, the User Management Service might need a <strong>hashed</strong> password for a <code>Person</code>. The Organization Management Service might have other attributes, or maybe even none at all except an id. </p>&#xA;&#xA;<p>Therefore you should have separate classes for those. Now there are different approaches to combine these two Aggregates depending on how they are designed. Let's start with the easiest:</p>&#xA;&#xA;<p>If the <code>Person</code> is completely owned by one service everything else just needs the id. In a web application you could (and possibly should) just include a link to the person resource and load any detail information you want to display using the owning service of the <code>Person</code>. You can also do that in your backend but that is a decision for another day.</p>&#xA;&#xA;<p>If both services have common attributes (e.g. you might want to include basic information like the <code>name</code> in the Organization Management Service so you can display something human readable even when the other service is not available.&#xA;In this case, you can use events to notify services of changes they need to apply to their version of the entity. </p>&#xA;&#xA;<p>Note that you want to decouple any event handling from the rest of your services so that. </p>&#xA;&#xA;<p>a) a problem in with the event infrastructure doesn't cause problems with the service trying to publish events.</p>&#xA;&#xA;<p>b) you have a process in place to sync information when the event infrastructure was unavailable for some time.</p>&#xA;&#xA;<p>One important property to maintain is that every attribute is only changed by one service. If you don't maintain that it gets really difficult to find out which is the correct variant if the values of an attribute diverge.</p>&#xA;"
40334073,40333799,1705598,2016-10-30T21:57:54,"<p>I don't think there's one good answer to this question, multiple approaches may be good, one better from a specific point of view, and another being better from another point of view.</p>&#xA;&#xA;<p>Just try to create an interface (not in the meaning of Go interface type) that hides DB specific behavior and types, which will leave you the option to easily switch to a new db implementation later on, given that all other parts of your code strictly access the persistent layer via this DB interface.</p>&#xA;&#xA;<p>The DB interface should define Go model types (modeling data stored in the persistent layer), and operations on these types, e.g. load, find, save.</p>&#xA;&#xA;<p>As an example, modeling a user:</p>&#xA;&#xA;<pre><code>package db&#xA;&#xA;type ID int64&#xA;&#xA;type User struct {&#xA;    ID   ID&#xA;    Name string&#xA;}&#xA;&#xA;// Manager contains the operations that involve the persistence layer.&#xA;type Manager interface {&#xA;    LoadUser(id ID) (*User, error)&#xA;    SaveUser(u *User) error&#xA;    FindUsersByName(name string) ([]*User, error)&#xA;    Close() error&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>And you may create an implementation (or multiple ones) of the <code>Manager</code> interface. An implementation using MongoDB:</p>&#xA;&#xA;<pre><code>package mongo&#xA;&#xA;import (&#xA;    ""db""&#xA;    ""gopkg.in/mgo.v2""&#xA;)&#xA;&#xA;// manager is a db.Manager implementation that uses MongoDB&#xA;type manager struct {&#xA;    // unexported fields, e.g. MongoDB session:&#xA;    sess *mgo.Sess&#xA;}&#xA;&#xA;func (m *manager) LoadUser(id db.ID) (*db.User, error) { ... }&#xA;&#xA;func (m *manager) SaveUser(u *db.User) error { ... }&#xA;&#xA;func (m *manager) FindUsersByName(name string) ([]*db.User, error) { ... }&#xA;&#xA;func (m *manager) Close() error {&#xA;    m.sess.Close()&#xA;    return nil&#xA;}&#xA;&#xA;func New(mongoURL string) (db.Manager, error) {&#xA;    // Create, initialize your manager, and return it:&#xA;    sess, err := mgo.Dial(url)&#xA;    if err != nil {&#xA;        return nil, err&#xA;    }&#xA;    return &amp;manager{sess: sess}, nil&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>A <code>db.Manager</code> instance (since it involves building a connection to a (Mongo)DB server) should be kept for as long as possible (e.g. global instance). Depending on the usage, <code>Manager.Copy()</code> and <code>Manager.Clone()</code> operations should be supported to acquire a copy or clone for short lived usage (e.g. serving an HTTP request).</p>&#xA;&#xA;<p>Using this example: Someone somewhere has to call <code>mongo.New()</code> to acquire a value of the <code>db.Manager</code>, but from there we only have to interact with the persistence layer via the <code>Manager</code>, leaving any db specific detail to the implementation.</p>&#xA;&#xA;<p>For example:</p>&#xA;&#xA;<pre><code>var mgr db.Manager&#xA;var err error&#xA;&#xA;mgr, err = mongo.New(""&lt;mongodburl&gt;"")&#xA;if err != nil {&#xA;    log.Printf(""Could not connect to db:"", err)&#xA;    return&#xA;}&#xA;defer mgr.Close()&#xA;&#xA;id := 123456&#xA;u, err := mgr.LoadUser(id)&#xA;if err != nil {&#xA;    log.Printf(""Failed to load user [id: %v]: %v\n"", id, err)&#xA;    return&#xA;}&#xA;fmt.Printf(""Loaded User: %+v\n"", u)&#xA;</code></pre>&#xA;"
43361965,30213456,4018558,2017-04-12T06:31:34,"<p>Personally I like the idea of Micro Services, modules defined by the use cases, but as your question mentions, they have adaptation problems for the classical businesses like banks, insurance, telecom, etc...</p>&#xA;&#xA;<p>Distributed transactions, as many mentioned, is not a good choice, people now going more for eventually consistent systems but I am not sure this will work for banks, insurance, etc....</p>&#xA;&#xA;<p>I wrote a blog about my proposed solution, may be this can help you....</p>&#xA;&#xA;<p><a href=""https://mehmetsalgar.wordpress.com/2016/11/05/micro-services-fan-out-transaction-problems-and-solutions-with-spring-bootjboss-and-netflix-eureka/"" rel=""nofollow noreferrer"">https://mehmetsalgar.wordpress.com/2016/11/05/micro-services-fan-out-transaction-problems-and-solutions-with-spring-bootjboss-and-netflix-eureka/</a></p>&#xA;"
44266416,35441660,4018558,2017-05-30T15:34:17,"<p>I actually developed a feasibility study to investigate the solution you mentioned. My conclusion is that it is totally viable to use Micro Service principles in a JBoss Platform.</p>&#xA;&#xA;<p>I used the combination of JBoss \ Spring Boot \ Netflix to create successful Micro Service stack, I personally do that to find a solution to the transaction problem (multiple micro services collaborating) and the fan out problem which caused because excessive Network communication and Serialization costs.</p>&#xA;&#xA;<p>I also wrote a blog about the subject, you might find more details there if you like to, here is the link.</p>&#xA;&#xA;<p><a href=""https://mehmetsalgar.wordpress.com/2016/11/05/micro-services-fan-out-transaction-problems-and-solutions-with-spring-bootjboss-and-netflix-eureka/"" rel=""nofollow noreferrer"">Micro Services – Fan Out, Transaction Problems and Solutions with Spring Boot/JBoss and Netflix Eureka</a></p>&#xA;"
44000693,32529742,4018558,2017-05-16T11:51:14,"<p>Well if you read little bit about the subject in the internet, it is a big debacle point at the moment but there is one answer that everybody agrees on it, distributed transactions are not way to go for it. They are too clumsy and buggy that we can't rely on them for data consistency.</p>&#xA;&#xA;<p>So what is our options then, people are the moment trying to coordinate micro service transactions via Apache Kafka or with Event Source (which concentrate on saving events which are changing the data instead of saving the data itself). So what is the problem with those? Well they are quite different then usual programming model that we get used to and at technical and organisational point of view quite complex, so instead of programming for Business Problems, you start programming against the technical challenge.</p>&#xA;&#xA;<p>So what is the alternative, I personally developed an another concept and wrote a blog about it, it might be interesting for you. In its basics, it uses full micro service design principles and Spring Boot + Netflix in a J2EE container and fully using the transactions, it is too long to write all details here, if you are interested you can read the from the link below.</p>&#xA;&#xA;<p><a href=""https://mehmetsalgar.wordpress.com/2016/11/05/micro-services-fan-out-transaction-problems-and-solutions-with-spring-bootjboss-and-netflix-eureka/"" rel=""nofollow noreferrer"">Micro Services and Transactions with Spring Boot + Netflix</a></p>&#xA;"
44000168,42204181,4018558,2017-05-16T11:24:59,"<p>actually because of the complications you mentioned about organizing transaction over multiple micro services over Apache Kafka, I developed another concept and wrote a blog about it.</p>&#xA;&#xA;<p>If you reach a state of complication that Kafka solution might not be feasible anymore, you might find it as an interesting read. It is too long to explain here but basically it uses a J2EE container fully with Micro Service principle and with full transaction support between the Micro Services with the help of the Spring Boot + Netflix.</p>&#xA;&#xA;<p><a href=""https://mehmetsalgar.wordpress.com/2016/11/05/micro-services-fan-out-transaction-problems-and-solutions-with-spring-bootjboss-and-netflix-eureka/"" rel=""nofollow noreferrer"">Micro Services Fanout and Transaction Problems and Solutions with Spring Boot and Netflix</a></p>&#xA;"
41351532,38863827,6707582,2016-12-27T20:05:56,"<p>If you already designed your micro service where users and messages are two different domain/micro services, then your best bet will be designing a edge service that combines the two services and provide what client needs. For example, the service could be</p>&#xA;&#xA;<pre><code>/messages_by_user/:id&#xA;</code></pre>&#xA;&#xA;<p>Remember a micro service will primarily provide <code>CRUD</code> operation on a domain resource, but client will need more than just CRUD operations on a resource and in that case you should always consider creating edge services to facilitate clients need.</p>&#xA;&#xA;<p>If you haven't implemented the services yet then I would suggest putting both users and messages into the same micro service and consider user as a sub-resource of messages. In that case all the bellow paths are valid.</p>&#xA;&#xA;<pre><code>/messages&#xA;/messages/:id&#xA;/message/:id/users&#xA;/messages/user/:id&#xA;</code></pre>&#xA;&#xA;<p>When you design a micro service, you need to strike the balance of what makes more sense. Although in theory every resource and their <code>CRUD</code> operation should have a separate micro service, but practically you can combine few related resources and make them a sub-resource as long as you are not compromising scalability, performance of the service.</p>&#xA;"
32549820,32532736,680318,2015-09-13T12:45:43,"<p>It sounds like you want a suitable collective noun. I suggest you Google ""collective nouns"", to find numerous lists. Read some of the lists and pick a noun that you think is appropriate.</p>&#xA;&#xA;<p>Alternatively, the term <em>cooperative</em> (or <em>co-op</em> for short) might be suitable if one of the defining characteristics of an instantiation collection of microservices is that they complement, or cooperate with, each other.</p>&#xA;"
49129600,49081095,484222,2018-03-06T11:19:02,"<p>Microservices advocates the idea of loosely coupled services, where each micro-service will handle his own domain.</p>&#xA;&#xA;<p>Following the microservices approach, if you understand that you had to create two different topics to publish your messages, probably it is because they have different scopes\domain, needing their own micro-service.</p>&#xA;&#xA;<p>In your description it is hard to identify if the domain of TopicA and TopicB are related, so we can not offer a good suggestion.</p>&#xA;&#xA;<p>In any case, if one service listen for both topics, let's assume TopicA handles 1000 messages and TopicB handles 100 per second.&#xA;In case you have to publish a new version of your application to handle changes on TopicB messages, you would have to stop the handling of TopicA, that was not necessary. So you are coupling the services, that to begin with should be two independent services, or both topics should be handle as a single one.</p>&#xA;&#xA;<p>Regarding your questions:</p>&#xA;&#xA;<blockquote>&#xA;  <p>1 My first question is, In most of the services instance will not get&#xA;  the message in Topic B, As Topic B has less traffic, So will it be&#xA;  waste of resources ?</p>&#xA;</blockquote>&#xA;&#xA;<p>Waste of resources is relative how you design your application, it might be if your service listen the queue\topic and handle it at the same time, and uses too much memory to keep running all the time. In this scenario, would be case to split them and make a <strong>Queue\Topic Listener</strong> and other <strong>Message Handler</strong> that will receive the message to process, and if it keep too long without processing messages you shut it down, leaving just the listener. You could also use actors instead of a service.</p>&#xA;&#xA;<blockquote>&#xA;  <p>2 Is it better to create different micro-services for Topic A and&#xA;  Topic B listeners, and create 10x instance of micro-service which&#xA;  listen to topic A and x instance of topic B listener service ?</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes for the services, regarding the the number of instances, it should be driven by the size of the queue, otherwise you would have too much listeners and also wasting resources, if you follow the approach of splitting the services, you would need one listener receiving the messages from the queue\topic and it would delivery the messages to multiple messages handlers(service instances\actors) and the queue\topic listener control the number of running instances at same time.</p>&#xA;&#xA;<blockquote>&#xA;  <p>3 Is create a message listener in azure service bus, keep on pulling&#xA;  message every time ? means continuously looking/ checking for message,&#xA;  message is there or not.</p>&#xA;</blockquote>&#xA;&#xA;<p>Is not the only approach, but it's correct.</p>&#xA;"
40610571,40564979,484222,2016-11-15T13:06:04,"<p>We did this way:</p>&#xA;&#xA;<ul>&#xA;<li>Created the service fabric projects in the same solution of our WebAPI.</li>&#xA;<li>Inside the ServiceFabricHost project we created our HttpListenerService to expose the ports using service fabric, the same you would do to a <a href=""https://www.asp.net/web-api/overview/hosting-aspnet-web-api/use-owin-to-self-host-web-api"" rel=""nofollow noreferrer"">SelfHosted WebApi</a>.</li>&#xA;<li>Configure the ServiceFabricHost to open the needed endpoints.</li>&#xA;<li>Add the reference to the WebApi project</li>&#xA;<li>Use the WebApi Startup to send the IAppBuilder and configure the api with our external(SF) configuration, like ports and URL.</li>&#xA;</ul>&#xA;&#xA;<p>The secret is: when you create a self hosted web api, your generaly create a console application, like the asp.net docs. With service fabric, you replace the console with the ServiceFabricService, that is similar to a ConsoleApplication, but in this case will be a statelessService.</p>&#xA;&#xA;<p>In this case, we used a stateless service for HttpListenerService, if you need a StateFull service, you will need to refactor your Api to use the reliable collections.</p>&#xA;&#xA;<p>The fellow Vaclac, created a nice tutorial for this:&#xA;<a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-reliable-services-communication-webapi/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/documentation/articles/service-fabric-reliable-services-communication-webapi/</a></p>&#xA;"
48864535,48835919,484222,2018-02-19T10:53:08,"<p>When you have this scale level, the best approach is use node types to handle external access separate from Internal.</p>&#xA;&#xA;<p>You would create a FrontEnd NodeType to host your APIs or WebServices that expose hard-coded ports(i.e.: 80, 443), and then create BackEnd NodeType to host your services with random ports. With this approach, you don't have to worry about the ports, because the services will run on different nodes according to their roles and only FE will be accessible from outside the cluster.</p>&#xA;&#xA;<p>If still need to handle this inside the same nodes, you can make use of the node type configuration ""Application Start Port"" &amp; ""Application End Port"" that will handle the list of ports your service will be assigned to at started.&#xA;So you would Hard-code your ports outside these lists, and let the BE services to use list from Application Ports, like you suggested on your 3rd option. </p>&#xA;&#xA;<p>Make sure that the Hard-Coded ports are configured on your load balancer, otherwise they won't be accessible from outside the cluster.</p>&#xA;"
47070280,47069673,233598,2017-11-02T07:59:33,<p>In your <code>application.properties</code> file you configure as below:</p>&#xA;&#xA;<pre><code>zuul.routes.external.path /external/**&#xA;zuul.routes.external.url http://urltoexternalservice.com/external&#xA;</code></pre>&#xA;
51557017,51515839,3058302,2018-07-27T11:28:23,"<p>App Engine Standard does support connecting to a MongoDB instance with the very same library that you were using.  <a href=""https://github.com/GoogleCloudPlatform/nodejs-docs-samples/tree/master/appengine/mongodb"" rel=""nofollow noreferrer"">This example</a> works for Standard and Flexible as well.</p>&#xA;&#xA;<p>The issue is with how you were connecting. You have to create the URI like this: </p>&#xA;&#xA;<pre><code>let uri = `mongodb://${user}:${pass}@${host}:${port}`;&#xA;</code></pre>&#xA;&#xA;<p>Where as, in your code, you have this:</p>&#xA;&#xA;<pre><code>const url = 'mongodb://testmongodb:27017';&#xA;</code></pre>&#xA;&#xA;<p>You are missing the user and password in your URI (assuming that <code>testmongodb</code> is your hostname).</p>&#xA;"
43581933,43545080,2014334,2017-04-24T07:15:02,<p>Actually I was hitting some different end point which was not the part of OAuth in turn I was getting the HTML response. Issue has been solved.Thanks guys.</p>&#xA;
50394184,50392109,1623678,2018-05-17T14:36:08,"<p>Cassandra is very good at what it does but it is not a drop-in replacement for an RDBMS. If you find that you need any of the following, I would not encourage you to migrate to Cassandra: </p>&#xA;&#xA;<ul>&#xA;<li>Strict consistency</li>&#xA;<li>ACID transactions</li>&#xA;<li>Support for ad-hoc queries, including joins, aggregates, etc. </li>&#xA;</ul>&#xA;&#xA;<p>Now as for you hitting some limits (or thinking you will hit them in the future) with MySQL, here are some thoughts: </p>&#xA;&#xA;<ul>&#xA;<li>Don't think that a limitation in MySQL is a limitation in RDBMS in general. Just so you don't think I am a $some_other_DB zealot, I've been using MySQL for almost 20 years, but it is not the best tool for all jobs.</li>&#xA;<li>If by 'changes' you mean 'schema changes', a lot of the pain can be alleviated by either:&#xA;&#xA;<ul>&#xA;<li>Using an RDBMS where they are implemented better (including perhaps a more recent MySQL version)</li>&#xA;<li>Using community supported tools such as pt-online-schema-change or gh-ost</li>&#xA;</ul></li>&#xA;</ul>&#xA;&#xA;<p>Good luck!</p>&#xA;"
43517345,43515765,7598462,2017-04-20T10:46:19,"<p>You can use <code>git filter-branch</code> with the <code>--subdirectory-filter</code> option to filter a subdirectory of your repository and thus make the repository contain the subfolder as root directory. This is described in step 5 <a href=""https://help.github.com/articles/splitting-a-subfolder-out-into-a-new-repository/"" rel=""nofollow noreferrer"">here</a>, documentation <a href=""https://git-scm.com/docs/git-filter-branch"" rel=""nofollow noreferrer"">here</a> might also help. You would have to clone your repository three times and run <code>filter-branch</code> in each of those for a different part of your project.</p>&#xA;&#xA;<p>Since (with said <code>--subdirectory-filter</code>) only subdirectories can be treated this way, you may have to rearrange your repository before. The advantage over the naive deletion of other parts is, however, that by using <code>filter-branch</code> you will only preseve history that concerns the actual content of your repository, and do not have any history of the parts filtered out.</p>&#xA;"
44200194,44183595,1312383,2017-05-26T11:16:16,"<p>I found fabric8 library that helped me to achieve this. &#xA;Still don't know if this is the correct answer, but it works :D</p>&#xA;&#xA;<p><a href=""https://github.com/fabric8io/kubernetes-client/tree/master/kubernetes-client"" rel=""nofollow noreferrer"">https://github.com/fabric8io/kubernetes-client/tree/master/kubernetes-client</a></p>&#xA;&#xA;<pre><code>@RequestMapping(""/"")&#xA;private String getResponse() {&#xA;    String ret = ""hello from Client L0L!!!\n"";&#xA;&#xA;    //Config config = new ConfigBuilder().withMasterUrl(""https://mymaster.com"").build();&#xA;    //KubernetesClient client = new DefaultKubernetesClient(config);&#xA;    KubernetesClient client = new DefaultKubernetesClient();&#xA;&#xA;&#xA;    ServiceList services = client.services().withLabel(""APIService"").list();&#xA;    Service server = null;&#xA;    log.warn(""----------------------------------------------&gt;"");&#xA;    for (Service s : services.getItems()) {&#xA;        log.warn(s.getMetadata().getName());&#xA;        log.warn(s.toString());&#xA;        if (s.getMetadata().getLabels().containsKey(""ServiceType"") &amp;&amp; s.getMetadata().getLabels().get(""ServiceType"").equals(""server""))&#xA;            server = s;&#xA;    }&#xA;&#xA;    log.warn(""----------------------------------------------&gt;"");&#xA;&#xA;    String s = """";&#xA;    if (server != null) {&#xA;        RestTemplate t = new RestTemplate();&#xA;        String url = ""http://"" + server.getMetadata().getName() + "":"" + server.getSpec().getPorts().get(0).getPort() + ""/"";&#xA;        log.warn(""Contacting server service on: "" + url);&#xA;        s = t.getForObject(url, String.class);&#xA;        log.warn(""Response: "" + s);&#xA;    } else {&#xA;        log.warn(""Didn't find service with label ServiceType=server!!!"");&#xA;    }&#xA;&#xA;    return ret + "" - "" + s;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>I create the two services and added the two labels used in the code.</p>&#xA;"
51158091,51153850,199551,2018-07-03T15:20:41,"<p>No, running multiple instances on a single machine will not make things run faster, it is only making execution less efficient.</p>&#xA;&#xA;<p>However, it might be that a single instance isn't giving you the expected performance even though your system monitoring indicates there are plenty of resources to spend but not used. In that case you might want tweak the configuration of your NServiceBus endpoint by configuration the amount of allowed parallel message execution.</p>&#xA;&#xA;<p>On the following link you see how you can increase the concurrency:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://docs.particular.net/nservicebus/operations/tuning"" rel=""nofollow noreferrer"">https://docs.particular.net/nservicebus/operations/tuning</a></li>&#xA;</ul>&#xA;&#xA;<p>You can further scaleout by actually using multiple machines but if all these endpoints share the same central database your network or database server can easily become the bottleneck. If you consider deploying or scaling out your endpoints across multiple machines make sure that any storage solutions are also scaled out for these not to become your bottleneck.</p>&#xA;&#xA;<h2>Zero downtime upgrades/deployments</h2>&#xA;&#xA;<p>The only reason to have multiple instance on the same box is for example when deploying a new version, you can temporarily run the current and the new version side-by-side to achieve zero downtime deployments.</p>&#xA;"
30333532,30273152,199551,2015-05-19T18:36:15,"<p>Depending on your needs you could use NServiceBus (<a href=""http://particular.net/nservicebus"" rel=""nofollow"">http://particular.net/nservicebus</a>). NServiceBus is communication middle ware which can be used with different types of queuing systems like MSMQ, RabbitMQ and others. It is essentially a servicebus which is very developer friendly and focused. It does not only facilitate asynchronous message based distributed communication but also: </p>&#xA;&#xA;<ul>&#xA;<li>Publish / Subscribe that is transport agnostic using automatic registration</li>&#xA;<li>Transports: Can be used with MSMQ, RabbitMQ, Azure Storage Queues, etc.</li>&#xA;<li>Security: Supports encryption of messages</li>&#xA;<li>BLOB's: Has support for storing large message payloads transparently with the data bus to allow for communicatie message larger then the transport allows.</li>&#xA;<li>Scalability: Out and upscaling to increase throughput</li>&#xA;<li>Reliability: Deduplication, idempotent processing without having distributed transactions.</li>&#xA;<li>Orchestration: Sagas can help in controlling message flow and routing.</li>&#xA;<li>Exception handling: Exceptions get automatically retried in two different stages.</li>&#xA;<li>Monitoring: Tools like Service Pulse, Service Insight and Windows Performance monitors to monitor performance and errors. See what errors occurred and </li>&#xA;<li>Serialization: Can use different serializers that support formats like xml, json, binary</li>&#xA;<li>Open Source: All source code is available</li>&#xA;<li>Auditing: Can move all processed message to an audit queue for archiving or audit requirements</li>&#xA;<li>Community: Has a large community of developers that are active on the forums but also supply additional transports, serializers and other features.</li>&#xA;</ul>&#xA;&#xA;<p>I must mention that I work for Particular but also that there are other options to consider. NServiceBus does not use SOAP for message exchange but a lightweight message in a format of choice as mentioned as the serialization bullet. It can integrate with services that require SOAP. It has the ability to expose an service (endpoint) as a WCF service for easy integration and it can use SOAP from within code to call external SOAP services using the features that the .net framework and visual studio provide.</p>&#xA;&#xA;<p>Good luck in choosing the right technology for your project.</p>&#xA;"
50227627,50225125,7683711,2018-05-08T06:55:50,<p>Its a rather usual case to have different microservices use different technologies. However they need an api to communicate with each other. Often that is a rest api. For example Microservice 1 listens on port x to requests. Depending on the requested url path (e.g /hello-world) it will return a string representation (e.g json) of the requested data.</p>&#xA;&#xA;<p>Docker can help you with setting the ports of the services and glueing everything together.</p>&#xA;
42879270,42462663,7683711,2017-03-18T20:07:16,"<p>When two microservices are tightly coupled I would suggest thinking hard about merging them. Why do you want to have microservices anyways? Is it a large project which should grow a lot, possibly with independent teams working on them? &#xA;Don't do microservices just because they are cool but because of the need. In a relatively small one person project I generally would not suggest using microservices.</p>&#xA;&#xA;<p>I would do some reading on microservices.io about when to use a microservice architecture and where to split.</p>&#xA;"
43033669,43032883,7683711,2017-03-26T19:44:55,"<p>If your servers share a secret key you can encrypt and decrypt the token on any server. Inside the token you can write who the user is and what permissions he has. Whenever the user wants to perform some restricted action you decrypt the token and check wheter he has permission to do what he wants. Don't forget to give the token a time to live so it won't last forever.</p>&#xA;&#xA;<p>I'm not a js guru so I won't paste code but you should be able to find an example <a href=""https://coderead.wordpress.com/2012/08/16/securing-node-js-restful-services-with-jwt-tokens/"" rel=""nofollow noreferrer"">here</a></p>&#xA;"
45791626,45791262,7683711,2017-08-21T07:37:40,"<p>Ideally you would describe your entire setup in one or multiple docker compose files. See the documentation for <a href=""https://docs.docker.com/compose/overview/"" rel=""nofollow noreferrer"">details</a>.</p>&#xA;&#xA;<p>Concerning networking and linking your services:&#xA;Docker compose supports <a href=""https://docs.docker.com/compose/networking/"" rel=""nofollow noreferrer"">networking</a>. You can define networks and all services which are in the same network can access each other with their serviceName and internal port (as the application knows it). E.g : <a href=""http://mySuperService:3001"" rel=""nofollow noreferrer"">http://mySuperService:3001</a></p>&#xA;&#xA;<p>In case you want multiple replicas of your services or maybe multiple machines you would need to look into orchestrators such as docker swarm (easyest to start with) or Kubernetes or others.</p>&#xA;"
45170945,45170527,7683711,2017-07-18T15:19:32,<p>Each microservice could provide a versionnumber either via API or by writing to a public file / shared DB. Each microservice would then contain all the expected versionnumbers of its dependencies with and check if the version numbers in the file / database match before startup.</p>&#xA;
49176772,45115860,4671615,2018-03-08T15:30:00,<p>An approach that I used to host Go microservices in Windows is simply run <code>go install &lt;main-package-file&gt;</code> which generated an exe file for Windows. Then I simply used SC to create a Windows service out of it. Then just run it as a normal Windows service. This way I did not need Nginx or any such engine..</p>&#xA;
31574196,31573823,682111,2015-07-22T21:20:40,"<p>Application licensing and creating students are orthogonal so option 2 doesn't make sense.</p>&#xA;&#xA;<p>Option 1 is more sensible but I would try not to build another service. Instead I would try to ""filter"" calls to student service through licensing middleware.</p>&#xA;&#xA;<p>This way you could use this middleware for other service calls (e.g. classes service) and changes in API of both licensing and students can be done independently as those things are really independent. It just happens that licensing is using number of students but this could easily change.</p>&#xA;&#xA;<p>I'm not sure how option 3, an event-based approach can help here. It can solve other problems though.</p>&#xA;"
47262539,47236551,1275007,2017-11-13T10:48:19,"<p>I will try to explain a few more bits in the hope to give you some more perspective on that matter and how you can achieve it in a reliable way in Lagom.</p>&#xA;&#xA;<p>We have a few concepts that we must keep in mind. The most important one which is the source of all is <strong>Event Sourcing</strong> itself. Event Sourcing means that any State in the system has its source in Events.</p>&#xA;&#xA;<p>The first State that we will deal with is the State of the PersistentEntity. This State is prominent because, together with the Command and Event Handler, it defines the consistency boundary of your model.</p>&#xA;&#xA;<p>But there other States in the system. Actually, we can create as much as we want because we have the Event Journal. A read-model is also a State and it’s also generated from the events.</p>&#xA;&#xA;<p>There are many reasons why you shouldn’t publish the State of the PersistentEntity to other systems. The first one being a matter of avoiding coupling. You don’t want your data to leak to other services. That’s all about having an anti-corruption layer (ACL).</p>&#xA;&#xA;<p>So, from here we could say: before publishing Order and Customer to Recommendation Service, I will transform it to OrderView and CustomerView (ACL 101).</p>&#xA;&#xA;<p>The question now is when will you do it? If you try to publish it in Kafka after you have handled a command, you don’t have any guarantee that the State will be published. There are no XA transactions between the event journal and the Kafka topic. So, there is a chance that the events are persisted, but for some reason, the State is not published in Kafka.</p>&#xA;&#xA;<p>If you want data to get out of a service in a reliable way and without creating coupling between services, you have the following options:</p>&#xA;&#xA;<p>Use the broker API and publish the events to a topic. You should not publish the events as they are, but transform them into the format of your external API (ACL).&#xA;Use a read-side processor to generate a view of it, again the external API format you want to make available. If you want, you can publish that ViewState to a topic so other services can consume it directly.</p>&#xA;&#xA;<p>That said, there is nothing wrong in publishing something in a topic that is not a real event, but some derived State. The problem is how you can guarantee that it is effectively published. Doing that from inside the PersistentEntity is risky because you have at-most-once semantics. The most reliable way of doing it is a read-side process that gives you at-least-once semantics.</p>&#xA;&#xA;<p>Further comments inline...</p>&#xA;&#xA;<blockquote>&#xA;  <p>Listen to domain events from customer and orders and rebuild the state&#xA;  in the recommandation service. This is a horrible idea because you&#xA;  would need to duplicate the logic that handles events across different&#xA;  bounded context</p>&#xA;</blockquote>&#xA;&#xA;<p>That's not a horrible idea. That's how you make your services independent from each other. The logic that you will need to implement to consume the events are not the same. As you said, it's a different bounded context, as such it only gets what it needs. </p>&#xA;&#xA;<p>Leaking the State from a BC to another is more problematic for the reasons I mentioned above (anti-corruption layer).</p>&#xA;&#xA;<p>To achieve decoupling you do need more coding and there is nothing wrong with that. At the end of the day, the reason for building microservices is to avoid coupling and be able to let the services evolve and scale without interfering with each other. There is a price to pay for that and the price is to write more code. You need to evaluate the thread-offs.</p>&#xA;&#xA;<p>You can consume your own events, produce an <strong>OrderView</strong> and <strong>CustomerView</strong> and publish into Kafka, but that's the same as consuming the events directly on the Recommendation Service.</p>&#xA;&#xA;<p>Note that you also need to store <strong>OrderView</strong> and <strong>CustomerView</strong> somewhere in the Recommendation Service. So you end up storing it three times. On the original service (view table), in Kafka and in the Recommendation Services. </p>&#xA;&#xA;<p>That's why publishing events in a topic is the best option to propagate data between services.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Every time we receive a domain event from customers or orders, go to&#xA;  them and ask them the state. This is horrible because if you have more&#xA;  than one microservice that needs their state, you will end up&#xA;  producing load on customers and orders</p>&#xA;</blockquote>&#xA;&#xA;<p>That is indeed a horrible idea because you will make the Recommendation Service be dependent on the other two services. If Order or Customer is down, the Recommendation will be down as well. That's what a broker helps to solve. </p>&#xA;&#xA;<blockquote>&#xA;  <p>Have customers and orders not only publish events but also state and&#xA;  having all the services that need to build materialized views listen&#xA;  the state they need How do you apply the last pattern with Lagom? We&#xA;  found no way to listen to state changes, just to events.  One solution&#xA;  we considered implied publishing with pubSub the state in the onEvent&#xA;  handler of a persistent entity but I am not sure this is the right&#xA;  place to make it happen.</p>&#xA;</blockquote>&#xA;&#xA;<p>Using pubSub in the onEvent handler is the worst solution of all. For the following reasons:</p>&#xA;&#xA;<ol>&#xA;<li><p>pubSub has at-most-once sematincs (see comments above)</p></li>&#xA;<li><p>Event handlers are called many times. Whenever you re-hydrate an Entity, the events are replayed and the the event handlers will be used for that. Which mean that you will re-publish the state each time. Actually, you would solve the at-most-once pubSub problem, but not the way you might expect/desire.</p></li>&#xA;</ol>&#xA;&#xA;<p>You could use the afterPersist callback for that, but that's not reliable neither because pubSub is at-most-once.</p>&#xA;&#xA;<p>PubSub inside a PersistentEntity should not be used for something that you need to be reliable. It's a best-effort capability, that's all. </p>&#xA;"
51423750,51423358,5211519,2018-07-19T13:19:39,"<p>According to the documentation, you should be able to install a package from any url resolving to a gzipped tarball.</p>&#xA;&#xA;<p>So if you expose your bucket content with cloudfront, it should work.</p>&#xA;&#xA;<blockquote>&#xA;  <p>A package is:</p>&#xA;  &#xA;  <ul>&#xA;  <li>a) a folder containing a program described by a package.json file</li>&#xA;  <li>b) a gzipped tarball containing (a)</li>&#xA;  <li>c) a url that resolves to (b)</li>&#xA;  <li>d) a @ that is published on the registry with (c)</li>&#xA;  <li>e) a @ that points to (d)</li>&#xA;  <li>f) a  that has a ""latest"" tag satisfying (e)</li>&#xA;  <li>g) a git url that, when cloned, results in (a).</li>&#xA;  </ul>&#xA;</blockquote>&#xA;&#xA;<p>doc : <a href=""https://docs.npmjs.com/all#what-is-a-package"" rel=""nofollow noreferrer"">https://docs.npmjs.com/all#what-is-a-package</a></p>&#xA;"
33680942,33680497,1281407,2015-11-12T20:42:49,"<p>A microservice is exactly like its name suggests. It's a tiny service that performs a very simple function.</p>&#xA;&#xA;<p>So yes, in terms of code, you're probably looking at a REST service. Note that any other API style would work. It doesn't have to be REST, however it must be <strong>language agnostic</strong> for you to draw all the benefits.</p>&#xA;&#xA;<p>But the idea is bigger than that. The idea behind it is that they are very specialized and there's no big business workflow attached to they. For example, if you have a service that processes payments, then writes an audit log, then notifies the customer. I wouldn't count that as a microservice. Writing an audit log, that'd probably be a microservice, notifying the customer too, processing the credit card as well. Your system would coordinate that business workflow (the 3 steps mentioned above) by calling the 3 necessary microservices. So your system is a coordinator and you don't worry about implementing the business functions.</p>&#xA;&#xA;<p>Microservices don't try to think too much, they just do as they're told, but they do it quickly.</p>&#xA;&#xA;<p>So in short. Take a very simple business function, put a REST API in front of it. And you have a microservice.</p>&#xA;&#xA;<p>There are several <strong>interesting properties</strong> with microservices:</p>&#xA;&#xA;<ol>&#xA;<li>They can be deployed independently. That's nice because you can deploy pieces of your app without taking everything down all at once.</li>&#xA;<li>They can run in their own silo. So if you have a microservice that is very memory hungry, it can be deployed on a separate server so it doesn't affect the rest of your system.</li>&#xA;<li>They can use different technology. You may have microservices in Java, some on .Net</li>&#xA;<li>They keep dependencies in check. Developers have a tendency to bleed dependencies from one component to the next when they live together. Here you can't.</li>&#xA;</ol>&#xA;&#xA;<p>But you should also consider some <strong>drawbacks</strong></p>&#xA;&#xA;<ol>&#xA;<li><p>It's a lot harder to maintain a consistent transaction across all these calls. If you need to roll back, you'll need JTA to rollback all the REST calls. That can be a pain and doesn't perform very well in my experience.</p></li>&#xA;<li><p>Tracking a transaction through the system in order to troubleshoot can be pretty rough if you don't have consistent logging and a consistent transaction id</p></li>&#xA;<li><p>It might get tricky to find where a defect was introduced if the ground is constantly shifting due to deployments that may have side effects.</p></li>&#xA;<li><p>Obviously all these REST calls can stack up to be pretty expensive. I'm not necessarily talking about the transport since it's likely all your microservices would live in the same data center. But every time you make a call over the network, you have to marshal/unmarshal your data and that could quickly get expensive CPU-wise.</p></li>&#xA;</ol>&#xA;"
39056700,38687434,48767,2016-08-20T17:04:02,"<p>Use a <a href=""https://www.nginx.com/blog/service-discovery-in-a-microservices-architecture/"" rel=""nofollow"">service registry</a> and look up the location of other services at run time. Here are some of the typical technologies used for this (there are others out there).</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://github.com/Netflix/eureka"" rel=""nofollow"">Eureka</a></li>&#xA;<li><a href=""http://zookeeper.apache.org/"" rel=""nofollow"">Apache ZooKeeper</a></li>&#xA;<li><a href=""https://www.consul.io/"" rel=""nofollow"">Consul</a></li>&#xA;<li><a href=""https://github.com/coreos/etcd"" rel=""nofollow"">Etcd</a></li>&#xA;</ul>&#xA;&#xA;<p>The service registry must exist in a known location.  This location should always be a configurable property in your microservice.  Never hard coded!  For improved flexibility, it's pretty typical to access the registry endpoint byway of a DNS.  So, your service looks for <code>https://registry-1</code> instead of a specific IP address, which might change.</p>&#xA;&#xA;<p>Depending on the communication mechanism you want in your system, a message queue will help your services communicate, but it will not help with discovery.  In this approach you would still use a DNS and configurable properties to tell each microservice the location of the message queue.  Individual services would then publish and subscribe messages to the queue.  Microservices would never be aware of other services (no discovery) and all communication would be through messages in the queue.</p>&#xA;&#xA;<p><a href=""http://amzn.to/2blwemN"" rel=""nofollow"">Sam Newman's book on microservices</a> goes into greater detail on these approaches and covers other areas of concern you are likely interested in.</p>&#xA;"
51262748,51227737,3197055,2018-07-10T10:20:30,"<p>This is a very long question. But I believe most can be summarised by answering below,</p>&#xA;&#xA;<p><strong>To my understanding, all the articles and examples I found assume you want access to eg. google calendar, profile info or emails if you eg. login with google,</strong> </p>&#xA;&#xA;<p>You do not necessarily use Access token (ID token in some occasions) to access the services offered by token issuer.You can consume tokens by your own APIs. What these Identity Providers (synonym to Authorization server, or IDP in shorthand) is to hold identities of end users. For example, typical internet have a Facebook account. With OAuth and OpenID Connect, the same user get the ability to consume your API or any OAuth/OIDC accepted service. This reduce user profile creation for end users.</p>&#xA;&#xA;<p>In corporate domain, OAuth and OIDC serves the same purpose. Having a single Azure AD account lets you to consume MS Word as well as Azure AD's OIDC will issue tokens which can be used to Authorise against an in-house API or an third party ERP product (used in organization) which support OIDC based authentication. Hope it's clear now</p>&#xA;&#xA;<p><strong>A note on the diagram is that the Authentication service could probably be built into the API Gateway - not sure if that would be better?</strong></p>&#xA;&#xA;<p>If you are planning to implement an API gateway, think twice. If things are small scale and if you think you can maintain it, then go ahead. But consider about API managers which could provide most of your required functionalities. I welcome you to read this article <a href=""https://medium.com/scalable/wso2-api-manager-in-a-nutshell-eaac20812f0c"" rel=""nofollow noreferrer"">about WSO2 API manger</a> and understand its capabilities (No I'm not working for them).</p>&#xA;&#xA;<p>For example, that API manager has built in authentication handling mechanism for OAuth and OIDC. It can handle API authentication with simple set of configurations. With such solution you get rid of the requirement of implement everything.</p>&#xA;&#xA;<p><strong>What if you can't use an API manager and has to do it yourself</strong></p>&#xA;&#xA;<p>OpenID Connect is for authentication. Your application can validate the id token and authenticate end user. To access APIs through API Gateway, I think you should utilise Access token. </p>&#xA;&#xA;<p>To validate the access token, you can use <a href=""https://tools.ietf.org/html/rfc7662"" rel=""nofollow noreferrer"">introspection endpoint</a> of the identity provider. And to get user information, you can use user-info endpoint. </p>&#xA;&#xA;<p>Once access token is validated, API gateway could create a session for a limited time (ideally to be less or equal to access token lifetime). Consequent requests should come with this session to accept by API gateway. Alternatively, you can still use validated access token. Since you validated it at the first call, you may cache for a certain time period thus avoiding round trips to validations.</p>&#xA;&#xA;<p>To validate user details, permission and other grants, well you must wither bind user to a session or else associate user to access token from API gateway at token validation. I'm also not super clear about this as I have no idea on how your DB logic works. </p>&#xA;"
50407291,50405838,2674303,2018-05-18T08:46:39,"<p>The problem dissappeared when I corrected <code>@HystrixCommand</code> annotation parameters:</p>&#xA;&#xA;<pre><code>    @HystrixCommand(groupKey = ""somethingProxy"", commandKey = ""retrieveSomething"", ignoreExceptions = {&#xA;            IllegalArgumentException.class }, fallbackMethod = ""reliable"", commandProperties = {&#xA;            @HystrixProperty(name = ""execution.isolation.strategy"", value = ""SEMAPHORE"") })&#xA;    public String hello() {&#xA;</code></pre>&#xA;"
47648193,47647560,1826943,2017-12-05T07:24:41,"<p>In terms of having your two micro services independent and having them also independent in the future I would also duplicate the code. We had the exact same situation before. Several microservices seem to use some ""common"" classes that can be put to a seperate jar.&#xA;In the end we had following situation:&#xA;- several (5+) services using the same JAR&#xA;- turned out that classes that we thought are the same, seemed to have slightly different semantics in different services&#xA;- a change on one of the classes more or less forced us to have a release on every microservice, when it came to releasing (no independency here anymore)&#xA;- developers tend to see ""common"" behavior everywhere, so you most likely end up with some ""Helper/Utility"" classes there as well which is in the meanwhile considered a code smell in OOP</p>&#xA;&#xA;<p>Long story short, in the meanwhile we switched to having the code duplicated, which gives us the freedom to handle our mircoservices really independently, as we only need to stick to the service contract. What happens internally is fully up to the service and we don't have to release all services in the end of an iteration. I'm not saying that the other option is wrong, but it turned out that it was not suitable for us. If you really see common classes between two services and you are sure you don't mess your common library up with other crap, your save to go.</p>&#xA;&#xA;<p><strong>EDIT</strong></p>&#xA;&#xA;<p>Maybe as follow up, we had the same discussion in regards of tests (unit and integration) having share test code in some common classes. In the end this was hell, as every slight change in code or acceptance criteria made 50% of tests fail. Meanwhile our strategy is to not share anything on test level and have everything  right at the tests place. By that you are super fast in eliminating or changing tests. In the end the lesson for us was to keep business code as clean and elegante as suitable and the test code in a way to give us the least headache possible.</p>&#xA;"
41814134,41814001,194175,2017-01-23T19:29:49,"<p>I prefer approach #1 (especially since I know you're using Heroku already).</p>&#xA;&#xA;<p>Docker is great, but the benefits are very limited if you're deploying on Heroku. The reason why is that Heroku already does everything docker does: manage dependencies, installation, and process management. Heroku does all this already for you without the extra work of Docker-izing your environment.</p>&#xA;&#xA;<p>Regarding the load balancing: it doesn't actually matter. In both of your cases, if you want to run on Heroku, you WILL be using the Heroku load balancer. This is because there is no way to 'bypass' that layer of the Heroku stack.</p>&#xA;&#xA;<p>If you want to use Docker / zuul, you'll need to do that OUTSIDE of Heroku for sure. This means you need to do all sorts of other stuff, find a docker host, manage your own infrastructure, etc.</p>&#xA;&#xA;<p>So, in my mind, #1 is a better option (if you're using Heroku) because:</p>&#xA;&#xA;<ol>&#xA;<li>Everything is already taken care of for you.</li>&#xA;<li>You can focus on writing scalable code instead of managing all sorts of other things.</li>&#xA;<li>All your services will be in the same AWS region, so even though they will talk to each other over HTTPs, it will be VERY FAST.</li>&#xA;</ol>&#xA;"
41811770,41795612,194175,2017-01-23T17:06:23,"<p>Heroku is a very simple Platform-as-a-Service company. The way Heroku works is very straightforward:</p>&#xA;&#xA;<ul>&#xA;<li>You have multiple projects (services) in Git repos.</li>&#xA;<li>You create a Heroku app for each project (each Git repo).</li>&#xA;<li>You then push your code from each Git repo to their respective Heroku app.</li>&#xA;<li>Heroku assigns you a public URL for each app you have.</li>&#xA;<li>If each of your services is now running on Heroku, they can send API requests to each other over public HTTPs.</li>&#xA;</ul>&#xA;&#xA;<p>Now -- regarding your question about service oriented architecture on Heroku.</p>&#xA;&#xA;<p>If you're doing SOA on Heroku, you'll need to have each service talk publicly with each other over HTTPS. This is the typical 'pattern'.</p>&#xA;&#xA;<p>Because Heroku provides free SSL for each application, and each application is on the same Amazon region -- talking back-and-fourth between your services over HTTPs is very fast + secure.</p>&#xA;&#xA;<p>Each Heroku app has automatic load balancing, so no need to worry about load balancers.</p>&#xA;&#xA;<p>The next option here (if you don't want to follow the typical patterns) is to use something like RabbitMQ or Amazon SQS (a queueing service), and share 'messages' between your different services.</p>&#xA;&#xA;<p>In this pattern, you would still have one Heroku app for each service, but instead of communicating with each other over HTTPs, you would instead communicate with your other services through a queueing protocol like Rabbit or SQS. This has some speed benefits.</p>&#xA;&#xA;<p>In regards to authentication services, there are several providers you can use to provide this functionality. The most popular one I know of is <a href=""https://elements.heroku.com/addons/stormpath"" rel=""noreferrer"">Stormpath</a>. If you look through the <a href=""https://elements.heroku.com/addons"" rel=""noreferrer"">Heroku addon marketplace</a>, you can find others as well.</p>&#xA;&#xA;<p>Finally, for database stuff: you can use any Database provider you want. The most popular one is likely <a href=""https://elements.heroku.com/addons/heroku-postgresql"" rel=""noreferrer"">Heroku Postgres</a>. It's a hosted versino of PostgreSQL that's very reliable / easy to use.</p>&#xA;&#xA;<p>You can either share one database amongst ALL your services, or you can have one databaes per service. Either strategy will work fine.</p>&#xA;"
47347013,47335582,7994751,2017-11-17T09:15:50,"<p>Apparently there was this magic property that made it work again:</p>&#xA;&#xA;<p><code>security.oauth2.resource.filter-order: 3</code> </p>&#xA;&#xA;<p>in <code>auth-service.properties</code></p>&#xA;&#xA;<p>Further reading here:&#xA;<a href=""https://stackoverflow.com/questions/42822875/springboot-1-5-x-security-oauth2"">SpringBoot 1.5.x + Security + OAuth2</a></p>&#xA;"
50586573,45047011,263296,2018-05-29T14:13:27,"<p>There's none preventing you to have an SFTP server containing CSV or database storing the results. You can do whatever make senses. Using messaging to pass gigabytes of data, or streaming through HTTP may or may not make senses for your case.</p>&#xA;"
40564677,40564595,1508736,2016-11-12T15:56:20,"<p>By default, docker monitors one single process, and the container is finished / relaunched (depending on launch flags) when this monitored process ends.</p>&#xA;&#xA;<p>There is a specific distribution which aims at doing what you are pursuing, i.e., have more than one process alive in the docker instance while monitoring / restarting all of them, this image is phusion/baseimage, you will find it here: <a href=""https://github.com/phusion/baseimage-docker"" rel=""nofollow noreferrer"">https://github.com/phusion/baseimage-docker</a></p>&#xA;&#xA;<p>On this image, what you do is that you create as many services as you want by creating subdirectories under /etc/service, and launching as CMD the one who will start and monitor all the services, like this:</p>&#xA;&#xA;<pre><code># Use phusion/baseimage as base image. To make your builds reproducible, make&#xA;# sure you lock down to a specific version, not to `latest`!&#xA;# See https://github.com/phusion/baseimage-docker/blob/master/Changelog.md for&#xA;# a list of version numbers.&#xA;FROM phusion/baseimage:&lt;VERSION&gt;&#xA;&#xA;# Use baseimage-docker's init system.&#xA;CMD [""/sbin/my_init""]&#xA;&#xA;# ...put your own build instructions here...&#xA;&#xA;# Clean up APT when done.&#xA;RUN apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*&#xA;</code></pre>&#xA;&#xA;<p>After that, just ensure you are creating a folder per service under /etc/service, and that inside there is a file called ""run"". This will be the entry point for your service, an example (from the doc) of the Dockerfile:</p>&#xA;&#xA;<pre><code>RUN mkdir /etc/service/memcached&#xA;ADD memcached.sh /etc/service/memcached/run&#xA;</code></pre>&#xA;&#xA;<p>So for your target, just create a couple of folders + run file, one for NGINX and other for the Rails/Puma server, and use this image as base.</p>&#xA;"
36462185,36461493,3608192,2016-04-06T20:57:00,"<p>Forwarding is often done by a filter, in this case the request does not even reach a controller. This would explain why your @ControllerAdvice does not work.</p>&#xA;&#xA;<p>If you forward in the controller than the @ControllerAdvice should work.&#xA;Check if spring creates an instance of the class annotated with @ControllerAdvice. For that place a breakpoint in the class and see whether it is hit.</p>&#xA;&#xA;<p>Add a breakpoint also in the controller method where the forwarding should happen. May be you accidently invoke another controller method than you inspect ?</p>&#xA;&#xA;<p>These steps should help you resolve the issue.</p>&#xA;&#xA;<p>In your class annotated with @ControllerAdvice add an ExceptionHandler method annotated with @ExceptionHandler(Exception.class), that should catch every Exception. </p>&#xA;&#xA;<p><strong>EDIT :</strong>&#xA;You can try to add your own filter that converts the error response returned by the Zuulfilter. There you can change the response as you like.</p>&#xA;&#xA;<p>How the error response can be customized is explained here :</p>&#xA;&#xA;<p><a href=""https://stackoverflow.com/questions/17715921/exception-handling-for-filter-in-spring"">exception handling for filter in spring</a></p>&#xA;&#xA;<p>Placing the filter correctly may be a little tricky. &#xA;Not exactly sure about the correct position, but you should be aware of the order of your filters and the place where you handle the exception.</p>&#xA;&#xA;<p>If you place it before the Zuulfilter, you have to code your error handling after calling doFilter().</p>&#xA;&#xA;<p>If you place it after the Zuulfilter, you have to code your error handling before calling doFilter().</p>&#xA;&#xA;<p>Add breakpoints in your filter before and after doFilter() may help to find the correct position.</p>&#xA;"
47634884,45524591,7346048,2017-12-04T13:49:33,"<p>Fallbacks are actually not handled by Feign itself, but by a circuit breaker. So, you need to put Hystrix (which is the Netflix circuit breaker) on your classpath and enable it in your application.yml file like this:</p>&#xA;&#xA;<pre><code>feign:&#xA;  hystrix:&#xA;    enabled: true&#xA;</code></pre>&#xA;&#xA;<p>If you're using 'cloud:spring-cloud-starter-openfeign' in your build.gradle or pom.xml file, Hystrix should be automatically on your classpath.</p>&#xA;"
48482671,48482639,3103616,2018-01-28T02:41:54,"<p>With express, have you tried middleware? You can chain a series of callback functions with a certain timeout after the article is created.</p>&#xA;"
49905269,49901574,9664896,2018-04-18T17:04:05,"<p>check ${project.artifactId} name. it contain capital words.""Eureka""&#xA;it make error for you.</p>&#xA;&#xA;<pre><code>[INFO] Image will be built as springio/cloud-microservice-projet-Eureka:latest&#xA;</code></pre>&#xA;&#xA;<p>change it to <code>cloud-microservice-projet-eureka</code></p>&#xA;&#xA;<p>problem in here is image name, was in capital letter and docker doesnt allow image names having caps</p>&#xA;"
48689453,36049030,460417,2018-02-08T15:47:29,"<p>Using a different GOPATH per project is a very good and simple approach. In my experience this also works better than <code>vendor</code> since you can also install binaries and keep them on different versions.</p>&#xA;&#xA;<p><a href=""https://github.com/GetStream/vg"" rel=""noreferrer"">vg</a> is a simple tool that helps managing workspaces, it integrates with your shell and detects automatically workspaces when you <code>cd</code> them.</p>&#xA;&#xA;<p>Disclaimer: I am one of the authors of the tool.</p>&#xA;"
36019928,35820780,1325185,2016-03-15T19:03:11,"<p>Ok so after some trial and failure I settled on using both git sub modules and python packages. The directory tree looks like this:</p>&#xA;&#xA;<pre><code>main (repo)&#xA;├── auth (repo)&#xA;│   ├── auth-cleint (repo)&#xA;│   │   └── setup.py&#xA;│   └── requirements (contains an editable entry to ./auth-client)&#xA;├── notify (repo)&#xA;│   ├── notify-client (repo)&#xA;│   │   └── setup.py&#xA;│   └── requirements (-e ./notify-client and git+auth-client)&#xA;├── docker-compose.yml&#xA;└── data (repo)&#xA;    └── requirements (git+notify-client and git+auth-client)&#xA;</code></pre>&#xA;&#xA;<p>The auth, notify, data are separate git repositories linked as sub modules under the main repo. Same goes for clients inside their service Django projects. I install the clients within their services in editable mode just to make my development easy. Once I'll go staging/prod I'll use git+ links.</p>&#xA;&#xA;<p>This works for now in here <a href=""https://github.com/PyPila/amok"" rel=""nofollow"">https://github.com/PyPila/amok</a> (not a commercial project - used to learn) if someone's interested to dive deeper.</p>&#xA;&#xA;<p>One hint in this moment of time I have to use a deprecated <code>--process-dependency-links</code> flag when using <code>pip</code> as my client packages depend on some other private packages. <code>pip</code> maintainers set that flag as deprecated but it will stay there until they figure out how to solve those 'deep dependencies'.</p>&#xA;"
43944971,43941794,6452097,2017-05-12T19:13:41,"<p>You can certainly make a request to the API from another server as well. Just as you can make a call to any REST endpoint from anywhere, you can perform server to server communication with GraphQL APIs as well.</p>&#xA;&#xA;<p>For example, at <a href=""https://scaphold.io"" rel=""nofollow noreferrer"">Scaphold</a>, we use Lambda for many webhooks and scheduled tasks. And from our microservice, we use the <a href=""https://github.com/request/request"" rel=""nofollow noreferrer""><code>request</code> library</a> to make POST requests to the Scaphold server's GraphQL API.</p>&#xA;&#xA;<p><a href=""https://docs.scaphold.io/coredata/mutations/#create"" rel=""nofollow noreferrer"">Here's an example of a create mutation that you can use from a Node server.</a></p>&#xA;&#xA;<p>Hope this helps!</p>&#xA;"
43985329,43983286,6452097,2017-05-15T17:18:04,"<p>Great question! Sounds like you're asking how to set up your architecture for GraphQL and microservices, and why.</p>&#xA;&#xA;<h1>Background</h1>&#xA;&#xA;<p>I would recommend using GraphQL since it's best use case is to consolidate data sources in a clean way and expose all that data to you via one standardized API. On the flip side, one of the main problems with using microservices is that it's hard to wrangle all the different functions that you can possibly have. And as your application grows, it becomes a major problem with consolidating all these microservice functions.</p>&#xA;&#xA;<p>The benefits of using these technologies are tremendous since now you essentially have a GraphQL API gateway that allows you to access your microservices from your client as if it were a single monolithic app, but you also get the many benefits of using microservices from a performance and efficiency standpoint.</p>&#xA;&#xA;<h1>Architecture</h1>&#xA;&#xA;<p>So the architecture I would recommend is to have a GraphQL proxy sitting in front of your microservices, and in your GraphQL query and mutation resolvers, call out to the function that you need to retrieve the necessary data.</p>&#xA;&#xA;<p>It doesn't really matter all that much between having a GraphQL gateway in front of GraphQL microservices or a GraphQL gateway in front of REST endpoints, although I would actually argue that it would be simpler to expose your microservice functions as REST endpoints since each function should theoretically serve only one purpose. You won't need the extra overhead and complexities of GraphQL in this case since there shouldn't be too much relational logic going on behind the scenes.</p>&#xA;&#xA;<p>If you're looking for microservice providers the best ones that I've seen are <a href=""https://aws.amazon.com/lambda/"" rel=""noreferrer"">AWS Lambda</a>, <a href=""https://webtask.io/"" rel=""noreferrer"">Webtask</a>, <a href=""https://azure.microsoft.com/en-us/services/functions/"" rel=""noreferrer"">Azure Functions</a>, and <a href=""https://cloud.google.com/functions/"" rel=""noreferrer"">Google Cloud Functions</a>. And you can use <a href=""https://serverless.com/"" rel=""noreferrer"">Serverless</a> as a way to manage and deploy these microservice functions.</p>&#xA;&#xA;<p><strong>For example:</strong></p>&#xA;&#xA;<pre><code>import request from 'request';&#xA;&#xA;// GraphQL resolver to get authors&#xA;const resolverMap = {&#xA;  Query: {&#xA;    author(obj, args, context, info) {&#xA;      // GET request to fetch authors from my microservice&#xA;      return request.get('https://example.com/my-authors-microservice');&#xA;    },&#xA;  },&#xA;};&#xA;</code></pre>&#xA;&#xA;<h1>GraphQL Service</h1>&#xA;&#xA;<p>This is something that we've been exploring at <a href=""https://scaphold.io"" rel=""noreferrer"">Scaphold</a> as well in case you'd like to rely on a service to help you manage this workflow. We first provide a GraphQL backend service that helps you get started with GraphQL in a matter of minutes, and then allow you to append your own microservices (i.e. custom logic) to your GraphQL API as a composition of functions. It's essentially the most advanced webhook system that's gives you flexibility and control over how to call out to your microservices.</p>&#xA;&#xA;<p>Feel free to also join the <a href=""https://www.meetup.com/serverless-graphql/"" rel=""noreferrer"">Serverless GraphQL Meetup</a> in SF if you're in the area :)</p>&#xA;&#xA;<p>Hope this helps!</p>&#xA;"
47049199,46014427,6786689,2017-11-01T06:12:42,"<p>I was able to solve my issue by moving consul server and port configuration from application.yml to bootstrap.yml. </p>&#xA;&#xA;<p>I don't know much about how it was solved and why it was unable to read from application.yml. If anyone has some details about it, pl. let me know.</p>&#xA;"
24830938,24787801,455553,2014-07-18T17:47:33,"<p>Thanks to @Richard for the mention of <code>spring.profiles.active</code> JVM variable. Since my question was specific to the way <strong>Spring Boot</strong> does this and since there is much more to the answer, I am inclined to answer this myself and include all the details of how I arrived at the answer in the hopes that it will save others time. </p>&#xA;&#xA;<p>First, you can indeed pick the correct profile on the <code>java</code> command line by adding <code>-Dspring.profiles.active=profile_name</code> when you are running your Spring Boot app. (this is assuming your deployment preference is an <strong>uber jar with embedded container</strong> - Tomcat in my case)</p>&#xA;&#xA;<p>I wanted to leave <strong>MySQL</strong> datasource configurations under the <em>default</em> profile and put <strong>H2</strong> in-memory datasource configuration under a <em>test</em> profile. However, the way Spring Boot picks the right <strong>datasource</strong> based on profile is not so obvious. Even though I had MySQL details under the <em>default</em> profile and I had the in-memory H2 datasource details under the <em>test</em> profile, it would still pick H2 as the datasource even when <code>spring.profiles.active</code> was omitted from the command line. This was contrary to my assumption  that <em>default</em> profile will be picked, well, by default :-)</p>&#xA;&#xA;<p>I ended up having to put H2 configuration under the <em>default</em> profile and then create a <strong><em>local</em></strong> profile that included the MySQL datasource configuration. Here's what I ended up with in my <code>application.yml</code></p>&#xA;&#xA;<pre><code>spring:&#xA;  profiles: default &#xA;&#xA;spring:&#xA;  datasource:&#xA;    driverClassName: org.h2.Driver&#xA;    url: jdbc:h2:mem:sampletest;MODE=MySQL&#xA;&#xA;---&#xA;spring:&#xA;  profiles: test&#xA;&#xA;spring.jpa:&#xA;    hibernate:&#xA;      ddl-auto: create-drop&#xA;&#xA;---&#xA;spring:&#xA;  profiles: local&#xA;&#xA;spring.datasource:&#xA;  driverClassName: com.mysql.jdbc.Driver&#xA;  url: jdbc:mysql://127.0.0.1/sampledev&#xA;  username: sample&#xA;  password: sample&#xA;&#xA;spring.jpa:&#xA;  hibernate:&#xA;    dialect: org.hibernate.dialect.MySQLInnoDBDialect&#xA;    ddl-auto: update&#xA;</code></pre>&#xA;&#xA;<p>This worked. I was able to switch between <em>default</em> profiles and the <em>local</em> profile by omitting or adding the <code>-Dspring.profiles.active=local</code> on the <code>java</code> command line. Because test profile inherits from default it is also using H2 </p>&#xA;&#xA;<p>One more nuance: I added <code>ddl-auto: create-drop</code> to the <em>test</em> profile which uses the  in-memory DB to facilitate automatic table creation / teardown for unit tests. But for the local profile which uses MySQL I changed it to <code>update</code>. Implication being that for the local profile I have to first create the database outside of the application. </p>&#xA;"
35590175,35589045,455553,2016-02-23T23:15:54,"<p>It depends how you want to make sense of the logs. You can use a rich log aggregation platform like <a href=""https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-1-7-logstash-1-5-and-kibana-4-1-elk-stack-on-ubuntu-14-04"" rel=""nofollow"">ELK</a> to collect and index all your logs from your different microservice instances. Using this you can search your logs based on multiple attributes like server IP, service name, date ranges, etc. Setting this up is fairly easy although in production you will want to use best practices for high availability.</p>&#xA;&#xA;<p>If you want to further <em>correlate</em> logs across different distributed components of your system you may have to do a little more work. For example, if you want to follow the same ""request"" through all the microservices that are being used, you could attach a <strong>common identifier</strong> (e.g. session ID) to log messages in each of those services. You will need to somehow communicate this common identifier across service boundaries. For example if your services are accessed through HTTP you can send the ID through a HTTP header and add a context listener to your microservice to detect the special header and include the ID in each log message. This can be made easier if you develop a thin wrapper around the standard logging library that will include the ID for you transparently to the developers. This approach has the added benefit of letting you enforce a log message format across all your services. (there are other ways to do this too, such as using filters within ELK) </p>&#xA;&#xA;<p>Then, when you search the aggregated logs later, you can search for a specific ID and follow a given request through all the microservices that were used. </p>&#xA;"
35636831,35589008,455553,2016-02-25T19:44:04,"<p>Not knowing too much detail about your architecture and services, here are some suggestions that represent (a subset of) the strategies that have been proven in systems i've worked on in production. For this I am assuming you are using one container/VM per micro service:</p>&#xA;&#xA;<ul>&#xA;<li>If your services are stateless (as they should be :-) and you have redundancy (as you should have :-) then you set up your load balancer to call your /health on each instance and if the health check fails then the load balancer should take the instance out of rotation. Depending on how tolerant your system is, you can set up various rules that define failure instead of just a single failure (e.g. 3 consecutive, etc.)</li>&#xA;<li>On each instance run a Nagios agent that calls your health check (/health) on the localhost. If this fails, generate an alert that specifies which instance failed. </li>&#xA;<li>You also want to ensure that a higher level alert is generated if none of your instances are healthy for a given service. You might be able to set this up in your load balancer or you can set up a monitor process outside the load balancer that calls your service periodically and if it does not get any response (i.e. none of the instances are responding) then it should sound all alarms. Hopefully this condition is never triggered in production because you dealt with the other alarms. </li>&#xA;<li>Advanced: In a cloud environment you can connect the alarms with automatic scaling features. In that way, unhealthy instances are torn down and healthy ones are brought up automatically every time an instance of a service is deemed unhealthy by the monitoring system</li>&#xA;</ul>&#xA;"
42657225,42653725,455553,2017-03-07T19:59:58,"<p>I wanted to add this as a comment but I hit the character limit :-) </p>&#xA;&#xA;<p>If the behavior of the cage is different for different types of cages, maybe each of them needs to be a different type of cage resource in your cages microservice. Each type of cage will then enforce whatever rules it needs to follow. </p>&#xA;&#xA;<p>The issue of where to persist the relationships with microservices is a complex one with no one size fits all answer. My preference is to try to have loose coupling as much as you can. Only when absolutely necessary, each resource should store it's relationships to other resources and in a <em>uni-directional</em> way. What happens when relationships get updated somewhere else? one way is to inform the related services (through messaging perhaps?) so they do the right thing for their own resources. </p>&#xA;"
37446494,37445823,2225281,2016-05-25T19:57:23,"<p>I think you don't understand how promises work. </p>&#xA;&#xA;<p>First of all, functions <em>always return immediately</em> so you won't ever block execution of the next lines of code (<code>flight.set</code> and <code>theNextSyncFunction()</code> in your case). This is the point of returning a promise: you get a promise <em>immediately</em> that you can attach a callback to (using <code>then()</code>) that will get invoked <em>later</em>. If you want code to wait for the promise to resolve you <em>have to put it in a <code>then()</code> callback.</em> </p>&#xA;&#xA;<p>Secondly, your <code>calculateFlightEndDate()</code> is <em>not returning anything at all</em> so <code>endDate = calculateFlightEndDate()</code> is simply setting <code>endDate</code> to <code>undefined</code>. </p>&#xA;&#xA;<h2>Solution</h2>&#xA;&#xA;<p>You should return a promise from <code>calculateFlightEndDate()</code> and put the code you want to execute afterwards inside a <code>then()</code> callback:</p>&#xA;&#xA;<pre><code>calculateFlightEndDate(periodTypeId, numberOfPeriods, startDate) {&#xA;    let plan = new Plan();&#xA;&#xA;    return plan.getFlightEndDate(periodTypeId, numberOfPeriods, startDate).then((response) =&gt; {&#xA;        // response is JSON: {EndDate: ""12/05/2016""}&#xA;        return response.EndDate;&#xA;    }, (error) =&gt; {&#xA;        log.debug(""There was an error calculating the End Date."");&#xA;    });&#xA;}&#xA;&#xA;if (moment(momentTime).isValid()) {&#xA;    if (dateField == ""StartDate"") {&#xA;        PlanLineActions.calculateFlightEndDate(periodTypeId, numberOfPeriods, momentTimeUnix).then((endDate) =&gt; {&#xA;            flight.set(""EndDate"", endDate);&#xA;            this.theNextSyncFunction(...);&#xA;        });&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>You could also look into using <a href=""https://jakearchibald.com/2014/es7-async-functions/"" rel=""nofollow"">ES7 <code>async</code> and <code>await</code></a> which allows you to <em>write</em> your async code so that it <em>looks</em> synchronous, but uses promises under the hood to accomplish the same thing.</p>&#xA;"
52077871,46092604,657026,2018-08-29T12:47:04,"<p>Separate <strong>events</strong> and <strong>commands</strong>.</p>&#xA;&#xA;<p><strong>Events</strong> is something that happened in the past, sending an event to the server responds only with an ok that it is received.</p>&#xA;&#xA;<p><strong>Commands</strong> is something that you use when you need a response, for instance an id, or when you create a user and you need to check if the e-mail is already in use or not. The command then send an event ""usercreated"" with the user information, and return the id of the user to the one who ran the command (e.g. frontend client).</p>&#xA;"
50262004,50261052,9766530,2018-05-09T21:02:59,"<p>This is a very open ended question much of which depends on your system. In general your server should be able to handle concurrent requests, but there are some things to watch out for.</p>&#xA;&#xA;<p>Performance costs. If the operation to retrieve and store data requires too much computational power, then it will cause strain on all requests processed by the server.</p>&#xA;&#xA;<p>Database connections. The server spends a lot of time waiting for database queries to complete. If you have one database connection for the entire application, and this connection is busy, they will have to wait until the database connection is free. You may want to look into database connection 'pooling'.</p>&#xA;"
46178254,46178051,2428802,2017-09-12T13:51:59,"<p>If you just want conditional logging you would use a <a href=""https://stackoverflow.com/questions/7404435/conditional-logging-with-log4j"">NDC/MDC and a filter</a> using the frameworks that support that feature.</p>&#xA;&#xA;<p>If you want something more general then for instance, setup a <code>com.foo.request</code> that is set to say <code>INFO</code> and a <code>com.foo.request.debug</code> that is set to some lower level.  Pick and choose the logger on request parameter.</p>&#xA;"
48890916,48869041,6758786,2018-02-20T17:24:24,"<p>I will suggest to design this applications using actor model in service fabric. </p>&#xA;&#xA;<p>There will be two categories of actors in general.</p>&#xA;&#xA;<ol>&#xA;<li><p>Actor which act as an orchestrator</p>&#xA;&#xA;<ul>&#xA;<li><p>Create a Base Orchestrator actor which will do common functionality such as recieving the message, logging or any other aspects which all your orchestrations will have</p></li>&#xA;<li><p>Application specific actor orchestrators which will derive from base orchestrator and will have specific steps for business/application specific. Use the micro service actors to call in a series of steps to perform the task.</p></li>&#xA;</ul></li>&#xA;<li><p>Micro Service Actors which will do individual tasks ( DB1 Update, CRM Update, Email, Azure Storage etc )</p></li>&#xA;</ol>&#xA;&#xA;<p>We have done similar design for one of our IOT application utilizing the same model.</p>&#xA;"
47923189,40222469,7878274,2017-12-21T10:43:23,"<blockquote>&#xA;  <p>Is this a valid use case? Is there a more canonical way to do filtering on deeply nested properties, along with sorting and paging?</p>&#xA;</blockquote>&#xA;&#xA;<p>Major part of original questing lies on segregating collections on different databases on separate microservices. In fact, it's nessasary to perform collection joining and subsequent filtering on some key, but it's directly impossible since there is no field in original collection to filter, sort or paginate.</p>&#xA;&#xA;<p>Strightforward solution is perform full or filtered queries to original collections, and then perform joining and filtering result dataset on application server, e.g. by lodash, such at your solution. In is possible for small collections, but in general case causes large data transfer and unefficent sorting since there is no index structure - real RB-tree or SkipList, so with quadratic complexity it's not very good.</p>&#xA;&#xA;<p>Dependent on resource volume on application server, special cache and index tables can be build there. If collection structure is fixed, some relations between collection entries and their fields can be reflected in special search table and update respectively on demain. It's like find &amp; search index creation, but not it database, but on application server. Of cource, it will consume resources, but will be more fast than direct lodash-like sorting.</p>&#xA;&#xA;<p>Also task can be solved from another side, if there is access to structure of original databases. Key is denormalization. In counter for classical relation approach, collections can have dublicate information for avioding further join operation. E.g., Articles collection can have some information from Authors collection, which is nessasary to perform filtering, sorting and pagination in further operations.</p>&#xA;"
39367669,39364901,2770306,2016-09-07T10:40:46,"<p>Correct.&#xA;All you need to do is to resolve to the right source,&#xA;You can either return a value or a promise for a value.</p>&#xA;"
39278457,39268365,2770306,2016-09-01T18:22:04,"<p>I can suggest you to use <a href=""http://crossbar.io"" rel=""nofollow"">WAMP protocol</a> and then build a network of all of your functionality.&#xA;Finally serve it under 1 GraphQL server</p>&#xA;"
50994814,50989454,1600898,2018-06-22T19:57:09,"<p>There are multiple problems with the code.</p>&#xA;&#xA;<p>First and foremost, <code>data_received</code> <em>never returns</em>. At the transport/protocol level, asyncio programming is single-threaded and callback-based. Application code is scattered across callbacks like <code>data_received</code>, and the event loop runs the show, monitoring file descriptors and invoking the callbacks as needed. Each callback is only allowed to perform a short calculation, invoke methods on transport, and arrange for further callbacks to be executed. What the callback cannot do is take a lot of time to complete or block waiting for something. A <code>while</code> loop that never exits is especially bad because it doesn't allow the event loop to run at all.</p>&#xA;&#xA;<p>This is why the code only spits out exceptions once the client disconnects: <code>connection_lost</code> is never called. It's supposed to be called by the event loop, and the never-returning <code>data_received</code> is not giving the event loop a chance to resume. With the event loop blocked, the program is unable to respond to other clients, and <code>data_received</code> keeps trying to send data to the disconnected client, and logs its failure to do so.</p>&#xA;&#xA;<p>The correct way to express the idea can look like this:</p>&#xA;&#xA;<pre><code>def data_received(self, data):&#xA;    self.i = 0&#xA;    loop.call_soon(self.write_to_client)&#xA;&#xA;def write_to_client(self):&#xA;    self.transport.write(b'&gt;&gt; %i' % self.i)&#xA;    self.i += 1&#xA;    loop.call_later(2, self.write_to_client)&#xA;</code></pre>&#xA;&#xA;<p>Note how both <code>data_received</code> and <code>write_to_client</code> do very little work and quickly return. No calls to <code>time.sleep()</code>, and definitely no infinite loops - the ""loop"" is hidden inside the kind-of-recursive call to <code>write_to_client</code>.</p>&#xA;&#xA;<p>This change reveals the second problem in the code. Its <code>MyProtocol.connection_lost</code> stops the whole event loop and exits the program. This renders the program unable to respond to the second client. The fix could be to replace <code>loop.stop()</code> with setting a flag in <code>connection_lost</code>:</p>&#xA;&#xA;<pre><code>def data_received(self, data):&#xA;    self._done = False&#xA;    self.i = 0&#xA;    loop.call_soon(self.write_to_client)&#xA;&#xA;def write_to_client(self):&#xA;    if self._done:&#xA;        return&#xA;    self.transport.write(b'&gt;&gt; %i' % self.i)&#xA;    self.i += 1&#xA;    loop.call_later(2, self.write_to_client)&#xA;&#xA;def connection_lost(self, exc):&#xA;    self._done = True&#xA;</code></pre>&#xA;&#xA;<p>This allows multiple clients to connect.&#xA;<hr>&#xA;Unrelated to the above issues, the callback-based code is a bit tiresome to write, especially when taking into account complicated code paths and exception handling. (Imagine trying to express nested loops with callbacks, or propagating an exception occurring inside a deeply embedded callback.) asyncio supports coroutines-based <em>streams</em> as alternative to callback-based transports and protocols.</p>&#xA;&#xA;<p>Coroutines allow writing natural-looking code that contains loops and looks like it contains  blocking calls, which under the hood are converted into suspension points that enable the event loop to resume. Using streams the code from the question would look like this:</p>&#xA;&#xA;<pre><code>async def talk_to_client(reader, writer):&#xA;    peername = writer.get_extra_info('peername')&#xA;    print('Connection from {}'.format(peername))&#xA;&#xA;    data = await reader.read(1024)&#xA;    i = 0&#xA;    while True:&#xA;        writer.write(b'&gt;&gt; %i' % i)&#xA;        await writer.drain()&#xA;        await asyncio.sleep(2)&#xA;        i += 1&#xA;&#xA;loop = asyncio.get_event_loop()&#xA;coro = asyncio.start_server(talk_to_client, &#xA;    os.environ.get('MY_SERVICE_ADDRESS', 'localhost'), &#xA;    os.environ.get('MY_SERVICE_PORT', 8100))&#xA;server = loop.run_until_complete(coro)&#xA;&#xA;loop.run_forever()&#xA;</code></pre>&#xA;&#xA;<p><code>talk_to_client</code> looks very much like the original implementation of <code>data_received</code>, but without the drawbacks. At each point where it uses <code>await</code> the event loop is resumed if the data is not available. <code>time.sleep(n)</code> is replaced with <code>await asyncio.sleep(n)</code> which does the equivalent of <code>loop.call_later(n, &lt;resume current coroutine&gt;)</code>. Awaiting <code>writer.drain()</code> ensures that the coroutine pauses when the peer cannot process the output it gets, and that it raises an exception when the peer has disconnected.</p>&#xA;"
34242396,30288968,2797186,2015-12-12T16:49:25,"<p>This is a case where <a href=""http://linkeddata.org/"" rel=""nofollow"">Linked Data</a> can help you.</p>&#xA;&#xA;<p>Basically the Floor attribute for the worker would be an URI (a link) to the floor itself. And Any other linked data should be expressed as URIs as well.</p>&#xA;&#xA;<p>Modeled with some JSON-LD it would look like this:</p>&#xA;&#xA;<pre><code>worker = {&#xA;  '@id': '/workers/87373',&#xA;  name: 'John',&#xA;  floor: {&#xA;    '@id': '/floors/123'&#xA;  }&#xA;}&#xA;&#xA;floor = {&#xA;  '@id': '/floor/123',&#xA;  'level': 12,&#xA;  building: { '@id': '/buildings/87' }&#xA;}&#xA;&#xA;building = {&#xA;  '@id': '/buildings/87',&#xA;  name: 'John's home',&#xA;  city: { '@id': '/cities/908' } &#xA;}&#xA;</code></pre>&#xA;&#xA;<p>This way all the client has to do is append the BASE URL (like api.example.com) to the @id and make a simple GET call. </p>&#xA;&#xA;<p>To remove the extra calls burden from the client (in case it's a slow mobile device), we use the gateway pattern with micro-services. The gateway can expand those links with very little effort and augment the return object. It can also do multiple calls in parallel. </p>&#xA;&#xA;<p>So the gateway will make a GET /floor/123 call and replace the floor object on the worker with the reply. </p>&#xA;"
41865083,41850142,1031042,2017-01-26T02:36:16,"<p>If I'm understanding your question correctly, you're trying to still use one MySQL instance but with many microservices.</p>&#xA;&#xA;<p>There are a couple of ways to make an SQL system work:</p>&#xA;&#xA;<ol>&#xA;<li><p>You could create a microservice-type that handles data inserts/reads from the database and take advantage of <a href=""https://en.wikipedia.org/wiki/Connection_pool"" rel=""nofollow noreferrer"">connection pooling</a>. And have the rest of your services do all their data read/writes through these services. This will definitely add a bit of extra latency to all your writes/reads and likely be problematic at scale.</p></li>&#xA;<li><p>You could attempt to look for a multi-master SQL solution (e.g. <a href=""https://www.citusdata.com/"" rel=""nofollow noreferrer"">CitusDB</a>) that scales easily and you can use a central schema for your database and just make sure to handle edge cases for data insertion (de-deuping etc.)</p></li>&#xA;<li><p>You can use data-streaming architectures like <a href=""https://kafka.apache.org/"" rel=""nofollow noreferrer"">Kafka</a> or <a href=""https://aws.amazon.com/kinesis/"" rel=""nofollow noreferrer"">AWS Kinesis</a> to transfer your data to your microservices and make sure they only deal with data through these streams. This way, you can de-couple your database from your data.</p></li>&#xA;</ol>&#xA;&#xA;<p>The best way to approach it in my opinion is #3. This way, you won't have to think about your storage at the computation layer of your microservice architecture.</p>&#xA;&#xA;<p>Not sure what service you're using for your microservices, but <a href=""https://stdlib.com"" rel=""nofollow noreferrer"">StdLib</a> forces a few conversions (e.g. around only transferring data through HTTP) that helps folks wrap their head around it all. AWS Lambda also works very well with Kinesis as a source to launch the function which could help with the #3 approach.</p>&#xA;&#xA;<p>Disclaimer: I'm the founder of <a href=""https://stdlib.com"" rel=""nofollow noreferrer"">StdLib</a>.</p>&#xA;"
35367479,35361819,707458,2016-02-12T16:25:18,"<p>At this time service fabric does not support autoscale, it will in the near future:</p>&#xA;&#xA;<p><a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-cluster-scale-up-down/"" rel=""nofollow"">service-fabric-cluster-scale-up-down</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>Auto-scale Service Fabric clusters</p>&#xA;  &#xA;  <p>At this time, Service Fabric clusters do not support auto-scaling. In the near future, clusters will be built on top of virtual machine scale sets, at which time auto-scaling will become possible and will behave similarly to the auto-scale behavior available in cloud services.</p>&#xA;</blockquote>&#xA;"
37273897,37257673,6345090,2016-05-17T10:45:16,"<p>As shankarsh15 says, Hystrix actually provides resilience (e.g. fallbacks) when errors and/or timeouts occur in API calls.</p>&#xA;&#xA;<p>I believe it's actually ribbon-loadbalance (<code>LoadBalancerContext.java</code> -> <code>getServerFromLoadBalancer()</code>) that determines which client to call.</p>&#xA;&#xA;<p>And this ultimately works in a similar way to doing <code>discoveryClient.getInstances(""service-name"")</code> (aka gets a list of service instances, then uses round robin to pick a service to use)</p>&#xA;"
37459888,37457765,6345090,2016-05-26T11:41:47,"<p>I'd say that the <a href=""https://spring.io/blog/2015/01/20/microservice-registration-and-discovery-with-spring-cloud-and-netflix-s-eureka"" rel=""nofollow"" title=""Spring Cloud guide on this"">Spring Cloud guide on this</a> is the best starting point.</p>&#xA;&#xA;<p>But in short, since you're using Spring Cloud (i.e. <code>@EnableDiscoveryClient</code>), I'd personally use Spring Cloud's feign client support to carry out the call. This will do the actual discovery service (eureka) lookup and HTTP calls for you.</p>&#xA;&#xA;<p>Firstly you'll need the <code>@EnableFeignClients</code> annotation on your config class, and the following dependency (assuming Maven):</p>&#xA;&#xA;<pre><code>&lt;dependency&gt;&#xA;    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&#xA;    &lt;artifactId&gt;spring-cloud-starter-feign&lt;/artifactId&gt;&#xA;&lt;/dependency&gt;&#xA;</code></pre>&#xA;&#xA;<p>Then within your user service project, you can add the following interface:</p>&#xA;&#xA;<pre><code>@FeignClient(""add-service"")&#xA;public interface AddServiceClient {&#xA;&#xA;    @RequestMapping(method = RequestMethod.POST, value = ""/add/{x}/{y}"", consumes=""application/json"")&#xA;    int addNumbers(@PathVariable(""x"") int x, @PathVariable(""y"") int y);&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>That's basically it really. You can then autowire <code>AddServiceClient</code> and use it:</p>&#xA;&#xA;<pre><code>@Autowired&#xA;private AddServiceClient addServiceClient;&#xA;&#xA;void someMethod() {&#xA;    addServiceClient.addNumbers(2, 4);&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>This assumes that you expose /add/{x}/{y} as a POST endpoint within your add service (e.g. via <code>@RestController</code> and <code>@RequestMapping</code>)</p>&#xA;&#xA;<p>EDIT: Sorry, I just seen where you said REST would be costly. Why do you think that? :)</p>&#xA;"
51224799,51224374,1763458,2018-07-07T16:05:11,"<p>API gateway sounds like what you need.&#xA;If you'll keep it simple, just to trigger internal API, it will not become your new monolith.</p>&#xA;&#xA;<p>It will allow you do even better processing when your application grows with new microservices, or when you have to support different clients (browser, mobile apps, watch, IOT, etc)</p>&#xA;&#xA;<p>BTW, the example you show sounds like a good exercise, in reality, for most webapps, it looks like over design. I would not break every DB call to its own microservices.</p>&#xA;"
31897619,30213456,458370,2015-08-08T19:17:22,"<blockquote>&#xA;  <p>What solutions are available to prevent this kind of data inconsistency from happening? </p>&#xA;</blockquote>&#xA;&#xA;<p>Traditionally, distributed transaction managers are used.  A few years ago in the Java EE world you might have created these services as <a href=""https://en.wikipedia.org/wiki/Ejb"" rel=""nofollow"">EJB</a>s which were deployed to different nodes and your API gateway would have made remote calls to those EJBs.  The application server (if configured correctly) automatically ensures, using two phase commit, that the transaction is either committed or rolled back on each node, so that consistency is guaranteed.  But that requires that all the services be deployed on the same type of application server (so that they are compatible) and in reality only ever worked with services deployed by a single company.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Are there patterns that allow transactions to span multiple REST requests?</p>&#xA;</blockquote>&#xA;&#xA;<p>For SOAP (ok, not REST), there is the <a href=""http://docs.oasis-open.org/ws-tx/wstx-wsat-1.2-spec.html"" rel=""nofollow"">WS-AT</a> specification but no service that I have ever had to integrate has support that.  For REST, JBoss has <a href=""https://issues.jboss.org/browse/JBTM-1468"" rel=""nofollow"">something in the pipeline</a>.  Otherwise, the ""pattern"" is to either find a product which you can plug into your architecture, or build your own solution (not recommended).  </p>&#xA;&#xA;<p>I have published such a product for Java EE: <a href=""https://github.com/maxant/genericconnector"" rel=""nofollow"">https://github.com/maxant/genericconnector</a></p>&#xA;&#xA;<p>According to the paper you reference, there is also the Try-Cancel/Confirm pattern and associated Product from Atomikos.</p>&#xA;&#xA;<p>BPEL Engines handle consistency between remotely deployed services using compensation.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Alternatively, I know REST might just not be suited for this use case. Would perhaps the correct way to handle this situation to drop REST entirely and use a different communication protocol like a message queue system? </p>&#xA;</blockquote>&#xA;&#xA;<p>There are many ways of ""binding"" non-transactional resources into a transaction:</p>&#xA;&#xA;<ul>&#xA;<li>As you suggest, you could use a transactional message queue, but it will be asynchronous, so if you depend on the response it becomes messy.</li>&#xA;<li>You could write the fact that you need to call the back end services into your database, and then call the back end services using a batch. Again, async, so can get messy.</li>&#xA;<li>You could use a business process engine as your API gateway to orchestrate the back end microservices.</li>&#xA;<li>You could use remote EJB, as mentioned at the start, since that supports distributed transactions out of the box.</li>&#xA;</ul>&#xA;&#xA;<blockquote>&#xA;  <p>Or should I enforce consistency in my application code (for example, by having a background job that detects inconsistencies and fixes them or by having a ""state"" attribute on my User model with ""creating"", ""created"" values, etc.)?</p>&#xA;</blockquote>&#xA;&#xA;<p>Playing devils advocate: why build something like that, when there are products which do that for you (see above), and probably do it better than you can, because they are tried and tested?</p>&#xA;"
43835256,42311050,3018627,2017-05-07T18:40:43,"<p>Suppose you have a microservice named ""LoginServer"" now, let's see how to register this service with discovery server (Eureka Server) at startup.</p>&#xA;&#xA;<p>Here Spring Boot startup class of LoginServer.java:</p>&#xA;&#xA;<pre><code>@EnableAutoConfiguration&#xA;@EnableDiscoveryClient&#xA;public class LoginServer {&#xA;public static void main(String[] args) {&#xA;&#xA;     // Will configure using login-server.yml&#xA;&#xA;     System.setProperty(""spring.config.name"", ""login-server"");&#xA;     SpringApplication.run(LoginServer.class, args);&#xA;&#xA;  }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>The @EnableDiscoveryClient - enables service registration and discovery. In this case, this process registers itself with the discovery-server service using its application name, that is configured in YML configuration file. </p>&#xA;&#xA;<p>let's see the complete setup:</p>&#xA;&#xA;<p>First create a login-server.yml (any name but extension should be .yml) file into src/main/resources package folder. And write those configurations and save.</p>&#xA;&#xA;<pre><code># Spring properties&#xA;spring:&#xA; application:&#xA;   name: login-server # This name used as ID so (""spring.config.name"", &#xA;                      #""login-server""); must be same.&#xA;&#xA;# Discovery Server Access&#xA;eureka:&#xA; client:&#xA;  serviceUrl:&#xA;   defaultZone: http://localhost:1111/eureka/&#xA;&#xA;# HTTP Server&#xA;server:&#xA; port: 2222   # HTTP (Tomcat) port&#xA;</code></pre>&#xA;&#xA;<p>Run the LoginServer and let it finish initializing. Open the dashboard by putting this URL <a href=""http://localhost:1111"" rel=""nofollow noreferrer"">http://localhost:1111</a> in your favorite browser and refresh. After few seconds later you should see the LOGIN-SERVER. Generally registration takes up to 30 seconds (by default) so wait or restart.</p>&#xA;&#xA;<p>And this is the microservice complete registration process.</p>&#xA;"
45365725,45363163,2371715,2017-07-28T05:46:00,"<p>Typically the backend services that perform the server side business operations (i.e. core) are not exposed publicly due to many reasons. They are shielded by some Gateway layer that also serves as reverse-proxy. Netflix <code>Zuul</code> serves as this gateway layer which easily gives you the capabilities as mentioned by @Apollo and <a href=""https://github.com/Netflix/zuul/wiki"" rel=""nofollow noreferrer"">here</a></p>&#xA;"
48249562,48241005,2371715,2018-01-14T12:26:04,"<p>Ideally, it should be the <code>gateway</code> that should interact with the <code>authorization</code> server and decide whether the underlying <code>services</code> should be allowed access.</p>&#xA;"
50812411,50771405,2371715,2018-06-12T08:19:21,"<p>One thing is not clear to me.</p>&#xA;&#xA;<ul>&#xA;<li>Is the data source being accessed by a single service (say S2) will make a decoupled architecture? Aren't the Services like S1 or S3..Sn are going to be coupled with S2 instead of the D1_Big, if the datasource itself is shared? To achieve true decoupling, the data sources should be decoupled among themselves which needs a careful data modelling -- microservices as a pattern helps having more bounded context which is good for decoupling.&#xA;<br><br>&#xA;Microservices or not, I would think instead of having a separate service S2 exposing <em>data</em> operations, let the data source be interacted by a separate codebase that produces a library/interface/jar which can be bundled along with the deploy-able that is produced for the interacting Service like S1 etc. Bundling of <em>data</em> jar within the S1 can be handled through a build process. The Business Operation to Data Source mapping/interaction will be more performant than a service interaction in between. The layer can handle the data specific needs in CRUD (Simple READ/WRITE) or any Sophisticated ways (e.g. Views in RDBMS) it deems fit and expose data needed for business functionality and keep it client agnostic and still remains as a Scalable option.</li>&#xA;</ul>&#xA;"
42492587,42490380,6371459,2017-02-27T17:56:12,"<blockquote>&#xA;  <p>Is the refresh token inside the jwt with the user info or is it in its own token with a different encryption for extra protection?</p>&#xA;</blockquote>&#xA;&#xA;<p>If you are asking about refresh tokens as defined in Oauth2, a refresh token is returned by authorization server after a successful user authentication. It is just a random string. With a refresh token, the client can get access tokens ( your JWT)</p>&#xA;&#xA;<blockquote>&#xA;  <p>If it is in its own token what should the other micro services respond with to the react app if the jwt is invalid and needs to be refreshed. Is there a common http status code used?</p>&#xA;</blockquote>&#xA;&#xA;<p>They must reject the request. Use 401- Unauthorized</p>&#xA;&#xA;<blockquote>&#xA;  <p>I have read that a refresh token should be more secure then your jwt cause it can be used to issue jwts and will have a longer active time. Is there any extra security past encryption that can be done server side or client side that isnt already done for jwts?</p>&#xA;</blockquote>&#xA;&#xA;<p>Use https to get the refresh tokens. Aditional encryption will not increase the security level because possesion of the token is the proof-of-authentication. But you need to keep it secure</p>&#xA;&#xA;<blockquote>&#xA;  <p>When should you refresh the refresh token with a new token and timestamp that it becomes invalid?</p>&#xA;</blockquote>&#xA;&#xA;<p>Depends on the system. Oauth2 does not specify it. Usually  are long lived but in some cases I have seen recommendations to renew it after each usage. </p>&#xA;"
46195998,46190467,8537987,2017-09-13T11:11:48,"<p>Inside your event store, you could track whether read-side replication was successful.&#xA;As soon as step 9 suceeds, you can flag the event as 'replicated'.</p>&#xA;&#xA;<p>That way, you could introduce a component watching for unreplicated events and trigger step 9. You could also track whether the replication failed multiple times.</p>&#xA;&#xA;<p>Updating the read-side (step 9) and flagigng an event as replicated should happen consistently. You could use a saga pattern here.</p>&#xA;"
48971631,48942153,8537987,2018-02-25T08:20:34,"<p>You are describing a compensation pattern as a replacement of a distributed transaction. In a microservice-oriented architecture, this is a good approach to focus on availability by utilizing eventual consistency: Instead of having one centrally coordinated, distributed transaction across services, each service will execute its sub-task without a transactional context. If something goes wrong, each service will be informed about the failure and execute some kind of (semantic) compensation of the previous action. Thus, the transactional operation is eventually undone.</p>&#xA;&#xA;<p>As you have already stated, communication can be done via a message bus system and there is no need for Event Sourcing or CQRS, the compensation pattern does not depend on these principles. </p>&#xA;"
38191400,37684678,4278915,2016-07-04T19:49:03,"<p>You can test using an ""consumer driven contract"" approach.&#xA;Like mentioned here:&#xA;<a href=""https://medium.com/@sagansystems/integration-testing-our-microservices-for-fun-and-profit-b749ef42703b#.q7dyxgfrn"" rel=""nofollow"">https://medium.com/@sagansystems/integration-testing-our-microservices-for-fun-and-profit-b749ef42703b#.q7dyxgfrn</a>&#xA;using this project:&#xA;<a href=""https://github.com/realestate-com-au/pact"" rel=""nofollow"">https://github.com/realestate-com-au/pact</a></p>&#xA;"
48756822,48753245,1726083,2018-02-12T22:49:08,"<p>I would strongly recommend running 2 albs for this.   Sure, it will cost you more (not double because the traffic costs won't be doubled), but it's much more straight forward to have an internal load balancer and an external load balancer.  Work hours cost money too!  Running 2 albs will be the least admin and probably the cheapest overall.  </p>&#xA;"
40006545,30213456,6706819,2016-10-12T19:10:13,"<p>Why not use API Management (APIM) platform that supports scripting/programming? So, you will be able to build composite service in the  APIM without disturbing micro services. I have designed using APIGEE for this purpose.   </p>&#xA;"
52082557,52051459,1923715,2018-08-29T16:53:46,<p>add server.use-forward-headers=true to your config when using the gateway </p>&#xA;
40967940,36083504,1599619,2016-12-05T05:57:19,"<p>I don't see how pgbouncer will solve any of the problems you would have with the first approach.   There are many reasons to use pgbouncer but I don't think they are really applicable here.</p>&#xA;&#xA;<p>Also, in my experience, while idle connections can be an issue, they probably will not be on the scale you are talking about.  I mean we are not talking hundreds of idle connections right?</p>&#xA;&#xA;<p>More critically, one key thing that a microservices approach would give you is an ability to move dbs off to other servers.  If you do this, then having your connection pool centrally managed makes this harder to do.</p>&#xA;&#xA;<p>Per-service pool is generally more flexible and it makes your infrastructure quite a bit more flexible too.</p>&#xA;"
51383106,51282283,5338243,2018-07-17T13:48:27,"<p>The solution we will use for this issue will be to use custom attributes as part of the Cognito user set-up. We will have text fields for additional attributes or groups that the user belongs to.</p>&#xA;&#xA;<p>The way this should be implemented can be found at the following links:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://aws.amazon.com/blogs/apn/managing-saas-identity-through-custom-attributes-and-amazon-cognito/"" rel=""nofollow noreferrer"">Managing SaaS Identity Through Custom Attributes and Amazon Cognito</a></li>&#xA;<li><a href=""https://aws.amazon.com/blogs/apn/managing-saas-users-with-amazon-cognito/"" rel=""nofollow noreferrer"">Managing SaaS Users with Amazon Cognito</a></li>&#xA;<li><a href=""https://aws.amazon.com/quickstart/saas/identity-with-cognito/"" rel=""nofollow noreferrer"">SaaS identity and isolation with Amazon Cognito (Example Guide)</a></li>&#xA;<li><a href=""https://d0.awsstatic.com/whitepapers/Multi_Tenant_SaaS_Storage_Strategies.pdf"" rel=""nofollow noreferrer"">SaaS Storage Strategies</a></li>&#xA;</ul>&#xA;&#xA;<p>With this data being automatically passed into each service as part of the Cognito credentials, we will be able to check that the user has the valid credentials for accessing data specific to each client.</p>&#xA;&#xA;<p>Examples of how to work with Cognito in a NodeJS application (sometimes with Serverless) include:</p>&#xA;&#xA;<p><a href=""https://serverless-stack.com/chapters/login-with-aws-cognito.html"" rel=""nofollow noreferrer"">https://serverless-stack.com/chapters/login-with-aws-cognito.html</a></p>&#xA;&#xA;<p><a href=""https://serverless-stack.com/chapters/add-a-create-note-api.html#configure-the-api-endpoint"" rel=""nofollow noreferrer"">https://serverless-stack.com/chapters/add-a-create-note-api.html#configure-the-api-endpoint</a></p>&#xA;&#xA;<p>This seems to be most easily achieved by using the <a href=""https://www.npmjs.com/package/aws-amplify"" rel=""nofollow noreferrer"">aws-amplify</a> package, which is primarily designed for front-end authentication, but which can be used in NodeJS for back-end authentication as specified <a href=""https://aws-amplify.github.io/amplify-js/media/authentication_guide.html"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;"
28566586,28565187,950147,2015-02-17T16:48:41,"<p>The best place to start is with the packages ending in a service similar to the one you want to connect to. You would have to check the docs of the service you want to target based on the OAuth version (twitter uses OAuth 1, facebook uses OAuth 2).</p>&#xA;&#xA;<p>Find out what OAuth version the service you want to target uses from the service's API docs and use a package that matches it the best with the names <code>accounts-xxx</code></p>&#xA;&#xA;<p>The packages are found at <a href=""https://github.com/meteor/meteor/tree/devel/packages"" rel=""nofollow"">https://github.com/meteor/meteor/tree/devel/packages</a>.</p>&#xA;&#xA;<p>Additionally there are loads of <code>accounts-xxx</code> package on atmospherejs.com with github links. Most of these are based off the above too.</p>&#xA;&#xA;<p>When you find a package that matches best to the service you want to target simply swap out the service names to the service you want.</p>&#xA;&#xA;<p>The packages typically use <code>accounts-oauth</code> (<a href=""https://github.com/meteor/meteor/tree/devel/packages/accounts-oauth"" rel=""nofollow"">https://github.com/meteor/meteor/tree/devel/packages/accounts-oauth</a>) which is very thoroughly code commented if you want to go much further and understand everything that is going on in detail.</p>&#xA;"
38089232,38045423,6520963,2016-06-29T02:13:54,"<p>One of the main benefits of Microservices in general is the ability to switch out underlying component pieces without the rest of the app/ecosystem having awareness of a change. The ambassador pattern allows for pieces to move and change while consuming code continues to connect to a single location. One of the links you provided had a great explanation/example: </p>&#xA;&#xA;<p>`&#xA;Rather than hardcoding network links between a service consumer and provider, Docker encourages service portability, for example instead of:</p>&#xA;&#xA;<p>(consumer) --> (redis)&#xA;Requiring you to restart the consumer to attach it to a different redis service, you can add ambassadors:</p>&#xA;&#xA;<p>(consumer) --> (redis-ambassador) --> (redis)&#xA;Or</p>&#xA;&#xA;<p>(consumer) --> (redis-ambassador) ---network---> (redis-ambassador) --> (redis)&#xA;When you need to rewire your consumer to talk to a different Redis server, you can just restart the redis-ambassador container that the consumer is connected to.&#xA;`</p>&#xA;&#xA;<p>In this instance the underlying component (redis) can be moved or updated without the code that uses redis knowing. It would even allow one to switch from redis to another kv store if the code were a bit more clever. </p>&#xA;"
50779279,50779254,3152516,2018-06-09T23:11:07,"<p>Sorry but with every one of these scenarios, the answer is, it depends. Part of your role as the architect is to determine this based on your scenario, and there are no real rules, just opinions. Also, there will be nobody standing over it judging you. At the end of the day the app just needs to be useful, scalable, reliable, and easy to maintain. This is the equivalent to a builder asking if the bricks should be round, square, or rectangle. It depends. If you equate this to a house, we are not talking about how the kitchen is arranged, more like the pipes in the basement. At some point nobody will care.</p>&#xA;"
51389782,51368809,3744900,2018-07-17T20:32:57,<p>I had to use below line in EmployeeClient application.yml</p>&#xA;&#xA;<pre><code>    defaultZone: http://localhost:8080/EurekaServer/eureka&#xA;</code></pre>&#xA;
36231241,36116835,1195908,2016-03-26T03:18:01,"<p>Sounds like you need a pubsub framework/product, e.g. ActiveMQ, RabbitMQ, ZeroMQ (NetMQ) or maybe even Redis pubsub/queues</p>&#xA;"
47291511,47287743,8934020,2017-11-14T17:08:33,"<p>Commercetools platform's services can be utilized in different scenarios such as</p>&#xA;&#xA;<ol>&#xA;<li>architectures which core e-commerce services are provided by commercetools.</li>&#xA;<li>microservice architectures which consist of several different services provided by different service providers - only one of them being commercetools.</li>&#xA;</ol>&#xA;&#xA;<p>Therefore commercetools services provide <strong>different concepts</strong> and actions that suit different levels of integration. For scenarios that are more like <em>1</em> it is just convenient that commercetools platform optionally performs validations or calculations in a <strong>highly integrated</strong> fashion. For scenarios that are more like <em>2</em> commercetools provides optional concepts that allow you to use all the services in a <strong>loosly coupled</strong> fashion.  </p>&#xA;&#xA;<p>Some examples:</p>&#xA;&#xA;<p><strong>Add a line item to cart</strong></p>&#xA;&#xA;<ul>&#xA;<li>Cart's <a href=""http://dev.commercetools.com/http-api-projects-carts.html#add-lineitem"" rel=""nofollow noreferrer"">addLineItem</a> action is meant to be used in combination with commercetools <em>Products</em> endpoint. If you use both, commercetools can automatically validate the product that you are adding to cart.</li>&#xA;<li>Cart's <a href=""http://dev.commercetools.com/http-api-projects-carts.html#add-customlineitem"" rel=""nofollow noreferrer"">addCustomLineItem</a> action is meant to be used with your products that are stored outside of commercetools or with dynamically generated lineItems, not necessarily being conventional products. Therefore it allows you to add you own products that commercetools does not need to know.</li>&#xA;</ul>&#xA;&#xA;<p><strong>Calculate shippingCosts</strong></p>&#xA;&#xA;<ul>&#xA;<li>If you use commercetools' cart and shippingMethods services commercetools <a href=""http://dev.commercetools.com/http-api-projects-carts.html#set-shippingmethod"" rel=""nofollow noreferrer"">setShippingMethod</a> action can automatically calculate shipping costs based on zones, countries and shippingMethods and even on more complex rules.</li>&#xA;<li>If you want to use commercetools cart service but you want to use your own shippingRate service you can use commercetools Carts <a href=""http://dev.commercetools.com/http-api-projects-carts.html#set-custom-shippingmethod"" rel=""nofollow noreferrer"">setCustomShippingMethod</a> to provide your externally calculated shippingRates.</li>&#xA;</ul>&#xA;&#xA;<p><strong>Create an Order</strong></p>&#xA;&#xA;<ul>&#xA;<li>If you use commercetools' cart and order services commercetools automatically validates your carts before converting it to orders if you perform the <a href=""http://dev.commercetools.com/http-api-projects-orders.html#create-order-from-cart"" rel=""nofollow noreferrer"">createOrderFromCart</a> action.</li>&#xA;<li>If you want to use commercetools orders but want to use your own cart services you can use commercetools <a href=""http://dev.commercetools.com/http-api-projects-orders-import.html#create-an-order-by-import"" rel=""nofollow noreferrer"">orderImport</a> service.</li>&#xA;</ul>&#xA;&#xA;<p>These are just three examples of different ways to use commercetools services in different scenarios. Usually whenever it makes sense you will find a way of loosely integrating commercetools services into your overall architecture.</p>&#xA;&#xA;<p>There is an <a href=""https://documenter.getpostman.com/view/2267334/commercetools-platform-api/6fR3nEb"" rel=""nofollow noreferrer"">unofficial postman collection</a> for the commercetools API that covers most of their endpoints and update actions (currently more then 400 actions).</p>&#xA;"
42217551,42216713,2127492,2017-02-14T03:48:12,"<p>As of Docker 1.10 you can use <a href=""https://docs.docker.com/engine/userguide/networking/"" rel=""nofollow noreferrer"">networking</a>.  What this does is binds the ip address of the docker container to a hostname that can be used in the container. </p>&#xA;&#xA;<p>If you use <a href=""https://docs.docker.com/compose/"" rel=""nofollow noreferrer""><code>docker-compose</code></a>, a network is automatically created and the two services can talk to one another.  Here is an example of one:</p>&#xA;&#xA;<pre><code>version: ""2""&#xA;&#xA;services:&#xA;  frontend:&#xA;    image: nginx&#xA;    networks:&#xA;      - my-network&#xA;    environment:&#xA;      BACKEND_URL: ""http://backend""&#xA;  backend:&#xA;    image: mariadb&#xA;    networks:&#xA;      - my-network&#xA;    environment:&#xA;      FRONTEND_URL: ""http://frontend""&#xA;&#xA;networks:&#xA;  my-network:&#xA;    driver: bridge&#xA;</code></pre>&#xA;&#xA;<p>You can read more about this in the <code>Getting started with multi-host networking</code> available <a href=""https://docs.docker.com/engine/userguide/networking/get-started-overlay/"" rel=""nofollow noreferrer"">here</a> or you can follow a guide like this <a href=""https://lockmedown.com/docker-devs-multiple-containers-docker-compose/"" rel=""nofollow noreferrer"">Composing Multi-container Networks with Docker Compose</a></p>&#xA;"
46328132,46311488,8622724,2017-09-20T17:16:39,"<p>Microservices do not necessarily rely strictly on point-to-point integration.</p>&#xA;&#xA;<p>The problems associated with direct communication are often managed in a microservice architecture using a message broker. If communication can be done asyncrounously -- ""fire and forget"" -- the application sending a message does not become inoperable if the receiver goes down. And the messages will still be there when the receiving service comes back up.</p>&#xA;&#xA;<p>If microservices are integrating over REST, the caller does need to know how to react if the other service doesn't respond. Since this gets hairy when you're saving data across systems (i.e. a distributed transaction), I like to use REST only for data retrieval APIs. And do all saves as a result of messages.</p>&#xA;&#xA;<p>It's important to note that there's a lot more to ESBs than just messaging. <a href=""https://stackoverflow.com/q/3280576/8622724"">See more on that in the answer here</a>.</p>&#xA;"
46265798,46244677,8622724,2017-09-17T15:33:00,"<p>I would recommend keeping your authentication service packaged up in another microservice and communicating with it to authenticate. (Basically your first option.)</p>&#xA;&#xA;<p>In a microservices ecosystem, you're likely to have 10s if not hundreds of applications running (ideally) pretty independent of one another. With the second option, you have added boilerplate every time you spin up a new service. But perhaps more importantly, you've coupled all of your microservices to your current authentication scheme. Best to put all of your auth behind an interface and hide all your auth logic in one, separated place. </p>&#xA;"
47929409,47918407,365444,2017-12-21T16:51:27,<p>I would go along the common storage idea.</p>&#xA;&#xA;<p>Have each microservice register itself with the common storage. Have each microservice register it has processed the message identifier when it does.</p>&#xA;&#xA;<p>You can work out which n services should process it and how many of the n service have processed it.</p>&#xA;&#xA;<p>No services need to be aware of each other.</p>&#xA;
48872632,40880443,7054212,2018-02-19T19:04:49,"<p>The answer for your question is yes! Microservices - composite UI is quite new in frontend world. Many big companies are dropping big monolithics apps and starting development in microservices way - so colled composite UI.</p>&#xA;&#xA;<p><strong>[Tailor Mosaic]</strong></p>&#xA;&#xA;<p>Each application can be built in different frameworks React, Angular, Vuejs by separate teams. You can base your concept application on Zalando stack - Mosaic Framework or just grab one part witch is Tailor.js with MIT licence and build your own one on top of that. It will definitely require from you additional parts like reverse proxy and communication bus/pipe.</p>&#xA;&#xA;<p><strong>[Polymer]</strong></p>&#xA;&#xA;<p>This is not microservices aproach but everything depends on scale of your project. Sometimes web components are more than enough. To face problem of reusable components built in different technologies you can use <a href=""https://www.polymer-project.org/"" rel=""nofollow noreferrer"">Polymer framework</a> - <a href=""https://www.webcomponents.org/"" rel=""nofollow noreferrer"">Web Components</a>.</p>&#xA;&#xA;<p><strong>[React way]</strong></p>&#xA;&#xA;<p>React is grate library for building complicated user interfaces. You can do almost everything – build generic components or whole apps that will be injected in some kind react common frame. Writing special mechanism for injecting applications in runtime from different origins as app bundle files is possible. </p>&#xA;&#xA;<p><strong>[Communication &amp; styles]</strong></p>&#xA;&#xA;<p>For sure you will face problems with communication between application / components. There is few possibilities to make sure that you're components/apps will talk with each other. Basic solution is to use some kind of API like <a href=""http://reactivex.io/rxjs/"" rel=""nofollow noreferrer"">RxJs</a> to build your own communication bus with custom functions or just use existing one pubs/sub event bus. If you will have different react apps with their own stors (the best solution is to have one store but nothing is stopping you from having many) you can think about some mechanisms like <a href=""http://nicolasgallagher.com/redux-modules-and-code-splitting/"" rel=""nofollow noreferrer"">redux modules</a> or <a href=""https://stackoverflow.com/questions/32968016/how-to-dynamically-load-reducers-for-code-splitting-in-a-redux-application"">dynamically loaded reducers</a> to <a href=""https://medium.com/front-end-hacking/code-splitting-redux-reducers-4073db30c72e"" rel=""nofollow noreferrer"">marge them together</a> and unplug them when no more needed.</p>&#xA;&#xA;<p>Building composite ui will also cause a lot problems with styling so it is a good idea to think about some kind of strategy like <a href=""https://medium.com/@Intelygenz/how-to-organize-your-css-with-oocss-bem-smacss-a2317fa083a7"" rel=""nofollow noreferrer"">BEM/SMACSS</a> or library like react css modules to avoid css redundancy.</p>&#xA;&#xA;<p><strong>Useful links:</strong></p>&#xA;&#xA;<p><a href=""https://www.mosaic9.org/"" rel=""nofollow noreferrer"">Project Mosaic | Microservices for the Frontend</a></p>&#xA;&#xA;<p><a href=""https://github.com/zalando/tailor"" rel=""nofollow noreferrer"">Zalando Tailor</a></p>&#xA;&#xA;<p><a href=""https://medium.com/@tomsoderlund/micro-frontends-a-microservice-approach-to-front-end-web-development-f325ebdadc16"" rel=""nofollow noreferrer"">Micro frontends—a microservice approach to front-end web development</a></p>&#xA;&#xA;<p><a href=""https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/multi-container-microservice-net-applications/microservice-application-design"" rel=""nofollow noreferrer"">Designing a microservice-oriented application</a></p>&#xA;&#xA;<p><strong>Working example</strong></p>&#xA;&#xA;<p><a href=""https://github.com/tsnolan23/tailor-react-spa"" rel=""nofollow noreferrer"">React frontend microservices architecture</a></p>&#xA;&#xA;<p><strong>Example communication bus</strong></p>&#xA;&#xA;<p><a href=""https://github.com/postaljs"" rel=""nofollow noreferrer"">Postaljs</a></p>&#xA;"
48013962,47934052,6203524,2017-12-28T20:29:13,<p>I've also managed to get it working by using just the cloud trace api by doing this before I create a span.</p>&#xA;&#xA;<pre><code>SpanContext spanContext = Trace.getSpanContextFactory().fromHeader(traceId);&#xA;Trace.getSpanContextHandler().attach(spanContext);&#xA;</code></pre>&#xA;&#xA;<p>Not sure if there is a negative of doing this.</p>&#xA;
47765542,47765389,182660,2017-12-12T04:51:27,"<p>I think this has nothing to do with Scala or HTTP. It is a limitation of the TCP protocol.</p>&#xA;&#xA;<p>Unfortunately you just can't create 1 million outbound TCP connections from a single machine. TCP connection is specified by two pairs (IP address, port) for client and server. The server might have all connections coming to the same port and distinguish them by the client information. But for any typical TCP/IP implementation each outbound connection will have its own unique TCP port assigned to it (and even if you roll out some custom implementation you still can't have two outgoing connections to the same server/port from the same client/port because they will be indistinguishable). <a href=""https://en.wikipedia.org/wiki/Transmission_Control_Protocol#TCP_ports"" rel=""nofollow noreferrer"">TCP port</a> is actually just a 16-bit number. It means there are only about 65k ports altogether which is obviously much less than 1 million you want. So to make 1 million connections you will require many machines (or at least virtual machines) running the test at the same time.</p>&#xA;"
48432561,48095718,2673284,2018-01-24T22:12:09,"<p>Service graph data must be generated in Jaeger. Currently it's possible with via a Spark job here: <a href=""https://github.com/jaegertracing/spark-dependencies"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/spark-dependencies</a></p>&#xA;"
31916653,31573823,2030468,2015-08-10T10:00:03,"<p>I know the question has been asked a while ago, but I think I have something to say that might be of value here.<br>&#xA;First of all, your approach will depend on the overall size of your final product. I tend to go with a rule of thumb: if I would have too many dependencies between individual micro-services, I tend to use something that would simplify and possibly remove these dependencies. I don't want to end up with a spider-web of services! A good thing to look at here are Message queues, like <a href=""https://www.rabbitmq.com/"" rel=""nofollow"">RabbitMQ</a> for example.<br>&#xA;However, if I have just a few services that talk to each other, I will just make them call each other directly, as any alternative solutions whilst simplifying the architecture, add some computing and infrastructure overhead.  </p>&#xA;&#xA;<p>Whatever approach you will decide to go with, design your services in a <a href=""http://alistair.cockburn.us/Hexagonal+architecture"" rel=""nofollow"">Hexagonal architecture</a> in mind! This will save you trouble when you decide to migrate from one solution to another. What I tend to do is design my DAOs as ""adapters"", so a DAO that calls Service A will either call it directly or via message queue, independent of the business logic. When I need to change it, I can just change this DAO for another one, without having to touch any of the business logic (at the end of the day business logic doesn't care how it gets the data). Hexagonal architecture fits really well with micro-service, TDD and black-box testing.</p>&#xA;"
48662919,38989659,2189445,2018-02-07T11:40:30,"<p>Faced the same issue, and after removing version from javax.servlet dependency it worked fine.</p>&#xA;"
42137517,42134186,2845061,2017-02-09T13:04:05,"<p>Try this in your service application:</p>&#xA;&#xA;<pre><code>const con = DDP.connect('http://localhost:3000');&#xA;&#xA;Examples = new Mongo.Collection('examples', {&#xA;  connection: con,&#xA;});&#xA;&#xA;console.log(Examples.find().count());&#xA;</code></pre>&#xA;&#xA;<p>Remember to start your main application first</p>&#xA;"
22514501,22513893,1471109,2014-03-19T18:06:20,"<p>I have not used REST but I use WSDL to allow the communication between the layers. Integration between services is very simple they talk to each other like nested functions at the back end or simply used XMLs and JSONs if the requests hop from server to server. </p>&#xA;&#xA;<p>Here server is the host of internal web-services. And based on requirement queuing can be provided to individual services. But at the end, only one response is sent to the caller from backend. </p>&#xA;"
44310769,30213456,8098437,2017-06-01T14:50:12,"<p>This is a classic question I was asked during an interview recently How to call multiple web services and still preserve some kind of error handling in the middle of the task. Today, in high performance computing, we avoid two phase commits. I read a paper many years ago about what was called the ""Starbuck model"" for transactions: Think about the process of ordering, paying, preparing and receiving the coffee you order at Starbuck... I oversimplify things but a two phase commit model would suggest that the whole process would be a single wrapping transaction for all the steps involved until you receive your coffee. However, with this model, all employees would wait and stop working until you get your coffee. You see the picture ?</p>&#xA;&#xA;<p>Instead, the ""Starbuck model"" is more productive by following the ""best effort"" model and compensating for errors in the process. First, they make sure that you pay! Then, there are message queues with your order attached to the cup. If something goes wrong in the process, like you did not get your coffee, it is not what you ordered, etc, we enter into the compensation process and we make sure you get what you want or refund you, This is the most efficient model for increased productivity.</p>&#xA;&#xA;<p>Sometimes, starbuck is wasting a coffee but the overall process is efficient. There are other tricks to think when you build your web services like designing them in a way that they can be called any number of times and still provide the same end result. So, my recommendation is:</p>&#xA;&#xA;<ul>&#xA;<li><p>Don't be too fine when defining your web services (I am not convinced about the micro-service hype happening these days: too many risks of going too far);</p></li>&#xA;<li><p>Async increases performance so prefer being async, send notifications by email whenever possible.</p></li>&#xA;<li><p>Build more intelligent services to make them ""recallable"" any number of times, processing with an uid or taskid that will follow the order bottom-top until the end, validating business rules in each step;</p></li>&#xA;<li><p>Use message queues (JMS or others) and divert to error handling processors that will apply operations to ""rollback"" by applying opposite operations, by the way, working with async order will require some sort of queue to validate the current state of the process, so consider that;</p></li>&#xA;<li><p>In last resort, (since it may not happen often), put it in a queue for manual processing of errors.</p></li>&#xA;</ul>&#xA;&#xA;<p>Let's go back with the initial problem that was posted. Create an account and create a wallet and make sure everything was done.</p>&#xA;&#xA;<p>Let's say a web service is called to orchestrate the whole operation.</p>&#xA;&#xA;<p>Pseudo code of the web service would look like this:</p>&#xA;&#xA;<ol>&#xA;<li><p>Call Account creation microservice, pass it some information and a some unique task id 1.1 Account creation microservice will first check if that account was already created. A task id is associated with the account's record. The microservice detects that the account does not exist so it creates it and stores the task id. NOTE: this service can be called 2000 times, it will always perform the same result. The service answers with a ""receipt that contains minimal information to perform an undo operation if required"".</p></li>&#xA;<li><p>Call Wallet creation, giving it the account ID and task id. Let's say a condition is not valid and the wallet creation cannot be performed. The call returns with an error but nothing was created.</p></li>&#xA;<li><p>The orchestrator is informed of the error. It knows it needs to abort the Account creation but it will not do it itself. It will ask the wallet service to do it by passing its ""minimal undo receipt"" received at the end of step 1.</p></li>&#xA;<li><p>The Account service reads the undo receipt and knows how to undo the operation; the undo receipt may even include information about another microservice it could have called itself to do part of the job. In this situation, the undo receipt could contain the Account ID and possibly some extra information required to perform the opposite operation. In our case, to simplify things, let's say is simply delete the account using its account id.</p></li>&#xA;<li><p>Now, let's say the web service never received the success or failure (in this case) that the Account creation's undo was performed. It will simply call the Account's undo service again. And this service should normaly never fail because its goal is for the account to no longer exist. So it checks if it exists and sees nothing can be done to undo it. So it returns that the operation is a success.</p></li>&#xA;<li><p>The web service returns to the user that the account could not be created.</p></li>&#xA;</ol>&#xA;&#xA;<p>This is a synchronous example. We could have managed it in a different way and put the case into a message queue targeted to the help desk if we don't want the system to completly recover the error"". I've seen this being performed in a company where not enough hooks could be provided to the back end system to correct situations. The help desk received messages containing what was performed successfully and had enough information to fix things just like our undo receipt could be used for in a fully automated way.</p>&#xA;&#xA;<p>I have performed a search and the microsoft web site has a pattern description for this approach. It is called the compensating transaction pattern:</p>&#xA;&#xA;<p><a href=""https://docs.microsoft.com/en-us/azure/architecture/patterns/compensating-transaction"" rel=""noreferrer"">Compensating transaction pattern</a></p>&#xA;"
40068679,40068318,1939589,2016-10-16T09:09:23,"<p>@LoadBalanced was missing from ribbon&#xA; and also added </p>&#xA;&#xA;<pre><code>client:&#xA;    serviceUrl:&#xA;      defaultZone: http://127.0.0.1:8761/eureka/&#xA;    lease:&#xA;      duration: 5&#xA;</code></pre>&#xA;&#xA;<p>code is working now, and can be taken from my github account</p>&#xA;"
38041666,37897058,301957,2016-06-26T18:18:15,"<p>I have swagger api documentation url in each micro services it self. Ex: <a href=""http://myservice1domain:8080/swagger-ui.html"" rel=""nofollow"">http://myservice1domain:8080/swagger-ui.html</a>. I haven't use ALPS. Moreover, I dont think you have to or you can specifically expose only HAL kind of stuff because those are bind with response data. Anyway HAL expose to user in swagger as sample request body and with response json right</p>&#xA;"
27123564,27116051,107397,2014-11-25T09:56:23,"<p>Yes, that is possible. You can use a <a href=""http://docs.cloudfoundry.org/devguide/services/user-provided.html"" rel=""nofollow"">user provided service</a>. </p>&#xA;&#xA;<p>That allows you to inject the environment variables into your app, that are needed to connect to RabbitMQ (like host, port, vhost, username, password). </p>&#xA;&#xA;<p>Once you create that service, you can bind it to your app. Inside your app code, you then can read the environment variables exactly the same way as you would do it, if you had used a RabbitMQ service provided by CloudFoundry.</p>&#xA;"
48572281,48571805,454533,2018-02-01T21:49:05,"<p><code>json</code> is for handling a request body from an incoming request to your server (<code>http.IncomingMessage</code>). <code>https.get</code> returns a <code>ClientRequest</code>. You need to use the callback of <code>https</code> to get the response:</p>&#xA;&#xA;<pre><code>https.get('https://jsonplaceholder.typicode.com/posts/1', async res =&gt; {&#xA;  const jsonData = await json(res, {&#xA;    encoding: 'utf8'&#xA;  });&#xA;  console.log(jsonData)&#xA;});&#xA;</code></pre>&#xA;&#xA;<p>You may want to use a library like <code>axios</code> to integrate promises with <code>IncomingMessage</code></p>&#xA;&#xA;<pre><code>const { data } = await axios({ url, responseType: 'stream' });&#xA;const jsonData = await json(data, { encoding: 'utf8' });&#xA;</code></pre>&#xA;&#xA;<p>However note that axios and other such libraries have built in JSON parsing for responses as well.</p>&#xA;"
48181267,48180454,8060293,2018-01-10T06:08:37,"<p>Your post contains many questions. Your asking about database sharding, architecture with rails engines and performances / scalability with the timeout issue.</p>&#xA;&#xA;<p><strong>Performances / Timeout</strong></p>&#xA;&#xA;<p>First your timeout issue is not related to the number of models as you suggest. The number of models as no impact at all on performances. To know where is the performance problem or the bottleneck you should use a monitoring tool. This one is my <em>favorite</em> <a href=""https://www.rorvswild.com"" rel=""nofollow noreferrer"">https://www.rorvswild.com</a> (DISCLAIMER: I'm the author ^_^). There is other competitors, use the one your like more.</p>&#xA;&#xA;<p>For the timeouts maybe you should check around your database config. We have not enough informations here to go deeper.</p>&#xA;&#xA;<p><strong>Database sharding</strong></p>&#xA;&#xA;<p>That is not trivial at all since you cannot JOIN and reference foreign keys when they are not in the same db. That is why you have to carefully chose where to shard your db. Ideally it's where you have the least joins. It's a long work which has an important impact on your code base. A stack overflow post is not enough to talk about sharding. Fortunately there is a lot of articles and gems to help you for that.</p>&#xA;&#xA;<p>Just be sure you understand that you can split the load across many databases, but it comes at an extra price on your code base.</p>&#xA;&#xA;<p>For the relationships across databases you cannot use the Rails builtin has_many and so on. You have to define the relationships yourself or use a &#xA; gem which will help for that. To give you an idea:</p>&#xA;&#xA;<pre><code>class User&#xA;  def products&#xA;    Product.where(user_id: id)&#xA;  end&#xA;end&#xA;</code></pre>&#xA;&#xA;<p><strong>Rails engines</strong></p>&#xA;&#xA;<p>They are great to built reusable piece of features across applications (<a href=""http://guides.rubyonrails.org/engines.html"" rel=""nofollow noreferrer"">http://guides.rubyonrails.org/engines.html</a>). Looks like reusing is not your goal. So I'm afraid your going in the wrong direction.</p>&#xA;&#xA;<p>If you don't want to reopen the class you can use a module:</p>&#xA;&#xA;<pre><code>module HasProducts&#xA;  def self.included(model)&#xA;    model.has_many(:products)&#xA;  end&#xA;&#xA;  def get_title_product&#xA;    products.pluck(:title)&#xA;  end&#xA;end&#xA;</code></pre>&#xA;"
52108560,52104915,400262,2018-08-31T04:43:42,"<p>First off you may want to consider a framework such as Chalice if you’re moving to microservices on Lambda. This will help reduce the sprawl of services some but each case is different and all depends on where you draw your bounded contexts. </p>&#xA;&#xA;<p>Speaking from a similar experience to what you’re embarking on, you will want to invest heavily in a few areas. First off having a consistent logging approach is key. You’ll want to ship logs consistently to a single log aggregation service so you can easily query across all services to get metrics. CloudWatch, Sumo Logic, etc can help with this. Also use X-Ray to get more detailed insight. </p>&#xA;&#xA;<p>You will also want to consider adding some automation into your CI/CD pipeline to produce documentation in Swagger or something similar. This should be done in a way that the result is a searchable catalog&#xA;Of all services with all necessary documentation. My experience has involved doing this with Swagger UI and some custom HTML that gets generated and deployed on each build job. </p>&#xA;&#xA;<p>One last recommendation is to invest in testing. Contract testing and backwards compatibility testing is key to saving yourself from deploying breaking changes. I would also add feature toggles as another key that can go hand and hand here. </p>&#xA;&#xA;<p>Good luck with this effort! </p>&#xA;"
49334214,26866479,1007494,2018-03-17T08:55:26,"<p>We're currently implementing an architecture similar to your second option. We encountered the following complexities while doing it: (feel free for anyone to chime in to this as it's still a work in progress)</p>&#xA;&#xA;<ul>&#xA;<li>There is still technically a monolithic app in your system (the user facing app). Each time a change is done in the REST api you'd have to change the front facing app to handle those new changes. Don't even get me started on how you introduce a new microservice behind it. So in essence, the more microservices you put behind it, the bigger that API Gateway gets. (<a href=""https://www.nginx.com/blog/building-microservices-using-an-api-gateway/"" rel=""nofollow noreferrer"">https://www.nginx.com/blog/building-microservices-using-an-api-gateway/</a>)</li>&#xA;</ul>&#xA;&#xA;<blockquote>&#xA;  <p>The API Gateway also has some drawbacks. It is yet another highly available component that must be developed, deployed, and managed. There is also a risk that the API Gateway becomes a development bottleneck. Developers must update the API Gateway in order to expose each microservice’s endpoints. It is important that the process for updating the API Gateway be as lightweight as possible. Otherwise, developers will be forced to wait in line in order to update the gateway. Despite these drawbacks, however, for most real‑world applications it makes sense to use an API Gateway.</p>&#xA;</blockquote>&#xA;&#xA;<ul>&#xA;<li>For re-usability I designed an abstraction layer that defines unique behavior for communicating with each microservice. One concrete implementation for each microservice. This introduced another layer of complexity because now we had to maintain what we called ""RPC connectors"" along with its corresponding microservice. Personally, this ate a lot of a developer's time since, on top of maintaining their respective microservice they had to maintain the connector. If any of those were out of date, the public app would fail. Also, changes in the connector would require a public app rebuild (we currently defined the connectors as jar dependencies). </li>&#xA;<li>While this is mentioned in another post and a blog, the foreign key relationship becomes a mess when dealing with multiple microservices handling its own db. (Database per service pattern) Your front facing app is now facing the problem of having to stitch them together. (""I need this data but I have to look up these keys to in each microservice to see who has what."") I'm not saying this is the right way to do it but if we're dealing with multiple rows returned then each of the ids have to be individually resolved from a microservice. I'm not sure how efficient this is though. I would be glad to hear suggestions.</li>&#xA;</ul>&#xA;"
41646377,41516276,6451334,2017-01-14T02:59:51,"<p>I ended up making a module, that could be initiated at one place and used in a component within it, which could then be imported in the main app.&#xA;So in my module i have --</p>&#xA;&#xA;<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">&#xD;&#xA;<div class=""snippet-code"">&#xD;&#xA;<pre class=""snippet-code-js lang-js prettyprint-override""><code>module.exports = {&#xD;&#xA;  initiate: funtion(token){&#xD;&#xA;    this.data = useTokenToGetData(token);&#xD;&#xA;    ... //downstream processing&#xD;&#xA;&#xD;&#xA;  },&#xD;&#xA;  getData: function() {&#xD;&#xA;    return this.data;&#xD;&#xA;  }&#xD;&#xA;}</code></pre>&#xD;&#xA;</div>&#xD;&#xA;</div>&#xD;&#xA;</p>&#xA;&#xA;<p>and --</p>&#xA;&#xA;<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">&#xD;&#xA;<div class=""snippet-code"">&#xD;&#xA;<pre class=""snippet-code-js lang-js prettyprint-override""><code>import {getData} from 'path/to/module';&#xD;&#xA;export default SomeComponent extends React.Component {&#xD;&#xA;  constructor(props) {&#xD;&#xA;    super(props);&#xD;&#xA;    this.state = {&#xD;&#xA;      data: getData()&#xD;&#xA;    }&#xD;&#xA;  }&#xD;&#xA;  render(){&#xD;&#xA;    const {data} = this.state;&#xD;&#xA;    return (&#xD;&#xA;      &lt;div&gt;{data}&lt;/div&gt;&#xD;&#xA;    );&#xD;&#xA;  }&#xD;&#xA;}</code></pre>&#xD;&#xA;</div>&#xD;&#xA;</div>&#xD;&#xA;</p>&#xA;&#xA;<p>Then in main app, i initiate the module once ---</p>&#xA;&#xA;<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">&#xD;&#xA;<div class=""snippet-code"">&#xD;&#xA;<pre class=""snippet-code-js lang-js prettyprint-override""><code>import {initiate} from ""path/to/external/module"";&#xD;&#xA;&#xD;&#xA;export default ParentComponent extends React.Component {&#xD;&#xA;  constructor(props){&#xD;&#xA;    super(props);&#xD;&#xA;    initiate(props.data);&#xD;&#xA;  }&#xD;&#xA;  render(){&#xD;&#xA;    return (&#xD;&#xA;      &lt;div&gt;{this.props.children}&lt;/div&gt;&#xD;&#xA;    );&#xD;&#xA;  }&#xD;&#xA;}</code></pre>&#xD;&#xA;</div>&#xD;&#xA;</div>&#xD;&#xA;</p>&#xA;&#xA;<p>and use the component i made in the module in children ---</p>&#xA;&#xA;<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">&#xD;&#xA;<div class=""snippet-code"">&#xD;&#xA;<pre class=""snippet-code-js lang-js prettyprint-override""><code>import SomeComponent from ""path/to/module/SomeComponent""&#xD;&#xA;  &#xD;&#xA;export default const ChildComponent = (props) =&gt; {&#xD;&#xA;  return (&#xD;&#xA;    &lt;div&gt;&#xD;&#xA;    Using Somecomponent With Data Initiated previously and processed by the module&#xD;&#xA;    &lt;SomeComponent /&gt;&#xD;&#xA;    &lt;/div&gt;&#xD;&#xA;  );&#xD;&#xA;} </code></pre>&#xD;&#xA;</div>&#xD;&#xA;</div>&#xD;&#xA;</p>&#xA;&#xA;<p>For processing that required serverside, i ended up making a node-express service for the module too. But I guess that isn't exactly a front-end microservice.</p>&#xA;"
46474871,46453447,8553862,2017-09-28T17:15:34,"<p>Seems to be a versioning problem as the alpha feature is supported for k8s version> 1.7 as mentioned here (<a href=""https://kubernetes.io/docs/admin/extensible-admission-controllers/#what-are-initializers"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/admin/extensible-admission-controllers/#what-are-initializers</a>). </p>&#xA;&#xA;<blockquote>&#xA;  <p>1.7 introduces two alpha features, Initializers and External Admission<br>&#xA;  Webhooks, that address these limitations. These features allow admission &#xA;  controllers to be developed out-of-tree and configured at runtime.</p>&#xA;</blockquote>&#xA;&#xA;<p>And it is possible to deploy a version of kubernetes >= 1.7.4 to Azure. Note sure about the deployed version using the portal. But if you use acs-egnine to generate the ARM template, it is possible to deploy a cluster with version 1.7.5. </p>&#xA;&#xA;<p>You can refer here for the procedures <a href=""https://github.com/Azure/acs-engine"" rel=""nofollow noreferrer"">https://github.com/Azure/acs-engine</a>. Basically it involves three steps. First, you should create the json file by referring to the clusterDefinition section. To use version 1.7.5, you should specify the attribute ""orchestratorRelaease"" to be ""1.7"" and also enable the RBAC by specifying the attribute ""enableRbac"" to be true. Second, use the acs engine (version >= 0.6.0) to parse the json file to ARM template (azuredeploy.json &amp; azuredeploy.parameters.json should be created). Lastly, use the command ""New-AzureRmResourceGroupDeployment"" in powershell to deploy the cluster to Azure. </p>&#xA;&#xA;<p>Hope this helps :) </p>&#xA;"
47319714,47319604,8948102,2017-11-16T00:40:53,"<p>Well, monolithic applications have everything in one file structure, as one application. Not necessarily in one file.</p>&#xA;&#xA;<p>There are many advantages that come with running micro services. But you are right, they still need to work.</p>&#xA;&#xA;<p>Often micro services consists of a suite of api’s that talk together, if you take a limited scope of Facebook for instance, they might have an api for posts, a different one for likes, another one for storage if pictures, authentication and so on. And a huge advantage is the ability to scale up and down depending on load. </p>&#xA;&#xA;<p>When developing, you have limited scopes so you don’t trash the whole codebase if one module is wonky. You can deploy new versions without depending on thousands of people (in facebooks case). You can have developers with different skill sets, it doesn’t really matter if you do it in java, php, python or so on. You can also choose the best technology for the task. It is easier to re-do pieces of the code, as it often is very easy to specify what a micro service shall do. </p>&#xA;&#xA;<p>With cloud, I am very certain that micro services will become the standard for all services that can be deployed in the cloud. Because the advantages are so great that businesses must open their eyes to it.</p>&#xA;"
38052377,38049334,1744774,2016-06-27T11:07:00,"<p>A Maven project is not in charge of managing the versions of its dependencies. It's the project that builds a dependency artifact that's responsible for its version.</p>&#xA;&#xA;<p>In your case you could create a (pseudo-)aggregator project that includes all your microservices as sub-modules:</p>&#xA;&#xA;<pre><code>&lt;modules&gt;&#xA;    &lt;module&gt;../ms-1&lt;/module&gt;&#xA;    &lt;module&gt;../ms-2&lt;/module&gt;&#xA;    &lt;module&gt;../ms-3&lt;/module&gt;&#xA;    &lt;module&gt;../...&lt;/module&gt;&#xA;    &lt;module&gt;../ms-15&lt;/module&gt;&#xA;&lt;/modules&gt;&#xA;</code></pre>&#xA;&#xA;<p>Assuming that your microservice projects are siblings to each other like:</p>&#xA;&#xA;<pre><code>+- (pseudo-)aggregator&#xA;|  +- pom.xml&#xA;+- ms-1&#xA;|  +- pom.xml&#xA;+- ms-2&#xA;|  +- pom.xml&#xA;+- ms-3&#xA;|  +- pom.xml&#xA;+- ms-...&#xA;|  +- pom.xml&#xA;+- ms-15&#xA;   +- pom.xml&#xA;</code></pre>&#xA;&#xA;<p>If they are not adapt the relative <code>&lt;module&gt;</code>s paths accordingly.</p>&#xA;&#xA;<p>Such you are asked for the release versions of all of the projects (default is current <code>&lt;version&gt;</code> minus <code>-SNAPSHOT</code>, to be confirmed by just <kbd>Enter</kbd>)  during a <code>.../(pseudo-)aggregator $ mvn</code><a href=""https://maven.apache.org/maven-release/maven-release-plugin/prepare-mojo.html"" rel=""nofollow noreferrer""><code>release:prepare</code></a>, which also updates your <code>&lt;dependencies&gt;/&lt;dependency&gt;/&lt;version&gt;</code>s accordingly.</p>&#xA;&#xA;<p>Or you can perform a <a href=""https://maven.apache.org/maven-release/maven-release-plugin/examples/non-interactive-release.html"" rel=""nofollow noreferrer"">non-interactive release</a> with <code>-B | --batch-mode</code> which uses the defaults without asking.</p>&#xA;&#xA;<p>And finally an amendment:</p>&#xA;&#xA;<p><code>release</code> is not a <em>phase</em> of <a href=""https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html#Lifecycle_Reference"" rel=""nofollow noreferrer"">Maven's build lifecycles</a>. It's the shortcut for the <a href=""http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.apache.maven.plugins%22%20AND%20a%3A%22maven-release-plugin%22"" rel=""nofollow noreferrer""><code>maven-release-plugin</code></a> that has various <a href=""https://maven.apache.org/maven-release/maven-release-plugin/index.html"" rel=""nofollow noreferrer""><em>goals</em></a>.</p>&#xA;"
43151224,43132158,7799943,2017-03-31T23:56:09,"<p>If you're using Mule runtime v3.8.x, and if the service is an HTTP/S listener, you can actually make it auto-discovered in the API Manager and have policies applied directly on it, even if the mule config is not generated using APIkit.</p>&#xA;&#xA;<p><a href=""https://docs.mulesoft.com/api-manager/api-auto-discovery"" rel=""nofollow noreferrer"">https://docs.mulesoft.com/api-manager/api-auto-discovery</a></p>&#xA;&#xA;<p>Choose the flow that you want the API Manager to manage and apply policies.</p>&#xA;&#xA;<p>Do note that you will need to have to right entitlement (API Gateway) in the Mule Runtime license and that it has the right Anypoint Platform Client ID/Secret pairs configured in the wrapper.conf. The IDs should be automatically configured if you've added the Mule Runtime server in the Anypoint Runtime Manager.</p>&#xA;"
40972366,40966759,2695,2016-12-05T10:44:20,"<p>TL;DR;  Your problem is that you're basically translating your data model into services. That is wrong, that's not how you model a microservice architecture. <strong>It is not about the data model, is about the functionality</strong>. (I'm basing this on the way you framed the question, you don't talk about functionality and responsibilities, you talk about relationships).</p>&#xA;&#xA;<p>I'm going to answer quickly the first part of your question as I think your problem is actually on your modeling.</p>&#xA;&#xA;<p><strong>About authentication and authorization</strong></p>&#xA;&#xA;<blockquote>&#xA;  <p>The question is: Where do I make these requests? Have the Gateway ask&#xA;  the Hotel Service before forwarding client request to Room Service, or&#xA;  let the Room Service ask the Hotel Service by itself. When to choose&#xA;  one over another ? What's the benefit ?</p>&#xA;</blockquote>&#xA;&#xA;<p>First of, on your model, the Room Service is the one with enough <strong>context</strong> to actually authorize the request. The gateway doesn't have (nor should it have) enough information to judge (the gateway should NOT understand anything about rooms or hotels or anything, its job is to forward requests, not to interpret them).</p>&#xA;&#xA;<p>Secondly, even though you can have the room service ask the hotel for authorization, it is better if the room service does it by himself or calling another service whose responsibility is to provide authorization (i.e. an authorization service).</p>&#xA;&#xA;<p>But most importantly, this microservice architecture doesn't make a lot of sense (as you've described it) and that is why the whole model is strange to work with.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p><strong>Why the model is wrong</strong></p>&#xA;&#xA;<p>The reason this modeling seems wrong is because it is.</p>&#xA;&#xA;<p>The problem with the term ""micro""service, is that people tend to focus on the ""micro"" part and forget about the ""service"" part. There're different views on what a microservice is, but a service is something that can be invoked on its own and provides a value that is shared across several clients of that service.</p>&#xA;&#xA;<p>Your Room Service makes no sense. <strong>You're basically translating your data model into services.</strong> A Hotel has rooms, so you define a Hotel Service and a Room Service. That is NOT what microservices are about... </p>&#xA;&#xA;<p>Without knowing your particular user requirements it is difficult to judge but my gut feeling is you probably do not need a microservice architecture in here. Just because that's the latest trend you don't need to solve every single problem with it.</p>&#xA;&#xA;<p>If your operations are stuff like ""Register a new room, add photos to a room, remove photos from a room, book a room, etc"", you're better off just having a backend service with a simple API that allows you to do all those simple kind of operations. Honestly, a Hotel Management System does not seem like the right kind of application to build using a microservice architecture. This feels more like a traditional MVC model to be honest.</p>&#xA;&#xA;<p>If I had to come up with a use case for a room microservice I'd say you may want to have a room service that is aware of ALL the rooms on all the hotels. Rooms can be registered by a hotel, edited and changed. Anyone can get a list of all the rooms available, filtered by date available, filtered by number of beds, etc.</p>&#xA;&#xA;<p>Note we now have two or three possible clients:&#xA; - Your frontrend for administering hotels.&#xA; - Your frontrend for seaching rooms.&#xA; - Someone's elses frontrend for your room service to search rooms??.</p>&#xA;&#xA;<p>Note also we've changed the system, from a Hotel Management System to a system that can be used to query different hotels for free rooms... useful, but a completely different kind of user needs.</p>&#xA;&#xA;<p>So now your service actually make sense... and then pieces will start failing into place.</p>&#xA;&#xA;<p>Because now you have anonymous users (or users from outside the system) it doesn't make sense to go to the hotel service anymore (after all a user doesn't need to administer a hotel any longer) so why would the hotel service know?.</p>&#xA;&#xA;<p>Now, how are you going to handle the users of your system? Would there be different users for each microservice? Or is there going to be a single user that is shared across all microservices? Probably the latter, so that hints at another service for authentication (or you can use oauth2 if that fits your model which is exactly that, a service that authenticates people).</p>&#xA;&#xA;<p>How are you going to manage your permissions (your authorization), do you want a central configuration for authorization or would each microservice have its own config? If it is the former, then you probably want another service that provides authorization to each microservice.</p>&#xA;"
50280545,50280470,2695,2018-05-10T19:52:19,"<p>Front-end A should call Microservice B directly. As in your solution 2.</p>&#xA;&#xA;<p>Simplifying, microservice architectures are supposed to be composed of small services that provide single functions in a self contained and self manage way.</p>&#xA;&#xA;<p>By having your solution 1, you break that principle.</p>&#xA;&#xA;<p>MicroserviceA does not need to call Microservice B to perform its function. Frontend A needs it. Keep microservices simple and focused, otherwise you're not doing microservice architecture, you're doing something else.</p>&#xA;&#xA;<p>The best way to approach a microservice architecture is to approach each microservice functionality in isolation, as if it was the only microservice that exists and then add dependencies (calls to other microservices) on demand, don't consider the needs of the clients of your microservice, think about what that particular service offers.</p>&#xA;"
48341678,48341375,1919879,2018-01-19T12:57:06,"<p>I believe best way is to place all authorization logic into library that is shared across microservices via private artifact repository. This library can configuration beans that can be component scanned when this JAR library is on classpath. component scan might be automatic, if you structure your packages wisely. Or you may want to make that scanning or including this shared logic explicit.</p>&#xA;&#xA;<p>This way you might remove OAuth authorization logic duplication, which would otherwise be present. </p>&#xA;&#xA;<p>Of course I am assuming that your microservices are Spring/Java based. If your microservices would vary programming languages, your authorization logic will be duplicated.</p>&#xA;&#xA;<p>BTW, via shared library, you can cover also other cross cutting concerns of your Java/Spring microservices (monitoring, error hangling...). I've seen this working very well for teams working on 20+ microservices.</p>&#xA;&#xA;<p><strong>EDIT:</strong> Original response container workd ""authentication"", but I meant ""authorization"". Sorry for that confusion. ""Authentication"" should be in OAuth performed by separate dedicated service. This is not cross cutting concern, because there will be only one service accepting crendentials and returning back Oauth token.  The best solution is to use third party OAuth provider, because you don't need to deal with credentials -> less security concerns for you. ""Authorization"" is cross cutting concern.</p>&#xA;"
32530833,32529742,1919879,2015-09-11T19:28:10,"<p>This situation can be handled only with JTA global/distributed transactions. JTA is part of Java EE standard and can have various implementors. Atomikos is often tool of choice.</p>&#xA;&#xA;<p><a href=""http://www.javaworld.com/article/2077963/open-source-tools/distributed-transactions-in-spring--with-and-without-xa.html"" rel=""nofollow"">Here is good writeup from Dave Syer (Spring ecosystem contributor).</a> It contain also working examples. It's little bit outdated, but still relevant. You can apply some more modern Spring abstractions on top of his examples.</p>&#xA;&#xA;<p>I created few <a href=""https://github.com/lkrnac/book-eiws-code-samples/tree/master/07-jta"" rel=""nofollow"">GitHub examples of JTA transactions</a> for my book. Notice that there are errors simulated and transaction is spread across JMS and JDBC datasources.</p>&#xA;&#xA;<p><strong>But also bear in mind that JTA transactions across various data sources are slow, because of 2-phased commit algorithm involved.</strong> So often people try to avoid them and rather deal with inconsistencies somehow pragmatically.</p>&#xA;"
38890401,38889466,1919879,2016-08-11T07:51:07,"<p>Extract that filter into separate JAR that will be used by each microservice. That JAR will be separate project versioned and deployed into your artifact repository independently. </p>&#xA;&#xA;<p>In fact in microservice environment, you will have much more ""common"" beans/functionality that should be shared across services. Therefore such common JAR is necessary in microservice environment. </p>&#xA;"
35228303,35223505,1919879,2016-02-05T15:55:28,"<p>Place common logic into separate thin JAR, place it in your artifact repository and version separately from services. This common library/ies will live it's life as standalone projects (similar to other JAR dependencies you use in your project).</p>&#xA;&#xA;<p>Each service will use this JAR/s as normal dependency.</p>&#xA;&#xA;<p>I was working in team, where we used this approach for:</p>&#xA;&#xA;<ol>&#xA;<li>Authentication code</li>&#xA;<li>AOP for logging</li>&#xA;<li>Some common validation code</li>&#xA;<li>Some common domain objects</li>&#xA;<li>Exception handling</li>&#xA;</ol>&#xA;"
38238643,31190685,1734279,2016-07-07T06:16:39,"<p>Microservices are new architectural style to develop services which does one thing and does that thing really well.  Microservices apply <strong>SRP</strong> at service layer. So each service exists for a unique reason. </p>&#xA;&#xA;<p>And I think, it makes sense to understand it from perspective of SOA which has been there for a while. Adrian Cockcroft at Netflix, describes Microservices as <strong>fine grained SOA</strong>. </p>&#xA;&#xA;<p>I have found few really interesting links which explains microservices quite well and in detail. Here are some of them:</p>&#xA;&#xA;<p><a href=""http://martinfowler.com/articles/microservices.html"" rel=""nofollow"">http://martinfowler.com/articles/microservices.html</a></p>&#xA;&#xA;<p><a href=""http://microservices.io/patterns/microservices.html"" rel=""nofollow"">http://microservices.io/patterns/microservices.html</a></p>&#xA;&#xA;<p>I have also blogged about it over <a href=""http://geekrai.blogspot.in/2016/07/understanding-microservices.html"" rel=""nofollow"">here</a>.</p>&#xA;"
38239273,29591967,1734279,2016-07-07T06:58:44,"<p>I would start with Your last question - <strong>It's only related to webservices?</strong>&#xA;That's debatable. I would say, NO. It's related to webservice (but not only to it.)</p>&#xA;&#xA;<p>Martin fowler describes microservices as a <strong>small subset of SOA</strong>, after all <strong>microservices are services</strong>, and SOA is a very generic and broad term. </p>&#xA;&#xA;<p>Below are <em>some</em> of the important aspects of Microservices:</p>&#xA;&#xA;<ol>&#xA;<li>Each service (or a set of few) should have it's own data store. </li>&#xA;<li>Services are organized around the business needs or functionality.</li>&#xA;<li>Each service is independent so they can be implemented in any language. Leads to polyglot programming culture in team. </li>&#xA;<li>Service can take request from client or from other services as well.</li>&#xA;<li>They are usually event driven and asynchronous so scaling becomes easier. </li>&#xA;<li>Services are dumb as they only do one thing (but they should be self sufficient to monitor themselves)</li>&#xA;<li>They can be helpful in continuous deployment or delivery as implement to deploy cycle is really small. </li>&#xA;<li><p>They are very small so there is not much of network overhead in deploying them. So they can be deployed across a cluster of nodes in few minutes. </p>&#xA;&#xA;<p><em>Also, I want to stress that, above are NOT only true about microservices. Companies like google, netflix, and Amazon have been doing similar thing even before the term was coined</em>. </p></li>&#xA;</ol>&#xA;"
50849267,50841501,6453895,2018-06-14T03:40:49,"<p>For your above use-case , if you are going to use kafka for inter microservices communication , there is no need for any spring-cloud-netflix component. You can publish to a topic and have consumers in microservices consume from the topic. Load balancing will automatically happen depending upon number of partitions in the topic. </p>&#xA;&#xA;<p>For example , lets consider your topic name is test and it has 4 partitions . If you have deployed 4 microservices with each consisting of a single Kafka Consumer consuming from topic test then each consumer will consume from 1 partition on the topic test. So , load balancing will automatically happen.</p>&#xA;&#xA;<p>The spring-cloud-netflix components are mainly meant for inter-microservices communication when there is network calls involved between microservices . </p>&#xA;&#xA;<p>For example -&#xA;Lets consider two applications A and B . You have 4 instances of Application A deployed and 4 instances of Application B deployed . The entry-point for the consumers of  your system is Application A . Here , you will use an Api Gateway like Zuul . As for Eureka , you will have all your instances deployed register in it. When a request comes into Zuul which needs to be forwarded to Application A . All the instances of Application A will be fetched from Eureka (4 in our case) and then being provided to a load balancer Ribbon who will chose which url should be called . Zuul will then forward the request to that instance of Application A.</p>&#xA;&#xA;<p>Some links you should look at are-</p>&#xA;&#xA;<p><a href=""https://github.com/Netflix/zuul/wiki/How-it-Works"" rel=""nofollow noreferrer"">https://github.com/Netflix/zuul/wiki/How-it-Works</a></p>&#xA;&#xA;<p><a href=""https://github.com/Netflix/ribbon/wiki/Working-with-load-balancers"" rel=""nofollow noreferrer"">https://github.com/Netflix/ribbon/wiki/Working-with-load-balancers</a></p>&#xA;&#xA;<p><a href=""https://blog.asarkar.org/technical/netflix-eureka/"" rel=""nofollow noreferrer"">https://blog.asarkar.org/technical/netflix-eureka/</a></p>&#xA;"
43777757,43763418,7961735,2017-05-04T08:33:47,"<p>for your question, no there is no out of the box way to update Certificate for Service Fabric on-premise cluster.&#xA;I opened a ticket with Microsoft for this issue: 117011115158708 and they replied it will be fixed on version 5.5&#xA;this version is out now and the problem still not fixed, they should get back to me with an answer about this issue, i will try to keep this post updated.</p>&#xA;"
40586032,40585401,42119,2016-11-14T10:00:53,"<p>This doesn't seem to be the correct way to use routing keys, if I understand you correctly. Routing keys tend to be used to define an operation, eg 'do-this' or 'do-that' - I wouldn't expect them to contain keys, unless those keys were a limited set that defined how the system operates. The Ids that you mention (they seem to be correlation Ids, so that you can match a received response to an outbound request, asynchronously) would be contained in the body of the message. So, you have the following setup:</p>&#xA;&#xA;<ol>&#xA;<li>Service B -> send message to exchange with routing key 'process-data'. The message contains the Id in its body.</li>&#xA;<li>Service A listens to messages on Queue A, which is is bound to the exchange with binding key 'process-data'. It then dequeue's Service B's message, whose body contains the Id</li>&#xA;<li>Service A performs processing</li>&#xA;<li>Service A -> send message to exchange with routing key 'data-processed'. The message contains the original Id in its body.</li>&#xA;<li>Service B listens to messages on Queue B, which is is bound to the exchange with binding key 'data-processed'. It then dequeue's Service A's message, whose body contains the Id</li>&#xA;</ol>&#xA;"
49827447,49708997,8301447,2018-04-14T02:58:27,"<p>You can try splitting an existing Monolithic application to gain perspective on microservice architecture.</p>&#xA;&#xA;<p>I wrote <a href=""https://medium.com/greedygame-media/how-we-broke-up-our-monolithic-django-service-into-microservices-8ad6ff4db9d4"" rel=""nofollow noreferrer"">this</a> article, which talks about splitting a Django App into microservices. Hope it helps.</p>&#xA;"
44433117,44432985,5168007,2017-06-08T10:24:12,<p>Have you opened the appropriate ports for the service in Azure?</p>&#xA;
45883282,39967784,5627271,2017-08-25T14:09:03,"<p>Regarding your question about the ""domain"", to my view, and I believe it is a main characteristic for Microservices: A Microservice should manage its own functional domain and data model. </p>&#xA;&#xA;<p>Let's say you have a products catalog application within your company. You probably would not like to have many other applications hitting the catalog persistence layer and abstracting (again) the catalog model as it would harden the model refactoring / evolution. Probably it would cause  concurrency issues between these applications preventing the catalog application to be scaled &#xA;Instead you would probably prefer to maintain a single catalog application, which would expose web service APIs (such as REST endpoints) consumed by other applications.</p>&#xA;&#xA;<p>I've read this comment in this <a href=""https://stackoverflow.com/questions/25501098/difference-between-microservices-architecture-and-soa"">other related question</a> ""Microservices = SOA - ESB"". Indeed, ESB are incompatible with this microservices characteristic: ""Smart endpoints and dumb points"" which means that when a microservice needs another one as a dependency, it should use it directly without  any routing logic / components handling the pipe.</p>&#xA;&#xA;<p>Finally, you could take a look to this <a href=""https://gist.github.com/remibantos/0ace22c8256dfacf19e7"" rel=""nofollow noreferrer"">cheat sheet</a> based on a Martin Fowler introduction to Microservices video.</p>&#xA;"
29592288,29591967,1373063,2015-04-12T17:23:30,"<p>My own definition: </p>&#xA;&#xA;<p><em>A microservice is a stand-alone, decoupled component that handles a single business concern, and is consumable from other services.</em></p>&#xA;&#xA;<p>Others might agree or disagree, and there is a lot of interesting discussion on this topic that make it a great study point for software engineers.</p>&#xA;&#xA;<p><strong>From a technical standpoint:</strong>&#xA;You can create microservices in almost any technology: Java EE, Java + Spring, Python, Rails, Grails, Node.js and so forth.  From what I have seen, it seems most commonly applied in the domain of web apps and back-end service-oriented ecosystems.  In the article you reference, the NetFlix model is a very interesting thing to study, because you can see all the elements of a microservice architecture in depth: service discovery, circuit-breaking, monitoring, dynamic configuration, and so on.</p>&#xA;&#xA;<p>Some things you might want to check out, if you are Java-oriented:</p>&#xA;&#xA;<p>Spring Cloud allows you to use some of these same NetFlix components with a minimum of hand-coding: <a href=""http://cloud.spring.io/spring-cloud-netflix/"" rel=""nofollow"">http://cloud.spring.io/spring-cloud-netflix/</a></p>&#xA;&#xA;<p>An actual operational example on github (not mine, but I have used it in my own learning on the topic): <a href=""https://github.com/ewolff/microservice"" rel=""nofollow"">https://github.com/ewolff/microservice</a></p>&#xA;&#xA;<p><strong>From a conceptual point of view</strong>, your question hints at a notorious microservice design dilemma.   There is not necessarily a ""correct"" level of granularity for a microservice.  The idea is to choose a level of granularity that has meaning within your business domain.  If you implement microservices at a very low level of granularity, (e.g. the CRUD level), then you will almost certainly end up with very chatty services and you will probably have to build more meaningful composite services over top.  If you choose too high a level of granularity, you could end up with a more monolithic application which may require refactoring into microservice-sized pieces later.</p>&#xA;"
25965771,25965275,1177991,2014-09-22T02:00:59,"<p>We switched from Finagle to <a href=""http://spray.io"" rel=""nofollow"">Spray</a>. It's minimal framework to write RESTful micro services, much smaller than Play. It's literally takes less than 10 lines of spray-based code to run new service.</p>&#xA;&#xA;<p>If you don't plan to build web UI with play I don't see why to use this heavy framework for building just REST services.</p>&#xA;"
45570252,45510905,5314440,2017-08-08T13:52:50,"<p>You need to write a filter. A zuul <strong>pre</strong> filter is what you need. You can access your auth server within the filter and if the token is invalid you don't call your microservice and return a response immediately. If it is valid you let the request go down to micro services.</p>&#xA;&#xA;<p>An example filter class:</p>&#xA;&#xA;<pre><code>public class AuthFilter extends ZuulFilter {&#xA;&#xA;    @Autowired&#xA;    RestTemplate restTemplate;&#xA;&#xA;    @Override&#xA;    public String filterType() {&#xA;        return ""pre"";&#xA;    }&#xA;&#xA;    @Override&#xA;    public int filterOrder() {&#xA;        return 1;&#xA;    }&#xA;&#xA;    @Override&#xA;    public boolean shouldFilter() {&#xA;        return true;&#xA;    }&#xA;&#xA;    @Override&#xA;    public Object run() {&#xA;        RequestContext ctx = RequestContext.getCurrentContext();&#xA;        //get your token from request context and send it to auth service via rest template&#xA;        boolean validToken = restTemplate.exchange(or getForObject or other methods of restTemplate which you find suitable for method and return type of your auth service controller method)&#xA;        if(!validToken) {&#xA;            ctx.setSendZuulResponse(false); //This makes request not forwarding to micro services&#xA;            ctx.setResponseStatusCode(HttpStatus.UNAUTHORIZED.value());&#xA;            ValidationResponse validationResponse = new ValidationResponse();&#xA;            validationResponse.setSuccess(false);&#xA;            validationResponse.setMessage(""Invalid Access..."");&#xA;            ObjectMapper mapper = new ObjectMapper();&#xA;            String responseBody = mapper.writeValueAsString(validationResponse);&#xA;            ctx.setResponseBody(validationResponse);&#xA;            ctx.getResponse().setContentType(""application/json"");&#xA;            //If you want to do any thing else like logging etc, you can do it.&#xA;        }&#xA;        return null;&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;"
43739159,42642688,5314440,2017-05-02T13:28:49,<p>As far as I understand you want zuul just as an api gateway. Write a zuulFilter and check the incoming request's token via authentication service and then pass the request actual microservice(if token is provided and valid). Microservice can check the token if necessary roles/permissions are in it. </p>&#xA;&#xA;<p>Short version is; if you want authorization in each microservice zuul has not much of a role in this case(just a simple api gateway.)</p>&#xA;
44115144,44114755,5314440,2017-05-22T14:13:20,"<p>It is an architecture(microservices) problem. Spring boot or netflix-oss do not offer a direct solution. You have to implement your own solution. Check with <a href=""http://microservices.io/patterns/data/event-driven-architecture.html"" rel=""nofollow noreferrer"">event driven architecture</a>. It can give you some ideas.</p>&#xA;"
46817875,46813736,613256,2017-10-18T19:33:45,"<p>It look like you need a distributed transactions in your system&#xA;<a href=""https://msdn.microsoft.com/en-us/library/windows/desktop/ms681205(v=vs.85).aspx"" rel=""nofollow noreferrer"">https://msdn.microsoft.com/en-us/library/windows/desktop/ms681205(v=vs.85).aspx</a></p>&#xA;&#xA;<p>Also there is a nice book devoted to microservices. It includes distributed transactions and other patters used in microservice bases apps.&#xA;<a href=""http://shop.oreilly.com/product/0636920033158.do"" rel=""nofollow noreferrer"">http://shop.oreilly.com/product/0636920033158.do</a></p>&#xA;"
47149088,47129599,4682632,2017-11-07T02:33:38,"<p>Yes, it does transaction tracking for every intra-component call across languages. Without code changes. When there is a slow transaction detail down to code level will show up in snapshots, this is what you'd use for diagnostics. Depending on your language you can also configure data collectors which do runtime instrumentation of custom data from the code. These show up on every call, and you can turn them into metrics too. Once again no code changes.</p>&#xA;"
42570606,42487685,4032392,2017-03-03T04:22:19,"<p>You will have to use <code>vm</code> module to achieve this. More technical info here <a href=""https://nodejs.org/api/vm.html"" rel=""noreferrer"">https://nodejs.org/api/vm.html</a>. Let me explain how you can use this:</p>&#xA;&#xA;<ol>&#xA;<li>You can use the API <code>vm.script</code> to create compiled js code from the code which you want run later. See the description from official documentation </li>&#xA;</ol>&#xA;&#xA;<blockquote>&#xA;  <p>Creating a new vm.Script object compiles code but does not run it. The&#xA;  compiled vm.Script can be run later multiple times. It is important to&#xA;  note that the code is not bound to any global object; rather, it is&#xA;  bound before each run, just for that run.</p>&#xA;</blockquote>&#xA;&#xA;<ol start=""2"">&#xA;<li>Now when you want to insert or run this code, you can use <code>script.runInContext</code> API. </li>&#xA;</ol>&#xA;&#xA;<p>Another good example from their official documentation:</p>&#xA;&#xA;<pre><code>'use strict';&#xA;const vm = require('vm');&#xA;&#xA;let code =&#xA;`(function(require) {&#xA;&#xA;   const http = require('http');&#xA;&#xA;   http.createServer( (request, response) =&gt; {&#xA;     response.writeHead(200, {'Content-Type': 'text/plain'});&#xA;     response.end('Hello World\\n');&#xA;   }).listen(8124);&#xA;&#xA;   console.log('Server running at http://127.0.0.1:8124/');&#xA; })`;&#xA;&#xA; vm.runInThisContext(code)(require);&#xA;</code></pre>&#xA;&#xA;<p>Another example of using js file directly:</p>&#xA;&#xA;<pre><code>var app = fs.readFileSync(__dirname + '/' + 'app.js');&#xA;vm.runInThisContext(app);&#xA;</code></pre>&#xA;&#xA;<p>You can use this approach for the conditional code which you want to insert. </p>&#xA;"
41085582,41082938,1757320,2016-12-11T11:22:10,"<p>This is typically done via <a href=""https://en.wikipedia.org/wiki/Vector_clock"" rel=""nofollow noreferrer"">vector clocks</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p>A vector clock is an algorithm for generating a partial ordering of events in a distributed system and detecting causality violations. </p>&#xA;</blockquote>&#xA;"
50066573,36573857,7731848,2018-04-27T16:20:30,"<p>Reliable collection in SF can be used for concurrent transactions with consistency is guaranteed by Service fabric. You will not face any issue due to partition if you are using the same dictiornary across your partitions. Problem becomes complex when you need to update two disctionries simulataneously and you have depended data in each dictionary. In that case you can use patterns like ""Saga"" pattern or ""Twophase commit"" patterns.</p>&#xA;&#xA;<p>Please refer <a href=""https://docs.microsoft.com/en-gb/azure/service-fabric/service-fabric-reliable-services-reliable-collections#persistence-model"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-gb/azure/service-fabric/service-fabric-reliable-services-reliable-collections#persistence-model</a> for more info:&#xA;Reliable Collections provide strong consistency guarantees out of the box to make reasoning about application state easier. Strong consistency is achieved by ensuring transaction commits finish only after the entire transaction has been logged on a majority quorum of replicas, including the primary. To achieve weaker consistency, applications can acknowledge back to the client/requester before the asynchronous commit returns.</p>&#xA;&#xA;<p>Reliable Dictionary: Represents a replicated, transactional, and asynchronous collection of key/value pairs. Similar to ConcurrentDictionary, both the key and the value can be of any type.</p>&#xA;"
50128736,50128046,8323209,2018-05-02T06:54:58,"<p>Microservice architecture is simple. Here we divide each task into separate services(like Spring-boot application).&#xA;Example in every application there will be login function,registration function so on..each of these will a separate services in micro-service architecture.</p>&#xA;&#xA;<p>1.You can store that in database, since in feature if you want add more values it is easy to add.&#xA;You can maintain separate or single db. Single db with separate collections or table for each microservices.</p>&#xA;&#xA;<ol start=""2"">&#xA;<li>Validation means you are asking about who can use which microservice(Role based access)???</li>&#xA;</ol>&#xA;&#xA;<p>3.I think you have to use local db.</p>&#xA;"
42219938,39690815,2758537,2017-02-14T07:10:59,<p>This error Comes When there is any Whitespace or Invalid character in command.Remove Whitespace or invalid character from command  </p>&#xA;
43888586,43661844,993417,2017-05-10T09:31:34,"<p>In my opinion, it would be better to use something like <a href=""http://flywaydb.org/"" rel=""nofollow noreferrer"">Flyway</a> or <a href=""http://www.liquibase.org/"" rel=""nofollow noreferrer"">Liquibase</a>, which are integrated really well in Spring Boot. You can find more information <a href=""https://docs.spring.io/spring-boot/docs/current/reference/html/howto-database-initialization.html#howto-use-a-higher-level-database-migration-tool"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;&#xA;<p>I prefer Liquibase, since it uses a higher level format to describe your database migrations, allowing you to switch databases quite easily. This way, you can also use different databases per environment, for example:</p>&#xA;&#xA;<ul>&#xA;<li>HSQLDB during local development</li>&#xA;<li>MySQL in DEV and TEST</li>&#xA;<li>Oracle in Production</li>&#xA;</ul>&#xA;&#xA;<p>It's also possible to export your current database schema from an existing database to have an initial version in Flyway or Liquibase, this will give you a good baseline for your scripts.</p>&#xA;"
27595184,26866479,1135241,2014-12-22T00:13:06,"<p>You trouble is how model your microservices.</p>&#xA;&#xA;<p>In term of microservices the second approach is most appropriate, which expose its logic through API.</p>&#xA;&#xA;<p>Always when you model your microservices keep in mind the follow facts.</p>&#xA;&#xA;<ul>&#xA;<li><p><strong>Loose Coupling</strong>: When services are loosely coupled, a change to one service should not require a change to another. The whole point of this Microservice thing is being able to make a change to one service, and deploy it, independent of a need to change any other part of the system. This is really quite important.</p></li>&#xA;<li><p><strong>Strong Cohesion</strong>: We want related behaviour to sit together, and unrelated behaviour to sit elsewhere. Why? Well, if we want to change behaviour, we want to be able to change it in one place, and release that change as soon as possible. </p></li>&#xA;</ul>&#xA;"
41319161,40586946,1344649,2016-12-25T06:36:35,"<p>I had the similar problem today when I attempted to follow the fabric8 get started instructions here: <a href=""https://fabric8.io/guide/getStarted/gofabric8.html"" rel=""nofollow noreferrer"">https://fabric8.io/guide/getStarted/gofabric8.html</a>.&#xA;I used:&#xA;<code>gofabric8 start --minishift</code> and received this error:</p>&#xA;&#xA;<p><code>DSKTP-000003:~ usr$ gofabric8 start --minishift&#xA;fabric8 recommends OSX users use the xhyve driver&#xA;xhyve driver already installed&#xA;Downloading https://github.com/jimmidyson/minishift/releases/download/v1.0.0-beta.1/minishift-darwin-amd64...&#xA;Downloaded /Users/brent.fisher/.fabric8/bin/minishift&#xA;kubectl is already available on your PATH&#xA;Downloading https://github.com/openshift/origin/releases/download/v1.3.1/openshift-origin-client-tools-v1.3.1-dad658de7465ba8a234a4fb40b5b446a45a4cee1-mac.zip...&#xA;Unable to unzip /Users/brent.fisher/.fabric8/bin/oc.zip zip: not a valid zip fileUnable to download client zip: not a valid zip file&#xA;using the executable /Users/brent.fisher/.fabric8/bin/minishift&#xA;Unable to get status fork/exec /Users/brent.fisher/.fabric8/bin/minishift: exec format errorDSKTP-000003:~ usr$</code></p>&#xA;&#xA;<p>I am using gofabric8 version 0.4.112:</p>&#xA;&#xA;<p><code>gofabric8 version&#xA;gofabric8, version 0.4.112 (branch: 'master', revision: '50d5d75')&#xA;  build date:       '20161129-10:39:49'&#xA;  go version:       '1.7.1'&#xA;</code>&#xA;It seems that the minishift option tries to download a version of openshift client (oc) that no longer exists [1.3.1] at that url.  I was able to get around the error by manually downloading open shift from here: <a href=""https://github.com/openshift/origin/releases/tag/v1.3.2"" rel=""nofollow noreferrer"">https://github.com/openshift/origin/releases/tag/v1.3.2</a>&#xA;and extracting it and renaming the extracted executable to <code>oc</code>.</p>&#xA;"
31380636,28767707,2528649,2015-07-13T10:14:01,"<p>This time I've read your question from different perspective, so here is a 'different opinion'. I know it may be too late but hope it helps with further development. </p>&#xA;&#xA;<p>It looks like <code>shared state</code> is a result of wrong decoupling. In 'right' microservice architecture all microservices have to be isolated functionally rather than logically. I mean all three <code>user profile information (A), user permissions (B), user groups (C)</code> look functionally the same and more or less functionally coherent. They seem to be a single <code>user microservice</code> with a coherent storage. I don't see here any reasons of decoupling them (or at least you haven't told about them).</p>&#xA;&#xA;<p>So the real problem is related to <em>microservice isolation</em>. Ideally each microservice can live as <strong>complete standalone product</strong> and deliver a well defined <strong>business value</strong>. When elaborating system architecture we break up it into tiny logical units (A, B, C, etc in your case, or even smaller) and then define functionally coherent subgroups. I can't tell you exact rules of how to do that, perhaps some examples. Complex communication/dependencies between the units, many common terms in their ubiquitous languages so it looks like such units belong to the same functional group and thus to microservice.</p>&#xA;&#xA;<p>So from your example, since there is a single storage you have only way of managing its consistency as you did.</p>&#xA;&#xA;<p>BTW I wonder what actual way you've solved your problem? Also if you like my idea fell free to accept it.</p>&#xA;"
50968693,50965591,1223817,2018-06-21T12:33:14,"<p>The problem is that in the instance object I didn't write instanceId (wasn't mentioned anywhere). I found this solution crawling the code and than in another project where there was also this field</p>&#xA;&#xA;<pre><code>instance: {&#xA;        app: 'node-microservice',&#xA;        instanceId: 'nodemicroservice',&#xA;        hostName: 'localhost',&#xA;        ipAddr: '127.0.0.1',&#xA;        port:  {&#xA;            '$': 3001,&#xA;            '@enabled': 'true',&#xA;        },&#xA;        vipAddress: 'nodemicroservice',&#xA;        statusPageUrl: 'http://localhost:3001/info',&#xA;        dataCenterInfo:  {&#xA;            '@class': 'com.netflix.appinfo.InstanceInfo$DefaultDataCenterInfo',&#xA;            name: 'MyOwn',&#xA;        },&#xA;        registerWithEureka: true,&#xA;        fetchRegistry: true&#xA;    },&#xA;</code></pre>&#xA;"
43130626,37715875,6032010,2017-03-31T00:46:14,"<p><a href=""https://sparktutorials.github.io/2015/07/30/spark-testing-unit.html"" rel=""nofollow noreferrer"">This</a> might help, and it's from the official spark-java website so it might be considered the <em>right method</em>, or at least the recommended one.</p>&#xA;"
36287423,36285239,15754,2016-03-29T14:35:57,"<p>The simple approach is to create a <strong>token</strong> that the client can use to check the status of the request at a later time. You will need to add a GetStatus method to the public API to return the status when requested.</p>&#xA;&#xA;<p>Create a GUID when the initial request is received, return this <strong>token</strong> to the calling client. This will be their acknowledgement that the request was received. Persist the <strong>token</strong> and record it with the request's current status, which will initially be Received.  Call the Validation API with the request and include the <strong>token</strong>.  </p>&#xA;&#xA;<p>When the Validation API has completed its work, it will update the status of the request.  If there is a validation issue, it will update the status with an appropriate error message, otherwise it will update the status to Validated and then call the Template API, passing along the request and the <strong>token</strong>.  </p>&#xA;&#xA;<p>Repeat the above with the Template API and Sending API.  When the Sending API has completed its work -- it should simply update the request's status to Complete.</p>&#xA;&#xA;<p>The client can call the GetStatus method on the API at any time simply by supplying the <strong>token</strong> returned by their initial request.  The method should simply return the request's status or a Not Found status if a non-existent GUID has been supplied.</p>&#xA;&#xA;<p>The advantages of this is that you don't have to get into callbacks and other craziness and the calling client only needs to worry about 2 things: making a request and checking its status. The client can be as concerned or unconcerned about the status as it wants to be. It also lets you add further services in the chain without having to rewrite the external interface.</p>&#xA;&#xA;<p>The nuances involve how and how long to persist the request statuses. Which really depend on system demand and available resources. Could be a database, a cache or some combination. </p>&#xA;"
44894255,44893574,2702730,2017-07-03T21:48:19,"<p>It is not directly possible (see <a href=""https://github.com/IdentityServer/IdentityServer3/issues/1918"" rel=""nofollow noreferrer"">this issue</a>)</p>&#xA;&#xA;<p>But you probably need to rethink your architecture. Assuming the business requirement (Business requirement is that user logged in to any application from group 1 can use any app from this group but can't use app or api from group 2.) is non negotiable, what you are trying to achieve may be dangerous if these microservices are handling something very particular to one or the other group (ie, I am talking about multitenancy here)</p>&#xA;&#xA;<p>The right approach in your case would be to have separate instances of microservices, dedicated to different groups, ideally running in their own containers or VMs</p>&#xA;"
45878440,45853546,2702730,2017-08-25T09:30:04,"<p>Global state is <strong>BAD</strong> for microservice architecture. </p>&#xA;&#xA;<p>To solve your problem, both Service A and B should own their states (is_running) in its own database which is <strong>NOT</strong> shared with any other service.</p>&#xA;&#xA;<p>Then expose that state via a simple API <code>/serviceA/state</code> and <code>/serviceB/state</code>. These endpoints should return the state (via JSON e.g.). </p>&#xA;&#xA;<p>Each Service then should call the API of the other service(s) to check the state of that other service.</p>&#xA;"
42344302,42344020,2833802,2017-02-20T12:13:14,"<p>This is called an <a href=""http://microservices.io/patterns/apigateway.html"" rel=""noreferrer"">API Gateway pattern</a>.</p>&#xA;&#xA;<blockquote>&#xA;  <p>API gateway is the single entry point for all clients. The API gateway handles requests in one of two ways. Some requests are simply proxied/routed to the appropriate service. It handles other requests by fanning out to multiple services.</p>&#xA;</blockquote>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/At93L.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/At93L.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>As the starting point, you may look into <a href=""https://github.com/aspnet/Proxy"" rel=""noreferrer"">Proxy Middleware</a> for ASP.NET Core.</p>&#xA;"
45121183,45111662,2833802,2017-07-15T17:55:15,"<p>Consider to create an additional microservice, that encapsulates all logic related to user details:</p>&#xA;&#xA;<ul>&#xA;<li>only this service should have access to the users table and provide endpoints to read/modify data</li>&#xA;<li>other services should do requests to that service and know nothing about users table</li>&#xA;</ul>&#xA;&#xA;<hr>&#xA;&#xA;<p>I have experience mostly with REST-based microservices (and so this part of my answer is mostly opinion-based) and here in most cases, we use direct HTTP requests to service when need to read/modify data. And when we want that service notifies different components about data modifications, we use queue-based message communication (publish-subscribe pattern):</p>&#xA;&#xA;<ul>&#xA;<li>service, that is the data owner, sends the message about modification event to queue;</li>&#xA;<li>other services subscribe to that queue and do actions when a new message arrives;</li>&#xA;<li>as you see, communication goes asynchronously. </li>&#xA;</ul>&#xA;&#xA;<p>One of the benefits of message queue over simple message broadcasting is that it automatically supports mechanism with retry (and delay retry upon failure).</p>&#xA;"
46083772,46069921,2833802,2017-09-06T20:24:13,"<p>Yes, it is a specific case of API Gateway. For me <a href=""http://samnewman.io/patterns/architectural/bff/#comment-2923121019"" rel=""nofollow noreferrer"">this comment</a> was helpful for understanding. It says you may think about the following cases when we are talking about API Gateway - Client relationships:</p>&#xA;&#xA;<ul>&#xA;<li>A single API gateway providing a single API for all clients.</li>&#xA;<li>A single API gateway provides an API for each kind of client.</li>&#xA;<li>A per-client API gateway providing each client with an API. This is the BFF pattern.</li>&#xA;</ul>&#xA;"
42807097,42788267,2833802,2017-03-15T10:30:17,"<blockquote>&#xA;  <p>The authorization code flow returns an authorization code (like it says on the tin) that can then be exchanged for an ID token and access token. This requires client authentication using a client id and secret to retrieve the tokens from the back end and has the benefit of not exposing tokens to the User Agent.</p>&#xA;  &#xA;  <p>This flow allows for long lived access (through the use of refresh tokens).&#xA;  <strong>Clients using this flow must be able to maintain a secret</strong>.</p>&#xA;</blockquote>&#xA;&#xA;<p>Accordingly to your description, you have service-to-service authorization flow, and as your service are not exposing client secret key it is totally OK to use the Code flow. Moreover, you should use it to allow long lived tokens.</p>&#xA;"
45612149,45579511,2833802,2017-08-10T11:13:53,"<p>If describe the high level, generic approach, you may do something like this:</p>&#xA;&#xA;<ol>&#xA;<li>Your clients need to have some parameters which will allow to uniquely identify them. Looks like you already have this.</li>&#xA;<li>Implement additional API service (let's call it Experiment API). This service should have at least one endpoint that receives client identifying attributes and says whether the client is involved in A/B testing or not. </li>&#xA;<li>On each incoming request, the Gateway API need to use that Experiment API endpoint to decide which microservice version (v1 or v2) uses for redirect/call.</li>&#xA;<li>To avoid calling Experiment API each time you may introduce some caching layer in the Gateway API. As another option, you may use some custom cookie (that contains whether client under ""experiment""), do call to Experiment API only if that cookie is not specified and return the cookie to client with the response.</li>&#xA;</ol>&#xA;"
45631274,45622414,2833802,2017-08-11T09:12:09,"<p>Authentication/authorization in most cases is needed for microservices that provide public API, as they are available/visible for the World. </p>&#xA;&#xA;<p>Why? Cause when someone from the World calls the API method, we (in most cases) want to know who the client is (do Authentication) and decide what client is allowed to do (do Authentication).</p>&#xA;&#xA;<p>On the other hand, for internal microservices (in most cases) the client's are well-known as they are other internal microservices. So until you don't need to provide different restrictions of use for different internal microservices there is no need for authorization. Note that I assume that internal components only available within the organization.</p>&#xA;"
42510663,42498492,2833802,2017-02-28T13:56:25,"<p>You may think about</p>&#xA;&#xA;<ul>&#xA;<li>splitting <code>Data Service</code> into few microservices, based on some business logic;</li>&#xA;<li>modify <code>Data Service</code> (if needed) to support more than one instance of service. Then use the load balancer to split requests between those instances.</li>&#xA;</ul>&#xA;&#xA;<blockquote>&#xA;  <p>A load balancer is a device that acts as a reverse proxy and distributes network or application traffic across a number of servers. Load balancers are used to increase capacity (concurrent users) and reliability of applications.</p>&#xA;</blockquote>&#xA;&#xA;<hr>&#xA;&#xA;<p>Regarding ""One database, multiple services"":</p>&#xA;&#xA;<p>Each microservice need to have own data storage, otherwise, you do not have a decomposition. If we are talking about relation database, then this can be achieved using one of the following patterns:</p>&#xA;&#xA;<ul>&#xA;<li>Private tables per Service – each service owns a set of tables that must only be accessed by that service</li>&#xA;<li>Schema perService – each service has a database schema that’s private to that service</li>&#xA;<li>Database per Service – each service has it’s own database.</li>&#xA;</ul>&#xA;&#xA;<p>If your services using separate tables from <code>Data Warehouse</code> database and <code>Data Service</code> only provides access layer to database without any additional processing logic, then yes, you may remove <code>Data Service</code> and move data access logic to corresponding services. But think on another hand - right now you have only one place (<code>Data Service</code>), that knows how to access and manipulate with <code>Data Warehouse</code> that is what microservices are about.</p>&#xA;"
44162345,44161386,2833802,2017-05-24T15:10:17,"<p>Looks like you need to change status flow for your orders. See, not only Invoice microservice may fail - any step may (payment could be rejected as an example). Instead of next flow:</p>&#xA;&#xA;<pre><code>create order --&gt; mark as completed --&gt; do payment --&gt; create invoice &#xA;</code></pre>&#xA;&#xA;<p>consider something like this</p>&#xA;&#xA;<pre><code>create order --&gt;  mark as ""waiting for payment"" --&gt; do payment --&gt;&#xA;--&gt; mark as ""ready for invoice"" --&gt; create invoice --&gt; mark as completed&#xA;</code></pre>&#xA;&#xA;<p>As the result, you will have orders with different statuses and will be able to process them differently in the case of fail. </p>&#xA;&#xA;<hr>&#xA;&#xA;<p>In additional, you need to think whether each step could be retried later if failed. As a rough example, you may have some scheduled job, that generates invoice mail for all orders with status ""ready for invoice"". So if the job is failed for order, it will re-try to generate mail again at the next round.</p>&#xA;"
46303808,46283367,8079754,2017-09-19T15:00:22,"<p>Hysterix is used to stop cascading failures, I'll give you an example to explain what I mean:&#xA;Lets pretend you have 3 components: 1) Frontend, 2) Backend A and 3) Backend B.<br>&#xA;Frontend talks to Backend A and Backend A asks backend B to do some sort of lookup.&#xA;The Frontend receives 50k requests per second, which means 50k requests are going to Backend A and another 50k requests going to Backend B.  If Backend B Becomes unhealthy, that is 50k sockets you're holding open between Backend B to Backend A, and another 50k sockets open between Backend A and the Frontend. What will end up happening is all the servers involved in the transaction will all start to hang because all the sockets are being held open. The sockets will fill up really fast, at 50k a second, with a 20 second timeout, thats 1 million open sockets between each server! the result of Backend B timing out will mean requests to Backend A will timeout which will mean requests to the Frontend will also time out.&#xA;Hysterix (or the idea of circuit breaking) is pretty much introducing a switch where when a server becomes unhealthy, it will have some sort of way to deal with the errors such as stopping all future requests and just give a predefined response instantly, resulting in the sockets closing straight away and no cascading failures occuring. This results in increased resilience and better fault tolerance.</p>&#xA;"
45769284,45766604,8344202,2017-08-19T08:12:21,"<p>As far as I know there is no way that docker routes HTTP requests to one or the other container. You can only map a port from your host to one container.</p>&#xA;&#xA;<p>What you will need is to run a reverse proxy (e.g. nginx) as your main container that then routes the request to the appropriate container.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>Here an example how to set it up</p>&#xA;&#xA;<h2>site1/Dockerfile</h2>&#xA;&#xA;<pre><code>FROM node:6.11&#xA;WORKDIR /site1&#xA;COPY site1.js .&#xA;CMD node site1.js&#xA;EXPOSE 80&#xA;</code></pre>&#xA;&#xA;<h2>site1/site1.js</h2>&#xA;&#xA;<pre><code>var http = require(""http"");&#xA;&#xA;http.createServer(function (request, response) {&#xA;   response.writeHead(200, {'Content-Type': 'text/plain'});&#xA;   response.end('Hello World 1\n');&#xA;}).listen(80);&#xA;</code></pre>&#xA;&#xA;<h2>site2/Dockerfile</h2>&#xA;&#xA;<pre><code>FROM node:6.11&#xA;WORKDIR /site2&#xA;COPY site2.js .&#xA;CMD node site2.js&#xA;EXPOSE 80&#xA;</code></pre>&#xA;&#xA;<h2>site2/site2.js</h2>&#xA;&#xA;<pre><code>var http = require(""http"");&#xA;&#xA;http.createServer(function (request, response) {&#xA;   response.writeHead(200, {'Content-Type': 'text/plain'});&#xA;   response.end('Hello World 2\n');&#xA;}).listen(80);&#xA;</code></pre>&#xA;&#xA;<h2>node-proxy/default.conf</h2>&#xA;&#xA;<pre><code>server {&#xA;    listen 80;&#xA;&#xA;    # ~* makes the /site1 case insensitive&#xA;    location ~* /site1 {&#xA;        # Nginx can access the container by the service name&#xA;        # provided in the docker-compose.yml file.&#xA;        proxy_pass http://node-site1;&#xA;    }&#xA;&#xA;    location ~* /site2 {&#xA;        proxy_pass http://node-site2;&#xA;    }&#xA;&#xA;    # Anything that didn't match the patterns above goes here&#xA;    location / {&#xA;        # proxy_pass http://some other container&#xA;        return 500;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<h2>docker-compose.yml</h2>&#xA;&#xA;<pre><code>version: ""3""&#xA;services:&#xA;&#xA;  # reverse proxy&#xA;  node-proxy:&#xA;    image: nginx&#xA;    restart : always&#xA;    # maps config file into the proxy container&#xA;    volumes:&#xA;      - ./node-proxy/default.conf:/etc/nginx/conf.d/default.conf&#xA;    ports:&#xA;      - 80:80&#xA;    links:&#xA;      - node-site1&#xA;      - node-site2&#xA;&#xA;  # first site&#xA;  node-site1:&#xA;    build:  ./site1&#xA;    restart: always&#xA;&#xA;  # second site&#xA;  node-site2:&#xA;    build:  ./site2&#xA;    restart: always&#xA;</code></pre>&#xA;&#xA;<p>To start the reverse proxy and both sites enter in the root of this folder <code>docker-compose up -d</code> and check with <code>docker ps -a</code> that all docker containers are running.</p>&#xA;&#xA;<p>Afterwards you can access this two sites with <a href=""http://localhost/site1"" rel=""nofollow noreferrer"">http://localhost/site1</a> and <a href=""http://localhost/site2"" rel=""nofollow noreferrer"">http://localhost/site2</a></p>&#xA;&#xA;<h2>Explanation</h2>&#xA;&#xA;<p>The folder site1 and site2 contains a small webserver build with nodejs. Both of them are listening on port 80. ""node-proxy"" contains the configuration file that tells nginx when to return which site.</p>&#xA;&#xA;<p>Here are some links</p>&#xA;&#xA;<ul>&#xA;<li>docker-compose: <a href=""https://docs.docker.com/compose/overview/"" rel=""nofollow noreferrer"">https://docs.docker.com/compose/overview/</a></li>&#xA;<li>nginx reverse proxy: <a href=""https://www.nginx.com/resources/admin-guide/reverse-proxy/"" rel=""nofollow noreferrer"">https://www.nginx.com/resources/admin-guide/reverse-proxy/</a></li>&#xA;</ul>&#xA;"
49976763,49973699,9661066,2018-04-23T08:42:07,"<p>If you wrap a return value in an interface you will always have to either type switch on it to process it based on its type in this kind of scenario.</p>&#xA;&#xA;<p>The alternative for the way you are approaching would be to insert the processing function into the Request objects dynamically by adding a method to the <code>Request</code> interface which allowed you register a handler and another method <code>Handle()</code> which you could call to process each message by calling the registered handler. Experience suggests that you would end up with a bunch of boilerplate in each Request type definition though.</p>&#xA;&#xA;<p>In go I would probably take a different approach and not use an OO kind of design where you make each request type it's own ""class"". I would probably define the network package more along the lines of the code below. (Caveat: this isn't tested, there might be a few typos and theres some error stuff that needs filled in)</p>&#xA;&#xA;<pre><code>type RequestType byte&#xA;&#xA;const (&#xA;    ARequest RequestType = iota&#xA;    BRequest&#xA;    CRequest&#xA;    EndRequestRange&#xA;)&#xA;&#xA;type HandlerFunc func([]byte) error&#xA;&#xA;type MessageInterface struct {&#xA;    handlers map[RequestType]HandlerFunc&#xA;    // other stuff&#xA;}&#xA;&#xA;func NewMessageInterface() *MessageInterface {&#xA;    m := &amp;MessageInterface {&#xA;        handlers: make(map[RequestType]HandlerFunc),&#xA;    }&#xA;    // Do other stuff&#xA;    return m&#xA;}&#xA;&#xA;func (m *MessageInterface) AddHandler(rt RequestType, h HandlerFunc) error {&#xA;    if rt &gt;= EndRequestRange {&#xA;        // return an error&#xA;    }&#xA;    m.handlers[rt]=h&#xA;&#xA;    return nil&#xA;}&#xA;&#xA;func (m *MessageInterface) ProcessNextMessage() error {&#xA;    buf := make([]byte, 100)&#xA;    _, _ := conn.Read(buf)&#xA;&#xA;    h, ok := m.handlers[ReqestType(buff[0])]&#xA;    if !ok {&#xA;        // probably return error&#xA;    }&#xA;&#xA;    return h(buff[1:])  &#xA;}&#xA;</code></pre>&#xA;&#xA;<p>In your main package you would then initialize <code>MessageInterface</code>, call <code>AddHandler()</code> to add each handler during initialization you want and repeatedly call <code>ProcessNextMessage()</code> during your main processing.</p>&#xA;&#xA;<p>It's a different approach from the design direction you were going in, but for me a bit more of a go approach.</p>&#xA;&#xA;<p>(Note as well - I didn't do anything to make sure the code was safe for concurrency and this kind of approach is normally used in a way that each handler is called in its own goroutine. If you made that change you might want some mutex protection around the map as well as checking a few other aspects)</p>&#xA;"
47646191,47633268,7565450,2017-12-05T04:38:10,"<p>When an error is returned in gRPC using golang it uses the <a href=""https://godoc.org/google.golang.org/grpc/status"" rel=""nofollow noreferrer"">status library</a> . When your gRPC Server returns an error you get a status code and a message. You can use the  <code>FromError</code> function to get the status code and message on the client side. When I get a db error on my server side I return a Message INTERNAL_SERVER_ERROR with the code <code>Internal Code = 13</code>. My client checks the code and determines what message to return to the user.</p>&#xA;"
48612279,47793065,7511690,2018-02-04T19:43:36,"<p>Take a look at Angular Elements (Custom Elements). <a href=""https://moduscreate.com/blog/angular-elements-ngcomponents-everywhere/"" rel=""nofollow noreferrer"">https://moduscreate.com/blog/angular-elements-ngcomponents-everywhere/</a></p>&#xA;&#xA;<p>The new Ionic version (4) is totally based on it to be able to be used on every version of Angular and on every JS frameworks.</p>&#xA;&#xA;<p>For that, they created <a href=""https://stenciljs.com/"" rel=""nofollow noreferrer"">https://stenciljs.com/</a> that will help you to create Custom Elements.</p>&#xA;&#xA;<p>But if all teams are using Angular, each of them can create a library using ngm for exemple.</p>&#xA;"
43380198,43379048,645002,2017-04-12T21:48:52,"<p>Consul provides a <a href=""https://www.consul.io/docs/agent/dns.html"" rel=""nofollow noreferrer"">DNS interface</a>. Don't use ip addresses, if you do you're not <em>really</em> doing any kind of service discovery.</p>&#xA;&#xA;<p>The best thing to do is set up <a href=""https://www.consul.io/docs/guides/forwarding.html"" rel=""nofollow noreferrer"">DNS Forwarding</a> in your DNS infrastructure, and then do a lookup for consul.service.consul.</p>&#xA;"
46702593,38461294,6350474,2017-10-12T06:11:57,"<p>Here is the sample code</p>&#xA;&#xA;<pre><code>public class Test {&#xA;&#xA;    public static void main(String[] args) {&#xA;        String jsonString = ""{\""id\"" : \""123\"",\""name\"" : \""Tom\"",\""class\"" : {\""subject\"" : \""Math\"",\""teacher\"" : \""Jack\""}}"";&#xA;        RestTemplate restTemplate = new RestTemplate();&#xA;        String url = ""http://192.1168.1.190:8080/test"" // url for second service&#xA;        System.out.println(responserEntityValue(jsonString,restTemplate,url,HttpMethod.POST,String.class));&#xA;    }&#xA;&#xA;    public ResponseEntity&lt;String&gt; responserEntityValue(final String body, final RestTemplate restTemplate,&#xA;            final String uRL, final HttpMethod requestMethod, final Class&lt;String&gt; stringClass) {&#xA;&#xA;        HttpHeaders headers = new HttpHeaders();&#xA;        // Set all headers&#xA;        headers.add(DatabaseConstants.CONTENT_TYPE, ""application/json"");&#xA;&#xA;        HttpEntity&lt;String&gt; request = new HttpEntity&lt;&gt;(body, headers);&#xA;&#xA;        return restTemplate.exchange(uRL, requestMethod, request, stringClass);&#xA;&#xA;    }&#xA;</code></pre>&#xA;"
39723861,39721025,4323978,2016-09-27T11:47:07,"<p>We're a small team (&lt;10 persons) and are using a microservices architecture and are getting a lot of benefits of it. But to be successful with a microservices approach you need to meet a bunch of prerequisites. (See <a href=""http://martinfowler.com/bliki/MicroservicePrerequisites.html"" rel=""nofollow"">http://martinfowler.com/bliki/MicroservicePrerequisites.html</a>) So if you need to deliver fast and you're not yet into continuous delivery and DEVOPS, I would stay away from it.</p>&#xA;&#xA;<p>My 2c</p>&#xA;"
39724647,32534401,4323978,2016-09-27T12:25:16,<p>Arguably the biggest benefit of a microservices architecture is the increase in speed with which you can roll out new features into production. In order to achieve that the microservices should be as much as possible independent from each other. WSDL-contracts have the tendency to result in a tighter coupling in comparison with REST-contracts. From this perspective it makes a lot of sense to opt for REST. But it all depends on the goals that you did want to accomplish when you decided to go for a microservices or a SOA architecture. </p>&#xA;
51857354,51855075,3802507,2018-08-15T10:51:18,"<p>Maybe you can have a look into this famous nginx proxy docker image ""<a href=""https://github.com/jwilder/nginx-proxy"" rel=""nofollow noreferrer"">https://github.com/jwilder/nginx-proxy</a>"". </p>&#xA;&#xA;<p>You can deploy multiple containers &amp; <code>docker-gen</code> will dynamically put entries into your nginx configs by listening to docker sock file. </p>&#xA;&#xA;<p>By default it works on domain basis, start any containers you want proxied with an env var <code>VIRTUAL_HOST=subdomain.youdomain.com</code></p>&#xA;&#xA;<p>If that doesn't solve the purpose, you can try creating your own <code>docker gen</code> templates for nginx configs in Golang using this image itself.<br>&#xA;Another great feature is you can easily scale up/down your app containers too. </p>&#xA;&#xA;<p>Good luck!</p>&#xA;"
46947465,46947124,5333643,2017-10-26T06:42:37,"<p>Scheduling a job must have these basic functionalities</p>&#xA;&#xA;<ol>&#xA;<li>Scheduling Job</li>&#xA;<li>Unscheduling/Deleting scheduled job</li>&#xA;<li>Processing based on priority (Will be useful in long-term), Maybe you &#xA; can prioritize based on the categories you have. Explore redis here as it has amazing capabilities to save ordered set</li>&#xA;</ol>&#xA;&#xA;<p>I would suggest keeping the job queueing service and job processing service (i.e workers) different as this will enable your application to work as an black box</p>&#xA;"
29641803,29636094,1144203,2015-04-15T04:49:08,"<p>The important thing to recognize here is that, when it comes to implementing user authentication, user authorization, and user session management solutions, there aren't significant conceptual differences between the solutions for micro-service architecture and any other form of distributed systems architecture.</p>&#xA;&#xA;<p>Conceptually, the solution will be an implementation of the <a href=""http://en.wikipedia.org/wiki/Kerberos_%28protocol%29"" rel=""noreferrer"">Kerberos protocol</a>, with the notion of a ""ticket"" (or ""token"" as it is more commonly called today) being passed around to establish the identity of users.</p>&#xA;&#xA;<p>So in your case, your user management service will serve as your backend authentication and authorization server, that all other services need to be aware. Your user management service will be responsible for issuing and verifying user tokens to establish your users' identity and roles.</p>&#xA;&#xA;<blockquote>&#xA;  <p>What can I do to only permit accessing the planning information through the user service without couple the two services?</p>&#xA;</blockquote>&#xA;&#xA;<p>When a user tries to access your .Net service, your service will have to rely on your user management service to establish the user's identity. In other words, your .Net service will need to know where your user management service is (i.e. its URL). You can utilize tools like <a href=""https://github.com/coreos/etcd"" rel=""noreferrer"">etcd</a> or <a href=""https://nats.io/"" rel=""noreferrer"">nats</a> to propagate the location of your user management service to the rest of your ecosystem, without coupling your microservices to it.</p>&#xA;&#xA;<blockquote>&#xA;  <p>How can I do to access billing informations from billing service corresponding to a user from the Mysql database?</p>&#xA;</blockquote>&#xA;&#xA;<p>The simplest solution is to pass in the (single?) key (say user ID) as a query parameter in the URL of the billing service. Of course, your billing service has to be implemented in such a way to expect the key being appended to the URL. If you have additional keys that need to be included (say for example, billing dates, order status etc.), maybe you can utilized something like either <a href=""http://memcached.org/"" rel=""noreferrer"">memcached</a> or <a href=""http://redis.io/"" rel=""noreferrer"">redis</a> to store those information at runtime. Of course, now you have to consider the trade-offs of adding these single-point-of-failure to your ecosystem :-)</p>&#xA;&#xA;<blockquote>&#xA;  <p>How to create authentication without duplicating authentication requests to the authentication service, from other services?</p>&#xA;</blockquote>&#xA;&#xA;<p>Again, once the user is authenticated, all its identity information can be stored in something like either <a href=""http://memcached.org/"" rel=""noreferrer"">memcached</a> or <a href=""http://redis.io/"" rel=""noreferrer"">redis</a> to avoid repeating the authentication and authorization process.</p>&#xA;"
48826961,48762736,8755083,2018-02-16T12:37:05,<p>I think using SSL connection to your microservice and pain text password in body of HTTP request is the general way of doing logins in client applications.</p>&#xA;
35449835,35441660,3391371,2016-02-17T06:53:16,"<p>By the definition what micro services are, then conceptually yes. A micro service is a service that is an independent unit, it could deployed, updated, and undeployed independently without affecting any unrelated part of your application. So that would mean having multiple instances of JBoss for MS and your application calling them through some sort of gateway or any other mechanism depending on your use case. If you plan to deploy all your MS in the same JBoss instance then it defeats the very purpose of a MS. Given that, JBoss wouldn't be a right choice for MS deployment because it will only make your MS deployment infrastructure quite heavy. </p>&#xA;&#xA;<p>Depending on what your client's requirements are, your could possibly keep your webapp in JBoss and deploy your MS containers separately. </p>&#xA;"
46306088,35135884,3391371,2017-09-19T17:04:23,"<p>Microservice sagas could be helpful here. More details on <a href=""http://microservices.io/patterns/data/saga.html"" rel=""nofollow noreferrer"">Sagas</a></p>&#xA;"
33521905,33399988,3391371,2015-11-04T12:26:00,"<p>A lot depends on your actual use case but I think write-behind or write-back could be one of your solution. <a href=""http://www.ehcache.org/generated/2.10.0/html/ehc-all/index.html#page/Ehcache_Documentation_Set/co-recipe_database_write_overload.html"" rel=""nofollow"">This</a> link talks about the technique with EhCache, I think there should be other caches supporting the feature, you may want to google a bit on that.</p>&#xA;"
32538059,32529742,4571787,2015-09-12T11:03:03,"<p>Don't do distributed transactions.</p>&#xA;&#xA;<p>For integration with your existing legacy system one approach could be a separate (micro)service which listens to update events from your userService and forwards the respective updates to the legalService.&#xA;Spring integration may be suitable for such a task.</p>&#xA;&#xA;<p>Cheers,&#xA;Michael</p>&#xA;"
47339947,47330878,2349963,2017-11-16T22:10:28,"<p>Seems like you don't need REST Endpoint on API2 at all. <br/> The suggestions above by @Dayo Adeyemi are correct, but if there is a service that you need to keep private then there is no need to write the REST Endpoint codes in there.</p>&#xA;"
45183089,35065875,260491,2017-07-19T06:59:39,"<p>GRPC Bus WebSocket Proxy does exactly this by proxying all GRPC calls over a WebSocket connection to give you something that looks very similar to the Node GRPC API in the browser. Unlike GRPC-Gateway, it works with both streaming requests and streaming responses, as well as non-streaming calls. </p>&#xA;&#xA;<p>There is both a server and client component.&#xA;The <a href=""https://github.com/gabrielgrant/grpc-bus-websocket-proxy-server"" rel=""nofollow noreferrer"">GRPC Bus WebSocket Proxy server</a> can be run with Docker by doing <code>docker run gabrielgrant/grpc-bus-websocket-proxy</code></p>&#xA;&#xA;<p>On the browser side, you'll need to install the <a href=""https://github.com/gabrielgrant/grpc-bus-websocket-proxy-client"" rel=""nofollow noreferrer"">GRPC Bus WebSocket Proxy client</a> with <code>npm install grpc-bus-websocket-client</code></p>&#xA;&#xA;<p>and then create a new GBC object with: <code>new GBC(&lt;grpc-bus-websocket-proxy address&gt;, &lt;protofile-url&gt;, &lt;service map&gt;)</code></p>&#xA;&#xA;<p>For example:</p>&#xA;&#xA;<pre><code>var GBC = require(""grpc-bus-websocket-client"");&#xA;&#xA;new GBC(""ws://localhost:8080/"", 'helloworld.proto', {helloworld: {Greeter: 'localhost:50051'}})&#xA;  .connect()&#xA;  .then(function(gbc) {&#xA;    gbc.services.helloworld.Greeter.sayHello({name: 'Gabriel'}, function(err, res){&#xA;      console.log(res);&#xA;    });  // --&gt; Hello Gabriel&#xA;  });&#xA;</code></pre>&#xA;&#xA;<p>The client library expects to be able to download the <code>.proto</code> file with an AJAX request. The <code>service-map</code> provides the URLs of the different services defined in your proto file as seen by the proxy server.</p>&#xA;&#xA;<p>For more details, see the <a href=""https://github.com/gabrielgrant/grpc-bus-websocket-proxy-client"" rel=""nofollow noreferrer"">GRPC Bus WebSocket Proxy client README</a></p>&#xA;"
40442271,35639882,1641868,2016-11-05T19:24:03,"<p>Using this approach you will have to validate the token in all your services, if you are okay with this then you are probably fine.</p>&#xA;&#xA;<p>The access token may have an expire time that will make it necessary to use a refresh token to get a new access token from the auth service:</p>&#xA;&#xA;<ul>&#xA;<li>When the access token expires you would return a 401 to the client, from the Service X that you are trying to talk to.</li>&#xA;<li>The client would have to call the Auth service providing a refresh token, getting a new access token&#xA;&#xA;<ul>&#xA;<li>Finaly the client would be hitting the Service X again with this new access token, have it validated and get the expected response from Service X.</li>&#xA;</ul></li>&#xA;</ul>&#xA;&#xA;<p>In my <a href=""http://www.davidkarlsson.no/readPost/Using_PassportJS_to_build_an_authentication_proxy"" rel=""nofollow noreferrer"">recent assignment</a> I wrote a micro-service that proxied all the requests validating their tokens, the proxy handled everything from login/auth to roles and sending 401's for expired tokens and revoking refresh tokens etc. I think this gave me a greater separation of concerns than having to deal with tokens in all services. However it ofcourse makes the proxy a possible bottle neck into the system, autoscaling of the auth service is meant to deal with this in my case.</p>&#xA;&#xA;<p>Also I did not user redis but stored a hashed key (consisting of hashed accesstoken properties + salt) in the accesstoken that I could validate by rehashing the other properties of the accesstoken+salt.</p>&#xA;&#xA;<p><strong>Important Note:</strong> In the refresh token scenario above my proxy only would experience load for an invalid/expired accesstoken, whilst in your scenario any service could be reached with invalid tokens, I don't know if this is of any concern in your particular case but it is probably worth mentioning...</p>&#xA;&#xA;<p>Another approach would be to let Service-A and Service-B call the auth service to validate the tokens, but this would infer a lot of more traffic between the services since each HTTP request with a token has to be validated. In this scenario as well a invalid token request would reach your Service X and hence infer some load on it...</p>&#xA;"
47683994,47656406,9064076,2017-12-06T21:48:24,"<p>A simple test could be asking below questions  -</p>&#xA;&#xA;<p>Does your “calculator” (as you refer it) hold the calculation result as an immutable state? — <em>if the answer is yes then it is a Value Object.</em></p>&#xA;&#xA;<p>Is the “Calculator” stateless and only exposes a “calculate” behaviour which accepts a request &amp; returns a calculation result? — <em>if the answer is yes then it is a Service, however, the “result” returned by this service may be classified as Value Object.</em></p>&#xA;"
43700964,43700905,819340,2017-04-29T22:13:09,"<p><strong>Known hostname solution</strong>. The <em>fixed part</em> would be the service domain name, for instance <code>xservice.com</code>. You can <em>query</em> this host using standard DNS tools (e.g., <code>dig</code> in your shell, etc). </p>&#xA;&#xA;<p>Finally, in the DNS bound to <code>xservice.com</code> you then add a <code>SRV</code> record with further details.</p>&#xA;&#xA;<p>A <code>SRV</code> record lists all the service details, including:</p>&#xA;&#xA;<ul>&#xA;<li>the symbolic service name;</li>&#xA;<li>the canonical hostname of the machine providing the service;</li>&#xA;<li>the TCP (or UDP) port on which the service is available. </li>&#xA;</ul>&#xA;&#xA;<p>There are many other info as well. Please see <a href=""https://en.m.wikipedia.org/wiki/SRV_record"" rel=""nofollow noreferrer"">Wikipedia</a> for the complete list.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>Please keep in mind this is a somewhat static solution. If you are looking for a more dynamic one, then <strong>Oswin</strong> answer might be a better fit :-) </p>&#xA;"
51392430,51388064,1933452,2018-07-18T01:51:45,"<p>AngularJs is pretty ancient man :)</p>&#xA;&#xA;<p>You have to look at the pain points to identify ways to tear down your monolith. Monolith pains are usually slow and painful dev cycle and difficult manual test phases. If you did the entire arquillian thing and have full continuouos integration with single button deployments, you've slain the beast the hard way. Not many braved this route. But if you're looking at mounting feature creep with code freezes and manual test cycles then yeah you kind of want to try to pull some of those features out into a service you can redeploy very quickly</p>&#xA;"
41662042,41661858,361100,2017-01-15T14:09:18,"<p>Your doing is the first step to fabricate a Microservice; making each service as compact/independent/isolate/unique as possible, expose them to well-known(REST) endpoint, and communicate them with messages. </p>&#xA;&#xA;<p>It is not suitable to describe the definition of Microservice in detail at this post but you could read lots of article about Actor model, Microservice, or their related patterns and applications.</p>&#xA;&#xA;<p>practically, there are frameworks to achieve Microservice architecture. <a href=""http://getakka.net/"" rel=""nofollow noreferrer"">Akka.net</a> and <a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-overview"" rel=""nofollow noreferrer"">Azure Service Fabric</a> are recommended for C# developers.</p>&#xA;&#xA;<p>In addition, you said <em>WebJob</em> and if it is <em>Azure WebJobs</em>, why don't you read this great article; <a href=""https://msdn.microsoft.com/en-us/magazine/mt595752.aspx"" rel=""nofollow noreferrer"">https://msdn.microsoft.com/en-us/magazine/mt595752.aspx</a> </p>&#xA;&#xA;<p>Hope this helps.</p>&#xA;"
34576135,34575783,361100,2016-01-03T11:43:09,"<p>Yes. Each actor can run fully independent, or we can say, an Actor must be able to work independently by design of Actor model. For that, you can create ServiceFabric with Stateful-Stateless interoperated manner on Reliable Actors API or use only one as your requirements.</p>&#xA;&#xA;<p>For multiple actors, <a href=""https://github.com/Azure-Samples/service-fabric-dotnet-getting-started/"" rel=""nofollow"">offical ServiceFabric samples</a> show good examples to use Reliable Services or Reliable Actors in stateful-stateless interoperable manner.</p>&#xA;"
37912448,37662379,2987916,2016-06-19T22:26:02,"<p>I've faced the same situation: The jdl is being imported but nothing is generated. As Vikas Joshi suggests try using jhipster-uml:</p>&#xA;&#xA;<pre><code>npm install -g jhipster-uml&#xA;jhipster-uml ./jhipster-jdl.jh -db mongodb&#xA;</code></pre>&#xA;&#xA;<p>jhispter-uml usually returns more feedback. Anyway take into account that in MongoDB as noSQL database that it is, some options have no sense (e.g relations between entities).</p>&#xA;"
50409064,50328886,4763788,2018-05-18T10:17:22,"<p>For what do you need the proxy? When it's for exposing the service to the outside world, have a look at Ingress / Ingress Controller: <a href=""https://kubernetes.io/docs/concepts/services-networking/ingress/"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/concepts/services-networking/ingress/</a></p>&#xA;"
31989715,31973473,4633996,2015-08-13T13:31:14,"<p>I've seen three general protocol design patterns with microservices architectures, when running at scale:</p>&#xA;&#xA;<ol>&#xA;<li>Message bus architecture, using a central broker such as ActiveMQ or Apache Qpid.</li>&#xA;<li>""Resilient"" HTTP, where some additional logic is built on HTTP to make it more resilient. Typical approaches here are Hystrix (Java), or SmartStack/Baker St (smart proxy).</li>&#xA;<li>Point-to-point asynchronous messaging using something like NSQ, ZMQ, or Qpid Proton.</li>&#xA;</ol>&#xA;&#xA;<p>By far the most common design pattern is #2, with a little bit of #1 mixed in when a queue is desirable.</p>&#xA;&#xA;<p>In theory, #3 offers the best of both worlds (resiliency AND scale AND performance) but the technologies are all somewhat immature. It turns out that with #2 you can get really very far (e.g., Netflix uses Hystrix everywhere).</p>&#xA;&#xA;<p>To answer your question directly, I'd say that #1 is very rarely used as an exclusive design pattern because it creates a single bottleneck for your entire system. #1 is common for a subset of the system. For most people, I'd recommend #2 today.</p>&#xA;"
32970468,32945623,4633996,2015-10-06T12:51:20,"<p>One approach is to use a client side LB like SmartStack (<a href=""http://nerds.airbnb.com/smartstack-service-discovery-cloud/"" rel=""nofollow"">http://nerds.airbnb.com/smartstack-service-discovery-cloud/</a>), Baker Street (<a href=""http://bakerstreet.io"" rel=""nofollow"">http://bakerstreet.io</a>), or Consul HAProxy (<a href=""https://hashicorp.com/blog/haproxy-with-consul.html"" rel=""nofollow"">https://hashicorp.com/blog/haproxy-with-consul.html</a>), along with a custom load balancing / routing algorithm.</p>&#xA;&#xA;<p>All of these systems us HAProxy to proxy your requests intelligently between producer/consumer. The proxy itself uses a service discovery mechanism to figure out what's available. You'd need to make some changes to the service discovery mechanism to return the right result based on your constraints noted above.</p>&#xA;"
32269648,32248896,4633996,2015-08-28T11:07:56,"<p>I don't think zip/unzip/etc should be viewed as business functions. Those are separate technical functions required to achieve a specific business function.</p>&#xA;&#xA;<p>To me, a business function is ""archive this data"" which might include compressing it, sticking it into some sort of archival storage system, and indexing it for future retrieval. </p>&#xA;"
33149230,33125508,4633996,2015-10-15T12:59:00,"<p>You should think about it as client side load balancing versus dedicated load balancing.</p>&#xA;&#xA;<p>Client side load balancers include Baker Street (<a href=""http://bakerstreet.io"" rel=""noreferrer"">http://bakerstreet.io</a>); SmartStack (<a href=""http://nerds.airbnb.com/smartstack-service-discovery-cloud/"" rel=""noreferrer"">http://nerds.airbnb.com/smartstack-service-discovery-cloud/</a>); or Consul HA Proxy (<a href=""https://hashicorp.com/blog/haproxy-with-consul.html"" rel=""noreferrer"">https://hashicorp.com/blog/haproxy-with-consul.html</a>).</p>&#xA;&#xA;<p>Client side LBs use a service discovery component (Baker Street uses a stateless pub/sub service discovery mechanism; SmartStack uses ZooKeeper; Consul HA Proxy uses Consul) as part of their implementation, but they provide the health checking / end-to-end functionality you're probably looking for.</p>&#xA;"
31747265,31717615,4633996,2015-07-31T13:22:51,"<p>There's no one size fits all solution.</p>&#xA;&#xA;<p>The general principle is that each microservice should make the right decision for itself in terms of what the right persistence architecture should be. It might be connected to a central SQL database, or it could be using a filesystem, or it could be using NoSQL data store, or memcached, or whatever. (This is why people talk about <a href=""https://en.wikipedia.org/wiki/Eventual_consistency"" rel=""nofollow"">eventual consistency</a> a lot with microservices.)</p>&#xA;&#xA;<p>You want to do it this way to really capture the benefits of microservices.</p>&#xA;&#xA;<ol>&#xA;<li>You want each microservice to be independently shippable, so that you're not blocked on anything. Stronger coupling to centralized infrastructure reduces the independence of the microservice.</li>&#xA;<li>Persistence requirements are highly variable. If you're running a search microservice, you don't need the ACID semantics of a typical SQL database. If you're doing payments, you need ACID. If you're storing and processing images, you might just use the filesystem. Etc.</li>&#xA;</ol>&#xA;"
29158306,22513893,4633996,2015-03-20T02:19:57,"<p>I think when you ask how often ""messaging"" is used in request/response, you probably mean asynchronous communication.</p>&#xA;&#xA;<p>I'll take a different (opposite) perspective than some of the answers here, and say that you should almost always use async, even in request/response. This is because async means you're not blocking your program waiting for a response, and your program can keep on doing some other processing while waiting for a response.</p>&#xA;&#xA;<p>For example, imagine you are implementing Twitter's homepage. You fire off a bunch of requests to different microservices (""get me recommended followers"", ""get me latest timeline"", etc.""). You don't want to block and serialize all of these requests. You want to fire them off async and that creates a much better user experience, because as the responses come back in, they update the UI in realtime.</p>&#xA;&#xA;<p>Twitter uses Finagle (<a href=""http://twitter.github.io/finagle/guide/index.html"" rel=""nofollow"">http://twitter.github.io/finagle/guide/index.html</a>) for exactly this (async RPC). It doesn't do some of the things a messaging system would do (in particular, it's really tied to the JVM, and it doesn't implement flow control or queues), but it does implement async request/response.</p>&#xA;"
46944861,46934916,6469772,2017-10-26T02:20:32,"<p><strong>What other architectures or technologies should I look into using (with spring boot)?</strong></p>&#xA;&#xA;<blockquote>&#xA;  <p>My answer is if you just want to simply provide RESTful service&#xA;  without Spring Cloud, then I think you can refer to following 2&#xA;  tutorials on Spring official website to achieve this:  </p>&#xA;  &#xA;  <p><a href=""https://spring.io/guides/gs/rest-service/"" rel=""nofollow noreferrer"">Building a RESTful Web Service</a><br>&#xA;  <a href=""https://spring.io/guides/gs/consuming-web-service/"" rel=""nofollow noreferrer"">Consuming a SOAP web service</a></p>&#xA;</blockquote>&#xA;&#xA;<p><strong>Is there a standard technology stack for this?</strong></p>&#xA;&#xA;<blockquote>&#xA;  <p>Currently, I will suggest you to use Spring Boot as your first choice. Because these are rich resources on the web and it does reduces much effort in development.</p>&#xA;</blockquote>&#xA;&#xA;<p><strong>Are there pitfalls I should be aware of?</strong></p>&#xA;&#xA;<blockquote>&#xA;  <p>If you finally choose Spring Boot, please be familiar to its components, you can start from <a href=""https://spring.io/guides"" rel=""nofollow noreferrer"">Guides</a> to realize how it works. Or you may mix up Spring Boot with tradition Spring framework.</p>&#xA;</blockquote>&#xA;"
47276080,47265946,6469772,2017-11-14T01:08:04,"<p>Actually, it doesn't matter to package all those services into ONE project. But in micro-service's opinion, you should separate them into many independent projects. There are several questions you can ask yourself before transforming original architecture.  </p>&#xA;&#xA;<ul>&#xA;<li>Is your application critical? Can user be tolerant of downtime while you must re-deploying whole package for updating only one service?</li>&#xA;<li>If there is no any dependency between services, why you want to put them together? Isn't it hard to develop or maintain?</li>&#xA;<li>Is the usage rate of each service the same? Maybe you can isolate those services and deploy them which are often to be invoked to a strong server.</li>&#xA;</ul>&#xA;&#xA;<p>Try to read this article <a href=""https://www.nginx.com/blog/microservices-at-netflix-architectural-best-practices/"" rel=""nofollow noreferrer"">Adopting Microservices at Netflix: Lessons for Architectural Design</a> to understand the best practices for designing a microservices architecture. And for developing with Spring Cloud, you can also read this post <a href=""http://cloud.spring.io/spring-cloud-static/spring-cloud.html#_spring_cloud_netflix"" rel=""nofollow noreferrer"">Spring Cloud Netflix</a> to know which components you should use in your architecture.</p>&#xA;"
47256324,47224077,6469772,2017-11-13T02:35:24,"<p>Your architecture is fine, but it still depends on your purpose.  </p>&#xA;&#xA;<p>Quote from this article - <a href=""http://cloud.spring.io/spring-cloud-static/spring-cloud.html#_spring_cloud_netflix"" rel=""nofollow noreferrer"">Spring Cloud Netflix</a>, there are several common components used to build a micro-service architecture with Spring Cloud Netflix.  </p>&#xA;&#xA;<ul>&#xA;<li>Service Discovery (Eureka)</li>&#xA;<li>Intelligent Routing (Zuul)</li>&#xA;<li>Circuit Breaker (Hystrix)</li>&#xA;<li>Client Side Load Balancing (Ribbon)</li>&#xA;</ul>&#xA;&#xA;<p>In my opinion, all these 4 components are required. So maybe you can explain what you want to achieve such as do you want to provide a service to many people or applications? Does your application need to be scaled out someday? Or if your service doesn't response after a long time, what will you do?</p>&#xA;"
42382429,42380585,7596684,2017-02-22T03:56:15,"<p>In my company we go to the first solution, however, I am not sure if it is an industry standard or not. The main reason is that it is very unlikely for us to change the configuration after building the docker container. </p>&#xA;&#xA;<p>Also, if you build different containers for different environments, passing <code>-Dspring.profiles.active=${environment}</code> parameter to the container run command is not very smart (it is always Prod for the production container). Instead, in <code>dockerFile</code>, you can just copy the appropriate <code>environment.properties</code>.</p>&#xA;"
37989992,37989803,1643661,2016-06-23T11:12:05,"<p>One solution would be assigning a public IP to the microservice you want to expose, and then perhaps have a VPN set-up that trusted entities use to access the other microservices.</p>&#xA;"
46450735,46388219,8132493,2017-09-27T14:39:20,"<p>For your first question: It seems to me that you are trying to publish a request message with a timeout (using the <code>nc.Request</code>). If so, the timeout is managed by the client. Effectively the client publishes the request message and creates a subscription on the reply subject. If the subscription doesn't get any messages within the timeout it will notify you of the timeout condition and unsubscribe from the reply subject.</p>&#xA;&#xA;<p>On your second question - are you using a queue group? A queue group in NATS is a subscription that specifies a queue group name. All subscriptions having the same queue group name are treated specially by the server. The server will select one of the queue group subscriptions to send the message to rotating between them as messages arrive. However the responsibility of the server is simply to deliver the message.</p>&#xA;&#xA;<p>To do what you describe, implement your functionality using request/reply using a timeout and a max number of messages equal to 1. If no responses are received after the timeout your client can then resend the request message after some delay or perform some other type of recovery logic. The reply message should be your 'protocol' to know that the message was handled properly. Note that this gets into the design of your messaging architecture. For example, it is possible for the timeout to trigger after the request recipient received the message and handled it but before the client or server was able to publish the response. In that case the request sender wouldn't be able to tell the difference and would eventually republish. This hints that such type of interactions need to make the requests idempotent to prevent duplicate side effects.</p>&#xA;"
31506902,26800452,2824333,2015-07-19T23:11:02,<p>As @Anatoly said <strong>trailing slashes</strong> make differences!</p>&#xA;
41277096,41019199,3684493,2016-12-22T06:36:00,<p>The CORS message is a red herring. If you look at the end of the error message you'll see this:</p>&#xA;&#xA;<blockquote>&#xA;  <p>The response had HTTP status code 500.</p>&#xA;</blockquote>&#xA;&#xA;<p>Usually the response will include some detail about the error. I suggest using a tool like Fiddler with HTTPS decryption enabled so you can see the content of the response.</p>&#xA;
42141450,42140285,7183757,2017-02-09T16:06:22,"<blockquote>&#xA;  <p>Steps a.- (save in Order DB) and b.- (publish the message) should be&#xA;  performed in a transaction, atomically. How can I achieve that?</p>&#xA;</blockquote>&#xA;&#xA;<p>Kafka currently does not support transactions (and thus also no rollback or commit), which you'd need to synchronize something like this. So in short: you can't do what you want to do. This will change in the near-ish future, when <a href=""https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging"" rel=""noreferrer"">KIP-98</a> is merged, but that might take some time yet. Also, even with transactions in Kafka, an atomic transaction across two systems is a very hard thing to do, everything that follows will only be improved upon by transactional support in Kafka, it will still not entirely solve your issue. For that you would need to look into implementing some form of <a href=""https://en.wikipedia.org/wiki/Two-phase_commit_protocol#Message_flow"" rel=""noreferrer"">two phase commit</a> across your systems.</p>&#xA;&#xA;<p>You can get somewhat close by configuring producer properties, but in the end you will have to chose between <em>at least once</em> or <em>at most once</em> for one of your systems (MariaDB or Kafka).</p>&#xA;&#xA;<p>Let's start with what you can do in Kafka do ensure delivery of a message and further down we'll dive into your options for the overall process flow and what the consequences are.</p>&#xA;&#xA;<p><strong>Guaranteed delivery</strong></p>&#xA;&#xA;<p>You can configure how many brokers have to confirm receipt of your messages, before the request is returned to you with the parameter <em>acks</em>: by setting this to <em>all</em> you tell the broker to wait until all replicas have acknowledged your message before returning an answer to you. This is still no 100% guarantee that your message will not be lost, since it has only been written to the page cache yet and there are theoretical scenarios with a broker failing before it is persisted to disc, where the message might still be lost. But this is as good a guarantee as you are going to get.&#xA;You can further reduce the risk of data loss by lowering the intervall at which brokers force an fsync to disc (<em>emphasized text</em> and/or <em>flush.ms</em>) but please be aware, that these values can bring with them heavy performance penalties.</p>&#xA;&#xA;<p>In addition to these settings you will need to wait for your Kafka producer to return the response for your request to you and check whether an exception occurred. This sort of ties into the second part of your question, so I will go into that further down.&#xA;If the response is clean, you can be as sure as possible that your data got to Kafka and start worrying about MariaDB.</p>&#xA;&#xA;<p>Everything we have covered so far only addresses how to ensure that Kafka got your messages, but you also need to write data into MariaDB, and this can fail as well, which would make it necessary to recall a message you potentially already sent to Kafka - and this you can't do.</p>&#xA;&#xA;<p>So basically you need to choose one system in which you are better able to deal with duplicates/missing values (depending on whether or not you resend partial failures) and that will influence the order you do things in.</p>&#xA;&#xA;<p><strong>Option 1</strong></p>&#xA;&#xA;<p><a href=""https://en.wikipedia.org/wiki/Two-phase_commit_protocol#Message_flow"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NB47L.png"" alt=""Kafka first""></a></p>&#xA;&#xA;<p>In this option you initialize a transaction in MariaDB, then send the message to Kafka, wait for a response and if the send was successful you commit the transaction in MariaDB. Should sending to Kafka fail, you can rollback your transaction in MariaDB and everything is dandy.&#xA;If however, sending to Kafka is successful and your commit to MariaDB fails for some reason, then there is no way of getting back the message from Kafka. So you will either be missing a message in MariaDB or have a duplicate message in Kafka, if you resend everything later on.</p>&#xA;&#xA;<p><strong>Option 2</strong></p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/NB47L.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4OiyF.png"" alt=""MariaDB first""></a></p>&#xA;&#xA;<p>This is pretty much just the other way around, but you are probably better able to delete a message that was written in MariaDB, depending on your data model.</p>&#xA;&#xA;<p>Of course you can mitigate both approaches by keeping track of failed sends and retrying just these later on, but all of that is more of a bandaid on the bigger issue.</p>&#xA;&#xA;<p>Personally I'd go with approach 1, since the chance of a commit failing should be somewhat smaller than the send itself and implement some sort of dupe check on the other side of Kafka.</p>&#xA;&#xA;<hr>&#xA;&#xA;<blockquote>&#xA;  <p>This is related to the previous one: I send the message with:&#xA;  orderSource.output().send(MessageBuilder.withPayload(order).build());&#xA;  This operations is asynchronous and ALWAYS returns true, no matter if&#xA;  the Kafka broker is down. How can I know that the message has reached&#xA;  the Kafka broker?</p>&#xA;</blockquote>&#xA;&#xA;<p>Now first of, I'll admit I am unfamiliar with Spring, so this may not be of use to you, but the following code snippet illustrates one way of checking produce responses for exceptions.&#xA;By calling flush you block until all sends have finished (and either failed or succeeded) and then check the results.</p>&#xA;&#xA;<pre class=""lang-java prettyprint-override""><code>Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(myConfig);&#xA;final ArrayList&lt;Exception&gt; exceptionList = new ArrayList&lt;&gt;();&#xA;&#xA;for(MessageType message : messages){&#xA;  producer.send(new ProducerRecord&lt;String, String&gt;(""myTopic"", message.getKey(), message.getValue()), new Callback() {&#xA;    @Override&#xA;    public void onCompletion(RecordMetadata metadata, Exception exception) {&#xA;      if (exception != null) {&#xA;        exceptionList.add(exception);&#xA;      }&#xA;    }&#xA;  });&#xA;}&#xA;&#xA;producer.flush();&#xA;&#xA;if (!exceptionList.isEmpty()) {&#xA;  // do stuff&#xA;}&#xA;</code></pre>&#xA;"
34752735,34406896,4171405,2016-01-12T20:00:12,"<p>I think you should be careful making a shared library and include it in your microservices because it can give you a point of coupling, meaning that if you have to make changes to the library, you might have to change all your services, and then you loose one of the benefits of this architecture -deploying small services without deploying the whole application.</p>&#xA;&#xA;<p>I think a better approach is to make it a microservice on its own.        </p>&#xA;"
49806459,49709956,3975309,2018-04-12T21:55:21,"<p>I agree with @MichelFloyd, Galaxy is designed for scaling, not running multiple apps on one container - it might face some challenges in scaling all your things for you. So you might need to look at other ways.</p>&#xA;&#xA;<p>If you use AWS or Digital Ocean (or any other similar hosting service), you can run multiple apps on the same box. You can either do this manually, or the excellent npm package <code>mup</code> makes it mostly easy for you. See <a href=""http://meteor-up.com/"" rel=""nofollow noreferrer"">http://meteor-up.com/</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>EASILY DEPLOY YOUR APP </p>&#xA;  &#xA;  <p>Meteor Up is a production quality Meteor app deployment tool.</p>&#xA;  &#xA;  <p>Install with one command: </p>&#xA;  &#xA;  <p>$ npm install --global mup</p>&#xA;</blockquote>&#xA;"
45151605,45146319,453005,2017-07-17T18:52:58,"<p>Assuming that you are unable to solve this problem with infrastructure&#xA;(which is the only correct way to solve this problem),&#xA;there are several (bad) techniques you can use:</p>&#xA;&#xA;<ol>&#xA;<li>IP Address White List - Create a list of good IP Addresses, reject any request from an address not on the list.</li>&#xA;<li>IP Address Zone - Variation of the white list.  Create a list of partial IP Addresses and reject any request that does not match one of the partial addresses.</li>&#xA;<li>Non-Routing IP Only - Variation of IP Address Zone.  Only accept requests from non-routing IP addresses only (these can only be on your local network).  Here is a <a href=""http://en.wikipedia.org/wiki/Reserved_IP_addresses"" rel=""nofollow noreferrer"">Wiki page of non-routing IP addresses</a></li>&#xA;<li>Magic token.  Only accept requests that include a specific token.  This is especially terrible, because somebody can watch your traffic and discover the token.</li>&#xA;</ol>&#xA;&#xA;<p>There are probably other options, but go with Infrastructure.</p>&#xA;"
51593567,51593335,789646,2018-07-30T12:03:37,"<p>I would choose to call the ""books"" microservice in the case ""books"" might need to orchestrate other things also except editing a page - pass this information to some other microservice. &#xA;Otherwise, if there is no orchestration need, it can call directly the exposed API of the ""pages"" microservice.</p>&#xA;"
31416571,26331854,1260174,2015-07-14T20:23:41,"<p>Microservices is more an architectural approach than framework or a set of  libraries. But in .Net world you can create independent process for web services using the OWIN (Open Web Interface for .NET).</p>&#xA;&#xA;<p>You can see an example for implementing a self hosting web service (it do not requires IIS) in:</p>&#xA;&#xA;<p><a href=""http://www.asp.net/web-api/overview/hosting-aspnet-web-api/use-owin-to-self-host-web-api"">http://www.asp.net/web-api/overview/hosting-aspnet-web-api/use-owin-to-self-host-web-api</a> </p>&#xA;&#xA;<p>With that, you can implement your microservices like self-hosting services in one or more machines. The kind of database or event source is up to your application.</p>&#xA;&#xA;<p>Then you can build some API Rest queried by your application, used like a central point of acces, a Facade pattern, every request will be turned to one of your minions-microservices. </p>&#xA;"
43643693,43636544,4275342,2017-04-26T20:11:52,"<p>Of course you do not have to specify port number if you do not need it. Service Fabric will automatically assign port to your service. I also do not define port number because we have 100+ services and it is ""a little bit hard"" to do that</p>&#xA;&#xA;<p>Just omit Port declaration in ServiceManifest.xml</p>&#xA;&#xA;<pre><code>&lt;Resources&gt;&#xA;    &lt;Endpoints&gt;&#xA;      &lt;Endpoint Protocol=""http"" Name=""UserHttpEndpoint"" Type=""Input"" /&gt;&#xA;      &lt;Endpoint Protocol=""tcp"" Name=""UserRpcEndpoint"" Type=""Input"" /&gt;&#xA;    &lt;/Endpoints&gt;&#xA;  &lt;/Resources&gt;&#xA;</code></pre>&#xA;"
31832250,31801057,5193614,2015-08-05T12:15:22,"<p>activiti is based on it database, all process actions implies an update in it. So you can make a micro service which use the Activiti API (ProcessEngine with the jobExecutor to false) and ask the database to find the usertask's status.</p>&#xA;"
39534297,39485766,4210830,2016-09-16T14:44:58,<p>After think about it a lot I came up with the following idea : </p>&#xA;&#xA;<p>all instances of a_service consume the Kafka topic equaly (no balancing) </p>&#xA;&#xA;<p>the results are stored in a  cache (local) </p>&#xA;&#xA;<p>the webclient needs to poll for the result with the UUID. </p>&#xA;&#xA;<p>In the Future I will replace the cache with a redis cluster or a clustered memcache. </p>&#xA;&#xA;<p>An alternativ to polling would be websockets. But this seems to have some downsizes. </p>&#xA;
48465066,48464383,1921678,2018-01-26T16:05:06,"<p>If you need to separate entities (or persistence objects in general) and public interface responses (such as REST etc.) depends a lot on your application design.</p>&#xA;&#xA;<p>If your persistence objects only contain simple, non-sensitive data, it is ok to use them for public interface responses as well. But here are some scenarios that would require separate objects:</p>&#xA;&#xA;<ol>&#xA;<li><p>Your persistence objects contain sensitive data (passwords, encryption keys, etc.) that is meant to be read by the outside. Then you would convert these objects into other types and stripping away that data to hide it from the public interface.</p></li>&#xA;<li><p>You have nested references in your persistence objects (relations between entities etc.) that reach very deep or even can form loops. If you would try to serialize these into a transport format (e.g. JSON), the process will fail because of the loop or your JSON object will be huge. Then it is easier to convert objects and removing the loops by just including IDs or whatever as your nested references.</p></li>&#xA;</ol>&#xA;&#xA;<p>In some applications I developed I even went one step further and settled on three different data object layers. The first layer is the persistence layer, could be entities, YAML or JSON representations or something like that.</p>&#xA;&#xA;<p>Since the application uses a lot of different storage backends, and supports multiple for the same data, I convert these specific objects into general domain objects, which form the second layer.The application itself only works on these domain objects, never touching the persistence objects directly.</p>&#xA;&#xA;<p>The third layer is the decorator layer that I use to hide or simplify information for output on my REST interface. They are used to counter the two scenarios mentioned above.</p>&#xA;"
42217509,42216713,3435918,2017-02-14T03:43:12,"<p>You can do this by passing an environment variable to the docker container when you run it.</p>&#xA;&#xA;<p>something like this:</p>&#xA;&#xA;<pre><code>docker run --name frontend -e MY_APP_BACKEND_IP=""192.168.7.2"" -e MY_APP_BACKEND_PORT=""3000"" ...&#xA;</code></pre>&#xA;&#xA;<p>And on the back-end, let's say you are using NodeJS, you can do:</p>&#xA;&#xA;<pre><code>var backend_ip   = process.env.MY_APP_BACKEND_IP;&#xA;</code></pre>&#xA;&#xA;<p>Note: Not a NodeJS pro, but some googling showed me how to do it</p>&#xA;"
36918179,36570654,6200718,2016-04-28T14:49:38,<p>If you have separate jobs that run pipelines you could just call <code>build [job name]</code> to invoke subsequent pipelines</p>&#xA;
50567138,50563413,7773582,2018-05-28T13:02:08,"<p>You can use different ways for different purposes to reach the service a microservice is offering to you. As mentioned <a href=""https://app.pluralsight.com/player?course=play-by-play-developing-microservices-mobile-apps-jhipster&amp;author=matt-raible&amp;name=c484eea8-eb58-4724-81c2-068b5f5583db&amp;clip=0&amp;mode=live"" rel=""nofollow noreferrer"">here</a> you don't have to use AngularJS or Angular, but you can also use React and/or Ionic.</p>&#xA;&#xA;<p>In general you can use more than one gateway to all or only a choice of microservices per gateway (a very good example implementing different gateways is shown <a href=""https://www.youtube.com/watch?v=JpbGrN21Aho"" rel=""nofollow noreferrer"">here</a>).</p>&#xA;"
36439931,36422243,2091662,2016-04-06T01:35:55,"<p>You can create a login page in your UI app, the login will call the authentication server like:</p>&#xA;&#xA;<pre><code>curl -X POST -vu client:secret http://localhost:8081/spring-security-oauth-server/oauth/token -H ""Accept: application/json"" -d ""password=password&amp;username=user&amp;grant_type=password&amp;scope=read%20write&amp;client_secret=secret&amp;client_id=client""&#xA;</code></pre>&#xA;&#xA;<p>You will receive a access token that will be used to access the endpoints in your resource server like:</p>&#xA;&#xA;<pre><code>curl http://localhost:8081/spring-security-oauth-resource/path/endpoint -H ""Authorization: Bearer &lt;Token&gt;""&#xA;</code></pre>&#xA;"
48251176,48251083,2092686,2018-01-14T15:34:14,"<p>I don't think you should do that. Think of AWS Lambda as a small piece of code which runs when it is triggered, and does its job. You surely can write a RESTful service with Lambda, but you have to define a Lamda for each route and set them up.</p>&#xA;&#xA;<p>I don't think you can do that since after packaging your app the jar file surely will exceed the size limit of a Lambda function.</p>&#xA;&#xA;<p>The simplest solution is to use Elastic Beanstalk. But if you are experienced in administering servers you will find <a href=""https://aws.amazon.com/ec2/"" rel=""nofollow noreferrer"">EC2</a> more useable.</p>&#xA;&#xA;<p>Take a look at this <a href=""https://aws.amazon.com/blogs/devops/deploying-a-spring-boot-application-on-aws-using-aws-elastic-beanstalk/"" rel=""nofollow noreferrer"">Spring Boot deploy to ElasticBeanstalk</a></p>&#xA;&#xA;<p>This <a href=""http://carlmartensen.com/step-by-step-guide-to-building-and-deploying-a-database-backed-spring-boot-web-application-in-aws"" rel=""nofollow noreferrer"">tutorial</a> might be helpful as well</p>&#xA;"
47077453,47077380,587406,2017-11-02T14:14:41,"<p>No...your frontend app should talk to Zuul which is already registered to Eureka and can query it to find the right backend server you need.  </p>&#xA;&#xA;<p>I <a href=""https://luizkowalski.net/netflix-oss-a-beginners-guide-pt3/"" rel=""nofollow noreferrer"">wrote</a> about it, hope it helps</p>&#xA;"
37679792,37679132,587406,2016-06-07T12:46:10,"<p>I think that, since you are using a static port number on your <code>AccountsServer</code>, Eureka will only register one instance...<br>&#xA;You can do something like this on the clients</p>&#xA;&#xA;<pre><code>server:&#xA;  compression.enabled: true&#xA;  port: 0&#xA;</code></pre>&#xA;&#xA;<p>(in this case, I'm using YAML to configure, not properties)</p>&#xA;&#xA;<p>on the gateway, you should specify that you want a load balancer, like that   </p>&#xA;&#xA;<pre><code> @Autowired&#xA; private LoadBalancerClient loadBalancer;&#xA; ....&#xA; private String getUrl(){&#xA;    return loadBalancer.choose(""your-service-name"").getUri().toString();&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>I've done something similar, if you are interested you can take a look <a href=""https://github.com/luizkowalski/microservices-netflix-sample"" rel=""nofollow"">here</a> </p>&#xA;"
45561287,33881958,7150223,2017-08-08T06:55:52,<p>Its quit simple. </p>&#xA;&#xA;<ol>&#xA;<li>Create a Config Class whitch is annotated with @Configutration</li>&#xA;<li>In this Class create a Bean Of type OAuth2ProtectedResourceDetails (interface) and create a ClientCredentialsResourceDetails Object in that method. Add your values to it and return it. </li>&#xA;<li>Create a second Bean of type OAuth2RestTemplate in the Configuration Class and create in that method a DefaultOAuth2ClientContext object with the default Constructor. Than create a OAuth2RestTemplate and add the OAuth2ProtectedResourceDetails bean and the DefaultOAuth2ClientContext object to it. Subsequently return the OAuth2RestTemplate Object.</li>&#xA;<li>Add it with @Autowired in your Controller Classes and Service implementations to use it. </li>&#xA;</ol>&#xA;
46658200,46643268,5775247,2017-10-10T03:31:34,"<p>There are a couple of technologies for service registration and discovery. Please see if the following articles help:</p>&#xA;&#xA;<ol>&#xA;<li><a href=""https://stackshare.io/stackups/consul-vs-eureka-vs-zookeeper"" rel=""nofollow noreferrer"">StackShare's comparison of Consul vs. ZooKeeper vs. Eureka</a></li>&#xA;<li><a href=""http://daviddawson.me/getting/started/with/microservices/2015/06/10/service-discovery-overview.html"" rel=""nofollow noreferrer"">A nice paper for service-discovery and guide on how to make the choice</a></li>&#xA;</ol>&#xA;"
50528478,50527363,9774735,2018-05-25T11:45:07,"<p>The problem is that I am using same model class (<code>Review.class</code>) with different microservices and this not allow me to execute request. </p>&#xA;&#xA;<p>Add method could not recognize <code>review</code> because I pass it from another project. </p>&#xA;&#xA;<p>After my recode it looks like that:</p>&#xA;&#xA;<pre><code>@RequestMapping(value = ""/addreview"", method = RequestMethod.PUT, produces = MediaType.APPLICATION_JSON_VALUE)&#xA;public ResponseEntity&lt;?&gt; createReviewForMovie(@RequestBody Review review) {&#xA;    Map&lt;String, String&gt; uriVariables = new HashMap&lt;String, String&gt;();&#xA;    uriVariables.put(""reviewContent"", review.getReviewContent());&#xA;    uriVariables.put(""userName"", review.getUserName());&#xA;&#xA;    ResponseEntity&lt;Boolean&gt; response = new RestTemplate().getForEntity(""http://localhost:9100/add/{reviewContent}/{userName}"", &#xA;            Boolean.class, uriVariables);&#xA;&#xA;    Boolean resultReview = response.getBody();&#xA;&#xA;    return new ResponseEntity&lt;Boolean&gt;(resultReview, HttpStatus.OK);&#xA;}&#xA;</code></pre>&#xA;&#xA;<hr>&#xA;&#xA;<pre><code>@RequestMapping(value = ""/add/{reviewContent}/{userName}"", method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE)&#xA;public ResponseEntity&lt;?&gt; add(@PathVariable(""reviewContent"") String reviewContent, @PathVariable(""userName"") String userName){&#xA;    if(reviewContent.length()&lt;10){&#xA;        return new ResponseEntity&lt;Boolean&gt;(false, HttpStatus.OK);&#xA;    }else if(userName.length()&gt;15){&#xA;        return new ResponseEntity&lt;Boolean&gt;(false, HttpStatus.OK);&#xA;    }&#xA;    return new ResponseEntity&lt;Boolean&gt;(true, HttpStatus.OK);&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>However I am still curious if there is any other way to use <code>Review.class</code>.</p>&#xA;"
49463333,49377817,8858255,2018-03-24T09:51:38,"<p>For the ‘API gateway’ front-end authentication you could use <a href=""https://developers.google.com/identity/protocols/OAuth2"" rel=""nofollow noreferrer"">OATH2</a> and for the back-end part you can use <a href=""http://openid.net/connect/"" rel=""nofollow noreferrer"">OpenID connect</a> which will allow you to use a key value that is relevant to the user, like for example a uuid and use this to set access control at the Microservice level, behind the API Gateway.</p>&#xA;&#xA;<p>You can find in the next link further information about <a href=""https://developers.google.com/identity/protocols/OpenIDConnect"" rel=""nofollow noreferrer"">OpenID connect</a> authentication.</p>&#xA;"
37451750,37427976,6384271,2016-05-26T04:41:35,"<p>The ref-app 'application' consists of several 'apps' and Predix 'services'. An app is bound to the service via an entry in the manifest.yml.  Thus, it gets the service endpoint and other important configuration information via this binding.  When an app is bound to a service, the 'cf env ' command returns the needed info.</p>&#xA;&#xA;<p>There might still be some Service endpoint info in a property file, but that's something that will be refactored out over time.</p>&#xA;&#xA;<p>The individual apps of the ref-app application are put in separate microservices, since they get used as components of other applications.  Hence, the microservices approach.  If there were startup dependencies across apps, the CI/CD pipeline that pushes the apps to the cloud would need to manage these dependencies. The dependencies in ref-app are simply the obvious ones, read-on.</p>&#xA;&#xA;<p>While it's true that coupling of microservices is not in the design.  There are some obvious reasons this might happen.  Language and function.  If you have a ""back-end"" microservice written in Java used by a ""front-end"" UI microservice written in Javascript on NodeJS then these are pushed as two separate apps.  Theoretically the UI won't work too well without the back-end, but there is a plan to actually make that happen with some canned JSON.  Still there is some logical coupling there.</p>&#xA;&#xA;<p>The nice things you get from microservices is that they might need to scale differently and cloud foundry makes that quite easy with the 'cf scale' command.  They might be used by multiple other microservices, hence creating new scale requirements.  So, thinking about what needs to scale and also the release cycle of the functionality helps in deciding what comprises a microservice.</p>&#xA;&#xA;<p>As for ordering, for example, the Google Maps api might be required by your application so it could be said that it should be launched first and your application second.  But in reality, your application should take in to account that the maps api might be down.  Your goal should be that your app behaves well when a dependent microservice is not available.</p>&#xA;&#xA;<p>The 'apps' of the 'application' know about each due to their name and the URL that the cloud gives it.  There are actually many copies of the reference app running in various clouds and spaces.  They are prefaced with things like Dev or QA or Integration, etc.  Could we get the Dev front end talking to the QA back-end microservice, sure, it's just a URL.  </p>&#xA;&#xA;<p>In addition to the aforementioned, etcd (which I haven't tried yet), you can also create a CUPS service 'definition'.  This is also a set of key/value pairs.  Which you can tie to the Space (dev/qa/stage/prod) and bind them via the manifest.  This way you get the props from the environment.</p>&#xA;"
49141606,35065875,388751,2018-03-06T23:18:11,"<p>Looking at the current solutions with gRPC over web, here is what's available out there at the time of writing this (and what I found):</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://github.com/improbable-eng/grpc-web"" rel=""nofollow noreferrer"">gRPC-web</a>: requires TypeScript for client</li>&#xA;<li><a href=""https://github.com/improbable-eng/grpc-web/tree/master/go/grpcwebproxy"" rel=""nofollow noreferrer"">gRPC-web-proxy</a>: requires Go</li>&#xA;<li><a href=""https://github.com/grpc-ecosystem/grpc-gateway"" rel=""nofollow noreferrer"">gRPC-gateway</a>: requires .proto modification and decorations</li>&#xA;<li><a href=""https://github.com/gabrielgrant/grpc-bus-websocket-proxy-server"" rel=""nofollow noreferrer"">gRPC-bus-websocket-proxy-server</a>: <s>as of writing this document it lacks tests and seems abandoned</s> (edit: look at the comments by the original author!)</li>&#xA;<li><a href=""https://github.com/konsumer/grpc-dynamic-gateway"" rel=""nofollow noreferrer"">gRPC-dynamic-gateway</a>: a bit of an overkill for simple gRPC services and authentication is awkward</li>&#xA;<li><a href=""https://github.com/paralin/grpc-bus"" rel=""nofollow noreferrer"">gRPC-bus</a>: requires something for the transport</li>&#xA;</ul>&#xA;&#xA;<p>I also want to shamelessly plug my own solution which I wrote for my company and it's being used in production to proxy requests to a gRPC service that only includes unary and server streaming calls:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://github.com/sepehr-laal/grpc-express"" rel=""nofollow noreferrer"">gRPC-express</a></li>&#xA;</ul>&#xA;&#xA;<p>Every inch of the code is covered by tests. It's an Express middleware so it needs no additional modifications to your gRPC setup. You can also delegate HTTP authentication to Express (e.g with Passport).</p>&#xA;"
51089762,51087521,1645712,2018-06-28T19:05:47,"<p>There are several approaches each having its pros and cons</p>&#xA;&#xA;<ul>&#xA;<li><p>what are you doing is simple authentication, looks simple, calls are done on behalve of a user, but you need to pass the user credentials and you have no simple meams to validate of the app2's session is still valid or not</p></li>&#xA;<li><p>another approach is using a token (e. g. jwt token) what app2 could validate without passing the user credentials or session cookie (token can be signed by app1 or an identity provider), or using oauth token where each app could validate the token with a common IdP (identity provider) </p></li>&#xA;<li><p>app1 could authenticate by its own application credentials (e. g. when app2 is behind an api gateway) </p></li>&#xA;</ul>&#xA;&#xA;<p>so there are multiple option, you could choose one to fit your environment and long time solution</p>&#xA;"
49853646,49848942,1645712,2018-04-16T09:15:33,"<p>The WSO2AM (API Manager) authorizes the clients using the OAuth protocol and the backend services should trust the WSO2AM providing service authorization.</p>&#xA;&#xA;<p>The API MAnager is not able (by default) validate the tokens of your backend services.</p>&#xA;&#xA;<p>As far I know WSO2 AM clears the ""Authorization"" header to the backend services. (correct me when I am wrong).</p>&#xA;&#xA;<p>your options:</p>&#xA;&#xA;<ol>&#xA;<li><p>Setup a proper environment, where APIM is used to authorize users. The APIM can send a JWT token to the backend services with user identity and attributes and the backend service will validate and trust the APIM JWT token containing user identity and attributes. I really suggest you stick to the way how APIM works and not try to force it working other way</p></li>&#xA;<li><p>If you really must using your own OAuth tokens, you could send the authorization token in different header (which will not get cleared)</p></li>&#xA;<li><p>You could create a custom mediation flow to to re-enter the authorization header into the request (I am not sure if you will need to update the exposed api mediation flow too or not to skip the default authorizer). </p></li>&#xA;</ol>&#xA;"
47066688,44392569,4926096,2017-11-02T02:35:46,<p>The docker team indicated recently they are very interested in looking at istio and docker swarm integration so stay tuned this may happen in the next few quarters before you know it :)</p>&#xA;
48887041,48863771,4473232,2018-02-20T14:03:08,"<p>In case where eureka server is not used to identify whether server is reachable or not , ribbon client uses ""RibbonConfiguration"" - pingUrl parameter.&#xA;By Default it is empty string that means it is going to ping list of server without any context to get an answer whether server is reachable or not.&#xA;So here you can do 2 things.</p>&#xA;&#xA;<ol>&#xA;<li><p>Create a service that is bound to root context of the server ""/"" and send a positive response.</p>&#xA;&#xA;<pre><code>@RequestMapping(value = ""/"")&#xA;public String status(HttpServletRequest request) {&#xA;    return """";&#xA;}&#xA;</code></pre></li>&#xA;<li><p>Or update Ribbon client Configuration (EmployeeConfiguration) and return PingUrl with relevant ""service-name"".</p>&#xA;&#xA;<pre><code>@Bean&#xA;public IPing ribbonPing(IClientConfig config) {&#xA;   return new PingUrl(false,""/employees"");&#xA;}&#xA;</code></pre></li>&#xA;</ol>&#xA;&#xA;<p>Go for the first one as it will solve the basic purpose to see if server is reachable or not.</p>&#xA;&#xA;<p>Ref: <a href=""https://spring.io/guides/gs/client-side-load-balancing/"" rel=""nofollow noreferrer"">https://spring.io/guides/gs/client-side-load-balancing/</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>Our IPing is a PingUrl, which will ping a URL to check the status of each server. Say Hello has, as you’ll recall, a method mapped to the / path; that means that Ribbon will get an HTTP 200 response when it pings a running Say Hello server. The IRule we set up, the AvailabilityFilteringRule, will use Ribbon’s built-in circuit breaker functionality to filter out any servers in an “open-circuit” state: if a ping fails to connect to a given server, or if it gets a read failure for the server, Ribbon will consider that server “dead” until it begins to respond normally.</p>&#xA;</blockquote>&#xA;"
40877368,40876048,1397648,2016-11-29T23:01:16,"<p>I guess there is no 'right' way necessarily. Here is how I would approach it.</p>&#xA;&#xA;<p>Based on :</p>&#xA;&#xA;<blockquote>&#xA;  <p>However, I could design it so that when a user is associated to a&#xA;  group, it disassociates it from a group that it was previously&#xA;  associated with.</p>&#xA;</blockquote>&#xA;&#xA;<pre><code>/groups/{id}/users     GET, POST, DELETE&#xA;                /{id}  GET, DELETE&#xA;</code></pre>&#xA;&#xA;<p>This endpoint would cause some issues in that the <code>Id</code> would change and you would therefore have to change the url you call after a user switches to a different group. So consecutive calls need to go to a new endpoint. </p>&#xA;&#xA;<p>For example : <code>/groups/1/users/1</code> could be valid one moment, but when the user moves groups <code>/groups/1/users/1</code> will no longer return a result. It would now become <code>/groups/2/users/1</code></p>&#xA;&#xA;<p>Whereas with:</p>&#xA;&#xA;<pre><code>/users/{id}/group      GET, POST, DELETE&#xA;</code></pre>&#xA;&#xA;<p>The endpoint won't change if the user switches groups, which keeps it cleaner and clearer in my opinion. The user is still the same and it's clear that the group is associated with this user, but it doesn't matter what <code>group</code> is being referred to as it will always return the associated <code>group</code> for the <code>user</code> with that <code>id</code></p>&#xA;&#xA;<p>I guess the bottom line is that the endpoint should reflect what you are trying to achieve, if you are getting the group for a user :</p>&#xA;&#xA;<pre><code>/users/{id}/group      GET, POST, DELETE&#xA;</code></pre>&#xA;&#xA;<p>or if you are getting all users for a group:</p>&#xA;&#xA;<pre><code>/groups/{id}/users     GET, POST, DELETE&#xA;</code></pre>&#xA;"
32373491,32369075,2385808,2015-09-03T10:47:05,"<p>In general the two concepts are <strong>not compatible</strong>. Using a universal data model for all of your services would clash with a couple of key ideas behind using Microservices, e.g. <a href=""http://martinfowler.com/bliki/PolyglotPersistence.html"" rel=""nofollow"">Polyglot Persistence</a>, separate development &amp; deployment of each service. Also, let's not forget that the ""Data Model Resource Book"" was last updated in 2009.</p>&#xA;&#xA;<p>However, <strong>if you must</strong> combine the two approaches, e.g. because management insists on it, you can encapsulate all access to the universal data model by a dedicated service and make your other services dependent on it.</p>&#xA;&#xA;<p>Some good thoughts on the subject can be found here: <a href=""http://plainoldobjects.com/2015/09/02/does-each-microservice-really-need-its-own-database-2/"" rel=""nofollow"">http://plainoldobjects.com/2015/09/02/does-each-microservice-really-need-its-own-database-2/</a></p>&#xA;"
38445452,38440876,559504,2016-07-18T20:27:10,<p>Nothing out of the box in compose to help you with this. You're probably looking at scripting a thin layer / cli on top of this if you definitely want to go down this route.</p>&#xA;
28139969,26616962,559504,2015-01-25T18:37:05,"<p>Components in a system use ""pipes"" (HTTP/S, queues, etc...) to communicate with each other. Usually these pipes flow through an ESB (Enterprise Service Bus) which does a number of things to the messages being passed between components. </p>&#xA;&#xA;<p>It might do:</p>&#xA;&#xA;<ul>&#xA;<li>Security checks</li>&#xA;<li>Routing</li>&#xA;<li>Business flow / validation</li>&#xA;<li>Transformation</li>&#xA;</ul>&#xA;&#xA;<p>Once it's completed these tasks the message will be forwarded onto the ""endpoint"" component. This is an example of ""smart pipes"" as lots of logic and processing reside inside the ESB (part of the system of ""pipes""). The endpoints can then be ""dumb"" as the ESB has done all the work.</p>&#xA;&#xA;<p>""Smart endpoints and dumb pipes"" advocates the opposite scenario. That the lanes of communication should be stripped of business processing and logic and should literally only distribute messages between components. It's then the components themselves that do processing / logic / validation etc... on those messages.</p>&#xA;"
50627628,50627087,180368,2018-05-31T15:16:11,"<p>If you have a service that stores and provides information about users then other services that need that information should communicate with the User service to get it.  That is, presumably, the reason the User service exists in the first place.</p>&#xA;&#xA;<p>Depending on the volatility of the users list and requirements for changes there to be respected in the Posts service you might consider some short-term caching in the Posts service, but I certainly wouldn't persist another copy of the user list there.</p>&#xA;"
39841944,39839881,6736784,2016-10-03T23:26:55,"<p>The way I see this is, if you have 2 different services trusting the same IDP(STS) then you can have service A requesting a token and then you can pass the same token to service B and let service B validate the token again. It is ok to pass the token as you dont want user to log in again for another service call.</p>&#xA;&#xA;<p>Also, it depends on the way you have services setup. If service A needs some data from service B to provide data back to the user then we should pass the same user token. I dont think that services should have their access token. </p>&#xA;&#xA;<p>Using user token really helps in identifying claims and data access at each service levels. So it is always better to pass the user token and let each service validate the token before sending the data out to the user. </p>&#xA;"
51457768,45099677,3921040,2018-07-21T15:54:40,<p>You can run multiple instance of zuul behind a load balancer. </p>&#xA;
51157985,51138147,65681,2018-07-03T15:16:03,"<p>If you want to do this, you'll need to <a href=""https://github.com/jhipster/jhipster-registry/releases"" rel=""nofollow noreferrer"">download the JHipster Registry</a>. Then run it with <code>java -jar jhipster-registry-*.war</code>. Then start your gateway and microservice apps as you normally would (with Maven/Gradle, or your IDE).</p>&#xA;"
34973901,34973165,1989009,2016-01-24T09:01:57,"<p>One possible solution is building each microservice as few packages and handle versioning by SBT:</p>&#xA;&#xA;<ul>&#xA;<li><strong>model</strong> - set of classes divided between microservices</li>&#xA;<li><strong>core</strong> -  microservice logic, depends on model package</li>&#xA;<li><strong>gateway</strong> - interface to main microservice, depends on model package, included by other microservices, it is possible to cache some info here</li>&#xA;</ul>&#xA;"
46609308,46600671,1794961,2017-10-06T15:38:10,"<p>Docker Swarm routes incoming requests based on the published port, you can't have two applications with the same port number in a single Swarm.</p>&#xA;"
44129954,44129347,6911633,2017-05-23T08:45:01,"<p>1) You can create coordinating service that will schedule tasks for senders using persistent storage like database table. This service will add send job records into table and sender services will scan table in a loop get job, mark it as processing so other instances will not get the same job.</p>&#xA;&#xA;<p>2) You can use queue like Azure ServiceBus to send jobs from coordinating service. </p>&#xA;&#xA;<p>Also if you are using micro services I will suggest to separate sending services by transport so you can scale them separately.  </p>&#xA;&#xA;<p>I can see next structure:</p>&#xA;&#xA;<p>NotificationSenderService - send coordinator you usually need only one instance of this. The responsibility of this service is to receive send notification request and create job using queue or database</p>&#xA;&#xA;<p>EmailNotificationService, SMSNotificationService, PuthNotificationService - actual senders. You can run as many instances of each as you need. They need to have access to database or queue of NotificationSenderService.</p>&#xA;"
44738367,41655915,818321,2017-06-24T16:09:59,"<p>I have been wondering the same thing.&#xA;Apparently, there are a number of ways to deal with atomicity of updating the db and publishing the corresponding event.<br/></p>&#xA;&#xA;<blockquote>&#xA;  <ol>&#xA;  <li><a href=""http://microservices.io/patterns/data/event-sourcing.html"" rel=""nofollow noreferrer"">Event sourcing</a></li>&#xA;  <li><a href=""http://microservices.io/patterns/data/application-events.html"" rel=""nofollow noreferrer"">Application events</a></li>&#xA;  <li><a href=""http://microservices.io/patterns/data/database-triggers.html"" rel=""nofollow noreferrer"">Database triggers </a></li>&#xA;  <li><a href=""http://microservices.io/patterns/data/transaction-log-tailing.html"" rel=""nofollow noreferrer"">Transaction log tailing</a></li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p>(<a href=""http://microservices.io/patterns/data/event-driven-architecture.html"" rel=""nofollow noreferrer"">Pattern: Event-driven architecture</a>)</p>&#xA;&#xA;<p>The <a href=""http://microservices.io/patterns/data/application-events.html"" rel=""nofollow noreferrer"">Application events</a> pattern sounds similar to your ideas.<br/>&#xA;An example could be:</p>&#xA;&#xA;<blockquote>&#xA;  <p>The Order Service inserts a row into the ORDER table and inserts an&#xA;  Order Created event into the EVENT table [in the scope of a single local db transaction].<br/>The Event Publisher thread&#xA;  or process queries the EVENT table for unpublished events, publishes&#xA;  the events, and then updates the EVENT table to mark the events as&#xA;  published.</p>&#xA;</blockquote>&#xA;&#xA;<p>(<a href=""https://www.nginx.com/blog/event-driven-data-management-microservices/"" rel=""nofollow noreferrer"">Event-Driven Data Management for Microservices</a>)</p>&#xA;&#xA;<p>If at any point the Event Publisher crashes or otherwise fails, the events it did not process are still marked as unpublished.<br/>&#xA;So when the Event Publisher comes back online it will immediately publish those events.<br/></p>&#xA;&#xA;<p>If the Event Publisher published an event and then crashed before marking the event as published, the event might be published more than once.<br/>&#xA;For this reason, it is important that subscribers de-duplicate received messages.</p>&#xA;&#xA;<p>Additionally, <a href=""https://stackoverflow.com/a/13490358/818321"">the answer</a> to a stackoverflow question — that might sound quite different to yours but in essence is asking the same thing — links to a couple of relevant blog posts.</p>&#xA;&#xA;<ul>&#xA;<li><a href=""http://blog.jonathanoliver.com/removing-2pc-two-phase-commit/"" rel=""nofollow noreferrer"">Removing 2PC (Two Phase Commit)</a> </li>&#xA;<li><a href=""http://blog.jonathanoliver.com/idempotency-patterns/"" rel=""nofollow noreferrer"">Idempotency Patterns</a></li>&#xA;<li><a href=""https://blogs.msdn.microsoft.com/clemensv/2012/07/30/transactions-in-windows-azure-with-service-bus-an-email-discussion/"" rel=""nofollow noreferrer"">Transactions in Windows Azure (with Service Bus)</a> [See: <strong>Q: What are the chances that you will build something that will support at least transactional handoffs between Service Bus the Azure SQL database?</strong>]</li>&#xA;</ul>&#xA;"
47535175,47515021,6261137,2017-11-28T15:28:01,"<p>I think you should to this.</p>&#xA;&#xA;<pre><code>declare var d3: any;&#xA;</code></pre>&#xA;&#xA;<p>I tested the method bellow and it works perfect but I dint install d3 using yarn, npm or any package dependency manager. I just used th CDN link that included in my index.html file.</p>&#xA;&#xA;<p>But as you installed using something like yarn i think you can do this.</p>&#xA;&#xA;<pre><code>import * as d3 from 'd3';&#xA;</code></pre>&#xA;"
40846729,40840470,2461073,2016-11-28T14:31:07,"<p>Using a guid is my prepared approach, the guids would be generated by the creator not the database, moreover, I would not recommend using referential integrity in a distributed system's persistence for transactional data as this will reduce the ability to scale out your component. </p>&#xA;&#xA;<p>Given eventual consistency and distribution of code to separate processes, your transactional data has no meaningful state to the outside world, the meaningful business state should be written to a view model data store once it is in a meaningful state (done creating a user, done verifying a user and so on).</p>&#xA;&#xA;<p>Make sense?</p>&#xA;"
38597492,38554037,2461073,2016-07-26T18:36:01,"<p>Microservices help solve the issue of coupling, to achieve that you need to keep your services/components autonomous, which means they can't share anything, not code assemblies or any resources, <strong>especially not the database.</strong></p>&#xA;&#xA;<p>If you need a read only copy of the data you can use patterns like publish subscribe to save a local copy of some information your component needs (subscribe to an event like <code>NewUserCreatedEvent</code>: new user was created and his id is this guid for example), there should be only one owner of the user data in the system and the owner is the only one who can modify the state of the data he owns, the rest of the components in the system can keep a local copy of reference data for read only purposes.</p>&#xA;&#xA;<blockquote>&#xA;  <p>this asset gets stored on a DB and a mongoID is returned. Then, using&#xA;  another protocol and the ID, there are calls to the socket server that&#xA;  needs to check that validity of that ID, thus, needs to read from DB</p>&#xA;</blockquote>&#xA;&#xA;<p>In your scenario the client that creates the user should provide the id (a guid), the component that owns the user (therefore it's the one that creates it) will publish an event the other component (service) subscribes to and stores the data it needs in it's own database.</p>&#xA;&#xA;<p>Does that make sense?</p>&#xA;"
38674999,38629237,2461073,2016-07-30T14:27:50,"<p>If I get your description correctly, it looks like the code above is in a 3rd service OR a shared code?</p>&#xA;&#xA;<p>When building autonomous components, they should not share resources or code, as this will defeat the basic idea of loose coupling...</p>&#xA;&#xA;<p>(micro)Service A (component A) should only modify and own the data in DB A&#xA;and the same for B</p>&#xA;&#xA;<p>Once A had done his task, it should raise an event B is subscribed to, and B will handle the event and do his work.</p>&#xA;&#xA;<p>If any of them fail they can raise an event like <code>xOperationFailed</code> (i.e. <code>CreatUserAccountFailed</code>) and the interested components subscribed to that event will take action according to the business logic (roll back, suspend, send an email to the user or to an admin to take corrective actions). Same goes for success (they can raise a success event).</p>&#xA;&#xA;<p>The hard part is to define the boundaries (responsibilities and data) of each component...</p>&#xA;&#xA;<p>Does that make sense?</p>&#xA;"
39617032,39591223,2461073,2016-09-21T12:48:50,"<p>A lot of people relate to Microservices as SOA 2.0, so If I get your question correctly, you definitely can use both architectures in one project...</p>&#xA;"
49361788,49349235,2461073,2018-03-19T11:23:48,"<p>To add to what was said by @Anunay and @Mohamed Abdul Jawad</p>&#xA;&#xA;<p>I'd consider writing the state from the units of work in your pipeline to a view (table/cache(insert only)), you can use messaging or simply insert a row into that view and have the readers of the state pick up the correct state based on some logic (date or state or a composite key). as this view is not really owned by any domain service it can be available to any readers (read-only) to consume...</p>&#xA;"
43306723,43125393,2461073,2017-04-09T12:32:25,"<p>Will this help? <a href=""https://docs.particular.net/nservicebus/architecture"" rel=""nofollow noreferrer"">https://docs.particular.net/nservicebus/architecture</a></p>&#xA;&#xA;<p>This is  NServisBus specific, but it can give you some direction </p>&#xA;"
44882589,44870461,2461073,2017-07-03T09:53:53,"<p>When distributing your code to achieve reduced coupling, you want to avoid resource sharing, and data is a resource you want to to avoid sharing.</p>&#xA;&#xA;<p>Another point is that only one component in your system owns the data (for state changing operations), other components can READ but NOT WRITE, they can have copies of the data or you can share a view model they can use to get the latest state of an object.</p>&#xA;&#xA;<p>Introducing referential integrity will reintroduce coupling, instead you want to use something like Guids for your primary keys, they will be created by the creator of the object, the rest is all about managing eventual consistency.</p>&#xA;&#xA;<p>Take a look at Udi Dahan's <a href=""https://channel9.msdn.com/Events/NDC/NDC-Oslo-2017/BRK03"" rel=""noreferrer"">talk in NDC Oslo for a more details</a></p>&#xA;&#xA;<p>Hope this helps</p>&#xA;"
40403140,40377377,2461073,2016-11-03T13:45:47,"<p>Having microservices do <strong>synchronous</strong> communication with each other is a risk, the big issue is coupling, it means the services are now coupled to each other, if one of them fails it's dependents will now fully or partially disabled/crash, a better solution would be to use asynchronous communication for state changing operations.</p>&#xA;&#xA;<p>You want to make a clear distinction between state changing operations and read operations (CQS Command Query Separation). For state changing operations you can use some kind of messaging infrastructure and go for fire and forget communication. For queries you would use Synchronous request response communication and could use an http API or just go directly to your data store.</p>&#xA;&#xA;<p>If you are using messaging then you can also look at publish subscribe for raising events between services.</p>&#xA;&#xA;<p>Another point to consider is (transactional) data sharing (as opposed to read only views) if you expose your internal state the reader might get the wrong state of your data, or the wrong version, and also will potentially lock your data?</p>&#xA;&#xA;<p>Last but not least, try to do everything you can to keep your services autonomous (at least at the logical level).</p>&#xA;&#xA;<p>Hope this makes sense.</p>&#xA;"
40704769,40645778,2461073,2016-11-20T13:48:22,"<p>I see a fundamental issue with this statement:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Service A can only perform its full functionality if it receives response from B and C</p>&#xA;</blockquote>&#xA;&#xA;<p>If your components (services) are not autonomous you have a serious flaw in the initial design. When decomposing a system you want to end up with a logical boundary that allows each ""Service""/""Component"" autonomous and explicitly a technical authority of the part of the system it owns.</p>&#xA;&#xA;<p>So <code>Service A</code> will do it's job and publish an event that <code>Service B</code> subscribes to and will do it's bit and then <code>B</code> will publish an event that <code>C</code> will subscribe to an ddo it's portion of the business process.</p>&#xA;&#xA;<p>If they can do the work in parallel then the creator (the client who send the initial commands with the data) can send the command directly to each component and you might have a component tracking the business process state by subscribing to events the component would publish when they complete their units of work.</p>&#xA;&#xA;<p>The actual design is very context dependent so it would be hard to decompose the domain without the specific details.</p>&#xA;&#xA;<p>Does that make sense?</p>&#xA;"
38978296,38966184,2461073,2016-08-16T15:00:29,"<p>Your question is not complete, but i can attempt to guess the question from the title...</p>&#xA;&#xA;<p>Each Component should have it's own business logic, components (or microservices) need to be totally autonomous, sharing nothing, no code no data nothing.</p>&#xA;&#xA;<p>Components can raise events (using messaging) to communicate the fact something append.</p>&#xA;&#xA;<p>Make sense?</p>&#xA;&#xA;<p>Here is an <a href=""http://docs.particular.net/samples/show-case/on-premise/"" rel=""nofollow"">example in .net using NServiceBus</a> </p>&#xA;"
42055432,42053559,2461073,2017-02-05T18:04:10,"<p>Adding to what @Gordon Linoff said, assuming durable messaging (something like MSMQ?) the method/handler is going to be transactional, so if it's all successful, the message will be written to the queue and the data to your view model, if it fails, all will fail...</p>&#xA;&#xA;<p>To mitigate the ID issue you will need to use GUIDs instead of DB generated keys (if you are using messaging you will need to remove your referential integrity anyway and introduce GUIDS as keys).</p>&#xA;&#xA;<p>One more suggestion, don't update the database, but inset only/upsert (the pending row and then the completed row) and have the reader do the projection of the data based on the latest row (for example) </p>&#xA;"
42121125,41096759,2461073,2017-02-08T18:41:05,"<p>Vertical slicing and domain decomposition leads to having each vertical slice have it's own well defined fields (not entities) that belong together (bounded context/business functions), defining a logical service boundary and decomposing the Service Boundary to Business Components and Autonomous Components (smallest unit of work). </p>&#xA;&#xA;<p>Each component owns the data it modifies and is the only one in the system that can change the state of that data, you can have many copies of the data for readers but only one logical writer.</p>&#xA;&#xA;<p>There for it makes more sense not to share these data models as they are internal to the component responsible for the data model's state (and is encapsulated).</p>&#xA;&#xA;<p>Data readers can use view models and these are dictated more by the consumer then the producer, the view/read models hold the ""real""/""current"" state of the transactional data (which is private to the data modifier), read data can be updated by the data processor after any logical state change.</p>&#xA;&#xA;<p>So it makes more sense to publish view/read models for read only consumers...</p>&#xA;&#xA;<p>Check out <a href=""https://www.youtube.com/watch?v=MTArpO7rSQE"" rel=""nofollow noreferrer"">Udi Dahan's video</a></p>&#xA;&#xA;<p>Does that make sense? </p>&#xA;"
39977883,39967784,2461073,2016-10-11T13:02:48,"<p>I think that this is a matter of interpretation:</p>&#xA;&#xA;<p>I'd argue that in SOA a <code>service</code> is not a physical process (windows service/app domain) but a logical boundary... and the same rules apply in SOA and Microservices in regards to the smallest autonomous component. they own (the technical authority and data owners meaning they are the only one component that can change the state of that piece of data) of a collection of one or more domain properties/fields.</p>&#xA;&#xA;<p>Now at runtime, I'd argue that if you don't need to distribute you process, then you can deploy them all in the same process (later when you need to scale, distribute your components to achieve better performance)...</p>&#xA;&#xA;<p>Make sense?</p>&#xA;"
40069723,40058568,2461073,2016-10-16T11:28:52,"<p>When building a distributed system (SOA/Microservices), you would use the CQS model (Command Query Separation), this can be translated to 2 different channels, one for commands and events, for state changes(asynchronous messaging) and another for queries, read only data models (synchronous reads).</p>&#xA;&#xA;<p>After you get that working you can look at creating view models that reflect the business state of your data (not your transient OLTP data) with a combination of pub/sub for data distribution (not full data sets but context data - i.e. product price change)</p>&#xA;&#xA;<p>And yes it makes things more complicated, but helps reduce coupling which helps you build better systems</p>&#xA;&#xA;<p>Make sense?</p>&#xA;"
50293294,50276356,2461073,2018-05-11T13:21:34,"<p>You can try NServiceBus: <a href=""https://docs.particular.net/nservicebus/sagas/"" rel=""nofollow noreferrer"">https://docs.particular.net/nservicebus/sagas/</a></p>&#xA;&#xA;<p>One of the core features in NServiceBus is Sagas among others.</p>&#xA;&#xA;<p>(Disclaimer: I work for Particular and am an NServiceBus user for many years)</p>&#xA;"
39774675,39617171,2461073,2016-09-29T15:43:23,"<blockquote>&#xA;  <p>He emphasises the need for asynchronous communication between&#xA;  miroservices, but says that APIs for external clients sometimes need&#xA;  to be synchronous (often REST).</p>&#xA;</blockquote>&#xA;&#xA;<p>When dealing with client - backend communications you can have a couple of types of operations and they should be handled seperetly (look at the idea of <a href=""http://martinfowler.com/bliki/CommandQuerySeparation.html"" rel=""nofollow"">CQS</a>):</p>&#xA;&#xA;<ul>&#xA;<li><p>State changing operations - they should be one way fire and forget using messaging (it can be the client calling an HTTP API and the api dispatching the message)</p></li>&#xA;<li><p>read operations: synchronous (request response) operations (using an HTTP API) and this is does not involve any messaging what so ever</p></li>&#xA;</ul>&#xA;&#xA;<p>Does that make sense?</p>&#xA;"
39374979,39306577,2461073,2016-09-07T16:19:48,"<p>In the scenrio you described, one solution can be to have a statefull orchestration component that can deal with the process's integrety, something like a <a href=""http://docs.particular.net/nservicebus/sagas/message-correlation"" rel=""nofollow"">Saga in NServiceBus</a>.</p>&#xA;&#xA;<p>You start the process by processing the <code>DeleteCountry</code> command that sends 2 messages, one to the country component and the other to the cities component.</p>&#xA;&#xA;<p>Both components will reply to the 'saga' and the saga will handle the completion or failure of the operation based on the state/results from the components doing the work.</p>&#xA;&#xA;<p>Does that make sense?</p>&#xA;"
47951913,47939726,2461073,2017-12-23T10:59:08,<p>You probably want to have the client or the creator of the account to generate the account Id using a <code>guid</code> then send a message to the account components and the wallet component to create the account and wallet using the generated aacountId and WalletId...</p>&#xA;&#xA;<p>Make sense?</p>&#xA;
40264818,40208887,2461073,2016-10-26T14:21:29,"<p>To add to what was answered above...</p>&#xA;&#xA;<p>When decomposing your domain, instead of using the single entity interaction model, use the properties/fields interaction and bounded context to compose your components/microservices.</p>&#xA;&#xA;<p>So for example a user account registration will have only the user's fields it is modifying (changing state and therefore own these properties of the entity).</p>&#xA;&#xA;<p>And another component might have the user's address, and another the user's age, gender and marital information, and another will have his credit card and bank details...</p>&#xA;&#xA;<p>You will end up with separate tables for each bounded context.</p>&#xA;&#xA;<p>How you host the tables is up to you and depends on your load and infrastructure (all tables in one database and one server or multiple databases/servers)</p>&#xA;&#xA;<p>Just to clarify, these tables have no referential integrity, if you had referential integrity you would re-introduce coupling and that will bring you back to the database contention issue...</p>&#xA;&#xA;<p>Watch <a href=""https://www.youtube.com/watch?v=MTArpO7rSQE"" rel=""nofollow"">this presentation by Udi Dahan</a> for a lot more details</p>&#xA;"
40265628,40081965,2461073,2016-10-26T14:56:56,"<p>Lookup tables will usually contain read only data (they are like view models), so they can be available across the system in whatever technical solution you choose.</p>&#xA;&#xA;<p>A shared read only database table, a distributed cache, on a CDN...</p>&#xA;&#xA;<p>Make sense? </p>&#xA;"
42455260,42353292,2461073,2017-02-25T11:09:28,"<p>In a distributed system, you will always have some amount of coupling, what you want to do is to reduce all aspects of coupling to a minimum.</p>&#xA;&#xA;<p>I'd argue that is does matter how you design your service location. if your code knows of the other service i.e. <code>OrderService.Send(SubmitOrderMessage);</code> (where 'OrderService' is an instance of the other service's proxy)&#xA;as opposed to <code>transportAgent.Send(SubmitOrderMessage);</code> (where 'transportAgent' is an instance of the transport's proxy i.e. the queuing service/agent and the actual address of the queue can be in your config), this reduces the coupling and your business logic code (Service) and delegates the routing to your infrastructure.</p>&#xA;&#xA;<p>Make Sense?</p>&#xA;"
44985750,44980176,2741393,2017-07-08T11:32:35,"<p>Are your inputs coming from something that looks like an event storage? I.e. a database that acts as an immutable source of truth, of append only events.</p>&#xA;&#xA;<p><strong>If that is the case, you have the foundation to use event sourcing, and additionally CQRS</strong>. (They're not the same thing)</p>&#xA;&#xA;<p>What you would have to realize is that the so called write side / command side... has already been done for you. </p>&#xA;&#xA;<p>The incoming batch of XML files with transactions... each transaction is an event already. It doesn't sound like you need to convert these to events, to then put these into Kafka. You can just map these to something you can put into Kafka, and then all subscribers of the topics can do stuff accordingly.</p>&#xA;&#xA;<p><strong>Effectively you would be implementing the read side of Event Sourcing + CQRS</strong>.</p>&#xA;&#xA;<p><strong>In practical terms</strong>, unless you are going to be doing things on the write side (where the xml files are generated / where user input is received)... <strong>I wouldn't worry too much about the subtleties</strong> of event sourcing as it relates to DDD and CQRS. I would <strong>simply think of what you're doing as a way to distribute your data to multiple services</strong>.</p>&#xA;&#xA;<p>And make sure to consider how caches, search engines, etc. will only be updated whenever you get those XML files.</p>&#xA;"
50709255,50707566,54929,2018-06-05T21:33:34,"<p>Kubernetes automatically injects services as environment variables. <a href=""https://kubernetes.io/docs/concepts/services-networking/service/#environment-variables"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/concepts/services-networking/service/#environment-variables</a></p>&#xA;&#xA;<p><strong>But you should not use this.</strong> This won't work unless all the services are in place when you create the pod. It is inspired by ""docker"" which also moved on to DNS based service discovery now. So ""environment based service discovery"" is a thing of the past.</p>&#xA;&#xA;<p>Please rely on DNS service discovery. Minikube ships with <code>kube-dns</code> so you can just use the <code>lucky-server</code> hostname (or one of <code>lucky-server[.default[.svc[.cluster[.local]]]]</code> names). Read the documentation: <a href=""https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/</a></p>&#xA;"
49537130,49536369,318097,2018-03-28T14:24:56,"<p>For me, the main benefit is an ability to refactor your domain model over time.</p>&#xA;&#xA;<p>With ORM you very often end up with database structure that is not easy to change. After several years, the cost of database structure change and data migration could be prohibitive.</p>&#xA;&#xA;<p>With Event Sourcing, your read models are calculated from event stream. You just create a new projection function and have a new database (your read model).</p>&#xA;&#xA;<p>And there a lot of other benefits explained in <a href=""https://www.youtube.com/watch?v=JHGkaShoyNs"" rel=""nofollow noreferrer"">classic Greg Young's presentation</a>.</p>&#xA;"
51942469,51942114,2880657,2018-08-21T06:00:23,"<p>It depends on your architecture.  If you're using some of the Domain Driven Design principles,  there would be little if any logic in the controllers/api. The controllers would be used to coordinate/ manage the communication between the domain services (i.e. AccountService), repositories (i.e. AccountRepo), and or infrastructure services (i.e. EmailService).  All logic would  be in the models and services.  Some advantages are...&#xA;1. Unit testable code&#xA;2. The code better models the business problem (controllers mean nothing to the business problem)&#xA;3. Controllers don't become a place to jam a lot of business logic into and result into a tangled mess&#xA;4. And more...</p>&#xA;&#xA;<p>Of course this all depends on whether maintainability is a priority </p>&#xA;"
51580595,51545971,7147666,2018-07-29T13:04:31,"<p>Use the Nginx <a href=""http://nginx.org/en/docs/http/ngx_http_auth_request_module.html"" rel=""nofollow noreferrer"">auth request module</a></p>&#xA;&#xA;<p>Then set up your directives in Nginx something like this:</p>&#xA;&#xA;<pre><code>location /service/users/ {&#xA;    auth_request /auth;&#xA;    ...&#xA;}&#xA;&#xA;location = /auth {&#xA;    internal;&#xA;    proxy_pass http://passport-service:3000/;&#xA;    proxy_pass_request_body off;&#xA;    proxy_set_header Content-Length """";&#xA;    proxy_set_header X-Real-Ip $remote_addr;&#xA;    proxy_set_header Authorization $http_authorization;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Now Nginx will make an internal subrequest to /auth whenever a client request url begins with /service/users, in this example passing the client IP and the authorization token in the headers, but you can configure it however to suit your needs.</p>&#xA;&#xA;<p>Your authentication server script receives the subrequest, does whatever authentication you need to do and you code it to return HTTP response code of 200 if you want to allow access and 401 if you dont.</p>&#xA;&#xA;<p>This is just a basic example which will allow or deny access, you can incorporate the <code>error_page</code> and/or <code>auth_request_set</code> directives to build a more comprehensive solution to redirect clients to a login page, custom error page, non admin page, whatever.</p>&#xA;"
50910714,48487098,407954,2018-06-18T13:32:11,"<p>The article <a href=""https://dzone.com/articles/patterns-for-microservices-sync-vs-async"" rel=""nofollow noreferrer"">Patterns for Microservices — Sync vs. Async</a> does a great job defining many of the terms used here and has animated gifs demonstrating sync vs. async and orchestrated vs. choreographed as well as hybrid setups.</p>&#xA;&#xA;<p>I know the OP <a href=""https://stackoverflow.com/questions/48487098/saga-choreography-implementation-problems#comment87372467_48487098"">answered his own question for his use case</a>, but I want to try and address the questions raised a bit more generally in lieu of the linked article.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Should a microservice issue the event in any process manager or should it be passed directly to the next microservice responsible for the chain of events?</p>&#xA;</blockquote>&#xA;&#xA;<p>To use a more general term, a process manager is an <em>orchestrator</em>. A concrete implementation of this may involve a stateful actor that <em>orchestrates</em> a workflow, keeping track of the progress in some way. Since a saga is workflow itself (composed of both forward and compensating actions), it would be the job of the process manager to keep track of the state the saga until completion (success or failure). This typically involves the actor sending <em>synchronous</em>* calls to services waiting for some result before going to the next step. Parallel operations can of course be introduced and what not, but the point is that this actor dictates the progression of the saga.</p>&#xA;&#xA;<p>This is fundamentally different from the choreography model. With this model there is no central actor keeping track of the state of a saga, but rather the saga <em>progresses</em> implicitly via the events that each step emits. Arguably, this is a more <em>pure</em> case of an <em>event-driven</em> model since there <em>is no</em> coordination.</p>&#xA;&#xA;<p>That said, the challenge with this model is observing the state at any given point in time. With the orchestration model above, in theory, each actor could be queried for the state of the saga. In this choreographed model, we don't have this luxury, so in practice a correlation ID is added to every message corresponding to (in this case) a saga. If the messages are queryable in some way (the event bus supports it or through some other storage means), then the messages corresponding to a saga could be queried and the saga state could be reconstructed.. (effectively an event sourced modeled).</p>&#xA;&#xA;<blockquote>&#xA;  <p>Who should know how to build the Saga chain of events? The first microservice that receives a certain work or the router?</p>&#xA;</blockquote>&#xA;&#xA;<p>This is an interesting question by itself and one that I have been thinking about quite a lot. The easiest and default answer would be.. hard code the saga <em>plans</em> and map them to the incoming message types. E.g. message A triggers plan X, message B triggers plan Y, etc.</p>&#xA;&#xA;<p>However, I have been thinking about what a control plane might look like that manages these <em>plans</em> and provides the mechanism for pushing changes dynamically to message handlers and/or orchestrators dynamically. The two specific use cases in mind are changes in authorization policies or dynamically adding new steps to a plan.</p>&#xA;&#xA;<blockquote>&#xA;  <p>If an event needs to pass a very large volume of data to the next Saga event, how is this done in terms of the request structure? Is it divided into multiple Sagas for example (as a result pagination type)?</p>&#xA;</blockquote>&#xA;&#xA;<p>The way I have approached this is to include references to the <em>large data</em> if these are objects such as a file or something. For data that are inherently streams themselves, a parallel channel could be referenced that a consumer could read from once it receives the message. I think the important distinction here is to decouple thinking about the messages driving the workflow from where the data is physically materialized which depends on the data representation.</p>&#xA;"
50001010,50000445,4603538,2018-04-24T11:53:11,"<p>The <a href=""https://docs.spring.io/spring-security/site/docs/3.0.x/reference/remember-me.html"" rel=""nofollow noreferrer"">remember-me</a> feature that comes with Spring Security might be what you're looking for. </p>&#xA;&#xA;<p>However, there are still quite a few limitations to this. If your cookies are wiped upon closing the browser, which is quite common in a lot of big companies, this won't work. Other than by the session cookie, you can't <em>safely</em> remember the user. You absolutely require <code>Service-Y</code> to be available to identify the user if his cookies have been cleared or have expired.</p>&#xA;&#xA;<p>The main selling point of OAuth2 is that you sign in using <code>Service-Y</code> without <code>Service-X</code> knowing your username/password <em>(unless the username is transmitted through the scope)</em>, so while you could ask the user to provide a password on the first time he signs in <code>Service-X</code> using <code>Service-Y</code> in case <code>Service-Y</code> goes down, which would <em>potentially</em> patch your issue, I really <strong>do not recommend it</strong> because it somewhat defeats the purpose of OAuth2.</p>&#xA;&#xA;<p>Now, if you do take that suggestion despite me not recommending it, you should make sure that <code>Service-X</code> <strong>checks</strong> that <code>Service-Y</code> is down before allowing the user to sign in using the password he provided for <code>Service-X</code>.</p>&#xA;&#xA;<p>Best case scenario, if your browser cookies aren't cleaned, go for the remember-me feature. I suggest you read the following: <a href=""http://www.baeldung.com/spring-security-oauth2-remember-me"" rel=""nofollow noreferrer"">http://www.baeldung.com/spring-security-oauth2-remember-me</a></p>&#xA;"
31216228,31206417,290962,2015-07-04T02:19:08,"<p>Assuming you have multiple systems providing the services, use a canonical data model to avoid embedding transformations in middleware. <a href=""http://www.enterpriseintegrationpatterns.com/CanonicalDataModel.html"" rel=""nofollow"">Here is a link to Gregor Hohpe's enterprise integration patterns site about canonical models</a>.</p>&#xA;&#xA;<blockquote>&#xA;  <p>a Canonical Data Model that is independent from any specific&#xA;  application. Require each application to produce and consume messages&#xA;  in this common format.</p>&#xA;</blockquote>&#xA;&#xA;<p>The idea is that there is an agreed upon standard which services use for inter-operations. Typically, each system that provides services has its own, internal data model that is different than the canonical model. This happens because changing legacy systems is too onerous, or the canonical model is a bad fit for a system's internal representation of data. Each system is then individually responsible for converting its internal schema to the canonical model for service I/O.</p>&#xA;&#xA;<p>Each system team can use whatever tool its wants to perform the transformations: XSLT, Python scripts, Java, or some WYSIWYG tool from Oracle or Microsoft. The upshot is that any system whose internal data model is not identical to the canonical model must perform some mapping. It's unavoidable.</p>&#xA;"
50968225,50966737,9274514,2018-06-21T12:07:55,"<p>A microservice application still runs on your infrastructure, you still control all of its nodes, state and infrastructure. So, despite being distributed (and even though the infrastructure might not be yours such as a 3rd party cloud), you still have the power to interfere in all of its aspects.</p>&#xA;&#xA;<p>The main selling point of a decentralized application is that theoretically no one can actually interfere in its infrastructure since it's not owned by a single entity. Theoretically anyone in the world (and the largest its user base, the more resilient the decentralized application becomes) can become a node in the infrastructure and the ""current valid state"" is calculated based on a kind of agreement between the nodes (so, unless you can interfere in a majority of the nodes, which you don't own, you can't change the application's state on your own).</p>&#xA;&#xA;<p>In a certain sense, you're right that they seem similar, since they're both distributed applications. The decentralized ones just go a step further to be not ""owned"" and ""controlled"" by a single entity and be the product of an anonymous community.</p>&#xA;&#xA;<p>EDIT</p>&#xA;&#xA;<p>So suppose you/your company makes a very cool microservice application and you host it on a bunch of 3rd party clouds around the world to make sure it's very redundant and always available. A change of heart on your part (or maybe being forced to do so by government regulations) can shutdown the application out of the blue or ban certain users from it or edit/censor content currently being published on it. You're in full control since it's <strong>your</strong> app. As good as your intentions might be, you are a liability, a single point of failure in the ecosystem.</p>&#xA;&#xA;<p>Now, if your app is decentralized... there's no specific person/entity to be hunt down to force such a behavior. You need to hunt thousands/millions of owners of single independent nodes providing infrastructure to the app and enforcing its agreed set of rules. So how would you go banning users / censoring content / etc? You (theoretically) can't... unless you can reach a majority of its nodes and that has already proven to be quite difficult and even brute force might be nearly impossible to achieve.</p>&#xA;"
50798127,50796780,9274514,2018-06-11T12:46:50,"<p>I'm not going to extend this answer into whether your bounded contexts and service endpoints are well defined since your question seems to be simplifying the issue to keep a well defined scope, but regarding your specific question:</p>&#xA;&#xA;<blockquote>&#xA;  <p>What is best practise to check ""ownership"" of entities cross database in micro services</p>&#xA;</blockquote>&#xA;&#xA;<p>Microservice architectures use strive for a ""share nothing"" principle. And that usually extends from code base to data base. So you're right to assume you're checking for this constraint ""cross-DB"" in your scenario.</p>&#xA;&#xA;<p>You have a few options on this particular case, each with their set of drawbacks:</p>&#xA;&#xA;<p>1) Your proposed <strong>""Does exists (personId=5,tenantId=1)""</strong> call from the <em>DocumentContext</em> to the <em>PersonContext</em> is not wrong on itself, but you will generate a straight dependency between these two microservices, so you must ask yourself whether it seems ok for you not to accept new documents if the <em>PersonManagement</em> microservice is offline. </p>&#xA;&#xA;<p>In specific situations, such dependencies might be acceptable but the more of these you have, the less your microservice architecture will behave as one and more like a ""distributed monolith"" which on itself it pretty much an anti-pattern.</p>&#xA;&#xA;<p>2) The other main option you have is that you should recognize that the <em>DocumentContext</em> is a very much interested in some information/behavior relating to <em>People</em> so it should be ok with modelling the <em>Person Entity</em> inside its boundaries. </p>&#xA;&#xA;<p>That means, you can have the <em>DocumentContext</em> subscribe for changes in the <em>PersonContext</em> to be aware of which <em>People</em> currently exist and what their characteristics are and thus being able to keep a local copy of such information.</p>&#xA;&#xA;<p>That way, your validation will be kept entirely inside the <em>DocumentContext</em> which will have its operation unhindered by eventual issues with the <em>PersonContext</em> and you will find out your modelling of the document related entities will be much cleaner than before.</p>&#xA;&#xA;<p>But in the end, you will also discover that a <em>""share nothing""</em> principle usually will cost you in what seems to be redundancy, but it's actually independence of contexts. </p>&#xA;"
50240283,50239555,9274514,2018-05-08T18:39:39,"<p>Usually microservice architectures adopt a ""share nothing"" concept, which mean your code bases should be ideally separate. Yes, that will mean you will write more code but will keep your microservices more manageable, uncoupled and probably lighter.</p>&#xA;&#xA;<p>Also, regarding the DDD-part do the question, you should really strive to keep well defined boundaries within your application, which means you shouldn't be scared to have ""redundant"" entities in different bounded contexts because the same concept usually mean different things to different domain areas of your application.</p>&#xA;&#xA;<p>Keeping onto the ""ERP"" theme, you'd expect the ""Order Placing"" context of your application to have quite a different view on the ""Product"" entity than that of the ""Tax"" context. Keeping those in distinct contexts in different code bases will allow you to model smaller aggregates with a higher level of cohesion that will be way less coupled to the other constructs of your model thus, making evolve your microservices way easier.</p>&#xA;"
50651810,50651256,9274514,2018-06-01T22:41:31,"<p>One of the key points many microservice practitioners would point out to you is that you should guide your microservice segregation by some factors:</p>&#xA;&#xA;<ul>&#xA;<li><p>Ownership: Ideally a team should own a microservice and be the sole responsible for evolving it. So, in your scenario, are these microservices owned by different teams or are they part of something a single team will be responsible for?</p></li>&#xA;<li><p>Service Relationships: A microservice unit should be a boundary of things that are closely related/coupled and thus must be deployed/monitored/scaled together. Again, in your scenario, is this your case?</p></li>&#xA;</ul>&#xA;&#xA;<p>In the end, one of the issues you might be facing is that your example might be too simple. Simple-CRUDs on their own usually won't justify real microservice architectures and over engineering these scenarios might do you more harm than good.</p>&#xA;&#xA;<p>If your scenario is more complex, you might want to think about the microservice boundaries with your more complex use cases and not with these support scenarios.</p>&#xA;"
50659095,50628676,9274514,2018-06-02T16:46:18,"<p>I'll try to answer your major concerns on a concept level without getting tied up with the specifics of frameworks and implementations. Hope this will help.</p>&#xA;&#xA;<blockquote>&#xA;  <p>There would be 2 ways to validate if the request is valid:</p>&#xA;  &#xA;  <p>. Write's side only get events (from event store, AKA zookeeper) to validate if the requested mutation is possible.</p>&#xA;  &#xA;  <p>. Write's side get a snapshot from a traditional database to validate the requested mutation.</p>&#xA;</blockquote>&#xA;&#xA;<p>I'd go by the first option. To execute a command, you should rely on the current event stream as authority to determine your model's current state. </p>&#xA;&#xA;<p>The read model of your architecture is only eventually consistent which means there is an arbitrary delay between a command happening and it being reflected on the read model. Although you can work on your architecture to try to ensure this delay will be as small as possible (even if you ignore the costs of doing so) you will always have a window where your read model is not still up to date.</p>&#xA;&#xA;<p>That being said, your commands should be run against your command model based off your current event store.</p>&#xA;&#xA;<blockquote>&#xA;  <p>The main difference between event sourcing and basic messaging is the usage of the event-store instead of a entity's snapshot? In this case, can we assume that not all MS need an event's store tactic (i mean, validating via the event store and not via a ""private"" database)? If yes, does anyone can explain when you need event's store and when not?</p>&#xA;</blockquote>&#xA;&#xA;<p>The whole concept of Event Sourcing is: instead of storing your state as an ""updatable"" piece of data which only reflects the latest stage of such data, you store your state as a series of actions (events) that can be interpreted to reach such state. </p>&#xA;&#xA;<p>So, imagine you have a piece of your domain which reads (on a free form notation):</p>&#xA;&#xA;<p>Entity A = { Id: 1; Name: ""Something""; } </p>&#xA;&#xA;<p>And something happens and a command arrives to change the name of such entity to ""Other Thing"".</p>&#xA;&#xA;<p>In a traditional storage, you would reach for such record and update it to:</p>&#xA;&#xA;<p>{ Id: 1; Name: ""Other Thing""; } </p>&#xA;&#xA;<p>But in an event-sourced storage, you wouldn't have such a record, you would have an event stream, with data such as: </p>&#xA;&#xA;<p>{Entity Created with Id = 1} > {Entity with Id = 1 renamed to ""Something""} > {Entity with Id = 1 renamed to ""Other Thing""}</p>&#xA;&#xA;<p>Now if you ""replay"" these events in order, you will reach the same state as the traditional storage, only you will ""know"" how your got to that state and the traditional storage will forget such history.</p>&#xA;&#xA;<p>Now, to answer your question, you're absolutely right. Not all microservices should use an event store and that's even not recommended. In fact, in a microservices architecture each microservice should have its own persistance mechanism (many times being each a different technology) and no microservice should have direct access to another's persistance (as your diagram implies with ""Another MS"" reaching to the ""Event Store"" of your ""Bill's MS"").</p>&#xA;&#xA;<p>So, the basic decision factor to you should be:</p>&#xA;&#xA;<ul>&#xA;<li><p>Is your microservice one where you gain more from actively storing the evolution of state inside the domain (other than reactively logging it)?</p></li>&#xA;<li><p>Is your microservice's domain one where your are interested in analyzing old computations? (that is, being able to restore the domain to a given point in time so you can understand its state's evolution pattern - consider here something as complex auditing where you want to understand past computations)</p></li>&#xA;<li><p>Even if you answer ""yes"" to both of these questions... will the added complexity of such architecture be worth it?</p></li>&#xA;</ul>&#xA;&#xA;<p>Just as a closing remark on this topic, note there are multiple patterns intertwined in your model:</p>&#xA;&#xA;<ul>&#xA;<li>Event Sourcing is just the act of storing state as a series of actions instead of an updatable central data-hub.</li>&#xA;<li>The pattern that deals with having Read Model vs Command Model is called CQRS (Command-Query Responsibility Segregation)</li>&#xA;</ul>&#xA;&#xA;<p>These 2 patterns are frequently used together because they match up so nicely but this is not a prerequisite. You can store your data with events and not use CQRS to split into two models AND you can organize your domain in two models (commands and queries) without storing any of them primarily as events.</p>&#xA;"
49677947,46236744,36498,2018-04-05T17:13:02,"<p>I think you can have definitions in Swagger even if they aren't used by any endpoints. Just declare any types that you need in a dedicated section, e.g. ""definitions"". You can refer ones that you're using in the endpoints as e.g. <code>""$ref"":  ""#/definitions/User""</code>, <a href=""http://petstore.swagger.io/v2/swagger.json"" rel=""nofollow noreferrer"">as per main Swagger live example</a>.</p>&#xA;&#xA;<p>I expect that code will be generated for them, so you can write tests against any of definitions.</p>&#xA;"
43187609,43187101,1153938,2017-04-03T14:42:26,"<p>Try ElasticCache <a href=""http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/WhatIs.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/WhatIs.html</a></p>&#xA;&#xA;<p>This makes it easy to share positions between nodes&#xA;They discuss using it for a score table but it might be possible to use it for positional data</p>&#xA;&#xA;<p>Combine ElasticCache with autoscaling <a href=""http://docs.aws.amazon.com/autoscaling/latest/userguide/WhatIsAutoScaling.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/autoscaling/latest/userguide/WhatIsAutoScaling.html</a> and you should be able to expand the environment with demand </p>&#xA;"
41903764,41893566,1490322,2017-01-27T22:04:10,"<p>Use Spring interceptors.  They have interceptors and factories for just about anything.  Lets you add instrumentation and common code to all aspects.  </p>&#xA;&#xA;<p>UPDATE:  samples interceptors and factories.  </p>&#xA;&#xA;<p><a href=""http://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/http/client/ClientHttpRequestInterceptor.html"" rel=""nofollow noreferrer"">ClientHttpRequestInterceptor</a><br>&#xA;<a href=""http://docs.spring.io/spring-boot/docs/current/api/org/springframework/boot/web/servlet/FilterRegistrationBean.html"" rel=""nofollow noreferrer"">FilterRegistrationBean</a><br>&#xA;<a href=""http://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/http/client/SimpleClientHttpRequestFactory.html"" rel=""nofollow noreferrer"">SimpleClientHttpRequestFactory</a><br>&#xA;<a href=""http://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/messaging/support/ChannelInterceptor.html"" rel=""nofollow noreferrer"">ChannelInterceptor</a>  </p>&#xA;"
30608350,30583466,1490322,2015-06-02T23:36:16,"<blockquote>&#xA;  <p>When an event isn’t received in the worker, the counters start to ‘drift away’ from the true MySQL count.</p>&#xA;</blockquote>&#xA;&#xA;<p>Interesting problem.  I think it is not too hard to make sure that you can recover from these transient outages and that certain level of expectations should be set on how up-to-date that stats are.</p>&#xA;&#xA;<p>A few things that such a sytem would require are as follow:</p>&#xA;&#xA;<p>1)  A request is deemed successful not only when it makes what ever changes it needs to but when it notifies the next system about the request and gets back a successful response.<br>&#xA;2)  Asynchronous tasks (like calculating stats) must first record the request to a local persistent queue, that it can process on startup should a failure occur, before responding with a successful response.<br>&#xA;3)  Asynchronous tasks must first process its persistent queue upon startup.<br>&#xA;4)  Use globally unique transaction ID's so that if a request gets submitted twice, due to race conditions, do not get processed more than once.  </p>&#xA;&#xA;<p>So now lets walk through this.  A request is made to an e-commerce service.  Said service processes the request and submits to a fast responding service any relevant stats about that request.  The async service would be responsible for recording the request to a persistent queue and quickly respond with a success.  The async service may be local to the e-commerce host, for purposes of performance.  Said async service might then have a thread to process those requests in earnest by handling them off to another service for further processing... the reason for this is to keep the e-commerce service fast and not cause it to slow down by accessing non-critical external hosts. The local async worker thread would send the stats to the next service, which would again record it then respond with a success.... and so forth and so on... hopefully the chain is not longer than two or three.  All requests must either be worked on to success or recorded (persistently) so that they can be processed later.</p>&#xA;&#xA;<p>All this ensures that the stats at the end of all this are accurate and if not accurate will soon be.  It is expected that there is a consistency delay for this type of data.  How up-to-date the data is can even be figured out and included in the responses for stats data.</p>&#xA;&#xA;<p>If needed, there could be periodic audits, like maybe nightly, where the stats are recalculated against another source, in your case it seems like this mysql DB might be the master record for all requests.</p>&#xA;&#xA;<p>I think you are close to this because you are already using </p>&#xA;"
31124042,31104540,1490322,2015-06-29T19:33:17,"<p>I agree with what @Udi Dahan said.  Just want to add to his answer.  </p>&#xA;&#xA;<p>I think you need to persist the request to the loyalty program so that if it fails it can be done at some other point.  There are various ways to word/do this.</p>&#xA;&#xA;<p>1)  Make the loyalty program API failure recoverable.  That is to say it can persist requests so that they do not get lost and can be recovered (re-executed) at some later point.  </p>&#xA;&#xA;<p>2)  Execute the loyalty program requests asynchronously.  That is to say, persist the request somewhere first then allow the service to read it from this persisted store.  Only remove from the persisted store when successfully executed.  </p>&#xA;&#xA;<p>3)  Do what Udi said, and place it on a good queue (pub/sub pattern to be exact).  This usually requires that the subscriber do one of two things... either persist the request before removing from the queue (goto 1) --OR-- first borrow the request from the queue, then after successfully processing the request, have the request removed from the queue (this is my preference). </p>&#xA;&#xA;<p>All three accomplish the same thing.  They move the request to a persisted place where it can be worked on till successful completion.  The request is never lost, and retried if necessary till a satisfactory state is reached.</p>&#xA;&#xA;<p>I like to use the example of a relay race.  Each service or piece of code must take hold and ownership of the request before allowing the previous piece of code to let go of it.  Once it's handed off, the current owner must not lose the request till it gets processed or handed off to some other piece of code.</p>&#xA;"
51917453,51916102,1490322,2018-08-19T12:02:41,"<p>In this case Wikipedia tackles this very question.  </p>&#xA;&#xA;<blockquote>&#xA;  <p>From a formal perspective, what is produced, published, propagated,&#xA;  detected or consumed is a (typically asynchronous) message called the&#xA;  event notification, and not the event itself, which is the state&#xA;  change that triggered the message emission. Events do not travel, they&#xA;  just occur. However, the term event is often used metonymically to&#xA;  denote the notification message itself, which may lead to some&#xA;  confusion. This is due to Event-Driven architectures often being&#xA;  designed atop message-driven architectures, where such communication&#xA;  pattern requires one of the inputs to be text-only, the message, to&#xA;  differentiate how each communication should be handled.</p>&#xA;</blockquote>&#xA;&#xA;<p><a href=""https://en.wikipedia.org/wiki/Event-driven_architecture"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Event-driven_architecture</a></p>&#xA;&#xA;<p>I'll be honest, I treated them the same when designing and writing code.  But I guess technically there is a difference, as per the paragraph quoted above.</p>&#xA;"
46182794,46180957,1490322,2017-09-12T17:59:55,"<p>When doing producer side testing you need to provide the JSON contract.  In those contracts you have the name of the producer.  From your provider tests, you need to refer to the location of the contracts and the name of the provider.</p>&#xA;&#xA;<pre><code>@RunWith(RestPactRunner.class) // Custom pact runner, child of PactRunner which runs only REST tests&#xA;@Provider(""myProducersNameAsItApearsInContract"") // Set up name of tested provider&#xA;@PactFolder(""path/to/pact/contracts"") // Point where to find pacts (See also section Pacts source in documentation)&#xA;</code></pre>&#xA;&#xA;<p>The name of the provider is in the contract as follows:</p>&#xA;&#xA;<pre><code>    ""provider"": {&#xA;    ""name"": ""myProvider""&#xA;},&#xA;</code></pre>&#xA;&#xA;<p>EDIT:  </p>&#xA;&#xA;<blockquote>&#xA;  <p>Problem here is the test is running fine and the annotated method is getting picked up when running from my local machine but when we are running from Jenkins that specific error is thrown during pact:verify.</p>&#xA;</blockquote>&#xA;&#xA;<p>You going to have to put the full pact folder relative to the Jenkins workspace for your project.  Since we keep our pact contracts in <code>src/test/resources/pacts</code> then our @PactFolder value is as follows:</p>&#xA;&#xA;<pre><code>@PactFolder(""src/test/resources/pacts"")&#xA;</code></pre>&#xA;"
32241044,32217639,1490322,2015-08-27T05:01:14,"<blockquote>&#xA;  <p>At the moment our architecture is something weird where we have microservices which actually all share the same DB (doesn't work well at all...). I am considering improving that but have some challenges on how to make them independant.</p>&#xA;</blockquote>&#xA;&#xA;<p>IMHO the architecture is more simple by having one OLTP database for orders, customers, and products since it allows you to make use of JOINS and stored procedures.  It could be the case that the DB could use some configuration and tuning TLC vs. software re-architecture.  Just keep that door open when you consider how to fix performance problems.</p>&#xA;&#xA;<blockquote>&#xA;  <p>How do you make the link between an order and a customer?  </p>&#xA;</blockquote>&#xA;&#xA;<p>In the <code>orders</code> table have a column for <code>customer_id</code>.  The <code>customer_id</code> field in the <code>orders</code> table would be a foreign key to the <code>id</code> field on the <code>customers</code> table.  This will give you the best performance.</p>&#xA;&#xA;<p>You can do either periodic cleanup or event based cleanup of deleted users (and their orders).  But please make sure that somewhere these old orders and customers are stored.  Maybe archive tables or back-end data-warehouse where reports and analysis (OLAP) can be done on this data. </p>&#xA;&#xA;<blockquote>&#xA;  <p>Let say both services share a customer ID, how do you handle data consistency? If you remove a customer on the customer service side, you end up with inconsistency. If your service has to notify the other services then you end up with tighlty coupled services which to me sounds like what you wanted to avoid in the first place. You could kind of avoid that by having an event mechanism which notify everyone but what about network errors when you don't even know who is supposed to receive the event?</p>&#xA;</blockquote>&#xA;&#xA;<p>There are various ways this can be done.  As mentioned you can either create an event to deal with customer deletions or do periodic db cleanups.  But one thing is for certain, the orders service does not NEED to be notified when this cleanup is done, unless you want it to.  Not a need but could be a want if you want order culling to be done via the order services.  The naive way to do this is to create a stored procedure that takes a customer_id (or list of customer_id's) as input and deletes all orders that match that customer_id from the orders table.  Please make sure to backup the data for future data analysis and auditing.</p>&#xA;&#xA;<blockquote>&#xA;  <p>I want to do a simple query : retrieve the customers from US that bought product A. Given that 3million people bought product A and we have 1 million customers in the US; How could you make that reasonably performant? (Our current DB would execute that in few milliseconds)</p>&#xA;</blockquote>&#xA;&#xA;<p>Again this is why it makes sense to keep the customers, products, and orders tables in the same DB as this query can more easily be made to execute quickly when they are on the same DB.  You can take advantage of your DB's designing and optimization tools, and EXPLAIN/DESCRIBE output to tweak your tables indexes and such.  If you are using Mysql you can change DB engines (I recommend TokuDB DB engine).  </p>&#xA;&#xA;<p>In the end my main suggestion is to leave in one DB for <a href=""https://en.wikipedia.org/wiki/Online_transaction_processing"" rel=""nofollow"">OLTP</a> as you will get more efficiency and performance for the same amount of hardware.  Splitting the DB into multiple DB's will have an overhead cost for your code, architecture, network, and CPU's.  The important thing is that your DB can scale horizontally and is finely tuned for the queries being done on it.  Move <a href=""https://en.wikipedia.org/wiki/Online_analytical_processing"" rel=""nofollow"">OLAP</a> to its own DB.  This can be done using <a href=""https://en.wikipedia.org/wiki/Extract,_transform,_load"" rel=""nofollow"">ETL</a> to move data from OLTP DB to OLAP DB.  The query in your example sounds like something that would be done in an OLAP DB.  For your OLAP database you can use a columnar DB, like Vertica or something equivalent that can easily scale horizontally.  The important thing to note is that by splitting up your OLAP and OLTP you can tune and configure each for their respective purpose.</p>&#xA;&#xA;<p>Whether you run your customer, orders, and products services as a monolith (my recommendation) or as microservices the DB design should not change.  What will cause your queries in your code to change is if you split the OLTP DB into multiple DB's because now you can not do simple JOINs or stored procedures.</p>&#xA;&#xA;<p>This is what Martin Fowler calls the Monolith First.  <a href=""http://martinfowler.com/bliki/MonolithFirst.html"" rel=""nofollow"">http://martinfowler.com/bliki/MonolithFirst.html</a> </p>&#xA;"
30261821,30237292,1490322,2015-05-15T14:18:04,"<p>I have never worked with EJB directly but I have worked with EJB teams, mainly creating microservices that interacted (both as server and client) with the EJB services.  </p>&#xA;&#xA;<p>IMHO, the big difference with EJB is that it is created to handle larger applications.  There are a many aspects and technologies that make up EJB.  Some of them seem overkill when considering a small microservice.  Those technologies have a higher return on the cost spent on implementing them when the project is bigger.  Again, this is a subjective opinion.  When creating small microservices you realize how simple the architecture is and that the technologies used can be kept to a minimum.</p>&#xA;&#xA;<p>With all that said, I think EJB can be used effectively in a microservices environment.  The key will be to keep the application containers small and nimble and able to easily work with other services over HTTP.</p>&#xA;"
30238516,30209442,1490322,2015-05-14T13:22:48,"<p>Authentication:</p>&#xA;&#xA;<p>I imagine an authentication service where a requesting API signs its request using an established protocol: e.g. concatenating parts of the request with a expirable-NONCE and application ID then hashing it to create a signature.  This signature is then encrypted with a private key.  All requests must contain this encrypted signature and the NONCE as well an application identifier.  The receiving service then does a lookup for the requesting application's public-key.  After verifying the NONCE has not expired, the receiving service decrypts the digest using the public key and verifies the signature is valid (by repeating the signing process and coming to the same signature).  A service would be required for obtaining the public key.  A service can cache the application ID to public key mapping.</p>&#xA;&#xA;<p>Authorization:<br>&#xA;This can be done using some sort of <a href=""http://en.wikipedia.org/wiki/Role-based_access_control"" rel=""nofollow"">role based access control</a> scheme.  Another service can be used to lookup whether the requesting service has access to the resources being requested.</p>&#xA;&#xA;<p>I think both the authorization and authentication can be done internally, depending on time and money and need for specialization.  If you are using Java take a look at <a href=""http://docs.spring.io/spring-security/site/docs/4.0.1.RELEASE/reference/htmlsingle/"" rel=""nofollow"">Spring Security</a>. If you decide to create custom code please justify it to your managers and get buy in.  Do a thorough search online for any other solution and include in your write-up as to why it would not fit and that a custom solution is still required.</p>&#xA;"
30198367,30173267,1490322,2015-05-12T18:06:54,"<blockquote>&#xA;  <p>I want this to stop for a variety of reasons- the biggest is that each request to my individual services spawns a request to the security service, which can turn into several extra hops once you account for load balancing, etc.</p>&#xA;</blockquote>&#xA;&#xA;<p>PROS of leaving as a separate service:<br>&#xA; -  Changes to security business logic only affect the security service and do not need a change to the client services.</p>&#xA;&#xA;<p>PROS of moving security logic into client services:<br>&#xA;-  Speed/performance.<br>&#xA;-  One less service to manage might mean reduced operation costs.</p>&#xA;&#xA;<p>Speed (performance) might trump here, depending on what the requirements are, but it will come with increased development costs.</p>&#xA;&#xA;<p>If you do move the security logic into its own re-usable module that can be called from within the other services just do a good job at encapsulating it and following basic lose-coupling-tight-cohesion design.  Also, since you might have to defend this decision for years to come, please have a good explanation so your future boss does not fire you when she asks why does it cost so much to update our security logic.  Have benchmarks readily available, people lie, numbers don't.  I once had a one page benchmark results for a new DB I begged for.  I was asked multiple times from different people as to why I went with the new DB... I would just send them the one page and never heard any further questions from that person.</p>&#xA;&#xA;<p>This video might make you feel better with regard sto bucking the trend:  <a href=""https://www.youtube.com/watch?v=StCrm572aEs"" rel=""nofollow"">https://www.youtube.com/watch?v=StCrm572aEs</a></p>&#xA;&#xA;<p>It shows how and why Netflix bucked the trend and did not go with REST architecture for their API's.  Basically architecture is a customer of requirements and cost, its not the other way around.</p>&#xA;&#xA;<p>EDIT:  Another big PRO for leaving as a service is that you might have to create multiple modules for each language being supported.  At my job our security services are used by client services across multiple languages.  </p>&#xA;"
30334904,30288968,1490322,2015-05-19T19:51:48,"<p>One thing I saw Netflix do (which i like) is create intermediary services for stuff like this.  So maybe a new intermediary service that can call the other services to gather all the data then create the unified output with the Country, Building, Floor, Worker.  </p>&#xA;&#xA;<p>You can even go one step further and try to come up with a scheme for providing as input which resources you want to include in the output.</p>&#xA;&#xA;<p>So I guess this closely matches your solution 2.  I notice that you mention for solution 2 that there are concerns with sorting/filtering in the DB's.  I think that if you are using NoSQL then it has to be for a reason, and more often then not the reason is for performance.  I think if this was done wrong then yeah you will have problems but if all the appropriate fields that are searchable are properly keyed and indexed (as @Roman Susi mentioned in his bullet points 1 and 2) then I don't see this as being a problem.  Yeah this service will only be as fast as the culmination of your other services and data stores, so they have to be fast. </p>&#xA;&#xA;<p>Now you keep your individual microservices as they are, keep the client calling one service, and encapsulate the complexity of merging the data into this new service.</p>&#xA;&#xA;<p>This is the video that I saw this in (<a href=""https://www.youtube.com/watch?v=StCrm572aEs"" rel=""nofollow"">https://www.youtube.com/watch?v=StCrm572aEs</a>)... its a long video but very informative.</p>&#xA;"
31600203,31573823,1490322,2015-07-24T00:16:26,"<p>IMHO, I would go with option 2.  A couple of things to consider.  If you are buying complete into SOA and furthermore microservices, you can't flinch everytime a service needs to contact another service.  Get comfortable with that.... remember that's the point.  What I really like about option 2 is that a successful student-service response is not sent until the license-service request succeeds.  Treat the license-service as any other external service, where you might wrap the license-service in a client object that can be published by the license-service JAR.</p>&#xA;&#xA;<ul>&#xA;<li>the API of licensing would have to leak into the student API so that you can specify licensing restrictions.</li>&#xA;</ul>&#xA;&#xA;<p>Yes the license-service API will be used.  You can call it leakage (someone has to use it) or encapsulation so that the client requesting the student-service need not worry about licensing.</p>&#xA;&#xA;<ul>&#xA;<li>it puts a lot of burden on the student-service because it has to handle consistency across all of the dependent services</li>&#xA;</ul>&#xA;&#xA;<p>Some service has to take on this burden.  But I would manage it organically.  We are talking about 1 service needing another one.  If this grows and becomes concretely troublesome then a refactoring can be done.  If the number of services that student-service requires grows, I think it can be elegantly refactored and maybe the student-service becomes the composite service and groups of independently used services maybe be consolidated into new services if required.  But if the list of dependency services that student-service uses is only used by student-service, then I do not know if its worth grouping them off into their own service.  I think instead of burden and leakage you can look at it as encapsulation and ownership.... where student-service is the owner of that burden so it need not leak to other clients/services.</p>&#xA;&#xA;<ul>&#xA;<li>as more services need to react when a student is created I could see the dependency graph quickly getting out of control and the service would have to handle that complexity in addition to the one from its own logic for managing students.</li>&#xA;</ul>&#xA;&#xA;<p>The alternative would be various composite services.  Like my response for the previous bullet point, this can be tackled elegantly if it surfaces as a real problem.</p>&#xA;&#xA;<p>If forced each of your options can be turned into viable solution.  I am making an opinionated case for option 2.</p>&#xA;"
30057614,29731394,1490322,2015-05-05T15:47:35,"<p>In past projects common dependencies go in their own module.  In my case its usually a JAR packaged maven project.</p>&#xA;&#xA;<p>The decision of whether or not to use multi-module maven project or independent projects is based on whether or not the dependent projects are similar enough to fit in one multi-module project.  I think putting the common code in its own project might allow for more flexibility.</p>&#xA;&#xA;<p>It is always a good idea to separate your interface code (the code that deals with HTTP) from the business logic.  Essentially a microservice then becomes some HTTP interfacing code that passes objects/data to and from business logic code that is in its own module.  The business logic code can then be used on other projects that may not even be related to HTTP, like maybe a daemon somewhere doing cleanup or operational automated tasks. </p>&#xA;"
30057307,29924248,1490322,2015-05-05T15:33:32,"<p>Having worked with multiple services in the past I can tell you that services are made to work across separate networks.  This is why there are security protocols like CAS, SAML, OAUTH, HTTPS, and HMAC to name a few. </p>&#xA;&#xA;<p>So as long as you are able to deal with the management of the networks, and you have good security around your services (and I assume you do), then I would not be worried about breaking some unspoken microservices rule.  Remember that microservices, if written well and are useful, are expected to be used across the Internet, especially for the Internet of Things, so they are expected to be used across multiple networks.</p>&#xA;"
30057444,29787063,1490322,2015-05-05T15:40:13,"<p>I think your description of using messaging bus architecture sounds like a good idea.  Kafka is one possible solution and there are a few others.  I think, more important than which messaging bus you use, is the architecture decision to go down that route.  Picking the right messaging bus can be done by looking up the pros and cons of each.  </p>&#xA;"
30057181,30053782,1490322,2015-05-05T15:27:03,"<p>The Billing service needs to protect itself.  In this situation you cannot rely on network security alone.  The way that the Billing service protects itself is by checking if the token/session is still active.  This should not be done by business logic but should be done by the security framework you are using.</p>&#xA;&#xA;<p>For example, in previous projects we have used CAS to protect our services.  <a href=""https://wiki.jasig.org/display/CASC/Configuring+the+Jasig+CAS+Client+for+Java+in+the+web.xml"" rel=""nofollow"">Here</a> is the instructions for adding CAS Filters to your Servlet container.  So to protect my services I just include those filters to my web.xml and the appropriate jar files.  Those CAS filters will intercept each request and verify that the token/session is active and that the user is logged in.  You might not be using CAS but the approach should be similar.  Hopefully you will not have to write custom code to inspect the HTTP request and verify that it has the appropriate active token/session in your Billing service.</p>&#xA;"
47146063,47145930,1490322,2017-11-06T21:25:12,"<p>The author of the book is correct in that it is difficult to update the version of an API, especially if it is popular.  This is because you will have to either hunt down all the users of the older version and have them upgrade or you will have to support two versions of your software in production at the same time.  </p>&#xA;&#xA;<p>But you should still version your API's, IMHO.  Just never change the API version.  The reason you should probably support a version in your API is because you never know when you may have to change it.  So add version but you should avoid every changing it.  On many cases it is easier to bring up a new API or extend an existing API in a way that does not break the current version contract.</p>&#xA;&#xA;<p>BTW, in the future you never know when new technology or design patterns will materialize that will allow two versions of one API to easily exist on the same software instance in production in a very elegant way.  If and when something like this comes out, then you may see more version changes in API's.</p>&#xA;"
31773549,31764926,1490322,2015-08-02T15:41:05,"<p>Microservices is a type of SOA (service orientated architecture).  So if there is a service that does the required unit of work then why not call it versus getting involved with the back-end of a data-model that is not directly tied to the calling service.  I would only go down the route of avoiding the UM service call if you hit some sort of performance or architectural issues.  Even performance issues maybe addressed while still calling the UM service from the OM service.</p>&#xA;&#xA;<blockquote>&#xA;  <p>As far as I understand services are supposed to be autonomous and as decoupled as possible - but now I got a OM service that will go down everytime UM service go down.</p>&#xA;</blockquote>&#xA;&#xA;<p>This argument can be used to show why you would use the UM service, so that the OM service does not have to get involved (coupled) with the inner working of the UM.  If OM is coupled to UM's back-end and UM decides to change completely how it works, say going from relational DB to no-sql implementation, then that will lead to more work cause it will affect OM.</p>&#xA;&#xA;<blockquote>&#xA;  <p>1.Use the event-based programming to replciate user data as soon as user data is created in UM module so it is always replicated into a table inside OM  </p>&#xA;</blockquote>&#xA;&#xA;<p>My concern with this is that you are taking the implementation of the UM service and putting it into the OM service.  Again I would only go down this route if you hit performance/arch issues and you can deal with having UM service logic coupled into the OM service.</p>&#xA;&#xA;<blockquote>&#xA;  <p>2.Use database's replciation mechanism to replicate data from UM onto OM database</p>&#xA;</blockquote>&#xA;&#xA;<p>Coupling UM's backend to the OM.</p>&#xA;&#xA;<blockquote>&#xA;  <p>3.Copy user data into order object as nested json to eliminate dependency for order query and update (but I suppose initial order creation will still need to call UM)</p>&#xA;</blockquote>&#xA;&#xA;<p>This is useful if it reduces the number of calls to UM.  Bear in mind the cost of doing this, which might be the performance hit of carrying this extra information, which may or may not be an issue.  The less the OM JSON is communicated and the more that the UM service is called, then the more ideal this solution is.  But if UM service is only called once (say at the end of the order process) but the OM JSON passed around a lot, maybe this solution is more costly than its worth.</p>&#xA;"
28729731,28727632,2147218,2015-02-25T21:14:14,"<p><strong>Typed</strong></p>&#xA;&#xA;<p>Incoming messages that are type tagged is very liberating, so long as it's possible to tell what the incoming message is without reading all of it. If so then you no longer care so much about message order. This is because it's easy for the recipient of the messages to handle whatever it is sent. So you can have an application which just sits there taking whatever it gets, and just does whatever is appropriate for each one.</p>&#xA;&#xA;<p><strong>Format</strong></p>&#xA;&#xA;<p>A schema language that allows you to define value and size constraints is very useful. It means that the sender of a message cannot accidentally send an invalid one. Moreover the receiver can automatically tell if an incoming message meets the schema. This is a real bonus in implementing a network service; the vast bulk of the message validation is done for you!</p>&#xA;&#xA;<p>By size constraint, I mean that you can specify how long an array is in the schema and the generated code will refuse to handle arrays longer or shorter. By value constraints, imagine a message field called ""bearing""; you might want to constrain that to be between 0 and 359. </p>&#xA;&#xA;<p>These both allow you to make a clear, unambiguous statement about what the interface is and have it enforced automatically. How many security bugs have there been recently where some network interface data validation has been badly implemented...</p>&#xA;&#xA;<p><strong>Options</strong></p>&#xA;&#xA;<p>One serialisation standard that does all this is ASN.1. The tools I've used take an ASN.1 schema and produce code to serialise and deserialise, automatically checking that the value and size constraints have been met and also telling you what an incoming message type is. The tools for ASN.1 can be quite elderly and are in need of updating. If updated it would be ideal for every purpose, with both binary and text wire formats available.</p>&#xA;&#xA;<p>There's now JSON schemas too, and they seem to have type, value and size constraints. This might be what you're looking for.</p>&#xA;&#xA;<p>I'm fairly sure that Google Protocol Buffers doesn't do type tagging very well, and doesn't do value and size constraints. I've seen comments in GPB schema along the lines of: </p>&#xA;&#xA;<p>// musn't be greater than 10.</p>&#xA;&#xA;<p>If that's what is being written into a schema, the schema language is arguably inadequate...</p>&#xA;&#xA;<p>I'm not sure of Thrift, I'm not sure it does value constraints (someone correct me if I'm wrong please!).</p>&#xA;&#xA;<p><strong>Disadvantages</strong></p>&#xA;&#xA;<p>Can't think of any! It can irritate developers; code they thought was good can be readily revealed to be producing junk messages, which annoys them intensely...</p>&#xA;"
50567285,50566406,2164883,2018-05-28T13:10:29,"<p>You can do it in two ways. </p>&#xA;&#xA;<ol>&#xA;<li><p>Split your express app into multiple express apps based on Modules you have.</p></li>&#xA;<li><p>You can keep only one express app and use the load balancer to route to your application.</p></li>&#xA;</ol>&#xA;&#xA;<p>eg : considering your user and cart module, if you have urls like</p>&#xA;&#xA;<p>myapp/cart/*&#xA;myapp/user/*</p>&#xA;&#xA;<p>then you have to deploy your application on respective instances and use the load balancer to route them accordingly.</p>&#xA;&#xA;<p>you can explore more on load balancers <a href=""https://aws.amazon.com/elasticloadbalancing/"" rel=""nofollow noreferrer"">here</a> </p>&#xA;"
45884835,45534252,2821011,2017-08-25T15:30:22,"<p>it should be <code>key-auth</code> &#xA;the problem is with indentation  </p>&#xA;&#xA;<pre><code>- key-auth:&#xA;       - proxy:&#xA;</code></pre>&#xA;&#xA;<p>it must be on one level </p>&#xA;&#xA;<p>basically this is an array:</p>&#xA;&#xA;<pre><code>policies: [{&#xA;    'key-auth':null&#xA;    },{&#xA;    proxy:{ ... }   &#xA;}]&#xA;</code></pre>&#xA;&#xA;<p>this will fix the issue.</p>&#xA;"
50500195,50499849,7152019,2018-05-24T02:48:40,"<p>To remove <code>slf4j binding exceptions</code>, you should probably remove <code>spring-boot-starter-logging</code> dependency from your <code>pom.xml</code>. </p>&#xA;&#xA;<p>Actually, this dependency is already added by <code>spring-boot-starter</code> artificat so you need to exclude it like below -</p>&#xA;&#xA;<pre><code>    &lt;dependency&gt;&#xA;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&#xA;        &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;&#xA;        &lt;exclusions&gt;&#xA;            &lt;exclusion&gt;&#xA;                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&#xA;                &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt;&#xA;            &lt;/exclusion&gt;&#xA;        &lt;/exclusions&gt;&#xA;    &lt;/dependency&gt;&#xA;    &lt;dependency&gt;&#xA;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&#xA;        &lt;artifactId&gt;spring-boot-starter-log4j2&lt;/artifactId&gt;&#xA;    &lt;/dependency&gt;&#xA;    &lt;dependency&gt;&#xA;       &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&#xA;       &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;&#xA;       &lt;scope&gt;test&lt;/scope&gt;&#xA;   &lt;/dependency&gt;&#xA;</code></pre>&#xA;"
49880263,49809651,13203,2018-04-17T14:04:23,"<p>I find that chaining workflows across queues can create more complex workflows than desired, where on the other hand, creating simpler consumer applications can make for more maintainable code.</p>&#xA;&#xA;<p>Do you gain or lose any scalability or simplicity in your code by splitting the first two steps? Without more detailed info to consider, I probably would not split up the first two parts of the functionality. I don't see anything wrong with directly storing the scraping results.</p>&#xA;&#xA;<p>I like your isolated consumer for sending email, though you might consider making a generic email sending consumer that any of your applications could use and have the message format contain the proper mail parts and have the consumer construct the mail and deliver it.</p>&#xA;&#xA;<p>I don't think there's a ""right"" answer to your architecture here other than to think about finding the right balance of simplicity/complexity, scalability, and maintainability.</p>&#xA;"
38169115,38164006,471955,2016-07-03T10:40:57,<p>Rather than delaying your requests you should have the requests to the <em>bottom service</em> occur on a <code>Scheduler</code> that limits parallel activity. For example:</p>&#xA;&#xA;<pre><code> int maxParallel = 4;&#xA; Scheduler scheduler = Schedulers.from(&#xA;     Executors.newFixedThreadPool(maxParallel));&#xA; ...&#xA; observable&#xA;   .flatMap(x -&gt; &#xA;       submitToBottomService(x)&#xA;         .subscribeOn(scheduler))&#xA;   .subscribe(subscriber);&#xA;</code></pre>&#xA;&#xA;<p>By the way you mention closing a connection. The <code>Observable.using</code> operator is designed for closing resources in a reactive context (it closes resources on termination and unsubscription). If you are not using it yet then give it a look.</p>&#xA;
29758910,29704842,4801939,2015-04-20T21:40:23,"<pre><code>package controller;&#xA;&#xA;&#xA;import static org.junit.Assert.assertThat;&#xA;&#xA;import java.io.File;&#xA;import java.net.URL;&#xA;&#xA;import mediator.CLPApplication;&#xA;&#xA;import org.hamcrest.Matchers;&#xA;import org.junit.After;&#xA;import org.junit.Before;&#xA;import org.junit.Test;&#xA;import org.junit.runner.RunWith;&#xA;import org.springframework.beans.factory.annotation.Value;&#xA;import org.springframework.boot.test.IntegrationTest;&#xA;import org.springframework.boot.test.SpringApplicationConfiguration;&#xA;import org.springframework.boot.test.TestRestTemplate;&#xA;import org.springframework.http.ResponseEntity;&#xA;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;&#xA;import org.springframework.test.context.web.WebAppConfiguration;&#xA;import org.springframework.web.client.RestClientException;&#xA;import org.springframework.web.client.RestTemplate;&#xA;&#xA;@RunWith(SpringJUnit4ClassRunner.class)&#xA;@SpringApplicationConfiguration(classes = CLPApplication.class)&#xA;@WebAppConfiguration&#xA;@IntegrationTest()&#xA;public class ControllerTest {&#xA;&#xA;    @Value(""${adapter.dependency.jar.location}"")&#xA;    private String adapterDependencyJarLocation;&#xA;&#xA;    @Value(""${adapter.dependency.jar.name}"")&#xA;    private String adapterDependencyJarName;&#xA;&#xA;    @Value(""${adapter.url}"")&#xA;    private String adapterURL;&#xA;&#xA;    @Value(""${mediator.url}"")&#xA;    private String mediatorURL;&#xA;&#xA;    private URL mediator;&#xA;    private URL adapter;&#xA;    private RestTemplate template;&#xA;    Process process = null;&#xA;&#xA;    @Before&#xA;    public void setUp() throws Exception {&#xA;&#xA;        adapter = new URL(adapterURL);&#xA;        template = new TestRestTemplate();&#xA;&#xA;        //&#xA;        // Start the Atomic adapter&#xA;        // &#xA;        System.out.println(adapterDependencyJarLocation);&#xA;        System.out.println(""Starting Adapter"");&#xA;&#xA;        try {&#xA;            process = new ProcessBuilder(""java"", ""-jar"", adapterDependencyJarName)&#xA;                .directory(new File(adapterDependencyJarLocation)).start();&#xA;&#xA;            // Try connecting 5 times with a 5 second pause between each&#xA;            // to see if it started. &#xA;            Thread.sleep(5000);&#xA;            for(int i = 0; i &lt;= 5; i++) {&#xA;                try{&#xA;                    System.out.println(""Testing to see if Adapter is up"");&#xA;                    template.getForEntity(adapter.toString(), String.class);&#xA;                    System.out.println(""Adapter Started"");&#xA;                    break;&#xA;                }&#xA;                catch(RestClientException rce){&#xA;                    System.out.println(""It's not up yet"");&#xA;                }&#xA;                Thread.sleep(5000);&#xA;            }&#xA;        } catch (Exception e) {&#xA;            e.printStackTrace();&#xA;        }&#xA;    }&#xA;&#xA;    @Test&#xA;    public void testMediator() throws Exception {&#xA;        mediator = new URL(mediatorURL);&#xA;        System.out.println(""Calling Mediator"");&#xA;        ResponseEntity&lt;String&gt; response = template.getForEntity(mediator.toString(), String.class);&#xA;        System.out.println(response.getBody());&#xA;        // Getting back JSON, so check to see if it starts with an open bracket&#xA;        assertThat(response.getBody(), Matchers.startsWith(""{""));&#xA;    }&#xA;&#xA;    @After&#xA;    public void tearDown() {&#xA;        if(process != null) {&#xA;            process.destroy();&#xA;        }&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
37940757,37939567,2576531,2016-06-21T09:37:32,"<p>An api gateway should be the only way your miroservices should be accessed. &#xA;So the microservices should use each other through the api service as well. &#xA;I would not like my microservices to call the webportal as an api service.</p>&#xA;&#xA;<p>Another point is, that you can't scale the api service (<strong>which is a bottle neck by design</strong>) independet of the ui.</p>&#xA;&#xA;<p><strong>So my suggestion is</strong>: Create the api gateway as a separate microservice.</p>&#xA;"
35677573,28581644,489363,2016-02-28T01:20:59,"<p>Upgrade to Kafka 0.9 and enable <a href=""http://kafka.apache.org/090/documentation.html#design_quotas"" rel=""nofollow"">Quotas</a>.</p>&#xA;"
44916759,44274982,3705491,2017-07-05T03:56:33,<p>I agree all above options and tried below option in my spring boot application.  It works perfectly fine now. Below is the code sample as a bean. Now just need to <code>@Autowire</code> <code>RestTemplate</code> wherever(<code>java class</code>) I need it. </p>&#xA;&#xA;<pre><code>   @Bean&#xA;    public RestTemplate restTemplate() {&#xA;        RestTemplate restTemplate = new RestTemplate();&#xA;        ((SimpleClientHttpRequestFactory) restTemplate.getRequestFactory()).setConnectTimeout(15000);&#xA;        ((SimpleClientHttpRequestFactory) restTemplate.getRequestFactory()).setReadTimeout(15000);&#xA;&#xA;        return restTemplate;&#xA;    }&#xA;</code></pre>&#xA;
43997071,43979597,3318419,2017-05-16T09:10:01,"<p>In our scenario, we had 50+ microservices and we performed logging on basis of use-cases like UserManagement could be one use case consisting of several microservices(user, role, policies etc).</p>&#xA;&#xA;<p><strong>So we had one index corresponding to one use-case.</strong> </p>&#xA;&#xA;<p>This way we leveraged efficiency in our reporting system because reports were around each use case like how many users logged-in and logged-out at what time, what were the changes in user role before and after etc.</p>&#xA;&#xA;<p>Similarly you can create Elastic Index corresponding to another use case too.&#xA;This way we segregated the elastic index data and any downtime/re-indexing of any one index will not affect another index.</p>&#xA;"
49183381,46586380,9364,2018-03-08T22:14:34,"<p>I figured out the issue by looking through Spring Boot's root debug logs. </p>&#xA;&#xA;<p>If you're using <code>yml</code>:</p>&#xA;&#xA;<pre><code>src/main/resources/application.yml&#xA;----------------------------------&#xA;logging:&#xA;  level:&#xA;    root: DEBUG&#xA;</code></pre>&#xA;&#xA;<p>Or if <code>properties</code>:</p>&#xA;&#xA;<pre><code>src/main/resources/application.properties&#xA;----------------------------------&#xA;logging.level.root=DEBUG&#xA;</code></pre>&#xA;&#xA;<p>I realized I was not passing in any user auth info with the <code>GET</code>:</p>&#xA;&#xA;<pre><code>o.s.s.w.a.ExceptionTranslationFilter: Access is denied (user is anonymous); ...&#xA;</code></pre>&#xA;&#xA;<p>So you can do 1 of 2 things:</p>&#xA;&#xA;<h1>1. Add creds through url params eg.</h1>&#xA;&#xA;<pre><code>curl -X GET \&#xA;  'http://localhost:9090/resource/endpoint?&#xA;  username=user1&amp;password=password1&amp;access_token=xxxx'&#xA;</code></pre>&#xA;&#xA;<p>or</p>&#xA;&#xA;<h1>2. Add creds through basic auth eg.</h1>&#xA;&#xA;<pre><code>curl -X GET \&#xA;  'http://localhost:9090/resource/endpoint?username=user1' \&#xA;  -H 'Authorization: Basic xxxxxxxxxxxxx&#xA;</code></pre>&#xA;&#xA;<p>Did you get this off the <strong>Building Microservices With Spring</strong> course on safaribooksonline, too? :)</p>&#xA;&#xA;<p>I discovered why the teacher wasn't having the issue. He must have authorized the username/password previously -- it seems to get cached somewhere because after you <code>GET</code> the resource once, if you call it again with only the <code>auth_token</code>, it works.</p>&#xA;"
50841813,50820762,2106516,2018-06-13T16:11:42,"<p>Once you have created you <em>service interface</em> (<code>com.poc.poc.services.ServiceEndPoint</code>) declaration and the concrete implementation (<code>SubscriptionService</code>), you should add the <strong>service provider binding</strong>.</p>&#xA;&#xA;<p>As per the <code>ServiceLocator</code> documentation, the binding should be inserted under a file named after you interface FQN, i.e. under <strong><em>META-INF/services/com.poc.poc.services.ServiceEndPoint</em></strong> (The whole directory structure goes under the project / module <em>resources</em> directory).</p>&#xA;&#xA;<p>The file will contain the actual interface implementation:</p>&#xA;&#xA;<pre><code>com.poc.poc.services.SubscriptionService&#xA;</code></pre>&#xA;"
44145918,44113228,2962068,2017-05-23T22:16:20,"<p>Feign client and Zuul are two entirely different components in Spring Cloud Netflix.</p>&#xA;&#xA;<p>Feign Client is a glorified REST Template with additions such as Retry, Fallbacks etc.  You can think along the lines of Apache HttpClient</p>&#xA;&#xA;<p>Zuul on the other hand is a proxy / reverse - proxy / gateway.  Typically Gateway should be a common entry point to your backend services.  It should be a separate layer which allows you to add common functionalities like Authentication, Auditing, Logging etc. As @ootero  mentioned, you can easily add Filters in Zuul to achieve this functionality.  </p>&#xA;"
33951338,28942614,358804,2015-11-27T06:21:04,"<p>While it's a good idea to use <code>--restart=always</code> as a failsafe, container restarting is relatively slow (5+ seconds with the simple Hello World Node server described <a href=""https://github.com/docker/docker/pull/18273"">here</a>), so you can minimize app downtime using something like <code>forever</code>.</p>&#xA;&#xA;<p>A downside of restarting the process within the container is that crash recovery can now happen <em>two</em> ways, which might have implications for your monitoring, etc.</p>&#xA;"
51588848,51586999,3243156,2018-07-30T07:19:12,"<p>Since the HashMap belongs to the HelperClass it will be not updated.</p>&#xA;&#xA;<p>I am assuming that you are going to use employee information for the HashMap in the application, because you don't want to  request the database everytime to fetch the records. You can use Spring Cahche mechanism to save the information in the SpringDefault Cache(Concurrent HashMap).&#xA;You can use @Cacheable on get methods, it will hit the database when the information is not present in the cache.   @CachePut annotation the update method will update the cache. </p>&#xA;&#xA;<p><a href=""https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-caching.html"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-caching.html</a></p>&#xA;&#xA;<p><a href=""http://www.baeldung.com/spring-cache-tutorial"" rel=""nofollow noreferrer"">http://www.baeldung.com/spring-cache-tutorial</a></p>&#xA;&#xA;<p><a href=""https://spring.io/guides/gs/caching/"" rel=""nofollow noreferrer"">https://spring.io/guides/gs/caching/</a></p>&#xA;"
42660633,42653725,7579796,2017-03-08T00:03:57,"<p>I'd <a href=""https://stackoverflow.com/questions/42449160/architecture-for-communication-between-two-microservices"">the same question</a> I think.</p>&#xA;&#xA;<p>Maybe together we'll have a complete response :)</p>&#xA;&#xA;<p>I'm using JHipster to generate my application.</p>&#xA;"
45022463,42949138,640404,2017-07-10T22:13:05,<p>I have been playing around with this problem and there are not any good documented solutions out there.  What I would do is build a service which managers the Users and your Access Control List (ACL).  </p>&#xA;&#xA;<ol>&#xA;<li>Identity Service - This is where the client will authorize itself for access to the other services in the API.  </li>&#xA;</ol>&#xA;&#xA;<p><code>login [POST]&#xA;email&#xA;password</code></p>&#xA;&#xA;<p>This will return a JWT token that in the payload section will have the following information:</p>&#xA;&#xA;<ol>&#xA;<li>User Id</li>&#xA;<li>ACL</li>&#xA;</ol>&#xA;&#xA;<p>The ACL could be an array of the allowed roles they have.  This way when the client makes a request to the API the service responding to the request can introspect on the User ID (Knock - current_user) and then you can get the ACL from the payload.  Very easy I think to then add a <code>before_action</code> to verify the appropriate role.</p>&#xA;
48766760,48707364,4053652,2018-02-13T12:23:01,"<p>Take a closer look at the description of the <a href=""https://www.consul.io/docs/agent/watches.html#type-service"" rel=""nofollow noreferrer"">service watch type</a> in the official documentation. It has an example, how you can specify it:</p>&#xA;&#xA;<pre><code>{&#xA;  ""type"": ""service"",&#xA;  ""service"": ""redis"",&#xA;  ""args"": [""/usr/bin/my-service-handler.sh"", ""-redis""]&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Note that it has no property <code>handler</code> and but takes a path to the script as an argument. And one more:</p>&#xA;&#xA;<blockquote>&#xA;  <p>It requires the ""service"" parameter</p>&#xA;</blockquote>&#xA;&#xA;<p>It seems, in you case you need to specify it as follows:</p>&#xA;&#xA;<pre><code>""watches"": [&#xA;    {&#xA;      ""type"": ""service"",&#xA;      ""service"": ""Apache"",&#xA;      ""args"": [""/fully/qualified/path/to/Consul-Script.sh""]&#xA;    }&#xA;  ]&#xA;</code></pre>&#xA;"
42927231,42922250,4053652,2017-03-21T12:30:57,"<p>It seems to me, you have to deregister a service and it's health checks on container restart. Consul API provide such an opportunity, you just have to use it in your microservices. How to exactly make it work, depends on the way your services are built. Otherwise, no way Consul will determine, that some service was restarted with another port.</p>&#xA;"
42850865,42843777,4053652,2017-03-17T06:41:15,"<p><strong>1. Can Consul be used to load balance microservices?</strong> </p>&#xA;&#xA;<p>Only with some third party loadbalancer. Consul doesn't have any embedded  loadbalancing functionality. </p>&#xA;&#xA;<p>Though it has an integration with HAProxy for that. You can read about it <a href=""https://www.hashicorp.com/blog/haproxy-with-consul/"" rel=""nofollow noreferrer"">here</a>. In that case, Consul is used for service discovering and populating HAProxy configuration in runtime. And HAProxy could be used as a software load balancer.</p>&#xA;&#xA;<p><strong>2. Can Consul be used at the microservice level to spin up instances as needed?</strong></p>&#xA;&#xA;<p>It's not really clear to me, what do you mean here.</p>&#xA;&#xA;<p><strong>3. If for whatever reason Consul monitors a microservice as offline, can it attempt to start it up again?</strong></p>&#xA;&#xA;<p>I suppose, that it could be done with the <a href=""https://www.consul.io/docs/agent/watches.html"" rel=""nofollow noreferrer"">watches</a>. They are used for monitoring for updates of some nodes, services, KV enties etc. When some change is detected, it can call some executable. You can provide this executable by your self, to restart your service or do whatever else you need.</p>&#xA;"
44021575,44000812,4053652,2017-05-17T09:58:45,"<p>I suppose, the reason is that your server is available only for <code>127.0.0.1</code> ip-address, which is <code>localhost</code> ip and available only from the same server. This can be seen here:</p>&#xA;&#xA;<pre><code>Client Addr: 127.0.0.1 (HTTP: 8500, HTTPS: -1, DNS: 8600)&#xA;Cluster Addr: 127.0.0.1 (LAN: 8301, WAN: 8302)&#xA;</code></pre>&#xA;&#xA;<p>You have to configure your server, to make it listening all network interfaces or some specific interface, which have to be available from other server.</p>&#xA;&#xA;<p>Try to run it with the <code>client</code> and <code>advertise</code> options set to <code>0.0.0.0</code> (or some specific ip). Read about it <a href=""https://www.consul.io/docs/agent/options.html#_client"" rel=""nofollow noreferrer"">here</a> and <a href=""https://www.consul.io/docs/agent/options.html#_advertise"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;&#xA;<p>And you might have to delete <code>-bind=127.0.0.1</code> from the client configuration, since it might be available from the server too.</p>&#xA;"
43973101,41618538,601493,2017-05-15T06:50:44,"<p>I suggest you have a look on Dave Sayer's great webinar on this topic. He suggest several possible options varying from Spring session to OAuth2.</p>&#xA;&#xA;<p>You can find the webinar here: <a href=""https://spring.io/blog/2014/11/07/webinar-replay-security-for-microservices-with-spring-and-oauth2"" rel=""nofollow noreferrer"">https://spring.io/blog/2014/11/07/webinar-replay-security-for-microservices-with-spring-and-oauth2</a></p>&#xA;&#xA;<p>There was also a tutorial with code samples but I can't find it right now.</p>&#xA;"
44591206,44585239,146325,2017-06-16T14:00:53,"<blockquote>&#xA;  <p>""suppose I have Orders and Customers tables, where ORDER has FK to CUSTOMER. For me these seems to be in different microservices. "" </p>&#xA;</blockquote>&#xA;&#xA;<p>Still nope to the foreign key. The Orders microservice  has a data store with its own Customers table. The Customer Update microservice has a data store with its own Customers table. The Customer Orders search would be a feature of the Orders microservice and so will search its data store not the Customer Update data store. </p>&#xA;&#xA;<p>The whole point about microservices is the absence of dependencies. They are entire, discrete systems in the their own right. This makes them easy to build and easy to deploy. The snag is the issue you are butting up against: data management. Most enterprises aspire to a single source of truth regarding their data. Which usually means a central database, which imposes constraints on applications because everything has to share the same data model and changes to common entities such as Customer cause major upheaval.</p>&#xA;&#xA;<p>Microservices appear to offer a solution to this by spinning out subsets of functionality which own their own data model. This inevitably means data integrity across the enterprise is looser, because it is handled asynchronously. There is no longer a single source of truth.</p>&#xA;&#xA;<p>So the Customer Update microservice will publish updates about Customers as messages which the Orders microservice will consume and apply. Likewise, if the Orders microservice can create new Customers then it will publish a similar stream of messages which the Customer Update microservice will consume and apply. What happens if the two microservices create records for the same new Customer in the same window between refreshes? Well, yes, a good question.</p>&#xA;&#xA;<p>The upshot is, the microservice will work in some scenarios and be absolutely disastrous in others. Certainly most enterprise applications will remain largely monolithic not just through inertia but because the benefits of centrally shared data outweigh the agility of microservices in many instances. </p>&#xA;"
49016840,49013500,6772373,2018-02-27T19:50:28,<p>I would honestly recommend for each backend service to implement the Authorization Grant. That is have an endpoint exposing the redirect to your provider. Then for each frontend app go to that endpoint to trigger the OAuth flow. After the flow has been completed handle the Authorization part in the callback url and return a token which will be stored on the frontend somewhere.</p>&#xA;&#xA;<p>Hope this helps.</p>&#xA;
44507999,44502869,7236501,2017-06-12T20:14:51,"<p>If you need to generate a URL for each of the entries in the archive, your best bet is to enumerate only the entries, which would <em>hopefully</em> only read the ZIP file's directory:</p>&#xA;&#xA;<pre><code>    List&lt;URL&gt; urls = new ArrayList&lt;&gt;();&#xA;&#xA;    ZipFile zip = new ZipFile( file );&#xA;&#xA;    Enumeration&lt;? extends ZipEntry&gt; entries = zip.entries();&#xA;    while( entries.hasMoreElements() )&#xA;        urls.add( new URL( ""http://localhost:8080/app/""&#xA;            + file.getName() + ""/"" + entries.nextElement().getName() ) );&#xA;&#xA;    zip.close();&#xA;</code></pre>&#xA;"
41581114,41556946,2125973,2017-01-11T01:01:40,"<p>You can check the MSF4J JWT claim sample. It explains scenario step by step the <a href=""https://github.com/wso2/msf4j/tree/v2.1.0/samples/jwt-claims"" rel=""nofollow noreferrer"">https://github.com/wso2/msf4j/tree/v2.1.0/samples/jwt-claims</a></p>&#xA;"
38700409,38698109,2125973,2016-08-01T13:40:17,<p>This will not work with the carbon kernel 4.4.x based products or older products since there are some additional MANIFEST headers (e.g carbon-component) that are been used to process at the runtime. This will be added in future wso2 products which will be based on carbon kernel 5.x</p>&#xA;
44769349,44769245,1668501,2017-06-26T22:05:24,"<p>Since you have hosted your Applications A and B as separately, you should have to navigate to absolute path instead of relative.</p>&#xA;&#xA;<p>You no need to build both the application modules. Only build the modules that are changed because both are hosted separately on your web server and it will work independently. </p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/KEJe5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KEJe5.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>I hope this information will helps</p>&#xA;"
36174809,36174288,1032085,2016-03-23T09:53:49,"<p>1)</p>&#xA;&#xA;<blockquote>&#xA;  <p>I think of docker as a fully functional virtual machine, and therefore I thought of transforming a server with a set of services, into a container.</p>&#xA;</blockquote>&#xA;&#xA;<p>NO NO and NO! It is not virtual machine </p>&#xA;&#xA;<p><strong>Run only one process per container</strong></p>&#xA;&#xA;<p>In almost all cases, you should only run a single process in a single container. Decoupling applications into multiple containers makes it much easier to scale horizontally and reuse containers. If that service depends on another service, make use of container linking.</p>&#xA;&#xA;<p>Read Best practices for writing Dockerfiles  <a href=""https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/"" rel=""nofollow"">https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/</a></p>&#xA;&#xA;<p>2)</p>&#xA;&#xA;<blockquote>&#xA;  <p>And if I create two different containers, which framework should I use to orchestrate them? Is it Docker compose appropriated for this task?</p>&#xA;</blockquote>&#xA;&#xA;<p>You need to start with docker-compose. When you will be more familiar with it you make you own well-founded decision. </p>&#xA;"
48296472,48294450,4513685,2018-01-17T08:19:28,"<p>You may use Kafka or Kinesis to choreograph event consistency between 2 micro services for critical data updates. For example reaction by Micro Service 1 [MS1] to an event triggers appropriate message in a topic which is then read by MS2 instantaneously. </p>&#xA;&#xA;<p>Other benefit of this approach would be that if there are multiple MS dependent on reaction of MS1 then all the other MS' can get that event.</p>&#xA;&#xA;<p>If events are complete and idempotent, then you may enable log compaction as well (not required though) to always get the latest copy over a period of time.</p>&#xA;&#xA;<p>Note: However, make sure to use one partition only within Kafka topic as ordering guarantees in Kafka is per partition only or always add keys to messages so that they land into same partition.</p>&#xA;&#xA;<p>In a nutshell,</p>&#xA;&#xA;<ol>&#xA;<li>Kafka / Kinesis as event orchestrator/brokers between microservices</li>&#xA;<li>Single Partition and/or Messages with Keys (with log compaction)</li>&#xA;<li>Retention [based on requirements]</li>&#xA;<li>3 x Replication [data availability]</li>&#xA;<li>acks=all [high levels of data consistency]</li>&#xA;</ol>&#xA;"
51105783,51099034,5695637,2018-06-29T16:12:31,"<p>Try this</p>&#xA;&#xA;<pre><code>    @Entity&#xA;    public class A implements Serializable {&#xA;    private static final long serialVersionUID = 9154946919235019012L;&#xA;    @Embedded&#xA;    @AttributeOverride(name = ""id"", column = @Column(name = ""b_id""))&#xA;    private B b;&#xA;    public A() {&#xA;    }&#xA;&#xA;    public A(B b) {&#xA;        this.b = b;&#xA;    }&#xA;&#xA;&#xA;    public B getB() {&#xA;        return b;&#xA;    }&#xA;&#xA;    public void setB(B b) {&#xA;        this.b = b;&#xA;    }&#xA;    }&#xA;</code></pre>&#xA;&#xA;<p>And here is class B</p>&#xA;&#xA;<pre><code>@Embeddable&#xA;@Entity&#xA;&#xA;public class B implements Serializable {&#xA;    private static final long serialVersionUID = 5579181803793008928L;&#xA;    @Id&#xA;    @Column(nullable = false)&#xA;    private Long id;&#xA;&#xA;    public B(Long id) {&#xA;        this.id=id;&#xA;    }&#xA;    public B(){&#xA;&#xA;    }&#xA;&#xA;    public void setId(Long id) {&#xA;        this.id = id;&#xA;    }&#xA;&#xA;    public Long getId() {&#xA;        return id;&#xA;    }&#xA;    }&#xA;</code></pre>&#xA;&#xA;<p>You don't have getters and setters, or an additional constructor besides the implicit no arg one. You should have both a no-args constructor and getter and setter methods. </p>&#xA;"
48668202,48633781,5434773,2018-02-07T16:03:05,"<p>I don't understand why you think that it is an issue with the internal DNS/service resolution within Kubernetes since when you perform the DNS lookup it works, but if you query that IP you get a connection timeout.</p>&#xA;&#xA;<ul>&#xA;<li>If you curl these services from outside the pod they all timeout, apart from the first service deployed, no matter if you used the IP or the domain name.</li>&#xA;<li>When you call these services from another container within the same pod, they do work as expected.</li>&#xA;</ul>&#xA;&#xA;<p>It seems an issue with the connection between pods more than a DNS issue therefore I would focus your troubleshooting towards that direction, but correct me if I'am wrong. </p>&#xA;&#xA;<p>Can you perform the classical networking troubleshooting (ping, telnet, traceroute)from a pod toward the IP given by the DNS lookup and from one of the container that is giving timeout to one of the other pods and update the question with the results?</p>&#xA;"
45400394,45400096,979349,2017-07-30T13:38:02,"<p>How you split your application depends on the type of modules you have. If the module contains business logic than it makes sense to create a new service and communicate via Http or Messaging. On the other hand if your module has no business logic, but just a set of helper functions in might be better to extract it to a separate public/private maven package and just use it as a dependency.</p>&#xA;&#xA;<p>Yes, <code>microservice</code> is a buzz-word that just recently became popular, but a concept has been around for a while. But it also comes with more than that. While it gives a benefits of scaling and independent service deployments, it comes with a price of complexity of managing and orchestrating big amount services.</p>&#xA;&#xA;<p>For example in monolith application when you just call a function from another module you know for sure that it is always available for calling. With microservices some of the services might go down because of disruption or deployment, in which case you have to think about handling these situations gracefully (for example apply <code>circuit breaker</code> pattern). </p>&#xA;&#xA;<p>There are many other things to consider when doing microservices and there are many literature available on this topic. I read <code>Microservices: From Design to Deployment</code> from Nginx. It's short and sweet.</p>&#xA;&#xA;<p>So when people ask you <code>Have you worked with microservices before?</code> I guess they want to know if you familiar and had some experience with all the pitfalls of this concept.</p>&#xA;"
45766622,45766604,5568405,2017-08-19T00:42:44,<p>You should use volumes so you will set it in <code>docker-compose.yml</code> like this</p>&#xA;&#xA;<pre><code>version: '3.2'&#xA;services:&#xA;    container1:&#xA;        volumes:&#xA;            - shared-files:/directory/files&#xA;    container2:&#xA;        volumes:&#xA;            - shared-files:/directory/files&#xA;    container3:&#xA;        volumes:&#xA;            - shared-files:/directory/files&#xA;&#xA;&#xA;volumes:&#xA;  shared-files:&#xA;</code></pre>&#xA;
44770606,44765491,7999894,2017-06-27T00:29:12,"<p>You can specify private dependencies in requirements.txt files. You would do this in the same way you would handle private dependencies for any other python package. You might directly specify a github url, or you might use a private repository such as an instance of <a href=""http://doc.devpi.net/latest/"" rel=""nofollow noreferrer"">devpi</a>.</p>&#xA;"
42399551,42397788,136598,2017-02-22T18:32:50,"<p>You can customize your app to perform differently based on the URL that was used.  </p>&#xA;&#xA;<p>For example, you can use <a href=""http://webapp2.readthedocs.io/en/latest/guide/routing.html"" rel=""nofollow noreferrer"">domain specific routes with webapp2</a> or you can check the domain in your handler by checking the value of <code>self.request.url</code> and responding accordingly.</p>&#xA;&#xA;<p>You could for example, have myapp.appspot.com return a 404 but have www.mydomain.com provide content to users.</p>&#xA;"
42608826,42607458,136598,2017-03-05T13:19:57,"<p>I believe you can do what you need with these three tools:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://github.com/Trii/NoseGAE"" rel=""nofollow noreferrer"">NoseGAE</a> -- ""NoseGAE sets up the GAE development environment before your test run. This means that you can easily write functional tests for your application without having to actually start the dev server and test over http. ""</li>&#xA;<li><a href=""http://docs.pylonsproject.org/projects/webtest/en/latest/"" rel=""nofollow noreferrer"">WebTest</a> -- ""With this you can test your web applications without starting an HTTP server, and without poking into the web framework shortcutting pieces of your application that need to be tested. The tests WebTest runs are entirely equivalent to how a WSGI HTTP server would call an application. ""</li>&#xA;<li><a href=""https://docs.python.org/3/library/unittest.mock.html"" rel=""nofollow noreferrer"">mock</a> -- ""It allows you to replace parts of your system under test with mock objects and make assertions about how they have been used.""</li>&#xA;</ul>&#xA;&#xA;<p>I'm not sure if you need mock.  I believe using NoseGAE and WebTest will allow your services to interact with each other in the testing environment.</p>&#xA;&#xA;<p>Even if you don't need mock, however, it is often useful to isolate your testing more effectively.  I use mock for mocking third-party services when testing my code, but you may also be able to use it to mock your session service when testing your dashboard service and vice versa.</p>&#xA;"
45368912,45167593,5904022,2017-07-28T08:47:01,"<p><strong>Followup:</strong></p>&#xA;&#xA;<p>I've been mulling intensely over this the during last week, and maybe I've discovered a flaw in my way of reasoning about the problem. I've thought about the post entity as residing exclusively in the post service, but maybe that's a troubling oversimplification.</p>&#xA;&#xA;<p>What if each service (i.e. bounded context) has its own concept of what a post is, and consequently arranges for the storage thereof? The event service keeps a table of posts posted in events, the wall service records posts on walls, etc. </p>&#xA;&#xA;<p>Such entities would be quite thin, consisting mostly of a GUID, identity of the poster, maybe its contents. They could also contain special attributes that are only used in that context. For example, events, but no other services, may allow posts to be pinned.</p>&#xA;&#xA;<p>(Nota bene: below the term ""event"" is concurrently used in a totally different mening, namely a message that is sent between processes using e.g. Apache Kafka, describing something that happened.)</p>&#xA;&#xA;<p>Every time a post is submitted to a service, an event is posted to the event bus. For example, when a user posts in an event, the event service creates a post entity and issues <code>events.post-posted {id: ..., authorId: ..., contents: ...}</code>. Similarly, the wall would post <code>wall.post-posted {id: ..., authorId: ..., receiverId: ..., contents: ...}</code>.</p>&#xA;&#xA;<p>The post service, in turn, listens for all such events. Each time a post is posted to another service, a <em>corresponding</em> post entity is created in the post service, sharing the ID of the original post. This is the ""smart"" post entity, with all features that are common to posts accross the application. It could deal with sending notifications, arranging threads, discovering abuse, recording likes etc.</p>&#xA;&#xA;<p>This means that each service has much more freedom in dealing with its post entities, as they no longer refer to a single source of information residing in a single service. It allows the gateway to choose among several ways of retrieving post data, depending on the situation. For example, in order to tell the UI that a post is pinned, it needs to talk to the event service, but in order to get the textual content, it might have to talk to the post service. Perhaps the post  entity in the wall service has special options to deal with birthday wishes posted on peoples' walls.</p>&#xA;&#xA;<p>Returning to the original question: this takes away the need for the services to communicate when dealing with the deletion of an event. Instead of deleting through the post service, it's the event service's job to receive requests to delete posts. Since it has information about both the author of the post and the hostship of the event, it can make the decision itself.</p>&#xA;&#xA;<p><strong>Criticism to this idea</strong>:</p>&#xA;&#xA;<p>Although I feel I'm on the right track here, I have two main concerns.</p>&#xA;&#xA;<p>The first one is that this obviously doesn't quite answer the original question about how to deal with authority when implementing a web application with microservices. Maybe this is just the answer to <em>one</em> hypothetical scenario, whereas other slight variations of the problem aren't alleviated at all by this approach.</p>&#xA;&#xA;<p>My second concern is that I'm falling into the <a href=""https://martinfowler.com/articles/201701-event-driven.html"" rel=""nofollow noreferrer"">passive-aggressive event trap</a>. I'm describing what's happening as a series of events, but maybe I'm really issuing commands? After all, the reason for posting the <code>event.post-posted</code> event is to trigger the creation of an event in the post service. On the other hand, the application wouldn't break if these events weren't listened to; events would just have really dry posts.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>What are your thoughts on this approach?</p>&#xA;"
48986864,48956086,3680662,2018-02-26T10:57:31,"<p>This is a very good start to read up on patterns &amp; best practices:</p>&#xA;&#xA;<p><a href=""https://blogs.msdn.microsoft.com/dotnet/2017/11/16/update-web-applications-with-asp-net-core-architecture-and-patterns-guidance-updated-for-asp-net-core-2/"" rel=""nofollow noreferrer"">Web Applications with ASP.NET Core Architecture and Patterns guidance</a></p>&#xA;&#xA;<p>Although not recommended you can have your microservies share one database. If you go that route each MS should have their own scheme and their own migration history table. This is easily done using EF Core. As pointed out already, each MS should have their own data models and they can not share tables, even if they might contain almost identical data.</p>&#xA;&#xA;<p>Microservices should not share data models.</p>&#xA;"
52119556,52097737,6566294,2018-08-31T16:31:14,"<p>The problem is with the Alpine package Manager,somehow it seem to cause an error with Ubuntu,so all what i did is use Apt instead of Apk.</p>&#xA;&#xA;<p>here's the new version of the task file</p>&#xA;&#xA;<pre><code>---&#xA;# File: install.yml - package installation tasks for Consul&#xA;&#xA;- name: Install OS packages&#xA;  package:&#xA;    name: ""{{ item }}""&#xA;    state: present&#xA;  with_items: ""{{ consul_os_packages }}""&#xA;  tags: installation&#xA;&#xA;- name: Read package checksum file&#xA;  local_action:&#xA;    module: stat&#xA;    path: ""{{ role_path }}/files/consul_{{ consul_version }}_SHA256SUMS""&#xA;  become: no&#xA;  run_once: true&#xA;  register: consul_checksum&#xA;  tags: installation&#xA;&#xA;- name: Download package checksum file&#xA;  local_action:&#xA;    module: get_url&#xA;    url: ""{{ consul_checksum_file_url }}""&#xA;    dest: ""{{ role_path }}/files/consul_{{ consul_version }}_SHA256SUMS""&#xA;  become: no&#xA;  run_once: true&#xA;  tags: installation&#xA;  when: not consul_checksum.stat.exists | bool&#xA;&#xA;- name: Read package checksum&#xA;  local_action:&#xA;    module: shell&#xA;      grep ""{{ consul_pkg }}"" ""{{ role_path }}/files/consul_{{ consul_version }}_SHA256SUMS"" | awk '{print $1}'&#xA;  become: no&#xA;  run_once: true&#xA;  register: consul_sha256&#xA;  tags: installation&#xA;&#xA;- name: Check Consul package file&#xA;  local_action:&#xA;    module: stat&#xA;    path: ""{{ role_path }}/files/{{ consul_pkg }}""&#xA;  become: no&#xA;  run_once: true&#xA;  register: consul_package&#xA;  tags: installation&#xA;&#xA;- name: Download Consul package&#xA;  local_action:&#xA;    module: get_url&#xA;    url: ""{{ consul_zip_url }}""&#xA;    dest: ""{{ role_path }}/files/{{ consul_pkg }}""&#xA;    checksum: ""sha256:{{ consul_sha256.stdout }}""&#xA;    timeout: ""42""&#xA;  become: no&#xA;  run_once: true&#xA;  tags: installation&#xA;  when: not consul_package.stat.exists | bool&#xA;&#xA;- name: Install unzip package&#xA;  apt:&#xA;    name: unzip&#xA;    state: present&#xA;  run_once: true&#xA;  when:&#xA;    - consul_install_dependencies | bool&#xA;&#xA;- name: Unarchive Consul package&#xA;  local_action:&#xA;    module: unarchive&#xA;    src: ""{{ role_path }}/files/{{ consul_pkg }}""&#xA;    dest: ""{{ role_path }}/files/""&#xA;    creates: ""{{ role_path }}/files/consul""&#xA;  become: no&#xA;  run_once: true&#xA;  tags: installation&#xA;&#xA;- name: Install Consul&#xA;  copy:&#xA;    src: ""{{ role_path }}/files/consul""&#xA;    dest: ""{{ consul_bin_path }}/consul""&#xA;    owner: ""{{ consul_user }}""&#xA;    group: ""{{ consul_group }}""&#xA;    mode: 0755&#xA;  tags: installation&#xA;&#xA;- name: Daemon reload systemd in case the binaries upgraded&#xA;  command: systemctl daemon-reload&#xA;  become: yes&#xA;  notify: restart consul&#xA;  when:&#xA;    - ansible_service_mgr == ""systemd""&#xA;    - consul_install_upgrade&#xA;&#xA;- name: Cleanup&#xA;  local_action: file path=""{{ item }}"" state=""absent""&#xA;  become: no&#xA;  with_fileglob: ""{{ role_path }}/files/consul""&#xA;  run_once: true&#xA;  tags: installation&#xA;</code></pre>&#xA;"
36841420,36832219,3228566,2016-04-25T13:02:46,"<p>it's possible to use the same Redis for multiple microservices, just make sur to well prefix your redis cache key to avoid conflict between all microservices.</p>&#xA;&#xA;<p>You can use multi db in the same redis instance (i.e one for each microservice) but it's discouraged because Redis is single threaded.</p>&#xA;&#xA;<p>The best way is to use one Redis for each microservices, then you can easily flush one of them without touch others.</p>&#xA;&#xA;<p>From my personal experience with a redis cache in production (with 2 millions keys), there is no problem using EXPIRE. I encourage you to use it.</p>&#xA;"
40473207,30649582,92822,2016-11-07T19:38:00,"<p>It's perfectly fine to correlate data across service boundaries by sharing identities (IDs). </p>&#xA;&#xA;<p>When your UI needs to display correlated data from two services, instead of replicating data between services (which goes against the loosely-coupled microservices grain as you've heard), you can use client-side composition to assemble the data on the client at runtime. Fetch the data from both services, build a ViewModel composing it, and then display it on the screen. The logic that composes it together need not be coupled to either service. It can take whatever data the services return and compose it dynamically. The service that does the client-side composition is sometimes referred to as the ""IT/Ops Service.""</p>&#xA;&#xA;<p>Here's an example of how that can be accomplished technically:</p>&#xA;&#xA;<p><a href=""https://particular.net/blog/secret-of-better-ui-composition"" rel=""nofollow noreferrer"">https://particular.net/blog/secret-of-better-ui-composition</a></p>&#xA;"
47337425,34357893,7434902,2017-11-16T19:14:16,"<p>Yes man see in <a href=""https://github.com/Netflix/Hystrix/wiki/How-To-Use#request-cache"" rel=""nofollow noreferrer"">docs</a> </p>&#xA;&#xA;<p>How to example of implementation:</p>&#xA;&#xA;<pre><code>public class UserService extends HystrixCommand&lt;Boolean&gt; {&#xA;     private final User user;&#xA;&#xA;     public User getUserById(String id) {&#xA;        super(HystrixCommandGroupKey.Factory.asKey(""AnyUser""));&#xA;        user = storage.get(id);&#xA;        return user;&#xA;    }&#xA;&#xA;    public void update(User user) { // SET&#xA;        super(HystrixCommandGroupKey.Factory.asKey(""AnyUser""));&#xA;        this.user = user;&#xA;        storage.put(user.getId(), user);&#xA;    }&#xA;&#xA;    @Override&#xA;    protected Boolean run() {&#xA;        return user == null || (user != null &amp;&amp; user.id == 0);&#xA;    }&#xA;&#xA;    @Override&#xA;    protected String getCacheKey() {            &#xA;        return (user != null ) ?String.valueOf(user.id):"""";&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
47829287,45996858,2914454,2017-12-15T09:26:50,"<p>It's true that a microservice must be self contained, which means it can be deployed and scale without other microservices. But usually a dbms or message broker or caching system are needed for service to start running. That's why most of CI system such as <a href=""https://docs.gitlab.com/ce/ci/docker/using_docker_images.html#what-is-a-service"" rel=""nofollow noreferrer"">Gitlab CI</a> or <a href=""http://docs.drone.io/services/"" rel=""nofollow noreferrer"">Drone</a> or Jenkins have feature to start other services before executing unit test. If you are not using any CI then it would be sensible to have compose file per component.</p>&#xA;"
47828859,46370014,2914454,2017-12-15T08:57:47,<p>Docker compose create a virtual network on you host and the services are not routable from outside of docker with those IPs. But inside the virtual network all services can access to each other with their names (<code>http://otherservice/</code>).&#xA;If you want to access to all deployed microservices on VM you will need a reverse proxy inside your docker compose. For example an nginx service that dispatch request to other services.</p>&#xA;
31398297,31201083,5027222,2015-07-14T05:28:30,"<p>I'm not sure if that's an option for you, but you could connect to the Consul agent using one of the libraries (e.g. <a href=""https://github.com/OrbitzWorldwide/consul-client"" rel=""nofollow"">consul-client</a>) and utilize it's capabilities for service discovery.</p>&#xA;"
44503927,44503237,3913366,2017-06-12T16:04:09,"<p>From <a href=""https://www.nginx.com/blog/building-microservices-using-an-api-gateway/"" rel=""noreferrer"">this</a> article:</p>&#xA;&#xA;<blockquote>&#xA;  <p>A major benefit of using an API Gateway is that it encapsulates the internal structure of the application. Rather than having to invoke specific services, clients simply talk to the gateway. </p>&#xA;</blockquote>&#xA;&#xA;<p>This simply means that clients talk to microservices through an API gateway and that gateway handles all the requests which comes for any of the underlying services.</p>&#xA;&#xA;<p>So, I think that if you need to do any authorisation for a request, it is beneficial to do it at API gateway itself because this way, an unauthorised request never reaches your service.</p>&#xA;&#xA;<p>But that can't be a complete a solution as well. Let's consider a service which actually authenticate a user. This service will always be accessed by users who haven't been authorised yet, so, it makes sense to implement security for this service at the service level instead of the API gateway.</p>&#xA;&#xA;<p>Also, consider a service which create orders. What if any internal service can call this service and create an order, without requiring any token (or any other authentication process). Now this problem can be solved using many ways like:</p>&#xA;&#xA;<ul>&#xA;<li>Restricted network access to internal services, such at not other service except API gateway can access this service.</li>&#xA;<li>Add/Proxy the security token (like JWT) to all services and validate a request against that token in each of the service.</li>&#xA;</ul>&#xA;&#xA;<p>Of course this is a high level overview, are my thoughts and I am willing to see what others have to say.</p>&#xA;"
44630069,37180375,5554070,2017-06-19T12:02:49,"<p>I know I am very late to answer&#xA;You can approach with prefilter of zuul. The steps you have to follow is given below.</p>&#xA;&#xA;<pre class=""lang-java prettyprint-override""><code> //1. create filter with type pre&#xA; //2. Set the order of filter to greater than 5 because we need to run our filter after preDecoration filter of zuul.&#xA; @Component&#xA; public class CustomPreZuulFilter extends ZuulFilter {&#xA;&#xA;  private final Logger logger = LoggerFactory.getLogger(this.getClass());&#xA;&#xA;@Override&#xA;public Object run() {&#xA;    final RequestContext requestContext = RequestContext.getCurrentContext();&#xA;    logger.info(""in zuul filter "" + requestContext.getRequest().getRequestURI());&#xA;    byte[] encoded;&#xA;    try {&#xA;        encoded = Base64.encode(""fooClientIdPassword:secret"".getBytes(""UTF-8""));&#xA;        requestContext.addZuulRequestHeader(""Authorization"", ""Basic "" + new String(encoded));&#xA;&#xA;        final HttpServletRequest req = requestContext.getRequest();&#xA;        if (requestContext.getRequest().getHeader(""Authorization"") == null&#xA;                &amp;&amp; !req.getContextPath().contains(""login"")) {&#xA;            requestContext.unset();&#xA;            requestContext.setResponseStatusCode(HttpStatus.UNAUTHORIZED.value());&#xA;&#xA;        } else {&#xA;              //next logic&#xA;            }&#xA;        }&#xA;&#xA;    } catch (final UnsupportedEncodingException e) {&#xA;        logger.error(""Error occured in pre filter"", e);&#xA;    }&#xA;&#xA;    return null;&#xA;}&#xA;&#xA;&#xA;&#xA;@Override&#xA;public boolean shouldFilter() {&#xA;    return true;&#xA;}&#xA;&#xA;@Override&#xA;public int filterOrder() {&#xA;    return 6;&#xA;}&#xA;&#xA;@Override&#xA;public String filterType() {&#xA;    return ""pre"";&#xA;}&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>requestContext.unset() will Resets the RequestContext for the current threads active request. and you can provide a response status code.</p>&#xA;"
40990159,39428356,56470,2016-12-06T07:39:37,<p>for me (./mvnw -Pprod);</p>&#xA;&#xA;<ul>&#xA;<li>I create new user (ubuntu / linux)</li>&#xA;<li>I login to that user</li>&#xA;<li>I run  ./mvnw -Pprod command</li>&#xA;<li>problem solved (<em>[ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.0:gulp</em>)</li>&#xA;</ul>&#xA;
50017533,37711051,2549398,2018-04-25T08:32:28,"<p>Microsoft provides a demo webshop application based on .NET Core showing how to apply the microservices pattern:&#xA;<a href=""https://github.com/dotnet-architecture/eShopOnContainers"" rel=""nofollow noreferrer"">https://github.com/dotnet-architecture/eShopOnContainers</a></p>&#xA;&#xA;<p>There is also an ebook available: <a href=""https://aka.ms/microservicesebook"" rel=""nofollow noreferrer"">https://aka.ms/microservicesebook</a></p>&#xA;"
51932898,51932616,1045142,2018-08-20T14:24:54,<p>microservices-dashboard-server is not compatible with Spring Boot 2</p>&#xA;&#xA;<p>And as I see the last commit is older than a year I assume that this is no longer developed.</p>&#xA;
46365006,46364880,1045142,2017-09-22T12:36:40,"<p>You can access header fields in a RestController like this:</p>&#xA;&#xA;<pre><code>@RequestMapping(""/demo"")&#xA;public Demo get(@RequestHeader(""Api-Version"") String apiVersion)  {&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Then you just have to check for the apiVersion.</p>&#xA;&#xA;<p>Personally I prefer the version in the URL.</p>&#xA;"
50270788,50264214,5829526,2018-05-10T10:28:19,"<p>As jjmerelo mentioned, <strong>installations are immutable</strong>, one solution is to download source code(include META6.json file), edit the code you want, then:</p>&#xA;&#xA;<pre><code>zef install . --/test&#xA;</code></pre>&#xA;&#xA;<p>For simple test, it's ok for me.</p>&#xA;&#xA;<p>As for <code>application/json; chartset=utf-8;</code> can't parse, I add a <code>;</code> in <code>MediaType.pm6</code>'s <strong><code>token</code></strong> token, make it possible to include <code>;</code> (maybe it's a bug, I don't know):</p>&#xA;&#xA;<pre><code>token token { &lt;[A..Za..z0..9;!#$%&amp;'*+^_`{|}~-]&gt;+ }&#xA;</code></pre>&#xA;&#xA;<p>installed locally, and it parse ok now.</p>&#xA;"
45223515,45215621,783510,2017-07-20T19:19:25,"<p>If your organization uses a monolithic code base (i.e., all code is stored in one repository), I would strongly recommend to use the same file. The alternative is only to copy the file but then you have to keep all the versions in sync.</p>&#xA;&#xA;<p>If you share protocol buffer file between the sender and the receiver, you can statically check that both the sender and the receiver use the same schema, especially if some new microservices will be written in a statically typed language (e.g., Java).</p>&#xA;&#xA;<p>On the other hand, if you do not have a monolithic code base but instead have multiple repositories (e.g., one per microservice), then it is more cumbersome to share the protocol buffers file. What you can do is to put them in separate repositories that can be added as an dependency to microservices that need them. That is what I have seen in my previous company. We had multiple small API repositories for the schema.</p>&#xA;&#xA;<p>So, if it is easy to use the same file, I would recommend to do so instead of creating copies. There may be situations, however, where it is more practical to copy them. The disadvantage is that you always have to apply a change at all copies. In best case, you know which files to update, then it is just tedious. In the worst case, you do not know which files to update, and your schema will get out of sync. Only when the code is released, you will find out.</p>&#xA;&#xA;<p>Note that monolithic code base does not mean monolithic architecture. You can have microservices and still keep all the source code together in one repository. The famous example is, of course, Google. Google also heavily uses protocol buffers for their internal communication. I have not seen their source code, but I would be surprised if they do not share their protocol buffer files between services.</p>&#xA;"
35614201,35613841,2199,2016-02-24T22:10:07,"<p>First - clarify your distinctions between ""real-time"", ""synchronous/asynchronous"" and ""one-way/two-way"". The things you rule out (queues, and pub/sub) can certainly be used for two-way request/response, but they are asynchronous.</p>&#xA;&#xA;<p>Second - clarify ""efficiency"" - efficiency on what metric? Bandwidth? Latency? Development time? Client support?</p>&#xA;&#xA;<p>Third - realize that (one of) the costs of microservices is latency. If that's an issue for you at your first integration, you're likely in for a long road.</p>&#xA;&#xA;<blockquote>&#xA;  <p>What's the different ways i could communicate between my main API and other microservice API's? Pros/cons of each approach?</p>&#xA;</blockquote>&#xA;&#xA;<p>Off the top of my head:</p>&#xA;&#xA;<ul>&#xA;<li>Host on single node(s) and IPC: pros: performance; cons: tight coupling to deployment topology; reachability to other nodes; client support; failure modes</li>&#xA;<li>REST/SOAP/etc. endpoints: pros: wide client and server support; debugging; web caching, cons: performance; failure modes</li>&#xA;<li>Binary protocol: pros: performance; cons: versioning; client and server support; debugging; failure modes</li>&#xA;<li>Messaging Queues: pros: asynchronicity; cons: complexity</li>&#xA;<li>ESB: pros: asynchronicity; client support; cons: complexity</li>&#xA;<li>Flat files: pros: bandwidth; simplicity; client support; cons: latency, general PITA</li>&#xA;</ul>&#xA;&#xA;<p>You'll note that this is the same list when we tie multiple applications together...because that's what you're doing. Just cause you made the applications smaller doesn't really change much except makes your system even <em>more</em> distributed. Expect to solve all the same problems ""normal"" distributed systems have, and then a few extra ones related to deployment and versioning.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Consider an idempotent GET request from a user like ""Get me question 1"". That client expects a JSON response of question 1. Simple. In my expected architecture, the client would hit api.myapp.com, which would then proxy a call via REST to question-api.myapp.com (microservice) to get the data, then return to user. How could we use pub/sub here? Who is the publisher, who is the subscriber? There's no event here to raise. My understanding of queues: one publisher, one consumer. Pub/sub topic: one publisher, many consumers. Who is who here?</p>&#xA;</blockquote>&#xA;&#xA;<p>Ok - so first, if we're talking about microservices and latency - we're going to need a more representative example. Let's say our client is the Netflix mobile app, and to display the opening screen it needs the following information:</p>&#xA;&#xA;<ol>&#xA;<li>List of trending movie ids</li>&#xA;<li>List of recently watched movie ids</li>&#xA;<li>Account status</li>&#xA;<li>For each movie id referenced: the name, # of stars, summary text</li>&#xA;<li>List of movie ids not available in your region (to be filtered out of trending/recently watched)</li>&#xA;</ol>&#xA;&#xA;<p>Each one of those is provided by a different microservice (we'll call them M1-M5). Each call from client -> datacenter has 100ms expected latency; calls between services have 20ms latency.</p>&#xA;&#xA;<p>Let's compare some approaches:</p>&#xA;&#xA;<h3>1: Monolithic service</h3>&#xA;&#xA;<ol>&#xA;<li>T0 + 100ms: Client sends request for /API//StartScreen; receives response</li>&#xA;</ol>&#xA;&#xA;<p>As expected, that's the lowest latency option - but requires everything in a monolithic service, which we've decided we don't want because of operational concerns. </p>&#xA;&#xA;<h3>2: Microservices</h3>&#xA;&#xA;<ol>&#xA;<li>T0 + 100ms: Client sends request to M1 /API/Trending</li>&#xA;<li>T1 + 100ms: Client sends request to M2 /API//Recent</li>&#xA;<li>T2 + 100ms: Client sends request to M3 /API//Account</li>&#xA;<li>T3 + 100ms: Client sends request to M4 /API/Summary?movieids=[]</li>&#xA;<li>T4 + 100ms: Client sends request to M5 /API//Unavailable</li>&#xA;</ol>&#xA;&#xA;<p>That's 500ms. Using a proxy w/this isn't going to help - it'll just add 20ms latency to each request (making it 600ms). We have a dependency between 1 + 2 and 4, and 3 and 5, but can do some async. Let's see how that helps.</p>&#xA;&#xA;<h3>3: Microservices - Async</h3>&#xA;&#xA;<ol>&#xA;<li>T0: Client sends request to M1 /API/Trending</li>&#xA;<li>T0: Client sends request to M2 /API//Recent</li>&#xA;<li>T0: Client sends request to M3 /API//Account</li>&#xA;<li>T0 + 200ms: (response from 1 + 2) Client sends request to M4 /API/Summary?movieids=[]</li>&#xA;<li>T0 + 200ms: (response from 3) Client sends request to M5 /API//Unavailable</li>&#xA;</ol>&#xA;&#xA;<p>We're down to 200ms; not bad - but our client needs to know about our microservice architecture. If we abstract that with our proxy, then we have:</p>&#xA;&#xA;<h3>4: Microservices - Async w/Gateway</h3>&#xA;&#xA;<ol>&#xA;<li>T0 + 100ms: client sends request to G1 /API/StartScreen</li>&#xA;<li>T1: G1 sends request to M1 /API/Trending</li>&#xA;<li>T1: G1 sends request to M2 /API//Recent</li>&#xA;<li>T1: G1 sends request to M3 /API//Account</li>&#xA;<li>T1 + 40ms: (response from 1 + 2) G1 sends request to M4 /API/Summary?movieids=[]</li>&#xA;<li>T1 + 40ms: (response from 3) G1 sends request to M5 /API//Unavailable</li>&#xA;</ol>&#xA;&#xA;<p>Down to 140ms, since we're leveraging the decreased intra-service latency.</p>&#xA;&#xA;<p>Great - when things are working smoothly, we've only increased latency by 40% compared to monolithic (#1). </p>&#xA;&#xA;<p>But, as with any distributed system, we also have to worry about when things aren't going smoothly. </p>&#xA;&#xA;<p>What happens when M4's latency increases to 200ms? Well, in the client -> async microservice route (#3), then we have partial page results in 100ms (the first batch of requests), unavailable in 200ms and summaries in 400ms. In the proxy case (#4), we have nothing until 340ms. Similar considerations if a microservice is completely unavailable.</p>&#xA;&#xA;<p>Queues are a way of abstracting producer/consumer in space and time. Let's see what happens if we introduce one:</p>&#xA;&#xA;<h3>5: Microservice with async pub/sub</h3>&#xA;&#xA;<ol>&#xA;<li>T0 + 100ms: client publishes request Q0 to P1 StartScreen, with a reply channel of P2</li>&#xA;<li>T1 + 20ms: M1 sees Q0, puts response R1 to P2 Trending/Q0</li>&#xA;<li>T1 + 20ms: M2 sees Q0, puts response R2 to P2 Recent/Q0</li>&#xA;<li>T1 + 20ms: M3 sees Q0, puts response R3 to P2 Account/Q0</li>&#xA;<li>T2 + 40ms: M4 sees R1, puts response R4a to P2 Summary/Q0</li>&#xA;<li>T2 + 40ms: M4 sees R2, puts response R4b to P2 Summary/Q0</li>&#xA;<li>T2 + 40ms: M5 sees R3, puts response R5 to P2 Unavailable/Q0</li>&#xA;</ol>&#xA;&#xA;<p>Our client, who is subscribed to P2 - receives partial results w/a single request and is abstracted away from the workflow between M1 + M2 and M4, and M3 and M5. Our latency in best case is 140ms, same as #4 and in worst case is similar to the direct client route (#3) w/partial results. </p>&#xA;&#xA;<p>We have a much more complicated internal routing system involved, but have gained flexibility w/microservices while minimizing the inevitable latency. Our client code is also more complex - since it has to deal with partial results - but is similar to the async microservice route. Our microservices are generally independent of each other - they can be scaled independently, and there is no central coordinating authority (like in the proxy case). We can add new services as needed by simply subscribing to the appropriate channels, and having the client know what to do with the response we generate (if we generate one for client consumption of course).</p>&#xA;&#xA;<h3>6 Microservices - Gateway with Queues</h3>&#xA;&#xA;<p>You could do a variation of using a gateway to aggregate responses, while still using queues internally. It would look a lot like #4 externally, but #5 internally. The addition of a queue (and yes, I've been using queue, pub/sub, topic, etc. interchangeably) still decouples the gateway from the individual microservices, but abstracts out the partial result problem from the client (along w/its benefits, though).</p>&#xA;&#xA;<p>The addition of a gateway, though, does allow you to handle the partial result problem centrally - useful if it's complex, ever changing, and/or reimplemented across multiple platforms. </p>&#xA;&#xA;<p>For instance, let's say that, in the event that M4 (the summary service) is unavailable - we have a M4b that operates on cached data (so, eg., the star rating is out of date). M4b can answer the R4a and R4b immediately, and our Gateway can then determine if it should wait for M4 to answer or just go w/M4b based on a timeout.</p>&#xA;&#xA;<p>For further info on how Netflix <em>actually</em> solved this problem, take a look at the following resources:</p>&#xA;&#xA;<ol>&#xA;<li>Their <a href=""http://techblog.netflix.com/2013/01/optimizing-netflix-api.html"" rel=""noreferrer"">API Gateway</a></li>&#xA;<li>The messaging patterns underlying their <a href=""http://techblog.netflix.com/2012/02/fault-tolerance-in-high-volume.html"" rel=""noreferrer"">fault-tolerance layer</a></li>&#xA;<li>More <a href=""https://www.nginx.com/blog/building-microservices-inter-process-communication/"" rel=""noreferrer"">about IPC in microservices</a></li>&#xA;</ol>&#xA;"
35614254,35613841,7831,2016-02-24T22:12:49,"<p>To start I would recommend breaking up your monolithic REST service into multiple micro-services, each specific to a single data entity or use case. For example: Sending emails out would be a single use case, and querying  results for a search page would be a single use case.</p>&#xA;&#xA;<p>Then as you roll out each individual service, you could modify your monolithic service to be a proxy to the new micro-services. This way your existing apps could keep calling your monolithic service, and the new app updates could call the micro-services appropriately.</p>&#xA;&#xA;<p>Basically, migrate from a monolithic service to micro-services incrementally, one at a time.</p>&#xA;&#xA;<blockquote>&#xA;  <p>The only approach that comes to mind is simple REST-based calls. Totally fine, but latency is the main issue here.</p>&#xA;</blockquote>&#xA;&#xA;<p>Using the monolithic web service as a Proxy to the micro-services will add some latency, but the individual micro-services won't add any latency, and you can scale those out to as many hosted instances behind an Azure Load Balancer as needed.</p>&#xA;&#xA;<p>If you do see query latency, then you could implement something like a Redis cache for any micro-services that exhibit performance problems with queries.</p>&#xA;&#xA;<p>If you do see write latency that becomes a problem, then I would recommend moving those problem writes to use some kind of Messaging Queue to allow for them to be handled asynchronously if possible.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Is there anything in Azure suited to this?</p>&#xA;</blockquote>&#xA;&#xA;<p>You may want to look into <a href=""https://azure.microsoft.com/en-us/services/app-service/api/"" rel=""nofollow"">Azure API Apps</a>, as they may offer some nice functionality for securing / authenticating with your micro-services via Azure more easily than securing the individually.</p>&#xA;&#xA;<blockquote>&#xA;  <p>What's the different ways i could communicate between my main API and other microservice API's?</p>&#xA;</blockquote>&#xA;&#xA;<p>It sounds like you need real-time communication between services. So, a message queue is out of the question, unless you have a task that can be performed in the background. You could use simple REST calls between services, or even something like SOAP if implemented with WCF. Although, it sounds like you currently are using REST, so it makes sense to keep it the same.</p>&#xA;&#xA;<p>Also, make sure your micro-services aren't communicating with each other through a database, as that can get messy real quick. Use a simple REST endpoint for them to communicate, similar to the REST endpoints used by the client apps.</p>&#xA;&#xA;<p>I like to follow the KISS (Keep It Simple Stupid) principle, so I would recommend you try not to over architect the solution and just keep it as simple as possible.</p>&#xA;"
35609762,35600997,1325207,2016-02-24T18:08:34,"<p>There are basically two choices to giving your workers access to the messages routed by ejabberd / MongooseIM. I'll focus on MongooseIM, since I know it better (<strong>DISCLAIMER</strong>: I'm in the dev team).</p>&#xA;&#xA;<p>The first is to scan the message archive in an async / polling fashion. The <a href=""http://xmpp.org/extensions/xep-0313.html"" rel=""nofollow"">Message Archive Management</a> describes XMPP level protocol for accessing it, but for your use case the important part is message persistence - so just making sure the relevant module (<code>mod_mam</code>) is enabled in server config and the messages will hit the database. The databases supported for MAM are PostgreSQL and Riak, though there was also some work on a Cassandra backend (YMMV). This doesn't require tinkering with the server / in Erlang for as long as there's a DB driver for your language of choice available. Since <a href=""https://github.com/esl/MongooseIM/pull/657"" rel=""nofollow"">PR#657</a> it's possible to store the messages in raw XML or even some custom format if you're willing to write the serialization module.</p>&#xA;&#xA;<p>The second option is to use the server mechanism of <a href=""http://mongooseim.readthedocs.org/en/latest/developers-guide/Hooks-and-handlers/"" rel=""nofollow"">hooks and handlers</a> (also available in ejabberd), which can trigger a server action on events like ""user sent a message"", ""user logged in"", ""user logged out"", ... This, however, requires a server side extension written in Erlang. In the simplest case the extension could forward any interesting event (with message content and metadata) via AMQP or just call some external HTTP/REST API - that way the real work is carried out by the workers giving you the freedom with regard to implementation language. This options also doesn't require to enable <code>mod_mam</code> or set up a database for message persistency (which you could still have with a persistent message queue...).</p>&#xA;&#xA;<p>In general, the idea is perfectly feasible.</p>&#xA;"
43462287,43460615,7187607,2017-04-18T01:51:44,"<p>I partially understood your question. Based on what I perceived from your question, I would recommend you to mention the Copy command to copy the script you want to run into the dockerfile. Once you build an image and run the container, start the db service. Then exec the container and get into the container, run the script manually. </p>&#xA;&#xA;<p>If you have <strong>CMD</strong> command in the dockerfile, then it will be overwritten by the command you specify during the execution. So, I don't think you have any other option to run the script unless you don't have CMD in the dockerfile.</p>&#xA;"
26617453,26611387,574103,2014-10-28T20:02:32,"<p>Humm, I don't really understand what is your real need ? What is the problem that you try to solve with trying to use DDD concepts or DTO...</p>&#xA;&#xA;<p>DTO is not comparable to aggregate root (an entity as well), an entity has datas and behaviors (that is not the case for DTO, that is simply data).</p>&#xA;&#xA;<p>So, the domain model, should be domain Driven ;-), and some building blocks are usefull to implement it, ex: Entity, Aggregate... And when you use an ORM, it can be hard to isolate your domain, so you have to try to have something pure in your Domain with less noise as you can. There are many strategies to solve it.</p>&#xA;&#xA;<p>You can find more there : <a href=""http://elegantcode.com/2009/11/13/dtos-ddd-the-anemic-domain-model/"" rel=""nofollow"">http://elegantcode.com/2009/11/13/dtos-ddd-the-anemic-domain-model/</a></p>&#xA;"
48271809,48271493,574103,2018-01-15T22:52:05,"<p>No relationship between Microservices and DDD... And yes you can definitely do one without the other.&#xA;Bounded context is domain level and microservice it's an implementation/infrastructure/architecture whatsoever, they can be related if you want to...</p>&#xA;&#xA;<p><a href=""https://martinfowler.com/tags/domain%20driven%20design.html"" rel=""nofollow noreferrer"">Domain Driven Design</a> is a set of tactical and strategical design principles introduced by Eric Evans in 2003... to tackle complexity in software </p>&#xA;&#xA;<p><a href=""https://martinfowler.com/articles/microservices.html"" rel=""nofollow noreferrer"">Microservices</a> is an architecture for designing an app as suites of independently deployable services</p>&#xA;&#xA;<p>CQRS and Event Sourcing are a way to implement a Domain Driven Design (could be seen as building block in DDD), but they could be used outside DDD like the other building blocks.</p>&#xA;"
40081264,39591223,510615,2016-10-17T08:02:35,"<p>Yes, you can absolutely have both architectures in the same environment. Here is why:</p>&#xA;&#xA;<p>Microservices Architecture is heavily inspired by SOA, so naturally principles used in both are very similar and there's often confusion between both. However, you should note that <strong>these paradigms differ in scope</strong>. <strong>Microservices Architecture</strong> is an approach to design an application in a specific way, while <strong>SOA</strong> aims to put order in communication between multiple heterogeneous applications (often in enterprise environment) in different domains, avoid point-to-point communication by having a middleware in place and generally reduce integration effort and increase ROI in the long term. SOA is not just an architectural approach but offers a whole new approach to optimize the IT domain in an enterprise. It often requires changes to processes and hierarchy, because a SOA governance body would need to be in place at some point to ideally enforce enterprise policies.</p>&#xA;&#xA;<p>So in practice you'll very often see something like this - many application architecture styles in the same general SOA:</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/SOhum.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/SOhum.png"" alt=""many application architecture styles in the same general SOA""></a></p>&#xA;&#xA;<p>Hope this helps.</p>&#xA;"
41096734,41094112,510615,2016-12-12T08:23:54,"<p>The very purpose of choosing to go with Microservice Architecture is to achieve scalability and agility through independence of services. Here is an excerpt from a <a href=""https://smartbear.com/learn/api-design/what-are-microservices/"" rel=""nofollow noreferrer"">good article on the matter, you might find interesting</a>. It is by a pretty credible source - SmartBear (the creators of SOAPUI):</p>&#xA;&#xA;<blockquote>&#xA;  <p>While there is no standard, formal definition of microservices, there&#xA;  are certain characteristics that help us identify the style. &#xA;  Essentially, microservice architecture is a method of developing&#xA;  software applications as a suite of independently deployable, small,&#xA;  modular <strong>services in which each service runs a unique process</strong> and&#xA;  communicates through a well-defined, lightweight mechanism to serve a&#xA;  business goal.</p>&#xA;</blockquote>&#xA;&#xA;<p>So yes, you should absolutely go for deployment as separate processes. You will be much more flexible this way. For example, running all 5 services in a single process could make it impossible to hot-swap new versions of single microservices without stopping all the others (even though they might not be logical interdependence between all services).</p>&#xA;"
43665378,43665011,2294168,2017-04-27T18:25:53,"<blockquote>&#xA;  <p>When a microservice registers itself with eureka whichever way, does this registry get replicated across all the eureka containers</p>&#xA;</blockquote>&#xA;&#xA;<p>Never tested by myself, but I think this happens. Registries contact themself and share all current registred microservices from everywhere.</p>&#xA;&#xA;<blockquote>&#xA;  <p>who know which eureka node in swarm cluster would service a particular microservice</p>&#xA;</blockquote>&#xA;&#xA;<p>Your microservice has an unique name in its <code>application.properties</code> that allows others microservices to send it an HTTP request, you don't control that mechanism in java so I hope it contacts the most suitable. </p>&#xA;&#xA;<blockquote>&#xA;  <p>Would it specify the ip address of the master node in the swarm cluster in its config for eureka server ?</p>&#xA;</blockquote>&#xA;&#xA;<p>I tryed several times to setup an hostname to let microservice contacts eureka &#xA; using a container based hostname but that doesn't work. I use <a href=""https://hub.docker.com/r/frolvlad/alpine-oraclejdk8/"" rel=""nofollow noreferrer"">frolvlad/alpine-oraclejdk8</a> as base image + bash and <a href=""https://github.com/iturgeon/wait-for-it"" rel=""nofollow noreferrer"">wait-for-it.sh</a>. I run everything with <code>docker-compose</code>.<br>&#xA;Keep in mind that this is the configuration server that tells microservices where is eureka. I endup to add a <code>bash</code> script in the configuration server to extract the <code>application.properties</code> from the <code>jar</code> file, put the IP of the registry, and rewrite the updated <code>application.properties</code> into the <code>jar</code>.</p>&#xA;"
44102527,43598939,2754712,2017-05-21T22:45:54,"<p>This depends about the Architecture that you are adopting.&#xA;If I understood the question, <strong>you already have the broker with the kafka message server.</strong>&#xA;I think that you can use the architecture publish/subscribe to assyncronous message.</p>&#xA;&#xA;<p>If in the backend architecture the legacy systems to support SLA, in this case you can use the rest endpoints necessary to the integration.</p>&#xA;&#xA;<p>This is the gain of if to utilize API Gateway Pattern in the Architecture.</p>&#xA;&#xA;<p>Thanks a lot.</p>&#xA;"
41745829,41745636,2380952,2017-01-19T15:36:39,"<p>Consider using <a href=""https://github.com/alferov/angular-file-saver"" rel=""nofollow noreferrer"">ngFileSaver</a></p>&#xA;&#xA;<p>Example:</p>&#xA;&#xA;<p><strong>JS</strong></p>&#xA;&#xA;<pre><code>function ExampleCtrl(FileSaver, Blob) {&#xA;  var vm = this;&#xA;&#xA;  vm.val = {&#xA;    text: 'Hey ho lets go!'&#xA;  };&#xA;&#xA;  vm.download = function(text) {&#xA;    var data = new Blob([text], { type: 'text/plain;charset=utf-8' });&#xA;    FileSaver.saveAs(data, 'text.txt');&#xA;  };&#xA;}&#xA;&#xA;angular&#xA;&#xA;      .module('fileSaverExample', ['ngFileSaver'])&#xA;      .controller('ExampleCtrl', ['FileSaver', 'Blob', ExampleCtrl]);&#xA;</code></pre>&#xA;&#xA;<p><strong>HTML</strong></p>&#xA;&#xA;<pre><code>&lt;div class=""wrapper"" ng-controller=""ExampleCtrl as vm""&gt;&#xA;  &lt;textarea&#xA;    ng-model=""vm.val.text""&#xA;    name=""textarea"" rows=""5"" cols=""20""&gt;&#xA;      Hey ho let's go!&#xA;  &lt;/textarea&gt;&#xA;  &lt;a href="""" class=""btn btn-dark btn-small"" ng-click=""vm.download(vm.val.text)""&gt;&#xA;    Download&#xA;  &lt;/a&gt;&#xA;&lt;/div&gt;&#xA;</code></pre>&#xA;"
51088766,48784283,2519704,2018-06-28T17:44:55,<p>The client credentials grant type is for when you want your application to contact the server with out a user. For example a weather app will contact the server to get the latest weather data. It does not need to user to login to do this. You want to client credentials because you do not want  everyone using your api. It is a way to protect your api and only allow your approved apps to access the api information.</p>&#xA;&#xA;<ol>&#xA;<li><p>Yes you will pass the client_id/client_secret from your mobile/desktop application to the /oauth/token url on your server so it will return an access token for you to use when getting all the information through the api.</p></li>&#xA;<li><p>The client_id and client secret do not need a user. It is for the client (Mobile/Desktop application). But it will be the same for every user since they all will be using the same mobile/Desktop application.</p></li>&#xA;</ol>&#xA;
50245683,50111276,958616,2018-05-09T04:45:33,"<p>I really like to connect pieces of tech to solve complex problems. Though it's not that complex.  </p>&#xA;&#xA;<ol>&#xA;<li><p>Solution 1: Cloud-based, specifically on AWS<br>&#xA;Use AWS <a href=""https://aws.amazon.com/lambda/"" rel=""nofollow noreferrer"">Lambdas</a>(Serverless compute) to hit the API to get prices or whatever info you are seeking and then store it in <a href=""https://aws.amazon.com/dynamodb/"" rel=""nofollow noreferrer"">DynamoDb</a>(A NoSQL DB). Use <a href=""https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html"" rel=""nofollow noreferrer"">CloudWatch Rules</a>(Serverless CRON job) to invoke your lambda periodically.<br>&#xA;Then SPA Single page application to view values stored in DynamoDb. It can be a <a href=""https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html"" rel=""nofollow noreferrer"">static website hosted on S3</a> also.<br>&#xA;Or<br>&#xA;A mobile app can also serve the purpose of viewing the data from DynamoDb.  </p></li>&#xA;<li><p>Solution 2: Mobile-Only<br>&#xA;Why not build the app purely for mobiles like iOS or Android. Check <a href=""https://play.google.com/store/apps/details?id=com.raevilman.bitcoinpriceindianexchanges"" rel=""nofollow noreferrer"">here</a> I've coded one app just to track the price of different alt-coins of different exchanges.<br>&#xA;With the mobile-only app, your app will fetch the prices periodically(Using alarms API in case of Android) and will store in its local database(SQLite in case of Android) and then you can open the app any time to see the latest values.</p></li>&#xA;</ol>&#xA;&#xA;<p>More solutions can be thought of, But I think above are a good approach for solving this problem rather than buying a VPS or blowing your laptop all 24X7 #ThinkCloud</p>&#xA;&#xA;<p>PS: Initial thoughts only, Ask more to enhance the solution... :)</p>&#xA;"
41289674,41287712,4106392,2016-12-22T18:34:06,<p>No in this case consul is acting purely as a DNS server and returning a healthy instance's ip address. As you have seen with your dig and ping commands.</p>&#xA;&#xA;<p>You have to specify port as you did with your curl call. </p>&#xA;&#xA;<p>To hack this you could modify your application to run on http default port 80. </p>&#xA;&#xA;<p>If this is not possible you can setup what is known as a reverse proxy.</p>&#xA;&#xA;<p>Nginx works great for this. Here is a snippet of a configuration that will allow you to handle this:</p>&#xA;&#xA;<pre><code>location / {&#xA;    proxy_pass http://127.0.0.1:3033;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>This will pass all your port 80 calls to localhost port 3033. Then your curl calls would work as expected without specifying port.</p>&#xA;
41471825,41293238,4106392,2017-01-04T19:30:05,"<p>Your design sounds good. With haproxy you can limit number of clients  connections to any given instance which is a huge issue with websockets. I would take the stateless approach and handle errors with code. When a client detects failure, just re-connect to any instance. Keep all persistent data in some kind of global store. Depending on use case, as you mentioned, Redis would work great.</p>&#xA;&#xA;<p>As a side note, Websockets keeps persistent connections open and depending on volume you might need a pool of haproxy instances. Would recommend at least two as starting point for availability.</p>&#xA;"
34159170,34158847,706456,2015-12-08T15:03:37,"<p><strong>#1</strong></p>&#xA;&#xA;<blockquote>&#xA;  <p>If I need to show 100 orders and each order has a unique customer_id, will I need to do 100 REST API call?</p>&#xA;</blockquote>&#xA;&#xA;<p>No, just make 1 request with 100 order_id(s) and return a dictionary of order_id &lt;=> customer_id</p>&#xA;&#xA;<p><strong>#2</strong><br/>&#xA;It's a single request</p>&#xA;&#xA;<pre><code>POST&#xA;/orders/new&#xA;{&#xA;    ""selected_service_provider_id"" : ""123""&#xA;    ...&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Which can return you <code>order_id</code> and you can print it locally for the customer or track progress or what have you.</p>&#xA;&#xA;<p>On the server side, you receive an order and process it. Processing can include sending an SMS at some stage. This functionality can be implemented inside original service that received this request or as a separate call to another dedicated service.</p>&#xA;"
46241104,46240289,6546584,2017-09-15T13:45:37,"<p>Try with something like this:</p>&#xA;&#xA;<pre><code>.authorizeRequests()&#xA;        .antMatchers(HttpMethod.OPTIONS, ""/**"").permitAll()&#xA;        .antMatchers(""/login/**"").permitAll()&#xA;        .anyRequest().authenticated();&#xA;</code></pre>&#xA;&#xA;<p>Notice that I've allowed options request in for every endpoint (since react exploits them before POST and PUT requests) and I've changed the login path regex to <code>/login/**</code>.</p>&#xA;"
46242482,46240639,6546584,2017-09-15T14:57:13,"<p>This is the default behaviour.</p>&#xA;&#xA;<p>You can add <strong>@CrossOrigin</strong> annotation to your controllers, or play a bit with Filters.</p>&#xA;"
31578875,31206417,404121,2015-07-23T05:31:06,"<p>I found this paper useful in shaping my thinking on the subject, it may be of help to you too.</p>&#xA;&#xA;<p>Pg. 21, but read the paper before this as you need the context</p>&#xA;&#xA;<p><a href=""http://wso2.com/whitepapers/practical-soa-for-the-solution-architect/"" rel=""nofollow"">Practical SOA for Solution Architect</a></p>&#xA;"
27071793,26616962,4280473,2014-11-21T22:58:52,"<p>I think Martin Fowlers’ article falls woefully short by mischaracterising the ‘ESB’ concept.  This mischaracterisation is widespread.  How many time have you seen a diagram that represents the ‘bus’ as a pipe down which messages flow?  I’ve certainly lost count and it makes me wince every time.  A ‘bus’ is not a pipe.  It is a mechanism for making ‘enabling services’ readily accessible across a distributed, service-orientated environment.  The classic analogy is a bus bar in a factory.  Although electricity flows through the bus bar, that’s not why it a ‘bus’.  It is a ‘bus’ because it runs the full length of the manufacturing floor.  Any machinery (service implementations) can tap easily into the bar to get power (from a generating service), wherever that machinery happens to be located.  The bus is a ubiquitous enabler which supports flexibility and change over time.</p>&#xA;&#xA;<p>The only things that live on a service bus are services, and as a general principle they are best implemented as microservices wherever possible or desirable.  The mantra of ‘smart endpoints, dumb pipes’ goes back way before the advent of microservices.  I first heard it from a member of the Microsoft BizTalk team many years ago in public debate with a leading advocate of ESB.   The ESB guy was advocating the desirability of very fine-grained stand-alone transformation services (microservices) rather than the typical BizTalk approach where transformations are applied at endpoints (smart endpoints).  The ESB guy was criticising BizTalk, claiming that it was ‘monolithic’ and therefore could not be used to implement such fine-grained, independently deployable services.  The BizTalk guy pointed out that he was factually wrong (as demonstrated subsequently in the BizTalk ESB toolkit), but that the main point was the general desirability of doing transformation at the message endpoints (from an integration perspective) rather than downstream in some intermediate service invoked in the interchange (conceptually, further down the ‘pipe’).</p>&#xA;&#xA;<p>This was a grown-up debate between serious practitioners.  I agreed with the BizTalk guy on this point – smart endpoints, dumb pipes.  Ironically, the ESB guy was effectively promoting a microservice approach in an ESB context.  To me, it’s a great example of how the microservice mantra, like any other philosophy, can be taken a step too far.</p>&#xA;"
51500763,51477318,6641561,2018-07-24T14:08:36,"<p>My company Progress DataDirect offers a <a href=""https://www.progress.com/cloud-and-hybrid-data-integration"" rel=""nofollow noreferrer"">Data API Gateway called Hybrid Data Pipeline</a> (HDP). HDP allows users to instantly <a href=""https://www.progress.com/tutorials/cloud-and-hybrid/instant-odata-rest-api-access-for-popular-data-sources"" rel=""nofollow noreferrer"">OData-enable popular databases</a>. And we also have a feature called Data Source Group. A Data Source Group contains references to multiple OData-enabled Data Source definitions, enabling you to access them all with the same Resource Path. Does this help you solve this problem? You can refer to this <a href=""https://documentation.progress.com/output/DataDirect/hybridpipeline/#page/hybrid%2Fcreating-a-data-source-group.html"" rel=""nofollow noreferrer"">documentation page</a> for more details.</p>&#xA;&#xA;<p>Do note that our OData endpoints are currently used by several enterprises to access their databases from Power BI Desktop.</p>&#xA;"
46596788,46583498,6064025,2017-10-06T00:54:53,"<p>Step 1 : Please create your account in hub.docker.com. , with your user name.</p>&#xA;&#xA;<p>Step 2 : Build your image locally using your Dockerfile&#xA;        <code>$ docker build -t=""mysql_mac"" -f mysql_dockerfile .</code>,In these case my image name is <code>mysql_mac</code></p>&#xA;&#xA;<p>Step 3 : It will create the image called mysql_mac and now tag it and push to hub.docker.com</p>&#xA;&#xA;<p>Step 4 : <code>$ docker tag mysql_mac aamir2292/mysql_mac</code>. <strong>Note</strong> my login name is <code>aamir2292</code> on docker-hub.</p>&#xA;&#xA;<p>Step 5 : <code>$ docker push aamir2292/mysql_mac</code></p>&#xA;&#xA;<p><strong>Congrats You have created a mysql image.</strong></p>&#xA;"
51964114,39967784,1546137,2018-08-22T09:48:15,"<p>I found a good explanation done by <a href=""https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/architect-microservice-container-applications/service-oriented-architecture"" rel=""nofollow noreferrer"">Microsoft</a> : </p>&#xA;&#xA;<blockquote>&#xA;  <p>Microservices derive from SOA, but SOA is different from microservices&#xA;  architecture. Features like big central brokers, central orchestrators&#xA;  at the organization level, and the Enterprise Service Bus (ESB) are&#xA;  typical in SOA. But in most cases, these are anti-patterns in the&#xA;  microservice community. In fact, some people argue that “<strong>The&#xA;  microservice architecture is SOA done right</strong>.”</p>&#xA;</blockquote>&#xA;"
50629902,50629814,7185049,2018-05-31T17:39:21,"<p>Interesting.Here is my attempt.</p>&#xA;&#xA;<pre><code>const inits = [&#xA;    {id: 1, name: ""test"", tagId: 1},&#xA;    {id: 2, name: ""test"", tagId: 15},&#xA;    {id: 3, name: ""test"", tagId: 5},&#xA;];&#xA;&#xA;// Get all ids&#xA;const ids = inits.map(init =&gt; init.tagId);&#xA;&#xA;const results = [&#xA;    {id: 1, name: ""tag1""},&#xA;    {id: 15, name: ""tag2""},&#xA;    {id: 5, name: ""tag3""},&#xA;];&#xA;&#xA;// Add results to object&#xA;const final = inits.map(init =&gt; {&#xA;    init.tag = results.find(res =&gt; res.id === init.tagId);&#xA;    return init;&#xA;});&#xA;</code></pre>&#xA;&#xA;<p>[Updated]</p>&#xA;&#xA;<p>If you can be certain order of the objects will remain the same, you can speed it up:</p>&#xA;&#xA;<pre><code>// Add results to object&#xA;const final = inits.map((init, index) =&gt; {&#xA;    init.tag = results.[index];&#xA;    return init;&#xA;});&#xA;</code></pre>&#xA;&#xA;<p>Why is JavaScript so sexy :(</p>&#xA;"
39450433,39435028,189849,2016-09-12T12:32:47,"<p>Microservices shouldn't be <em>sub-modules</em> of some <em>core</em> module.</p>&#xA;&#xA;<p>Imagine microservices as quite independent entities.&#xA;They might be written <em>eventually</em> in the same programming language as the core module but it's not necessary. They are communicating through REST API after all. You can process REST API with nearly all programming languages.</p>&#xA;&#xA;<p>The least is the compile or runtime dependency between such kind of services it's the better. The best if services can be implemented by different group of developers, probably from different companies or cultures. Also, it's good if (micro)services can ride out temporary outages of other (micro)services in production.</p>&#xA;&#xA;<p>A <a href=""http://shop.oreilly.com/product/0636920033158.do"" rel=""nofollow"">book</a> to read.</p>&#xA;"
43140824,43132158,7797657,2017-03-31T12:34:41,"<p>We can create endpoint with a proxy or select Basic endpoint if you create your API outside API Manager, for example, you created the API using Mule ESB. You don’t need a proxy in this case. So policies will be applied to API. For more details go through the link. &#xA;<a href=""https://docs.mulesoft.com/api-manager/setting-up-an-api-proxy"" rel=""nofollow noreferrer"">https://docs.mulesoft.com/api-manager/setting-up-an-api-proxy</a></p>&#xA;"
36057246,36049030,1254810,2016-03-17T10:04:04,<p>just set GOPATH according to your go files:</p>&#xA;&#xA;<blockquote>&#xA;  <p>GOPATH=$PROJECT_PATH/app/posts </p>&#xA;</blockquote>&#xA;&#xA;<p>then put your source codes under</p>&#xA;&#xA;<blockquote>&#xA;  <p>$PROJECT_PATH/app/posts/src/package</p>&#xA;</blockquote>&#xA;
51569171,50702676,403098,2018-07-28T07:38:48,"<p>I have a feeling you are using event store as a channel of communication, instead of using it as a database. If you want <em>micro-service 2 to feed on the data from micro-service 1</em>, then you should communicate with REST services.</p>&#xA;&#xA;<p>Of course, relying on REST services might make you less resilient to outages. In that case, using a piece of technology dedicated to communication would be the right way to go. (I'm thinking MQ/Topics, such as RabbitMQ, Kafka, etc.)</p>&#xA;&#xA;<p>Then, once your services are talking to each other, you will still need to persist your data... but only at <strong>one single</strong> location. &#xA;Therefore, you will need to define <em>where</em> you want to store the data. </p>&#xA;&#xA;<p>Ask yourself:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Who will have the governance of the data persistance ?</p>&#xA;</blockquote>&#xA;&#xA;<ul>&#xA;<li>Is it Microservice1 ? if so, then everytime Microservice2 needs to read the data, it will make a REST call to Microservice1. </li>&#xA;<li><p>is it the other way around ? Microservice2 has the governance of the data, and Microservice1 consumes it ?</p></li>&#xA;<li><p>It could be a third microservice that you haven't even created yet. It depends how you applied your separation of concerns.</p></li>&#xA;</ul>&#xA;&#xA;<p>Let's take an example :</p>&#xA;&#xA;<ul>&#xA;<li>Microservice1's responsibility is to process our data to export them in PDF and other formats</li>&#xA;<li>Microservice2's responsibility is to expose a service for a legacy partner, that requires our data to be returned in a very proprietary representation. </li>&#xA;</ul>&#xA;&#xA;<p>who is going to store the data, here ?</p>&#xA;&#xA;<ul>&#xA;<li>Microservice1 should not be the one to persist the data : its job is only to convert the data to other formats. If it requires some data, it will fetch them from the one having the governance of the data. </li>&#xA;<li>Microservice2 should not be the one to persist the data. After all, maybe we have a number of other Microservices similar to this one, but for other partners, with different proprietary formats.</li>&#xA;<li>If there is a service where you can do CRUD operations, this is your guy. If you don't have such a service, maybe you can find an existing Microservice who wouldn't have conflicting responsibilities.</li>&#xA;</ul>&#xA;&#xA;<p>For instance : if I have a Microservice3 that makes sure everytime an my <code>ObjectX</code> is changed, it will send a PDF-representation of it to some address, and notify all my partners that the data are out-of-date. In that scenario, this Microservice looks like a good candidate to become the ""governor of the data"" for this part of the domain, and be the one-stop-shop for writing/reading in the database.</p>&#xA;"
45180062,45179621,1141658,2017-07-19T02:43:32,"<p>Implementation of Microservices is often different from system to system or developer to developer. However some of the essential characteristics of microservices are,</p>&#xA;&#xA;<ul>&#xA;<li>Small focus (do one thing and to do that one thing well)</li>&#xA;<li>Loosely Coupled (Operates independently with one another)</li>&#xA;<li>Language Neutral </li>&#xA;<li>Bounded Context (do not need to know the implementation of other microservices)</li>&#xA;</ul>&#xA;&#xA;<p>We can make use of AWS lambda in achieving above characteristics. But maintaining and managing Lambda endpoints could be tricky and often a hassle. You can make use of <a href=""https://serverless.com/"" rel=""nofollow noreferrer"">Serverless Framework</a> to easily manage lambda functions. You can define a service configuration in simple YAML format, and the framework creates a CloudFromation stack and deploy it. </p>&#xA;&#xA;<p>With serverless framework you can define multiple services. Each service could be small focus, loosely coupled, language neutral and bounded context. Eg: User CRUD, Email Notifications, Scheduled Jobs etc... When you deploy services with Serverless Framework, it creates different API Gateways linked with respective lambdas. You can make use of API Gateway endpoints to interact with each service. </p>&#xA;&#xA;<p>As you have mentioned a lambda could be linked with other AWS services. But i think,  as long as it does not depend upon other microservices and has a clearly defined integration points to services like Dynamodb table or a S3 bucket we can make use of Lambda to develop microservices. </p>&#xA;"
39405919,39405067,2929562,2016-09-09T07:18:34,"<p>I was able to solve this issue. you need to make some changes in mapping request URL. I wonder how code in the block is working without error.</p>&#xA;&#xA;<p>As per my understanding one possible cause is that you have two or more applicable mappings for that URL call. <code>CustomerRest.java</code> and <code>ProductRest.java</code> both were mapped to <code>/</code>.  </p>&#xA;&#xA;<pre><code>@Path(""/"")&#xA;</code></pre>&#xA;&#xA;<p>And</p>&#xA;&#xA;<pre><code>@Path(""/"")&#xA;</code></pre>&#xA;&#xA;<p>The <code>Jersey</code> have no way of telling what method is actually supposed to be called and gives this error.</p>&#xA;&#xA;<p>You need to correct following files:&#xA;<strong>OrderRest.java</strong></p>&#xA;&#xA;<pre><code>@Named&#xA;@Path(""/"")&#xA;public class OrderRest {&#xA;    private long id = 1;&#xA;&#xA;    @Inject&#xA;    private RestTemplate restTemplate;&#xA;&#xA;    @GET&#xA;    @Path(""order"")&#xA;    @Produces(MediaType.APPLICATION_JSON)&#xA;    public Order submitOrder(@QueryParam(""idCustomer"") long idCustomer,&#xA;            @QueryParam(""idProduct"") long idProduct,&#xA;            @QueryParam(""amount"") long amount) {&#xA;&#xA;        Customer customer = restTemplate.getForObject(""http://localhost:8080/customer/getCustomer?id={id}"", Customer.class, idCustomer);&#xA;        Product product = restTemplate.getForObject(""http://localhost:8080/product/getProduct?id={id}"", Product.class,idProduct);&#xA;&#xA;        Order order = new Order();&#xA;        order.setCustomer(customer);&#xA;        order.setProduct(product);&#xA;        order.setId(id);&#xA;        order.setAmount(amount);&#xA;        order.setOrderDate(new Date());&#xA;        id++;&#xA;        return order;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>In <strong>CustomerRest.java</strong> - use this</p>&#xA;&#xA;<pre><code>@GET&#xA;    @Path(""/getCustomer"")&#xA;    @Produces(MediaType.APPLICATION_JSON)&#xA;    public Customer getCustomer(@QueryParam(""id"") long id) {&#xA;        Customer cli = null;&#xA;        for (Customer c : customers) {&#xA;            if (c.getId() == id)&#xA;                cli = c;&#xA;        }&#xA;        return cli;&#xA;    }&#xA;</code></pre>&#xA;&#xA;<p>In <strong>ProductRest</strong> - use this</p>&#xA;&#xA;<pre><code>@GET&#xA;    @Path(""/getProduct"")&#xA;    @Produces(MediaType.APPLICATION_JSON)&#xA;    public Product getProduct(@QueryParam(""id"") long id) {&#xA;        Product prod = null;&#xA;        for (Product p : products) {&#xA;            if (p.getId() == id)&#xA;                prod = p;&#xA;        }&#xA;        return prod;&#xA;    }&#xA;</code></pre>&#xA;&#xA;<p>Here is the output:&#xA;<a href=""https://i.stack.imgur.com/mQcd3.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mQcd3.png"" alt=""enter image description here""></a></p>&#xA;"
39476600,39476480,2929562,2016-09-13T18:18:53,"<p>I've had experinced this issue and as per research this error mainly occurs if the jar file may be corrupted. Try removing the all content of your</p>&#xA;&#xA;<pre><code> C:\Users\[your username]\.m2\repository\ folder. &#xA;</code></pre>&#xA;&#xA;<p>Then right click your project, select Maven, Update Project, check on Force Update of Snapshots/Releases. Or simply to go the location where logs says invalid LOC header (bad signature) and delete it manually. in Your scenario, you need to delete all from </p>&#xA;&#xA;<pre><code>C:\Users\user\.m2\repository\org\glassfish\jersey\core\jersey-client\2.22.1\&#xA;</code></pre>&#xA;&#xA;<p>and </p>&#xA;&#xA;<pre><code>C:\Users\user\.m2\repository\org\eclipse\jetty\jetty-servlet\9.2.13.v20150730\&#xA;</code></pre>&#xA;&#xA;<p>and again compile the code using <code>mvn clean install -DskipTests</code></p>&#xA;"
40332675,40296029,2152602,2016-10-30T19:20:30,"<p>There isn't really anything Kubernetes can do for you. You need to increase the number of inodes on your filesystem.</p>&#xA;&#xA;<p>Without specifics on the size of your disks I can't give you exact advice. But, the details on the arch wiki about bytes per inode is the correct advice: <a href=""https://wiki.archlinux.org/index.php/ext4#Bytes-per-inode_ratio"" rel=""nofollow"">https://wiki.archlinux.org/index.php/ext4#Bytes-per-inode_ratio</a></p>&#xA;"
50064429,50064156,9630106,2018-04-27T14:20:32,"<p>If I understand well your problem, you could solve it by using the Jenkins multijob plugin. And without updating your current existing job.</p>&#xA;&#xA;<p>Your workflow would look like:</p>&#xA;&#xA;<pre><code>Parent Job&#xA;    Service A --&gt; Build --&gt; Package --&gt; QA --&gt; Staging --&gt; Production&#xA;    Service B --&gt; Build --&gt; Package --&gt; QA --&gt; Staging --&gt; Production&#xA;    Service C --&gt; Build --&gt; Package --&gt; QA --&gt; Staging --&gt; Production&#xA;</code></pre>&#xA;&#xA;<p>Where your parent job trigger all your child jobs.</p>&#xA;&#xA;<p>I have seen a similar problem solved this way. Also sometime your child service must be released together, or one of your service must always be ahead (server). You can add some python/shell validation scripts in your parent job in order to make sure that your final product is always releasable.</p>&#xA;&#xA;<p><a href=""https://wiki.jenkins.io/display/JENKINS/Multijob+Plugin"" rel=""nofollow noreferrer"">https://wiki.jenkins.io/display/JENKINS/Multijob+Plugin</a></p>&#xA;"
40484902,40458770,204699,2016-11-08T10:45:45,"<p><strong>Disclaimer</strong>: I never used Keycloak, but the tag wiki says it's compliant with OAuth2 so I'll trust that information.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>At a really high-level view, you seem to have two requirements:</p>&#xA;&#xA;<ol>&#xA;<li>authenticate actions triggered by an end user while he's using your system.</li>&#xA;<li>authenticate actions triggered by your system at an unknown time and where there is no requirement for a end-user to be online.</li>&#xA;</ol>&#xA;&#xA;<p>You already met the first one, by relying on a token-based authentication system and <strong>I would do exactly the same for the second point, the only difference would be that the tokens would be issued to your system using the OAuth2 client credentials grant</strong> instead of the other grants that are targeted at scenarios where there is an end-user.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/rB4C3.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/rB4C3.jpg"" alt=""Client Credentials Grant""></a></p>&#xA;&#xA;<p>(source: <a href=""https://auth0.com/docs/architecture-scenarios/application/server-api#client-credentials-grant"" rel=""noreferrer"">Client Credentials Grant</a>)</p>&#xA;&#xA;<p>In your case, Keycloak would play the role of Auth0 and your client applications are microservices which can maintain client secrets used to authenticate themselves in the authorization server and obtain access tokens.</p>&#xA;&#xA;<p><strong>One thing to have in mind is that if your system relies on the <code>sub</code> claim for much more than authentication and authorization then you may need to make some adjustments</strong>. For example, I've seen systems where performing action A required to know that it was targeted at user X and Y, but the payload for the action only received user Y and assumed user X was the current authenticated principal. This works fine when everything is synchronous, but by merely switching the payload to specify both users would mean that the action could be done asynchronously by a system authenticated principal.</p>&#xA;"
40214773,40205675,204699,2016-10-24T09:18:52,"<p><strong>They are both valid options and as always it's the exact scenario where you want to apply them that will dictate the most appropriate</strong>. As it's generally the case, each option will have its pros and cons and you already mentioned a few, so I'll try my best to add something new to the discussion.</p>&#xA;&#xA;<p>The major difference is in the contents, by-value will contain the actual values while the reference is just a random sequence of bits. If you want to represent sensitive information in the token, this may tip the scale in favor of reference tokens.</p>&#xA;&#xA;<p>It's true that if you use JWT you can encrypt them to ensure confidentiality, but this adds a significant layer of complexity and the support for JWT encryption in the available libraries is most likely not as good as the support for signing given that encryption does not have a similar widespread use.</p>&#xA;&#xA;<p>Regarding the size, I don't think this should be a deciding factor. Yes, if you go with by-value tokens you need to keep them sufficiently small as to not cause significant overhead on the channel, but you should not pick one vs the other only because of this constraint.</p>&#xA;&#xA;<p>One thing you did not mentioned, but I believe it's important is that <strong>reference tokens seem more suitable for situations where the authorization server and resource server belong to the same entity</strong>. It's true that there is already a <a href=""https://tools.ietf.org/search/rfc7662"" rel=""nofollow"">standard to cover token introspection</a> so that an external resource server can query information about a reference token in an interoperable way. However, it's still true that, when both actors are within the same security boundary, reference tokens should be easier to scale and implement.</p>&#xA;&#xA;<p>The suggestion on the article it's also interesting, you get reduced overhead on the external network and no issues with information disclosure and then do the upgrade to by-value tokens in one central place meaning that all other services behind it may still gain from the simplicity of having the required information already within the token itself.</p>&#xA;&#xA;<p>In summary, if information disclosure is an issue to you you'll probably go for reference tokens in order to not take the cost of JWT encryption; otherwise, you might as well make life simple for you and go with value tokens.</p>&#xA;"
35185616,35179495,687515,2016-02-03T19:09:24,<p>We are wrapping up everything in docker containers using Kubernetes to manage multiple instances of those.</p>&#xA;
34357318,34341322,687515,2015-12-18T13:57:15,<p>You could go for asynchronous execution. Just add <code>@EnableAsync</code> to your application class and annotate the method calling the notification service with <code>@Async</code>.</p>&#xA;
34323509,34301614,687515,2015-12-16T22:39:39,<p>It is possible to achieve what you are trying to do by giving the correct Zuul config. Lets assume you have the user-cardholder service running on port 8081 and the user-account service on 8082 so that you can successfully answer requests going against:</p>&#xA;&#xA;<pre><code>http://localhost:8081/user-cardholder/accountholders/{id} &#xA;http://localhost:8082/user-account/accountholders/{id}/accounts/{accid}&#xA;</code></pre>&#xA;&#xA;<p>If this is working then you can achive what you are trying for these two services by using the following zuul config:</p>&#xA;&#xA;<pre><code>zuul:&#xA;  routes:&#xA;    cardholder:&#xA;      path: /user/accountholders/*&#xA;      url: http://localhost:8081/user-cardholder/accountholders/&#xA;    account:&#xA;      path: /user/accountholders/*/accounts/**&#xA;      url: http://localhost:8082/user-accounts/accountholders/&#xA;</code></pre>&#xA;&#xA;<p>Unfortunately you will also have to add more configs – even when they go against the same backend service – due to the fact that internal and external urls are differing. Otherwise you could just add the option <code>stripPrefix: false</code> and use the same internally as externally.</p>&#xA;
37990118,37985551,687515,2016-06-23T11:17:54,"<p>You already tagged your question accordingly with spring-cloud and Zuul. I'd suggest this is exactly the way to go cause this way you can avoid the browsers sandbox as well as to manage CORS headers on your services. Just use <a href=""http://start.spring.io"" rel=""nofollow"">start.spring.io</a> and include Zuul as a requirement and define your rules. I suggest you start with static routes and if you then are familiar and confident with Zuul, embed a service registry and discovery like Eureka, consul or etcd.</p>&#xA;"
42609903,42608033,5343387,2017-03-05T15:01:15,"<p>If security is not important:</p>&#xA;&#xA;<ul>&#xA;<li>Redis: <a href=""https://redis.io/"" rel=""nofollow noreferrer"">https://redis.io/</a></li>&#xA;<li>Consul: <a href=""https://www.consul.io/"" rel=""nofollow noreferrer"">https://www.consul.io/</a></li>&#xA;<li>Etcd: <a href=""https://coreos.com/etcd"" rel=""nofollow noreferrer"">https://coreos.com/etcd</a></li>&#xA;<li>Zookeeper: <a href=""https://zookeeper.apache.org/"" rel=""nofollow noreferrer"">https://zookeeper.apache.org/</a></li>&#xA;</ul>&#xA;&#xA;<p>If security is important:</p>&#xA;&#xA;<ul>&#xA;<li>Vault: <a href=""https://www.vaultproject.io/"" rel=""nofollow noreferrer"">https://www.vaultproject.io/</a></li>&#xA;<li>Conjur: <a href=""https://www.conjur.com/"" rel=""nofollow noreferrer"">https://www.conjur.com/</a></li>&#xA;<li>Thycotic: <a href=""https://thycotic.com/"" rel=""nofollow noreferrer"">https://thycotic.com/</a></li>&#xA;<li>Docker Secrets: <a href=""https://docs.docker.com/engine/swarm/secrets/"" rel=""nofollow noreferrer"">https://docs.docker.com/engine/swarm/secrets/</a></li>&#xA;</ul>&#xA;&#xA;<p>Honestly there are a bunch more options, but these are some of the most famous and vetted by the DevOps community.</p>&#xA;"
39098772,39068073,6329755,2016-08-23T10:33:18,"<p>I found issue about this problem. There are also advices how to fix it <a href=""https://github.com/spring-cloud/spring-cloud-netflix/issues/1108"" rel=""nofollow"">https://github.com/spring-cloud/spring-cloud-netflix/issues/1108</a></p>&#xA;"
44603862,44601394,8099995,2017-06-17T11:06:28,"<p>I can suggest you 2 different ways:</p>&#xA;&#xA;<ol>&#xA;<li><p>Write custom filter for your security framework. Setup pre authentication filter for URLs like <code>/company/{companyName}/**</code>, which will fill your context or token with all authorities user has for current company.</p></li>&#xA;<li><p>If you have complex logic for extracting authorities (e. g. can user view transaction or no), then it's probably better to move it to service layer and check authorities on demand before executing main logic.</p></li>&#xA;</ol>&#xA;&#xA;<p>Every modern security framework supports filters or something similar, so you should try to go with 1st option.</p>&#xA;"
41257360,33187526,6270875,2016-12-21T07:30:43,"<p>You can also use <a href=""https://hazelcast.org/"" rel=""nofollow noreferrer"">Hazelcast</a> as distributed Map/Cache. It is really easy to use (see code examples), but if you want some enterprise features you have to pay a lot.</p>&#xA;"
39406048,39338259,6270875,2016-09-09T07:26:10,"<p>You can use <strong><a href=""http://avro.apache.org/docs/current/gettingstartedjava.html"" rel=""nofollow"">Apache Avro</a></strong> for serializing (predefined avro schema) your objects between two service or if you want to use a queue bewteen them. On top you will lower the needed network bytes, because apache avro serialization is really good.</p>&#xA;&#xA;<p>You can also use CatainProto or ProtoBuffer, but I like the apache avro method.</p>&#xA;"
43135038,43104831,1556123,2017-03-31T07:33:11,<p>The API gateway model is a scalable solution for microservice based architectures.</p>&#xA;&#xA;<p>You have the gateway distributed over 2 data-centres which helps provide High Availability to the gateway - you could even consider spreading it over 3 for full multi-region in the future.</p>&#xA;&#xA;<p>If your microservices each have a replica of 3 and they are distributed into the 2 data-centres then yes you have 6 instances of that microservice running however unless you have the two data-centres sharing resources then it is 3 of each microservice in each data-centre.</p>&#xA;
48962652,48962157,1911196,2018-02-24T12:04:40,"<p>You need to write a simple microservice, which has access to the database and expose endpoints to the repositories. </p>&#xA;&#xA;<blockquote>&#xA;  <p>For services that are non-Java based, you have a choice of implementing the client part of eureka in the language of the service <a href=""https://github.com/Netflix/eureka/wiki/Eureka-at-a-glance#non-java-services-and-clients"" rel=""nofollow noreferrer"">[1]</a></p>&#xA;</blockquote>&#xA;&#xA;<p>You cannot register a PostgresSQL Database as a service to Eureka directly.</p>&#xA;&#xA;<p>EDIT: Since every microservice serves a specific concern, it should has its own data store. If you centralize the data store, it becomes your bottleneck and you limit the scalability of the microservices using the data store. </p>&#xA;"
49777511,47178056,2829212,2018-04-11T14:18:40,"<p>I have solved this issue using the Spring Cloud Kubernetes Dependencies</p>&#xA;&#xA;<p><code>&lt;spring.cloud.kubernetes&gt;0.2.0.RELEASE&lt;/spring.cloud.kubernetes&gt;</code></p>&#xA;&#xA;<pre><code>&lt;dependency&gt;&#xA;        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&#xA;        &lt;artifactId&gt;spring-cloud-kubernetes-ribbon&lt;/artifactId&gt;&#xA;        &lt;version&gt;${spring.cloud.kubernetes}&lt;/version&gt;&#xA;    &lt;/dependency&gt;&#xA;&#xA;    &lt;dependency&gt;&#xA;        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&#xA;        &lt;artifactId&gt;spring-cloud-kubernetes-config&lt;/artifactId&gt;&#xA;        &lt;version&gt;${spring.cloud.kubernetes}&lt;/version&gt;&#xA;    &lt;/dependency&gt;&#xA;&#xA;    &lt;dependency&gt;&#xA;        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&#xA;        &lt;artifactId&gt;spring-cloud-kubernetes-core&lt;/artifactId&gt;&#xA;        &lt;version&gt;${spring.cloud.kubernetes}&lt;/version&gt;&#xA;    &lt;/dependency&gt;&#xA;    &lt;dependency&gt;&#xA;        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&#xA;        &lt;artifactId&gt;spring-cloud-kubernetes-discovery&lt;/artifactId&gt;&#xA;        &lt;version&gt;${spring.cloud.kubernetes}&lt;/version&gt;&#xA;    &lt;/dependency&gt;&#xA;</code></pre>&#xA;&#xA;<p>What was very important for me was the ribbon dependency as it makes use of a load balanced rest template in order to substitute service names for the correct pod IP's that are found in your kubernetes cluster.</p>&#xA;&#xA;<p>I have created a git repo as part of answering a larger set of questions but it should be more than sufficient if someone is looking a way to implement Kubernetes service discovery in place of Eureka or Consul.</p>&#xA;&#xA;<p><a href=""https://github.com/foundery-rmb/kubernetes-service-discovery"" rel=""nofollow noreferrer"">https://github.com/foundery-rmb/kubernetes-service-discovery</a></p>&#xA;"
37066719,36582126,479900,2016-05-06T07:27:51,"<p>When Lagom talks about ""cluster"", it's referring to Akka clusters.  Each service may be deployed as an Akka cluster, that is, a service may be a cluster of nodes.  So you don't have multiple services in a cluster, you just have one clustered service.</p>&#xA;&#xA;<p>Lagom service calls map down in a fairly straight forward fashion to idiomatic REST.  So, when talking to an external service, the thing at that IP should be a REST service.  Likewise, when an external service is talking to Lagom, it should use REST.</p>&#xA;"
34178870,34109946,5659036,2015-12-09T12:29:42,"<p>The Forrester article provides the answer to some extent ""...and leveraging edge-of-network cache functionality for increasingly dynamic data. CDNs such as those provided by Akamai, along with delivery optimization solutions like Instart Logic, application delivery controllers like Riverbed Stingray, and on-premises in-memory database caches, fulfill this today...""</p>&#xA;&#xA;<p>To answer the question “which software should be used in the Delivery tier?” let us consider the following for a moment:&#xA;Forrester suggests 3 possible solutions other than CDNs and lets only focus on them.  </p>&#xA;&#xA;<ol>&#xA;<li>Application Delivery controllers (ADC) </li>&#xA;<li>Application Delivery Optimisation (ADO)</li>&#xA;<li>in-memory caches</li>&#xA;</ol>&#xA;&#xA;<p><b>Application delivery controllers  (ADCs) </b> are  computer network devices in a datacenter, often part of an application delivery network, that helps perform common tasks such as those done by web sites to remove load from the web servers themselves. Many also provide load balancing. ADCs are often placed in the DMZ, between the firewall or router and a web farm. {Wikipedia}.   That is the role of the appliances provided by Riverbed like Stingray.   </p>&#xA;&#xA;<p>That leaves us 2 possible software choices - ADO or cache</p>&#xA;&#xA;<p><b>Application Delivery Optimization (ADO) solutions </b>&#xA;Browsers typically download an entire multi-megabyte set of components — HTML, CSS, images, JavaScript — before displaying the content and executing the JavaScript code.  Instead of sending everything all the time or sending everything fully, can we  leverage a Modern HTML5 browser’s different caching and storage mechanisms, each with their own performance and capacity characteristic, to improve application performance.   That is sort of the principle behind ADO.</p>&#xA;&#xA;<p>So, Application Delivery Optimization (ADO) solutions try to speed up content delivery by deciding in real-time how each element of an application (such as an image or code fragment) should be placed in different tiers of a browser cache.    They also utilise learnings from how users typically perceive completeness of an element to decide the rate at which to stream elements.  For instance, an image needn’t be fully streamed before it can be recognised.  Products like Instart Logic, offer multiple components addressing each of these techniques.   I guess there may be some open source implementations but haven’t come across / explored them personally.</p>&#xA;&#xA;<p><b> In-Memory Database Cache </b>&#xA;With regards to in-memory database cache,  any in-memory database could be used as a starting point.   For e.g. Oracle Ten times DB, a Key Value store like Memcached, or if you are hosting in AWS then Amazon ElasiCache or a In memory data grid like Coherence etc.     Refer this good slide show at <a href=""http://www.slideshare.net/MaxAlexejev/from-distributed-caches-to-inmemory-data-grids"" rel=""nofollow"">http://www.slideshare.net/MaxAlexejev/from-distributed-caches-to-inmemory-data-grids</a> for more info.    Which in-memory database would depend on the particular problem situation you are trying to solve.</p>&#xA;&#xA;<p>Typically the in-memory distributed data caches would be useful at data / object level and will need a service / web / application to front it as you would expect. Refer a good / simple Azure example here with a simple diagram which might help to understand. www.asp.net/aspnet/overview/developing-apps-with-windows-azure/building-real-world-cloud-apps-with-windows-azure/distributed-caching </p>&#xA;&#xA;<p><b>Web Cache </b> &#xA;In addition to the three above, it is worthwhile to delineate the discourse between web  cache and database cache.   Web cache at the Reverse Proxy / Loadbalancer level is not the same as in-memory database cache.   Most servers like Nginx, Squid etc. also double up as Reverse Proxy, Web cache.  For instance refer: <a href=""http://nginx.com/blog/nginx-caching-guide"" rel=""nofollow"">http://nginx.com/blog/nginx-caching-guide</a> on how to use nginx for web caching.</p>&#xA;&#xA;<p>Thus, the answer to the question as to which software should be used in the Delivery tier as you see from the above is that it “May or May not be a single software/solution”.  </p>&#xA;&#xA;<p>Depending on a)the type of content an application is serving, b)where its clients are, c)how they access it, d)how they might perceive the content if it is only partially delivered etc. one would determine which combination of solutions as in ADC or ADO or Web Cache or In Memory database Cache is suitable for the overall solution.   </p>&#xA;&#xA;<p>Based on the above, choose products including Nginx (or equivalent Web cache),  Stingray (or equivalent ADC), Instart Logic (or equivalent ADO), various in-memory caches (as noted above).</p>&#xA;&#xA;<p>(PS:  Created a diagram but putting all this together, but upload to stackoverflow is failing...)</p>&#xA;"
45637540,45637090,1727132,2017-08-11T14:22:57,"<p>Your approach is definitely possible. But I'd take another one and try using microservices. Then you will move parts of your code one by one to little cute microservices and eventually have microservice architecture. I like this approach because it allows very fast switching... Of everything. You can build your microservices with Java, Node, Go - everything you want. And if you suddenly discover that node.js is not up to your expectations (for example, if you have hardcore math modules) - just throw this microservice out and quickly impliment it in any other language and framework. The most important part is defining communication architecture. REST API is already a thing of the past and you will possibly want to use some message broker like RabbitMQ.</p>&#xA;"
27192661,27054162,3042204,2014-11-28T16:22:35,"<p><sub>Disclaimer: most of this post is subjective. No attempt has been made here to strictly define anything, just trying to contextualize and give a global overview of the concepts and how they relate to each other.</sub></p>&#xA;&#xA;<blockquote>&#xA;  <p>I thought I knew what REST/""RESTFul"", restfulservices, webservices,&#xA;  SOA and microservices</p>&#xA;</blockquote>&#xA;&#xA;<p>I'd say that all these terms fall into the umbrella of <em>Service Oriented Architectures</em> (SOA). Web services is a SOA using web-related technologies. REST and its subset RESTful are a set of practices to implement web services. Finally, microservices are a new set of SOA practices.</p>&#xA;&#xA;<blockquote>&#xA;  <p>I hope to have a clear understanding of what the aforementioned terms&#xA;  represent</p>&#xA;</blockquote>&#xA;&#xA;<p>I'll try to address this point, but using informal definitions and without entering into advantages and disadvantages. That'd be way too long and I think the biggest points should be obvious from the explanations.</p>&#xA;&#xA;<p><strong>SOA</strong></p>&#xA;&#xA;<p>I think the name is self-explanatory in this case: SOA -- which stands for <em>Service Oriented Architecture</em> -- refers to architectures designed with a focus on services. Now, the tricky part here is what you may or may not consider a service and that is a whole different topic.</p>&#xA;&#xA;<p><strong>Web services</strong></p>&#xA;&#xA;<p>This accounts for the subset of SOA using web-related technologies. This typically involves HTTP and XML but it could also use FTP. I think the term <em>web</em> here is quite vague as it generally refers to standard Internet technologies.</p>&#xA;&#xA;<p><strong>REST(ful)</strong></p>&#xA;&#xA;<p>REST is a subset of web services -- and hence a SOA -- that revolves around using HTTP for communication. There are a certain set of common practices such as a certain given relevance to URLs.</p>&#xA;&#xA;<p>About 10 years ago when I was introduced to REST, RESTful was presented to me as a more strict REST implementation where a resource would have a unique URI and it would be managed through CRUD operations mapped to HTTP verbs -- <em>Create = POST, Read = GET, Update = PUT, Delete = Delete</em>.</p>&#xA;&#xA;<p>Updating user information through a HTTP GET or POST request o a <code>/users/1/update</code> URL would be perfectly valid in REST, but it would not be RESTful. For the latter, the approach would be to use an HTTP <a href=""https://stackoverflow.com/a/19844272/3042204"">PUT or PATCH</a> over <code>/users/1</code> (which would also be the URL for the rest of the operations, simply varying the HTTP verb).</p>&#xA;&#xA;<p>I find that this distinctions has become blurred over the years. However, it still stands that RESTful is a more strict subset of REST. (The exact requirements may be debatable.)</p>&#xA;&#xA;<p><sub><em>EDIT - A more formal definition:</em></sub></p>&#xA;&#xA;<p><sub>REST stands for <em>Representational State Transfer</em> and was <a href=""http://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm"" rel=""nofollow noreferrer"">presented by Roy Fielding in his Ph.D. thesis</a> as an architectural style for distributed hypermedia systems. The emphasis goes into the hypermedia and self-containment decoupling the client from most beforehand knowledge among others. A website is an example: it consists on a single URI (the website root) and a media type (HTML) through which the server offers all the information a client needs regarding resources and all possible interactions.</sub></p>&#xA;&#xA;<p><sub>I'd say 99% people talking about REST really mean <a href=""http://etherealbits.com/2012/12/debunking-the-myths-of-rpc-rest/"" rel=""nofollow noreferrer"">RPC or HTTP-based interfaces</a>: using HTTP endpoints to invoke certain actions or query data. Fielding himself <a href=""http://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven"" rel=""nofollow noreferrer"">has tried to clarify this</a>. Any API formed by a set of predefined URLs expecting certain HTTP verbs and some parameters falls into that 99%. So does my description above. However, I doubt the term itself can ever survive its misuse and I think we must accept its new meaning.</sub></p>&#xA;&#xA;<p><strong>Microservices</strong></p>&#xA;&#xA;<p>This is the more recent term; it promotes implementing applications as a set of simple independently deployable services. This contrasts with the classic approach of SOA architectures as a set of quite complex services used to build complex systems, tipically involving an enterprise service bus. However, it is important to note that although typically SOA gets associated with such systems, it is a broader term and indeed, microservices are also a subset of SOA.</p>&#xA;&#xA;<p>Microservices usually appear hand-by-hand with modern JavaScript full-stacks -- i.e. using JavaScript for all the vertical components, from the server up to user interface. Arguably this is so because using these JavaScripts full-stacks may speed up development thanks to the simplified integration. These stacks, and hence microservices implemented using them, are usually architected through REST but from a theoretical point of view, there is nothing preventing you from using a different approach to the same philosophy.</p>&#xA;"
49309867,49308482,159388,2018-03-15T21:54:26,"<p>The <a href=""https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md"" rel=""nofollow noreferrer"">gRPC over HTTP/2 protocol documentation</a> defines that the path is constructed as follows:</p>&#xA;&#xA;<blockquote>&#xA;  <p><strong>Path</strong> → "":path"" ""/"" Service-Name ""/"" <em>{method name}</em></p>&#xA;</blockquote>&#xA;&#xA;<p>with this additional note</p>&#xA;&#xA;<blockquote>&#xA;  <p>Some gRPC implementations may allow the <strong>Path</strong> format shown above to be overridden, but this functionality is strongly discouraged. gRPC does not go out of its way to break users that are using this kind of override, but we do not actively support it, and some functionality (e.g., service config support) will not work when the path is not of the form shown above.</p>&#xA;</blockquote>&#xA;&#xA;<p>So, the Node gRPC client is following the specification, and the alternate format used by go-micro appears to be hard coded in their code generation plugin (<a href=""https://github.com/micro/protoc-gen-micro/blob/b2a3e2d9e6160a0a88b2514d763be57607705b8b/plugin/micro/micro.go#L218"" rel=""nofollow noreferrer"">here</a>). I would consider that to be a bug.</p>&#xA;&#xA;<p>That being said, there is a viable workaround to match that method name format in the Node gRPC library. When you load a <code>.proto</code> file in the Node each client constructor function has a <code>service</code> member which is a plain JavaScript object that describes the service. It is a map of method names to method definitions, and each method definition includes a <code>path</code> member. You can modify the path of each method to match the pattern that go-micro uses, then pass the resulting service object to <code>grpc.makeGenericClientConstructor</code> to get a new client constructor that connects to the modified service.</p>&#xA;"
42734113,31044380,2587036,2017-03-11T10:34:23,"<p>In order to manage authentication in a microservices architecture, you must have a different point of view.</p>&#xA;&#xA;<p>Remember when you worked on a monolith, you had a single authentication process.</p>&#xA;&#xA;<p>As an example in PHP app, you find your user in a database with it's corresponding credentials, then you created a session a the user is ""authenticated"".</p>&#xA;&#xA;<p>With microservices, the workflow is kinda the same. The only thing that changes now is that you are not able to open a session in different services. Furthermore, you don't need to get the authenticated user. You only need to be sure that he is authorized to perform the current call on your microservices.</p>&#xA;&#xA;<p>Thanks to oauth2, having a valid access_token gives you this information.</p>&#xA;&#xA;<p>This should answer the frontend part. In the backend part (I mean behind the api gateway), you should not manage access_token because it is not relevant to microservices. You can use a functional key to find any information relevant to the user inside microservices like a uuid for example.</p>&#xA;&#xA;<p>In order to get a uuid while using oauth2 I suggest to use openid connect too. It is user with this protocol to manage specific user information and it gives you access to a specific endpoint ""/userinfo"".</p>&#xA;&#xA;<p>Hope this schema will make this answer clearer.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/DDw1c.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/DDw1c.png"" alt=""enter image description here""></a></p>&#xA;"
45637100,39485459,2587036,2017-08-11T14:01:16,"<blockquote>&#xA;  <p>What is the best practice to adapt for versioning in a Microservice Based Architecture</p>&#xA;</blockquote>&#xA;&#xA;<p>One startegy that I use in my microservice project is to version through routing.</p>&#xA;&#xA;<p>We use JSON RPC 2.0 so it's a little different from REST but it can be applied.</p>&#xA;&#xA;<p>We only use major versioning and try to live with the old and the new version at the same time to let the time to consumer to be updated.</p>&#xA;&#xA;<p><strong>Things to take care of</strong></p>&#xA;&#xA;<ul>&#xA;<li><p>Avoid having more than two versions of a service in production (really hard to manage if you have some model updates).</p></li>&#xA;<li><p>Find a way to tell the consumer that the version there are using is deprecated.</p></li>&#xA;</ul>&#xA;&#xA;<p>I know that some microservice architecture lives without versioning but it means that you have a strong coupling between consumer and producer so I may not recommand this approach.</p>&#xA;&#xA;<p>To answer the second question </p>&#xA;&#xA;<blockquote>&#xA;  <p>how the consumers would be able to use different versions?</p>&#xA;</blockquote>&#xA;&#xA;<p>It sounds not possible because a consumer should only use one version at a time.</p>&#xA;&#xA;<p>If you want it to know that the version is available to use it at runtime you can walk through feature flipping coupled with a service discovery.</p>&#xA;&#xA;<p>Your consumer will ask regularly to a service discovery if the route x exists. If true then use the feature, otherwise continue with the current version. It could works but it is kinda tricky to manage.</p>&#xA;"
45640432,44928556,2587036,2017-08-11T17:08:35,"<p>You have to take care of several things.</p>&#xA;&#xA;<p><strong>First</strong></p>&#xA;&#xA;<p>Going to microservice will be slower (90% of the time) than a monolith because you introduce latency. So never forget it when you go with it.</p>&#xA;&#xA;<p><strong>Second</strong></p>&#xA;&#xA;<p>You ask if it is a good way to go with kafka. I may answer yes in most of the case but you mentioned that today the process is synchronous. If it is for transactional reasons, you won't be able to solve it with a message broker I guess because you'll update your strong consistency system to an eventually one. <a href=""https://en.wikipedia.org/wiki/Eventual_consistency"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Eventual_consistency</a></p>&#xA;&#xA;<p>I am not saying that it is a bad solution only that it change your workflow and may impact some business rules.</p>&#xA;&#xA;<p>As a solution I offer this:</p>&#xA;&#xA;<p>1 - Break the seams in your monolith by introducing functional key and api composition inside the monolith (read Sam Newman's book to help).</p>&#xA;&#xA;<p>2 - Introduce the eventual consistency inside the monolith to test if it fits the purpose. It will be easier to rollback if not.</p>&#xA;&#xA;<p>No you have to possibility:</p>&#xA;&#xA;<p>The second step went well  so go ahead and put the code of the service into a microservice out of the monolith.</p>&#xA;&#xA;<p>The second step did not fit then think about doing the risky thing in a specific service or use distributed transactions (be careful with this solution it could be hard to manage). </p>&#xA;"
48871560,48791411,2587036,2018-02-19T17:46:35,"<p>RabbitMQ is just a broker to route messages. You should not use it to manage your business because you will have a lots of complicated routing rules.</p>&#xA;&#xA;<p>I personally solved this issue with a single consumer. Its goal is to listen on a queue, push events in a stack depending on a business rule. If you still need to scale up the number of consumer you can use reactive architecture pattern which let's you orchestrate the load balancing between all forked instances.</p>&#xA;&#xA;<p>Another solution using RabbitMQ is to set the priority of the message and create consumer listening on specific priority. <a href=""https://www.rabbitmq.com/priority.html"" rel=""nofollow noreferrer"">https://www.rabbitmq.com/priority.html</a></p>&#xA;&#xA;<p>If this two solutions still not fix your problem, you can set a queue by consumer and send the sequence in the corresponding queue when you requeue the message. With this simple solution, you will always be sure that a given message is read by the same consumer but you'll loose the benefits of the load balancing.</p>&#xA;"
37818868,29591967,1475228,2016-06-14T17:48:45,<p>Service to microservice is Java is to JavaScript. Don't think about it that way. Instead microservice can be thought of as the following:</p>&#xA;&#xA;<ol>&#xA;<li>A small problem domain.</li>&#xA;<li>Built and deployed by itself.</li>&#xA;<li>Runs in its own process.</li>&#xA;<li>Integrates via well-known interfaces.</li>&#xA;<li>Owns its own data storage.</li>&#xA;</ol>&#xA;
46464911,46462610,4362652,2017-09-28T08:50:13,"<p>Yes, there is a lot of discussion about submodules being good / evil, but i tend to think that some are using them wrong. Just like hitting a nail with a sledgehammer and not expecting any drawbacks, just because they have similar names.</p>&#xA;&#xA;<p>A few things to consider when using them:</p>&#xA;&#xA;<ul>&#xA;<li><p>They are entirely isolated between them and between the superproject.&#xA;There will be no detailed history in the superproject of the commits&#xA;made in the submodule, just the ones that updated it.</p></li>&#xA;<li><p>When you make a commit on a submodule, you need to push it, because your in another repo. If you don't and you go to the superproject to update the submodule, you will end up with a reference to a commit that was not pushed. And this will lead to even more problems and confusion.</p></li>&#xA;</ul>&#xA;&#xA;<p>These are examples of situations i found very often and people complain about them, but i don't consider these as cons, just things that we need to be aware of so we can decide if it suits our project or not.</p>&#xA;&#xA;<p>Were using it in a project that has another one embedded, more like a module.&#xA;They are both developed separately, and from time to time we update the module to a new version, or just keep it at a certain one for compatibility reasons. We don't need to know who is working on the module, or what they made, we just want to know when they released a new version / fixed something.</p>&#xA;&#xA;<p>Maybe someone with more experience will share it, we sure need it.</p>&#xA;"
43619284,43238799,6595740,2017-04-25T19:23:38,"<p>The <strong>performance monitoring, service health-check and the dashboards</strong> can be done via Consul. The service discovery is awesome and will cater quite nicely to your APIs. </p>&#xA;&#xA;<p>Check out their software comparisons page here: </p>&#xA;&#xA;<p><a href=""https://www.consul.io/intro/vs/index.html"" rel=""nofollow noreferrer"">https://www.consul.io/intro/vs/index.html</a></p>&#xA;&#xA;<p>When it comes to <strong>log aggregation and stack tracing</strong>, I personally would use <a href=""https://www.splunk.com/"" rel=""nofollow noreferrer"">Splunk</a> or <a href=""https://stackify.com/"" rel=""nofollow noreferrer"">Retrace</a> but as you're looking for open source I would go for <a href=""https://www.graylog.org/"" rel=""nofollow noreferrer"">GrayLog</a>.</p>&#xA;"
49314520,49176544,7452828,2018-03-16T06:32:25,<p>management.endpoints.web.exposure.include=*&#xA;how to write it in yaml</p>&#xA;
24789761,24787801,1620636,2014-07-16T20:11:28,"<p>this article shows how to use spring profiles, available in spring 3.1 and later.  It will do exactly what you want.</p>&#xA;&#xA;<p><a href=""http://chariotsolutions.com/blog/post/spring-3-1-environment-profiles-2/"" rel=""nofollow"">http://chariotsolutions.com/blog/post/spring-3-1-environment-profiles-2/</a></p>&#xA;&#xA;<p>set a JVM variable like this: spring.profiles.active=development</p>&#xA;&#xA;<p>then in your configuration xml you can wrap environment specific xml with the profile tags</p>&#xA;&#xA;<pre><code>&lt;beans profiles=""development""&gt;  &#xA;  &lt;bean id=""dataSource"" class=""...""&gt;&lt;/bean&gt;&#xA;  &lt;bean id=""messagingProvider"" class=""...""&gt;&lt;/bean&gt;&#xA;&lt;/beans&gt;&#xA;</code></pre>&#xA;&#xA;<p>You can also set the profile on annotation-driven classes with @Profile(""development"") at the beginning of the class.  That class will only be autowired if the profile matches.</p>&#xA;&#xA;<p>For unit tests you can set the active profile on a test class with @ActiveProfile(profiles = ""test"", ""CI""), it will run using test and CI resources</p>&#xA;"
51417830,51416552,1430428,2018-07-19T08:27:16,"<p>Alternately, you can also link the two containers together by <code>--link</code>. Assuming you want container1 as client to container2, you can use below:</p>&#xA;&#xA;<pre><code>sudo docker run --link container2 --name=container1 -d image_name&#xA;</code></pre>&#xA;"
51512149,51511923,1430428,2018-07-25T06:26:30,"<p>You can use this approach:</p>&#xA;&#xA;<ol>&#xA;<li>For the buggy microservice, in your POD configuration file open up the java JPDA port. So instead of</li>&#xA;</ol>&#xA;&#xA;<p><code>java -jar yourapp.jar</code></p>&#xA;&#xA;<p>give this:</p>&#xA;&#xA;<pre><code>java -Xdebug -Xrunjdwp:transport=dt_socket,server=y,address=&lt;port&gt; -jar &lt;yourapp&gt;.jar&#xA;</code></pre>&#xA;&#xA;<ol start=""2"">&#xA;<li><p>Expose the jpda port as mentioned in <code>&lt;port&gt;</code> parameter in above command via your POD configuration YAML file. This will ensure that your pod is accessible to external network during boot.</p></li>&#xA;<li><p>Connect via eclipse remote debugger. </p></li>&#xA;</ol>&#xA;"
40115189,40092551,406429,2016-10-18T18:11:04,"<p>We do not recommend using Vagrant anymore, but you should favor using minikube or minishift instead.</p>&#xA;&#xA;<p><a href=""http://fabric8.io/guide/getStarted/index.html"" rel=""nofollow"">http://fabric8.io/guide/getStarted/index.html</a></p>&#xA;"
48341508,48338676,9131933,2018-01-19T12:48:05,<p>It sounds like there is a service/API call  missing from you architecture. Moving a service into production should be no different than recovering from a a failure and should not require any additional steps. Perhaps the messages should be consumed from the queue by another service that can then be queried for the complete list of products.</p>&#xA;
47197712,47196334,6210973,2017-11-09T09:04:39,"<p>Your Rest Controller might not be detected/scanned automatically - as you suggested. You could try to <a href=""https://docs.oracle.com/cd/E24329_01/web.1211/e24983/configure.htm#RESTF189"" rel=""nofollow noreferrer"">manually register your class</a> which exposes the ""users/"" resource. </p>&#xA;"
38123603,38121112,5426333,2016-06-30T12:48:32,"<p>As far as i understand you don't want to use <code>JwtBearerAuthentication</code> in <code>genericapi</code>. In this case you can write custom authentication middleware(send jwt to <code>authapi</code> and validate it, then set current user) for <code>genericapi</code>. Then just use <code>[Authorize]</code> attribute. </p>&#xA;&#xA;<p>To write custom authentication middleware take a look at <a href=""https://stackoverflow.com/a/37415902/5426333"">https://stackoverflow.com/a/37415902/5426333</a></p>&#xA;&#xA;<p><strong>However</strong> if possible, i wouldn't go with your way. I would use <code>JwtBearerAuthentication</code> for <code>genericapi</code>. Then i would use <code>OnTokenValidated</code> event to handle other validations. </p>&#xA;&#xA;<pre><code>        app.UseJwtBearerAuthentication(new JwtBearerOptions()&#xA;        {&#xA;            Events = new JwtBearerEvents()&#xA;            {&#xA;                OnTokenValidated = (context) =&gt;&#xA;                {&#xA;                    // send jwt to auth api&#xA;                    // validate it&#xA;                    if (!valid)&#xA;                    {&#xA;                        context.SkipToNextMiddleware();&#xA;                    }&#xA;                    return Task.FromResult(0);&#xA;                }&#xA;            }&#xA;        });&#xA;</code></pre>&#xA;"
47735417,47730089,577181,2017-12-10T02:53:26,"<p>The error is correct.  MicroProfile Config is an API, a specification.  it is not an implementation.  To have an implementation you could use something like <a href=""https://github.com/apache/geronimo-config/tree/geronimo-config-1.0#adding-to-your-project"" rel=""nofollow noreferrer"">Geronimo Config</a> as a standalone library.</p>&#xA;&#xA;<p>Also note, you may need to place the properties file in your build under <code>src/main/resources/META-INF</code>, in case the property fails to load.</p>&#xA;"
45533782,45530657,5951133,2017-08-06T15:51:12,"<p>Using <a href=""http://semver.org/"" rel=""nofollow noreferrer"">semantic versioning</a> can help in this case by reducing or eliminating the amount of breaking changes your team experiences when moving between different versions of internal libraries. The core concept of semantic versioning is that major version increments signify the addition of backwards incompatible changes (such as removing or changing the signature of a method in the public API). By contrast, such backwards-incompatible changes must not occur between different minor or patch versions within the same major version. By following such conventions, your team should be able to avoid issues with your internal libraries not being compatible with each other (within the same major version). The <a href=""https://msdn.microsoft.com/en-us/library/system.obsoleteattribute(v=vs.110).aspx"" rel=""nofollow noreferrer""><code>Obsolete</code></a> attribute is your friend here as well for marking code that will be removed in the next major version.</p>&#xA;&#xA;<p>Once your team uses semantic versioning for your libraries, a project can then (usually) safely reference different versions of the same assembly like in the second of your examples. These different versions can be <a href=""https://docs.microsoft.com/en-us/dotnet/framework/configure-apps/redirect-assembly-versions"" rel=""nofollow noreferrer"">unified</a> through binding redirects to use the same version of that assembly (usually the higher version of the two). If you are using Visual Studio 2013 or later, these can be automatically generated and updated if <a href=""https://docs.microsoft.com/en-us/dotnet/framework/configure-apps/how-to-enable-and-disable-automatic-binding-redirection"" rel=""nofollow noreferrer"">this tag</a> is present in the project files that consume your libraries. For your first example, if Av2.0 and Av1.1 have incompatible APIs, using Nuget will help by allowing you to update to Av2.0, test out that version, and rollback if something isn't working the way it should.</p>&#xA;&#xA;<p>From personal experience at work, I highly recommend using Nuget and semantic versioning instead of copy-pasting code or leaving all internal libraries at the same assembly version. Copying code around is usually considered <a href=""https://en.wikipedia.org/wiki/Copy_and_paste_programming"" rel=""nofollow noreferrer"">bad practice</a> (see <a href=""https://stackoverflow.com/a/2490897/5951133"">this answer</a>). Never changing the version number of assemblies makes it difficult to tell what code is actually in that assembly file without decompiling it.</p>&#xA;"
47636478,45559979,4446877,2017-12-04T15:18:42,"<p>I have found below 3 options:</p>&#xA;&#xA;<ul>&#xA;<li><p>If each microservice is verifying the token then we can pass the same token. But the problem is - in between same token can be expired.</p></li>&#xA;<li><p>If we use client_credentials grant then there we are having two issues: one is, we need to send the username/id in next microservice. Another one is, we need to request two times - first for getting the access token, next for actual call.</p></li>&#xA;<li><p>If we do the token verification in API gateway only (not in microservices) then from the API gateway we need to send the username in every microservices. And microservices implementation needs to be changed to accept that param/header.</p></li>&#xA;</ul>&#xA;"
47655947,39839881,4446877,2017-12-05T14:27:00,"<p>I have found below 3 options:</p>&#xA;&#xA;<ul>&#xA;<li><p>If each microservice is verifying the token then we can pass the same token. But the problem is - in between same token can be expired.</p></li>&#xA;<li><p>If we use client_credentials grant then there we are having two issues: one is, we need to send the username/id in next microservice. Another one is, we need to request two times - first for getting the access token, next for actual call.</p></li>&#xA;<li><p>If we do the token verification in API gateway only (not in microservices) then from the API gateway we need to send the username in every microservices. And microservices implementation needs to be changed to accept that param/header.</p></li>&#xA;</ul>&#xA;"
46498125,46476622,191389,2017-09-29T23:35:28,"<p>So you want your Kong instance to communicate with your upstream server with some secret that only you know, so you can be certain that the traffic to your upstream server is coming from your Kong instance?</p>&#xA;&#xA;<p>Sure thing! There are a few ways to do that. You could use <a href=""https://getkong.org/plugins/request-transformer/"" rel=""nofollow noreferrer"">https://getkong.org/plugins/request-transformer/</a> to add a header that contains some secret string (and then potentially use <a href=""https://getkong.org/plugins/response-transformer/"" rel=""nofollow noreferrer"">https://getkong.org/plugins/response-transformer/</a> to remove that header before responding to the client). </p>&#xA;&#xA;<p>Really, it depends on what your upstream server supports in terms of validation of requests. </p>&#xA;&#xA;<p>You also mention giving your end-users API keys - I'm hopeful you'll use <a href=""https://getkong.org/plugins/key-authentication/"" rel=""nofollow noreferrer"">https://getkong.org/plugins/key-authentication/</a> to do that. </p>&#xA;"
43128448,43104831,191389,2017-03-30T21:13:11,"<p>In this scenario you'd want to deploy a single Kong cluster across multiple data centers - have a look at <a href=""https://getkong.org/docs/0.10.x/clustering/"" rel=""nofollow noreferrer"">https://getkong.org/docs/0.10.x/clustering/</a> </p>&#xA;&#xA;<p>Kong supports two datastores (Postgres and Cassandra) you'd probably want to pick Cassandra, but you could make Postgres work - have a look at <a href=""https://getkong.org/docs/0.10.x/configuration/#datastore-section"" rel=""nofollow noreferrer"">https://getkong.org/docs/0.10.x/configuration/#datastore-section</a></p>&#xA;"
50240404,50217813,191389,2018-05-08T18:48:01,"<p>Additionally, it is possible to configure separate roles for multiple Kong nodes - one (or more) can be ""control plane"" nodes that only you can access, and that are used to set and review Kong's configuration, access metrics, etc.</p>&#xA;&#xA;<p>One (or more) other Kong nodes can be ""data plane"" nodes that accept and route API proxy traffic - but that doesn't accept any Kong Admin API commands. See <a href=""https://konghq.com/blog/separating-data-control-planes/"" rel=""nofollow noreferrer"">https://konghq.com/blog/separating-data-control-planes/</a> for more details.</p>&#xA;"
50971019,50966737,1191070,2018-06-21T14:28:31,"<h2>Microservice</h2>&#xA;&#xA;<p>Microservice is rather software architecture. The idea is that you have many small applications - microservices, each focused on addressing only a single goal, but doing it really well. </p>&#xA;&#xA;<p>Specific instance of a microservice can be for example application running HTTP server for managing users. It could have HTTP endpoints for adding, viewing and deleting users in a database. You would then deploy such application together with database on some server. </p>&#xA;&#xA;<p>With fair degree of simplification, we could say that microservice is not all that different than a web browser you are running on your computer. The difference between your web browser and a microservice is that microservice will be running on server, exposing some sort of network interface, whereas you browser runs on your personal computer and it doesn't expose network interface for others to interact with.</p>&#xA;&#xA;<p><strong>The bottom line is that singe microservice is just an application running on a server, you can modify its code anytime, you can stop it any time, you can change data in the database it's using.</strong></p>&#xA;&#xA;<h2>Decentralized application</h2>&#xA;&#xA;<p>Decentralized application is deployed to blockchain. Blockchain is network of computers (Ethereum MainNet has tens of thousands of nodes), all running the same program. When you write decentralized application (called smart contract, in terms of Ethereum blockchain) and you ""deploy it"", what happens is that you basically insert your code into this networks of computers and each of them will have it available.</p>&#xA;&#xA;<p>Once code of your application is in the network, you can interact with it - you can the interface you defined in your decentralized application, by sending JSON-RPC requests to a server which is part of this blockchain network. </p>&#xA;&#xA;<p>It then takes some time until your request for execution is picked up by network. If everything goes right, your request is eventually distributed to the network and executed by every single computer connected to blockchain. </p>&#xA;&#xA;<p>The consequence of that is that if some computer in the network tries to lie about the result, the fraudulent attempt would be noticed by the rest of the network.</p>&#xA;&#xA;<p><strong>The bottom line here is that decentralized application is not executed on one computer, but many (possibly thousands) and even as a creator, you cannot modify it's code or data (you can only to limited degree)</strong></p>&#xA;"
47836584,47727201,7758750,2017-12-15T16:45:31,"<p>Did you try use correlation ID?</p>&#xA;&#xA;<p>Something like this:</p>&#xA;&#xA;<pre><code>from(""direct:myCall"")&#xA;.setProperty(""myCorrelationId"", constant(""1""))&#xA;.setBody(constant(""MESSAGE""))&#xA;.to(""socket-tcp://SOCKET:16010"")&#xA;.toD(""direct:aggregator"")&#xA;.end();&#xA;&#xA;from(""direct:aggregator"")&#xA;.aggregate(property(""myCorrelationId""),(oldExchange, newExchange) -&gt; {&#xA;    if(oldExchange != null) {&#xA;        /*do somenthing*/&#xA;    }&#xA;    if(newExchange != null) {&#xA;        /*Do something*/&#xA;    }&#xA;    return newExchange;&#xA;}).eagerCheckCompletion().completionSize(2)&#xA;.to(""log:foo"")&#xA;.end();&#xA;</code></pre>&#xA;&#xA;<p>In this example you will have a call with a Correlation ID and an Aggregate route which will wait for 2 messages with the same correlation ID for work using this information.</p>&#xA;"
51650694,33399988,1402270,2018-08-02T09:57:09,"<p>It really depends on your scalability requirements, and how/if your microservice instances need to cooperate to provide a single result. It helps to know what the trade-offs are:</p>&#xA;&#xA;<p><strong>Keeping it all in one database</strong></p>&#xA;&#xA;<ul>&#xA;<li><p>Easier configuration</p></li>&#xA;<li><p>No coordination or communication with other instances of your service&#xA;needed </p></li>&#xA;<li>Easier to discover your full dataset </li>&#xA;<li>System performance limited by database performance</li>&#xA;</ul>&#xA;&#xA;<p><strong>Keeping the databases separate</strong></p>&#xA;&#xA;<ul>&#xA;<li><p>The full answer for a request may be spread across microservice&#xA;instances In that case you have increased communication and&#xA;negotiation to resolve the request Handling data when you loose that&#xA;microservice node (even when the database is still up, you can't get&#xA;at it until a new one with the right configuration is stood back up)</p></li>&#xA;<li><p>Increased configuration complexity</p></li>&#xA;</ul>&#xA;"
28755746,27891019,355742,2015-02-27T00:59:02,"<p>I decided to go with fig. </p>&#xA;&#xA;<p>If a microservice depends on another microservice fig will reference this service using ../</p>&#xA;&#xA;<p>This assumes that all services are checked out as siblings. This can also easily be accomplished using git sub trees.</p>&#xA;&#xA;<p>My dev_ops repro contains my Vagrant file for boot2docker, etc.</p>&#xA;"
47351304,47324184,1692830,2017-11-17T13:02:35,"<p>I think we spoke really briefly on Gitter, and there I already said that it depends on your use case. I'll do a brief rundown of typical use cases, and where you need which kind of additional implementation.</p>&#xA;&#xA;<h1>Machine to Machine communication</h1>&#xA;&#xA;<p>If you need two systems to talk to each other from the backends, and these systems trust each other, you can use the OAuth2 ""Client Credentials Flow"". In this case, there is no ""end user"" identity involved, only the two systems which explicitly trust each other.</p>&#xA;&#xA;<p>For this scenario, Kong is everything you need - you just ask Kong's API Token end point (<code>&lt;address of kong&gt;:8443/your_api/oauth2/token</code> for URI based routing, or <code>fqdn.of.kong:8443/oauth2/token</code> if you're using host based routing) for an access token using your client ID and Secret, and you will get one back.</p>&#xA;&#xA;<p>Example:</p>&#xA;&#xA;<pre><code>curl --insecure -d 'grant_type=client_credentials&amp;client_id=&lt;...&gt;&amp;client_secret=&lt;..&gt;' https://&lt;address of kong&gt;:8443/your_api/oauth2_token&#xA;</code></pre>&#xA;&#xA;<p>Your backend service will get some extra headers injected, such as <code>X-Consumer-Id</code> and <code>X-Custom-ConsumerId</code> which maps to the consumer you created in Kong.</p>&#xA;&#xA;<h1>Confidential Web Application with End User context</h1>&#xA;&#xA;<p>In case you need to use your API from a confidential (=classic) web application, and you need to have an end user context with each call, you might want to use the OAuth2 ""Authorization Code Grant"". In this case, you will also need an Authorization Server which you need to implement yourself.</p>&#xA;&#xA;<p>The Task of the Authorization Server is to establish an end user identity (mind you: This is <strong>not</strong> specified in OAuth2 how this is done, and is up to you; you can federate to some other IdP, you can ask for username and password,...) and then to decide on which rights (=""scopes"") the user gets when accessing the API. This is completely up to you, and part of your business logic how to decide this.</p>&#xA;&#xA;<p>The flow goes like this:</p>&#xA;&#xA;<ol>&#xA;<li>You (re-)direct a user to the web page of the authorization server</li>&#xA;<li>The AS authenticates the user (by whatever means) and decides on the scopes (by whatever other means)</li>&#xA;<li><p>The AS talks to Kong on two different levels</p>&#xA;&#xA;<ul>&#xA;<li>Via the Kong Admin API, to retrieve the <code>provision_key</code> of the desired API</li>&#xA;<li>Via the <code>[/your_api]/oauth2/authorize</code> end point, which it uses to get a redirect URI which includes an authorization code, in the context of the authenticated user and his scope (<code>scope</code> and <code>authenticated_userid</code>); to call this end point, you will need <code>response_type=code</code>, <code>client_id</code>, <code>client_secret</code>, <code>provision_key</code>, <code>authenticated_userid</code> (whatever is suitable) and optionally <code>scope</code> (scopes need to be defined on the API as well if you want to use this)</li>&#xA;</ul></li>&#xA;<li><p>If successful, the AS redirects back to the web application, using the redirect URI returned by Kong</p></li>&#xA;<li>The web app calls Kong's <code>[/your_api]/oauth2/token</code> end point with its <code>client_id</code>, <code>client_secret</code> and <code>code</code>, using the <code>grant_type=code</code></li>&#xA;</ol>&#xA;&#xA;<p>Now you will have an access token (and a refresh token) which lets your web application access the API <strong>on behalf of the authenticated user</strong>.</p>&#xA;&#xA;<p>The Authorization Server has to be implemented by you; this is not super complicated, but you still need to make sure you know how to authenticate a user, and/or how you delegate this to some other IdP.</p>&#xA;&#xA;<h1>Public Client (Single Page Application) with End User Context</h1>&#xA;&#xA;<p>In case you need access to an API from a Single Page Application (like from an Angular app or similar), you should look at the OAuth2 ""Implicit Flow"", which is a simpler flow than the authorization code grant, but which has other drawbacks, like not being able to use refresh tokens.</p>&#xA;&#xA;<p>This flow works in the following way:</p>&#xA;&#xA;<ol>&#xA;<li>Just like for the Authorization Code grant, you redirect the user to the Authorization Server</li>&#xA;<li>The AS establishes identity and decides on scope (once again, this is up to you)</li>&#xA;<li>The AS calls the authorize end point, just like with the Authorization Code grant, but this time with <code>response_type=token</code></li>&#xA;<li>Kong, if successful, will return a redirect URI which <strong>already contains a token</strong></li>&#xA;<li>The AS redirects back to the SPA, using the redirect URI from Kong, which has the access token in the ""fragment"" of the URI (e.g. <code>https://your.app.com/#access_token=&lt;...&gt;&amp;token_type=bearer&amp;...</code>)</li>&#xA;</ol>&#xA;&#xA;<p>Your SPA will now be able to use the access token to access the API, just like with the Authorization Code grant, <strong>on behalf of the authenticated user</strong>.</p>&#xA;&#xA;<p>The drawback with this approach is that you can't (that) easily refresh the token, and that it's somewhat less secure than the Authorization Code grant. But dealing with SPAs, there are not many other secure ways of delegating access to it.</p>&#xA;&#xA;<h1>Mobile Applications</h1>&#xA;&#xA;<p>The last scenario I would like to touch here is Mobile Applications, like Android or iOS apps. For these, the last OAuth2 flow, the ""Resource Owner Password Grant"" can be used. In short, with this grant you exchange the actual user credentials (username and password) against an access token and a refresh token, so that you don't have to store username and password on the mobile device more than temporarily.</p>&#xA;&#xA;<p>This flow also needs an Authorization Server to be able to use with Kong, albeit a less complicated one this time, even though you must implement an additional <code>token</code> end point (in addition to the one Kong has), which is not ideally described in the Kong documentation.</p>&#xA;&#xA;<p>It'd go like this:</p>&#xA;&#xA;<ol>&#xA;<li>The mobile app uses its <code>client_id</code> (NOT the secret, the secret should not be deployed with the application), the username and password to call the Authorization Server's token end point</li>&#xA;<li>The Authorization Server checks username and password (by whatever means, you know the story) and decides on the scope (...)</li>&#xA;<li>The AS talks to Kong over admin API again, getting the <code>client_secret</code> for the provided <code>client_id</code> and the <code>provision_key</code> for the desired API</li>&#xA;<li><p>The AS issues a call to Kong's token end point <code>[/your_api]/oauth2/token</code>, like this:</p>&#xA;&#xA;<p>curl --insecure -d 'grant_type=password&amp;provision_key=&lt;...>&amp;client_id=&lt;...>&amp;client_secret=&lt;...>&amp;authenticated_userid=&lt;...>&amp;scope=' https://:8443/your_api/oauth2/token</p></li>&#xA;</ol>&#xA;&#xA;<p>Note that this call does <strong>not</strong> contain username and password; those don't belong here, you must check username and password against your own source of identity, Kong will not help you with that.</p>&#xA;&#xA;<p>This call should return both an access token and a refresh token which you then store (as safely as possible) on your device. These <strong>replace</strong> the username and password, which <strong>must not</strong> be stored on the device. The access token can as with the other end user context flows (Authorization Code Grant, Implicit Grant) be used to access the API <strong>on behalf of the authenticated user</strong>.</p>&#xA;&#xA;<p>I hope this could help shedding some light on how you can use Kong with OAuth2. It's tricky and involved, but Kong can really help getting this right and separate your concerns.</p>&#xA;"
46522273,46521195,3340702,2017-10-02T08:48:04,"<h3>TL;TR</h3>&#xA;&#xA;<p>It is possible to run multiple services inside a single container, but I highly discourage you to do it. </p>&#xA;&#xA;<p>Anyway, you can do this bundling all services inside the same image and then (1) running a container with many services using, or (2) running them using a supervisor.</p>&#xA;&#xA;<p>If your constraint is only the single image, you can do better, (3) running multiple containers using the same image, customizing the service to run on each.</p>&#xA;&#xA;<h3>Why running multiple processes in a container should be avoided</h3>&#xA;&#xA;<p>The <a href=""https://docs.docker.com/engine/admin/multi-service_container/"" rel=""nofollow noreferrer"">Docker documentation says</a> (bold is mine):</p>&#xA;&#xA;<blockquote>&#xA;  <p>It is generally recommended that you separate areas of concern by using <strong>one service per container</strong>. That service may fork into multiple processes (for example, Apache web server starts multiple worker processes).</p>&#xA;</blockquote>&#xA;&#xA;<p>and then it continues:</p>&#xA;&#xA;<blockquote>&#xA;  <p>It’s ok to have multiple processes, but to get the most benefit out of Docker, <strong>avoid one container being responsible for multiple aspects of your overall application</strong>.</p>&#xA;</blockquote>&#xA;&#xA;<p>This is because Docker containers are attached to a single process (usually defined by <code>ENTRYPOINT</code>/<code>CMD</code> in Dockerfiles) and when this process dies, the entire container is stopped.</p>&#xA;&#xA;<p>Containers are designed to isolate services: if you want a isolated environment with many, non isolated services (like in the first two ways described above), probably it's better to use a virtual machine.</p>&#xA;&#xA;<h3>The base</h3>&#xA;&#xA;<p>The common idea of every approach is to pack every application you have in the same final image using a single Dockerfile:</p>&#xA;&#xA;<pre><code>FROM ubuntu&#xA;&#xA;// RUN install dependencies for every app you have&#xA;// COPY all your binaries/apps (e.g. service 1, 2, 3) or build them&#xA;</code></pre>&#xA;&#xA;<h3>The wrapper script way (1)</h3>&#xA;&#xA;<p>Following the first example in the <a href=""https://docs.docker.com/engine/admin/multi-service_container/"" rel=""nofollow noreferrer"">Docker documentation</a> you can start multiple services using a wrapper script that starts them in the background and the check every minute if all of them are running. When a service crashes, the entire container is stopped. </p>&#xA;&#xA;<p>In this case your image will end with a command line like <code>CMD ./my_wrapper_script.sh</code>.</p>&#xA;&#xA;<h3>The supervisor way (2)</h3>&#xA;&#xA;<p>As suggested in the comment above, you can use <a href=""https://docs.docker.com/engine/admin/multi-service_container/"" rel=""nofollow noreferrer"">a supervisor inside the container</a> to run multiple services avoiding the problem above. In this way you have many processes managed by the supervisor and if the supervisor crashes, all your service will be taken down (you have a single point of failure).</p>&#xA;&#xA;<p>In this case your image will end with a line like <code>CMD start-supervisor</code>.</p>&#xA;&#xA;<h3>The custom command way (3)</h3>&#xA;&#xA;<p>If your constraint is only the single image and you can have multiple container this is the best approach. Just run multiple containers starting them using an explicit <a href=""https://docs.docker.com/engine/reference/run/"" rel=""nofollow noreferrer""><code>command</code></a> as the last parameter:</p>&#xA;&#xA;<pre><code>docker run your-image your-service-1&#xA;docker run your-image your-service-2&#xA;docker run your-image your-service-3&#xA;</code></pre>&#xA;&#xA;<p>You can do it also <a href=""https://docs.docker.com/compose/compose-file/#command"" rel=""nofollow noreferrer"">using a docker-compose file</a>.</p>&#xA;&#xA;<p>With this approach you don't ""break"" <strong>one service per container</strong> rule having a more resilient deploy.</p>&#xA;"
37995316,37989803,3340702,2016-06-23T15:00:53,"<p>You've read a lot, but not too much. Please give a look at:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""http://samnewman.io/books/building_microservices/"" rel=""nofollow"">Building microservices</a> (it talks about every aspect of MSs, and also about security)</li>&#xA;<li><a href=""http://the-cloud-book.com/"" rel=""nofollow"">The Practice of Cloud System Administration</a> (not strictly MS relatad but containts many useful information)</li>&#xA;</ul>&#xA;"
43553143,43552529,1246623,2017-04-21T22:50:46,"<p>Spring Cloud Config also supports native file system backend instead of git or svn. You can use it with the below options</p>&#xA;&#xA;<pre><code>spring.profiles.active=native&#xA;spring.cloud.config.server.native.searchLocations= xxxx&#xA;</code></pre>&#xA;&#xA;<p>You can find more defailts in 'File System Backend' section of official doc : <a href=""http://cloud.spring.io/spring-cloud-static/spring-cloud-config/1.3.0.RELEASE/"" rel=""nofollow noreferrer"">http://cloud.spring.io/spring-cloud-static/spring-cloud-config/1.3.0.RELEASE/</a></p>&#xA;"
45298191,45278205,1246623,2017-07-25T08:43:27,"<p>From your <strong>server</strong> properties, try to remove the below line. It makes eureka client inside your server not fetch other servers info from eureka server.</p>&#xA;&#xA;<pre><code>eureka.client.fetch-registry=false&#xA;</code></pre>&#xA;&#xA;<p>Also check that your Application class of server has <code>@EanbleEurekaClient</code> or <code>@EnableDiscoveryClient</code> annotation. Your server application also must be a Eureka client to fetch others.</p>&#xA;&#xA;<p>From your <strong>service</strong> properties, remove the below line. Overriding instance-id is not required at all (but, it might not be related to your problem)</p>&#xA;&#xA;<pre><code>eureka.instance.instance-id=${spring.application.name}&#xA;</code></pre>&#xA;&#xA;<p>If you need a full example, try to check this <a href=""https://github.com/yongsungyoon/spring-cloud-workshop"" rel=""nofollow noreferrer"">example</a> code. (Readme.md is written in Korean, but codes are very simple. So there will be no problem to read the code without readme)</p>&#xA;"
47798007,47276560,6687135,2017-12-13T16:31:56,"<p>Assuming you are using also Apollo in the client you can send a token or any other parameter you use for authentication alongside to the websocket connection.&#xA;You can read more here:&#xA;<a href=""https://www.apollographql.com/docs/react/features/subscriptions.html#authentication"" rel=""nofollow noreferrer"">https://www.apollographql.com/docs/react/features/subscriptions.html#authentication</a></p>&#xA;&#xA;<p>In the Apollo Server you can then authenticate the user for example onConnect:&#xA;<a href=""https://www.apollographql.com/docs/graphql-subscriptions/authentication.html"" rel=""nofollow noreferrer"">https://www.apollographql.com/docs/graphql-subscriptions/authentication.html</a></p>&#xA;&#xA;<p>You may also bake some claims into the token which define if a user can connect, publish or receive message and then pass along the token or verified users to Redis if you will otherwise call your user/auth service accordingly to verify the user and then proceed.</p>&#xA;&#xA;<p>You can additionally add verification in onOperation parsing the subscription/message to validate for example a user on subscribing to a channel/topic.&#xA;I highly recommend you to read this as a great sample:&#xA;<a href=""https://medium.com/react-native-training/building-chatty-part-7-authentication-in-graphql-cd37770e5ab3"" rel=""nofollow noreferrer"">https://medium.com/react-native-training/building-chatty-part-7-authentication-in-graphql-cd37770e5ab3</a></p>&#xA;&#xA;<p>Hope that helps.</p>&#xA;"
52058042,52057828,266143,2018-08-28T12:33:43,"<p>There's not really a template for creating a microservice in .NET, because any application that is deployable in a standalone way and that is reachable over some form of communication protocol (be it HTTP, message queues, or anything else) to perform some sort of action can be called a microservice.</p>&#xA;&#xA;<p>See also <a href=""https://www.martinfowler.com/articles/microservices.html"" rel=""nofollow noreferrer"">Martin Fowler: Microservices</a> and <a href=""https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/architect-microservice-container-applications/microservices-architecture"" rel=""nofollow noreferrer"">.NET microservices - Architecture e-book</a>.</p>&#xA;&#xA;<p>So to create your service that will <em>""[accept] two latitude and longitude values and [return] the distance""</em>, you can simply create an ASP.NET Core Web API with one action method, and that's your microservice.</p>&#xA;"
41124005,41036545,532383,2016-12-13T14:51:13,"<p>Configure an ObjectMapper bean in Spring/Javaconfig and set property to ignore missing properties. Inject the same bean where needed.</p>&#xA;&#xA;<pre><code>@Bean&#xA;public ObjectMapper objectMapper() {&#xA;    return new ObjectMapper()&#xA;       .configure(com.fasterxml.jackson.databind.DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);&#xA;    }&#xA;</code></pre>&#xA;"
50510943,48824086,9841311,2018-05-24T13:43:48,"<p>In your HttpSecurity http, add <code>.anonymous().disable()</code> such that you have&#xA;your HttpSecurity like <code>http.anonymous().disable()</code>.</p>&#xA;"
30637374,30213456,471199,2015-06-04T07:15:26,"<p>IMHO one of the key aspects of microservices architecture is that the transaction is confined to the individual microservice (Single responsibility principle). </p>&#xA;&#xA;<p>In the current example, the User creation would be an own transaction. User creation would push a USER_CREATED event into an event queue. Wallet service would subscribe to the USER_CREATED event and do the Wallet creation.</p>&#xA;"
43543184,43532494,18591,2017-04-21T12:50:39,"<p>First of all, if you really have a pure caching use case, there should be no correctness impact only a performance one. Which of course can in itself be a bad thing for your application.</p>&#xA;&#xA;<p>But effectively, if you want to use caching to provide performance and at the same time have a multi-node ability without sticky sessions, you have to move into the realm of distributed caching. This will give you the ability to share the cache content amongst the different nodes and thus make it (more) transparent for a given request in a conversation to hit any node of your application.</p>&#xA;&#xA;<p>In the Ehcache world, this means backing the cache with a Terracotta server, see <a href=""http://www.ehcache.org/documentation/3.3/clustered-cache.html"" rel=""nofollow noreferrer"">the documentation</a> for details.</p>&#xA;"
34455323,33556707,154527,2015-12-24T16:10:57,"<p>This is an interesting question.  All of the components you mention are independent with clearly separate strengths and roles in a microservice architecture.  The unusual part is in using messaging rather than HTTP. I think it's a valuable departure since it enables more flexible computing patterns (work producer doesn't need to be product consumer or even be notified).  A particular beauty of skipping HTTP is avoiding the cost (both OPEX and service latency), complexity, and added fault modes of load balancers.</p>&#xA;&#xA;<ol>&#xA;<li><p>Docker: to manage the packaging of individual services and delivery onto infrastructure&#xA;ZeroMQ: to manage efficient peer-to-peer or brokered communications between services&#xA;Consul: service discovery (ex. find out where the user service is)</p></li>&#xA;<li><p>Autoscaling isn't performed by Docker.  You could do this with your own microservice that inspects the load metrics (ex. load average, phys/swap memory usage, etc.) and spins up replicas and updates Consul.</p>&#xA;&#xA;<p>Alternatively, you could lean on autoscaling solutions detailed by @drhender: Kubernetes, Mesosphere DCOS, or AWS Autoscaling Groups.  Note, however, that using AWS Autoscaling Groups signicantly limits the portability of your solution.</p>&#xA;&#xA;<p>Whatever autoscaling mechanism you choose, make sure your ZeroMQ message patterns support services being added or removed.  The <a href=""http://zguide.zeromq.org/"" rel=""nofollow"">ZeroMQ Guide</a> has good advice on this topic.</p></li>&#xA;<li><p>Consul can provide both service discovery and service configuration needs.  Bear in mind storage security concerns if you are storing sensitive data such as PHI or PII.  Those are better stored with at-rest protections such as Vault supplies.</p>&#xA;&#xA;<p><a href=""https://consul.io/docs/agent/telemetry.html"" rel=""nofollow"">The Consul agent collects telemetry</a> that you can monitor for issues.  There is also a fairly simple <a href=""https://github.com/hashicorp/consul/tree/master/bench"" rel=""nofollow"">Consul KV benchmarking suite</a> that you can use to test the configuration storage.</p>&#xA;&#xA;<p>ZeroMQ isn't designed for service discovery directly.  You can choose a centrally brokered architecture that makes separate service discovery unnecessary, but that has different scalability and fault tolerance implications. It has a per-message payload overhead assuming you use multi-part messages to and topic subscriptions when binding SUB sockets.  There are many ways you could do service discovery with ZeroMQ alone, but it would be non-trivial particularly when factoring in faukt-tolerance and consensus.</p></li>&#xA;<li><p>Node failure is an interesting challenge.  Consul could know via health checks, but ZeroMQ creates some challenges depending on messaging patterns.  For example, using a REQ-REP pair if a request is sent and the responder dies after delivery the REQ socket will block forever in my experience.  There are ways around this with timeouts.</p>&#xA;&#xA;<p>I would lean on Consul for this and be prepared to interrupt or respawn REQ sockets on failures.  Avoiding RPC style interactions entirely by using a Staged Event Driven Architecture (SEDA) where producers of inputd are not the consumers of outputs side-steps this almost entirely.  You always have the challenge of losing the queued inputs or outputs on failure so you need system level monitoring and retry mechanisms if losing work is fatal.</p></li>&#xA;</ol>&#xA;&#xA;<p><a href=""http://containerbuddy.io/"" rel=""nofollow"">ContainerBuddy</a> allows you to put any launchable application in a Docker container and have it register with Consul.  It could simplify things for you.</p>&#xA;"
33660445,33659658,3634591,2015-11-11T21:46:24,"<p>By default, tasks run on .NET's thread pool, which is why the scheduler might be trying to run on more than 2 threads at a time. Lots more info on MSDN <a href=""https://msdn.microsoft.com/en-us/library/dd537609(v=vs.110).aspx#Anchor_14"" rel=""nofollow"">here</a>. </p>&#xA;&#xA;<p>So to limit your concurrency to <a href=""https://msdn.microsoft.com/en-us/library/system.environment.processorcount(v=vs.110).aspx"" rel=""nofollow"">Environment.ProcessorCount</a>, you would need to create your own custom TaskScheduler that doesn't use the ThreadPool and limits concurrency to your specific value--the docs for the <a href=""https://msdn.microsoft.com/en-us/library/system.threading.tasks.taskscheduler(v=vs.110).aspx"" rel=""nofollow"">TaskScheduler</a> has a nice example that illustrates how to do this. Just plug your scheduler into your own TaskFactory instance and then use that factory to spawn your async tasks.</p>&#xA;&#xA;<p>The <a href=""https://code.msdn.microsoft.com/ParExtSamples"" rel=""nofollow"">Samples for Parallel Programming</a> that the docs link to has a whole bunch of schedulers that you might consider.</p>&#xA;"
28324983,28319278,1085343,2015-02-04T15:23:36,"<p>Yeah easy. Each client of a micro service has an API key. Micro services only accept requests from clients with a valid API Key.</p>&#xA;&#xA;<p>Also, its good to know that REST is simply a protocol that allows communication between bounded contexts. </p>&#xA;&#xA;<p>It doesn't have to be over HTTP. The requirement is that it has a uniform interface (this is why HTTP is used with its PUT, POST, GET, DELETE... methods) and that it is stateless (all state being transferred through a URI).</p>&#xA;&#xA;<p>So if all your micro services run on the same box, all you need to do is something like this:</p>&#xA;&#xA;<pre><code>class SomeClass implements RestfulMethods {&#xA;&#xA;    public function get(params){ // return something}&#xA;    public function post(params){ // add something}&#xA;    public function put(params){ // update something}&#xA;    public function delete(params){ // delete something}&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Micro services then communicated by interacting with the RestfulMethod implementations of other services.</p>&#xA;&#xA;<p>But if your micorservices are on different machines, its probably best to use HTTP as the transport mechanism.</p>&#xA;"
47026529,47013636,832671,2017-10-31T01:16:45,"<p>It depends whether you're talking about port of the mock service in the consumer tests, or the port of the provider in the verification step.</p>&#xA;&#xA;<p>In the consumer tests, is it possible to provide a test implementation of the part of the SDK that looks up the port? Perhaps you could contact the provider team to see they could supply one that would allow you to set a known port?</p>&#xA;&#xA;<p>In regards to the provider, you would typically run the verification step against a locally running provider in the CI build, not against one deployed into a live environment, so a known port should be able to be used.</p>&#xA;"
45362886,45325062,832671,2017-07-28T00:07:53,"<p>The team that wrote Pact in the first place was responsible for both ends of the integration, and they still found contract testing valuable. Just because you're developing both sides now, doesn't mean that you will continue to be responsible for both sides in the future. Contract tests will ensure that changes made by future developers will not break anything. </p>&#xA;"
49352820,49344182,832671,2018-03-18T21:07:56,"<p>You could ask this on the Pact gitter channel <a href=""https://gitter.im/realestate-com-au/pact"" rel=""nofollow noreferrer"">https://gitter.im/realestate-com-au/pact</a></p>&#xA;"
51976361,51974383,832671,2018-08-22T23:12:14,"<p>It's always a good idea to choose the right tool for the right job, and Pact may not be the best tool for what you're trying to do if you're trying to use it in a way for which it was not designed.</p>&#xA;&#xA;<p>Please come and have a talk to us on <a href=""https://slack.pact.io"" rel=""nofollow noreferrer"">https://slack.pact.io</a> about what you're trying to achieve, as your use case does not make sense to me, and I can't give you a general answer that I would be happy to leave as a guide for anyone else with a similar question (which is what stackoverflow is good for).</p>&#xA;"
46354698,46343630,832671,2017-09-21T23:32:38,"<p>This functionality isn't currently implemented, however, I would suggest using a <code>Pact.eachLike</code>, which will just match on types. The actual values of the fields are rarely important.</p>&#xA;"
50634463,50584687,832671,2018-06-01T00:13:15,"<p>Firstly, no, Pact does not have support for javascript code. It will just treat the body as as String, as you have noticed. You can still do contract tests with string bodies, but they will be very brittle, as one character difference will make the match fail.</p>&#xA;&#xA;<p>Secondly, what behaviour would you expect if there <em>was</em> a matcher for javascript?</p>&#xA;&#xA;<p>Thirdly, Pact is not a good tool for testing a third party API. It is specifically written for consumer driven contract testing, not general API mocking. Please read the docs here: <a href=""https://docs.pact.io/getting-started/what-is-pact-good-for"" rel=""nofollow noreferrer"">https://docs.pact.io/getting-started/what-is-pact-good-for</a></p>&#xA;"
48846788,48840474,9223839,2018-02-17T23:04:33,"<p>For the update I would change the where clause to </p>&#xA;&#xA;<pre><code>WHERE convert(date, @DownloadDate) = convert(date, DownloadDate)&#xA;and (s.Identifier1 = @Identifier1 OR s.Identifier2 = @Identifier2)&#xA;</code></pre>&#xA;&#xA;<p>and for the insert</p>&#xA;&#xA;<pre><code>WHERE sec.Identifier1 = @Identifier1 OR sec.Identifier2 = @Identifier2&#xA;</code></pre>&#xA;&#xA;<p>That should work even if I haven't verified it myself. I am assuming that the given values for identifier1 and identifier2 can not match two different rows in the Instrument table.</p>&#xA;"
51734998,51733502,311455,2018-08-07T20:34:28,"<blockquote>&#xA;  <p>does Trasaction maanger have ability to handle concurrency issue in distributed database scenario's .</p>&#xA;</blockquote>&#xA;&#xA;<p>Transactions and concurrency are two independent concepts and though Transactions become most siginificant in context where we also see concurrency , transactions can be important without concurrency. </p>&#xA;&#xA;<p>To answer your question : No , Transaction Manager generally does not concern itself with handling issues that arise with concurrent updates. It takes a very naive and simple ( and often most meaningful ) approach : if after the start of a transaction , it detects that the state has become inconsistent ( because of concurrent updates ) it would simply raise it as an exception and Rollback the transaction. If only it can establish that all the conditions of the ACID properties of the transaction are still valid will it commit the transaction.</p>&#xA;"
43384610,43384538,1163423,2017-04-13T06:02:27,"<p>You are correct, each microservice should use its own data store that fits best to its needs. There might be a service that want's to store its data in a blob storage, another may store its data in a table storage or DocumentDb or SQL Database. </p>&#xA;&#xA;<p>You probably want to use <em>Database-as-a-Service</em> thus <em>not</em> hosting your own db because you don't have to worry about availabilty, scaling, backups...</p>&#xA;"
39672906,39450504,2344607,2016-09-24T05:33:57,"<p>What I've done with my own code is to pull all code out of the handler.js files and put it inside modules.  These modules would be required into the handler.js files and a simple function would then be called.</p>&#xA;&#xA;<p>usersModule.js: </p>&#xA;&#xA;<pre><code>export const doSomething = () =&gt; {&#xA;    // Do something here.&#xA;};&#xA;</code></pre>&#xA;&#xA;<p>users/handler.js: </p>&#xA;&#xA;<pre><code>import {doSomething} from '../.../usersModule.js';&#xA;&#xA;export const handler = ( event, context, callback ) =&gt; {&#xA;    doSomething();&#xA;    // Do other stuff...&#xA;    callback( null, ""Success"" );&#xA;};&#xA;</code></pre>&#xA;&#xA;<p>This way, you can place the meat of your code wherever you would like to have it, organized in whatever way makes sense to you.</p>&#xA;&#xA;<p>You would, however, still have to have a single API defined.  Or use the answer from RyanG-AWS to merge the APIs.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>If you'd still like to keep code and API definitions separate, you could create the users API and the products API separately.  You would then have another combined API that would call either of these APIs.  So this way, you would have a single service with a single base URL that you would call.  You can do this with integration type HTTP.  I have not tried this, so I don't know how well it would work.</p>&#xA;"
48601646,46131196,4314527,2018-02-03T20:23:28,"<p>You need to create Eureka Registry Server which is another microservice. It's main class incase of an SpringBoot application, should have @EnableEurekaServer annotation.</p>&#xA;&#xA;<p>Then in your Eureka Client you will have to mention the  Registry server URL in appliation.yml as below : </p>&#xA;&#xA;<pre><code>spring:&#xA;  application:&#xA;    name: stock-service&#xA;&#xA;server:&#xA;  port: 8083&#xA;&#xA;&#xA;eureka:&#xA;  client:&#xA;    registerWithEureka: true&#xA;    fetchRegistry: true&#xA;    serviceUrl:&#xA;      defaultZone: http://localhost:8084/eureka/&#xA;  instance:&#xA;    hostname: localhost&#xA;</code></pre>&#xA;&#xA;<p>where defaultzone should hold the value of your Eureka Registry.</p>&#xA;&#xA;<p>Once you do all these configurations you need to Get the Eureka Registry microservice up and then you can get the Eureka client up. Once your Registry is up you will not face this exception.</p>&#xA;"
46848496,46807757,1047711,2017-10-20T11:51:27,"<p>I have talked this with the Amazon support team and they respond the following:&#xA;""...using Service Discovery on AWS ECS[..] just with ALBs.</p>&#xA;&#xA;<p>So, there could be three options here: &#xA;1) Using ALB/ELB as service endpoints (Target groups for ALBs, separate ELBs if using ELBs)</p>&#xA;&#xA;<p>2) Using Route53 and DNS for Service Discovery</p>&#xA;&#xA;<p>3) Using a 3rd Party product like Consul.io in combination with Nginx. </p>&#xA;&#xA;<p>Let me speak about each of these options.</p>&#xA;&#xA;<h2>Using ALBs/ELBs</h2>&#xA;&#xA;<p>For this option the idea is to use the ELBs or ALB Target groups in front of each service.&#xA;We define an Amazon CloudWatch Events filter which listens to all ECS service creation messages from AWS CloudTrail and triggers an Amazon Lambda function. &#xA;This function identifies which Elastic Load Balancing load balancer (or an ALB Target group) is used by the new service and inserts a DNS resource record (CNAME) pointing to it, using Amazon Route 53. &#xA;The Lambda function also handles service deletion to make sure that the DNS records reflect the current state of applications running in your cluster.</p>&#xA;&#xA;<p>The down side here is that it can incur higher costs if you are using ELBs - as you need an ELB for each service. And it might not be the simplest solution out there.&#xA;If you wish to read more on this you can do so here[1]</p>&#xA;&#xA;<h2>Using Route53</h2>&#xA;&#xA;<p>This approach involves the use of Route53 and running a simple agent[2] on your ECS container instances. &#xA;As your containers stop/start the agent will update the Route53 DNS records. It creates a SRV record. Likewise it will delete said records once the container is stopped. </p>&#xA;&#xA;<p>Another part of this method is a Lambda function that performs health checks on ECS container instances - and removes them from R53 in case of a failure. </p>&#xA;&#xA;<p>You can read up more on this method, on our blog post here[3].</p>&#xA;&#xA;<h2>Using a 3rd Party tool like Consul.io Using tools like Consul.io on ECS, will work - but is not supported by AWS. So you are free to use it, but we - unfortunately - do not offer support for it.</h2>&#xA;&#xA;<p>So, in conclusion - there are a few ways of implementing service discovery on AWS ECS - the two ways I showed here that use AWS resources, and of course the way of using 3rd party applications.&#xA;""</p>&#xA;"
28325197,28319278,1572767,2015-02-04T15:32:12,<p>The easiest way is to only enable access from the IP address that your microservices are running on.</p>&#xA;
47294130,45579511,6357920,2017-11-14T19:52:55,"<p>To expound a bit on @Set's answer. You'll need to introduce some instrumentation code into your gateway API to make the decision about what downstream endpoint to call. If, and only if, the only component of your distributed backend that is concerned with this is the gateway API, the above solution is over-engineered: you can get by with just a library. But it's likely that you will soon discover that one or more of your other services needs to know about the experiment, in which case you DO need a standalone service.</p>&#xA;&#xA;<p>Generally speaking, building a robust experimentation framework is a hard task though. You will quickly run into unexpected problems, e.g. experience stability (how to guarantee the same experience to return visitors) or how to change the allocation proportion (or perhaps completely turn off the new code) without the need to restart the host application. You ought to investigate the open source frameworks out there, or even commercial server side instrumentation. (We have one at <a href=""http://www.getvariant.com"" rel=""nofollow noreferrer"">Variant</a>).</p>&#xA;"
43058478,43058415,1074592,2017-03-28T00:07:29,"<p>Webpack has <code>require.ensure</code> which lets you load modules async. This can improve performance if you have a lot of rarely used modules. For react components, you can use <a href=""https://github.com/webpack-contrib/react-proxy-loader"" rel=""nofollow noreferrer""><code>react-proxy-loader</code></a> to cut down on boilerplate. </p>&#xA;&#xA;<p>I have a fork of it that works with es6 modules.</p>&#xA;&#xA;<pre><code>npm install 'brigand/react-proxy-loader#use-exports-default'&#xA;</code></pre>&#xA;"
26800645,26800452,4221344,2014-11-07T11:51:15,<p>You can try to change Nginx config like as below:</p>&#xA;&#xA;<pre><code>listen 80;&#xA;&#xA;server_name  &lt;your pc name or localhost or 127.0.0.1&gt;&#xA;</code></pre>&#xA;
34519871,31031865,884640,2015-12-29T22:10:05,"<h1>Overview</h1>&#xA;&#xA;<p>Long post!</p>&#xA;&#xA;<ul>&#xA;<li><code>ENTRYPOINT</code> is your friend</li>&#xA;<li><a href=""http://shop.oreilly.com/product/0636920033158.do"" rel=""noreferrer""><em>Building Microservices</em></a> by Sam Newman is great</li>&#xA;<li>Inter-service security tip: 2-way TLS may work, but may present latency issues</li>&#xA;<li>I will get into a real example from my team. We <strong>could not</strong> use a configuration server, and things have gotten ... interesting. Manageable for now. But may not scale as the company has more services.</li>&#xA;<li>Configuration servers seem like a better idea</li>&#xA;</ul>&#xA;&#xA;<p><em>Update</em>: Almost two years later, we might move to <a href=""http://kubernetesbyexample.com/"" rel=""noreferrer"">Kubernetes</a>, and start using the <a href=""https://github.com/coreos/etcd"" rel=""noreferrer"">etcd-powered</a> <a href=""https://kubernetes.io/docs/tasks/configure-pod-container/configmap/"" rel=""noreferrer"">ConfigMaps</a> feature that ships with it. I'll mention this again in the configuration servers section. The post could still be worthwhile reading if you are interested in these subjects. We'll still be using <code>ENTRYPOINT</code> and some of the same concepts, just some different tools.</p>&#xA;&#xA;<h1><code>ENTRYPOINT</code></h1>&#xA;&#xA;<p>I suggest that <code>ENTRYPOINT</code> is the key to managing environment-specific configuration for your Docker containers.</p>&#xA;&#xA;<p><strong>In short: create a script to bootstrap your service before starting, and use <code>ENTRYPOINT</code> to execute this script.</strong></p>&#xA;&#xA;<p>I will go into detail contextualizing this, and also explain how how we do this <em>without</em> a configuration server. It gets a bit deep, but it's not unmanageable. Then, I end with details on configuration servers, a better solution for many teams.</p>&#xA;&#xA;<h1><em>Building Microservices</em></h1>&#xA;&#xA;<p>You're right that these are common concerns, but there just aren't one-size-fits-all solutions. The most general solution is a configuration server. (The <em>most</em> general but still not one-size-fits-all.) But perhaps you cannot use one of these: we were barred from using a configuration server by the Security team.</p>&#xA;&#xA;<p>I strongly recommend reading <a href=""http://shop.oreilly.com/product/0636920033158.do"" rel=""noreferrer""><em>Building Microservices</em></a> by Sam Newman, if you haven't yet. It examines all the common challenges and discusses many possible solutions, while also giving helpful perspective from a seasoned architect. (Side note: don't worry about a perfect solution to your configuration management; start with a ""good enough"" solution for your current set of microservices and environments. You can iterate and improve, so you should try to <a href=""https://twitter.com/samnewman/status/608624424728666112"" rel=""noreferrer"">get useful software to your customers ASAP</a>, then improve in subsequent releases.)</p>&#xA;&#xA;<h1>Cautionary tale?</h1>&#xA;&#xA;<p>Rereading this again ... I cringe a little at how much it takes to explain this fully. From the <a href=""https://www.python.org/dev/peps/pep-0020/"" rel=""noreferrer"">Zen of Python</a>:</p>&#xA;&#xA;<pre><code>If the implementation is hard to explain, it's a bad idea.&#xA;If the implementation is easy to explain, it may be a good idea.&#xA;</code></pre>&#xA;&#xA;<p>I'm not thrilled with the solution we have. Yet it's a workable solution, given we couldn't use a configuration server. It's also a real world example.</p>&#xA;&#xA;<p>If you read it and think, ""Oh god no, why would I want all that!"" then you know, you need to look hard into configuration servers.</p>&#xA;&#xA;<h1>Inter-service security</h1>&#xA;&#xA;<p>It seems like you are also concerned with how different microservices authenticate each other.</p>&#xA;&#xA;<p>For artifacts and configuration related to this authentication ... treat them like any other configuration artifacts.</p>&#xA;&#xA;<p>What are your requirements around inter-service security? In your post, it sounds like you're describing app-tier, username/password authentication. Maybe that makes sense for the services you have in mind. But you should also consider <a href=""https://www.owasp.org/index.php/Transport_Layer_Protection_Cheat_Sheet#Client-Side_Certificates"" rel=""noreferrer"">Two-Way TLS</a>: ""this configuration requires the client to provide their certificate to the server, in addition to the server providing their's to the client."" Generating and managing these certificates can get complicated ... but however you choose to do it, you'll shuffle around the config/artifacts like any other config/artifacts.</p>&#xA;&#xA;<p>Note that 2-way TLS may introduce latency issues at high volumes. We're not there yet. We are using other measures besides 2-way TLS and we may ditch 2-way TLS once those are proven out, over time.</p>&#xA;&#xA;<hr>&#xA;&#xA;<h1>Real-world example from my team</h1>&#xA;&#xA;<p>My current team is doing something that combines two of the approaches you mentioned (paraphrased):</p>&#xA;&#xA;<ul>&#xA;<li>Bake configuration at build-time</li>&#xA;<li>Pull configuration at run-time</li>&#xA;</ul>&#xA;&#xA;<p>My team is using <a href=""http://projects.spring.io/spring-boot/"" rel=""noreferrer"">Spring Boot</a>. Spring Boot has really complex <a href=""http://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-external-config.html"" rel=""noreferrer"">Externalized Configuration</a> with a ""profiles"" system. Spring Boot's configuration handling is complex and powerful, with all the pros/cons that go with that (won't get into that here).</p>&#xA;&#xA;<p>While this is out-of-the-box with Spring Boot, the ideas are general. I prefer <a href=""http://www.dropwizard.io/"" rel=""noreferrer"">Dropwizard</a> for Java microservices, or <a href=""http://flask.pocoo.org/"" rel=""noreferrer"">Flask</a> in Python; in both of those cases, you could do similar thing to what Spring Boot has going on ... You'll just have to do more things yourself. Good and bad: These nimble little frameworks are more flexible than Spring, but when you're writing more code and doing more integrations, there's more responsibility on YOU to QA and test your complex/flexible config support.</p>&#xA;&#xA;<p>I'll continue with the Spring Boot example because of first-hand experience, but <em>not</em> because I'm recommending it! Use what is right for your team.</p>&#xA;&#xA;<p>In the case of Spring Boot, you can activate multiple profiles at a time. That means you can have a base configuration, then override with more specific configuration. We keep a base configuration, <code>application.yml</code> in  <code>src/main/resources</code>. So, this config is packaged with the shippable JAR, and when the JAR is executed this config is <em>always</em> picked up. Therefore we include all default settings (common to all environments) in this file. Example: the configuration block that says, ""Embedded Tomcat, always use TLS with these cipher suites enabled."" (<code>server.ssl.ciphers</code>)</p>&#xA;&#xA;<p>When just one or two variables needs to be overwritten for a certain environment, we leverage <a href=""http://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-external-config.html"" rel=""noreferrer"">Spring Boot's support for getting configuration from environment variables</a>. Example: we set the URL to our Service Discovery using an environment variable. This overrides any default in the shipped/pulled configuration files. Another example: we use an environment variable <code>SPRING_PROFILES_ACTIVE</code> to specify which Configuration Profiles are active.</p>&#xA;&#xA;<p>We also want to make sure <code>master</code> contains a tested, working config for development environments. <code>src/main/resources/application.yml</code> has sane defaults. In addition we put dev-only config in <code>config/application-dev.yml</code>, and check that in. <a href=""http://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-external-config.html#boot-features-external-config-application-property-files"" rel=""noreferrer"">The <code>config</code> directory is picked up easily, but <em>not</em> shipped in the JAR.</a> Nice feature. Developers know (from the README and other documentation) that in a dev environment, all of our Spring Boot microservices require the <code>dev</code> profile to be activated. </p>&#xA;&#xA;<p>For environments besides <code>dev</code>, you can probably already see some options... Any one of these options could do (almost) everything you need. You can mix and match as you need.  These options have overlap with some ideas you mention in your original post.</p>&#xA;&#xA;<ol>&#xA;<li>Maintain environment-specific profiles like <code>application-stage.yml</code>, <code>application-prod.yml</code>, and so on,&#xA;that override settings with deviations from defaults (in a very heavily-locked-down git repository)</li>&#xA;<li>Maintain modular, vendor-specific profiles like <code>application-aws.yml</code>, <code>application-mycloudvendor.yml</code>&#xA;(where you store this will depend on whether it contains secrets). These may contain values that cut across&#xA;stage, prod, etc.</li>&#xA;<li>Use environment variables to override any relevant settings, at runtime; including picking profile(s) from 1 and 2</li>&#xA;<li>Use automation to bake in hardcoded values (templates) at build or deployment time (output into&#xA;a heavily-locked down repository of some sort, possibly distinct from (1)'s repository)</li>&#xA;</ol>&#xA;&#xA;<p>(1), (2), and (3) work well together. We are happily doing all three and it's actually pretty easy to document,&#xA;reason about, and maintain (after getting the initial hang of it).</p>&#xA;&#xA;<p>You said ...</p>&#xA;&#xA;<blockquote>&#xA;  <p>I suppose you could create a repo of per-environment properties files or script [...] You would need a ton of scripts, though.</p>&#xA;</blockquote>&#xA;&#xA;<p>It can be manageable. The scripts that pull or bake-in config: these can be uniform across all services. Maybe the script is copied when somebody clones your microservice template (btw: you should have an official microservice template!). Or maybe it's a Python script on an internal PyPI server. More on this after we talk about Docker.</p>&#xA;&#xA;<p>Since Spring Boot has such good support for (3), and support for using defaults/templating in YML files, you may not need need (4). But here's where things get very specific to your organization. The Security Engineer on our team wanted us to use (4) to bake in some specific values for environments&#xA;beyond <code>dev</code>: passwords. This Engineer didn't want the passwords ""floating around"" in environment variables, mainly because then -- who would set them? The Docker caller? <strong>AWS ECS Task Definition (viewable through AWS web UI)</strong>? In those cases, the passwords could be exposed to automation engineers, who wouldn't necessarily have access to the ""locked-down git repository"" containing <code>application-prod.yml</code>. (4) might not be needed if you do (1); you could just keep the passwords, hardcoded, in the tightly-controlled repository. But maybe there are secrets to generate at deployment-automation time, that you don't want in the same repository as (1). This is our case.</p>&#xA;&#xA;<p>More on (2): we use an <code>aws</code> profile and Spring Boot's ""configuration as code"" to make a startup-time call to get AWS metadata,&#xA; and override some config based on that. Our AWS ECS Task Definitions activate the <code>aws</code> profile. <a href=""http://cloud.spring.io/spring-cloud-netflix/spring-cloud-netflix.html"" rel=""noreferrer"">The Spring Cloud Netflix documentation gives an example like this:</a></p>&#xA;&#xA;<pre><code>@Bean&#xA;@Profile(""aws"")&#xA;public EurekaInstanceConfigBean eurekaInstanceConfig() {&#xA;  EurekaInstanceConfigBean b = new EurekaInstanceConfigBean();&#xA;  AmazonInfo info = AmazonInfo.Builder.newBuilder().autoBuild(""eureka"");&#xA;  b.setDataCenterInfo(info);&#xA;  return b;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Next, Docker. Environment Variables are a very good way to pass in configuration arguments in Docker. We don't use any command-line or positional arguments because of some gotchas we encountered with <code>ENTRYPOINT</code>. It's easy to pass <code>--env SPRING_PROFILES_ACTIVE=dev</code> or <code>--env SPRING_PROFILES_ACTIVE=aws,prod</code> ... whether from command-line, or from a supervisor/scheduler such as <a href=""https://aws.amazon.com/ecs/"" rel=""noreferrer"">AWS ECS</a> or <a href=""https://mesosphere.github.io/marathon/"" rel=""noreferrer"">Apache Mesosphere/Marathon</a>. Our <code>entrypoint.sh</code> also facilitates passing JVM flags that have nothing to do with Spring: we use the common <code>JAVA_OPTS</code> convention for this.</p>&#xA;&#xA;<p>(Oh, I should mention ... we also use <a href=""https://docs.gradle.org/current/release-notes"" rel=""noreferrer"">Gradle</a> for our builds. At the moment  ... We wrap <code>docker build</code>, <code>docker run</code>, and <code>docker push</code> with Gradle tasks. Our <code>Dockerfile</code> is templated, so again, option #4 from above. We have variables like <code>@agentJar@</code> that get overrwritten at build time. <strong>I really don't like this, and I think this could be better handled with plain old configuration (<code>-Dagent.jar.property.whatever</code>). This will probably go way.</strong> But I'm just mentioning it for completeness. Something I <strong>am</strong> happy about with this: nothing is done in the build, <code>Dockerfile</code>, or <code>entrypoint.sh</code> script, that is coupled tightly to a certain deployment context (such as AWS). All of it works in dev environments as well as deployed environments. So we don't have to deploy the Docker image to test it: it's portable, as it should be.)</p>&#xA;&#xA;<p>We have a folder <code>src/main/docker</code> containing the <code>Dockerfile</code> and <code>entrypoint.sh</code> (the script called by <code>ENTRYPOINT</code>; this is baked into the <code>Dockerfile</code>). Our <code>Dockerfile</code> and <code>entrypoint.sh</code> are nearly completely uniform across all microservices. These are duplicated when you clone our microservice template. Unfortunately, sometimes you have to copy/paste updates. We haven't found a good way around this yet, but it's not terribly painful.</p>&#xA;&#xA;<p>The <code>Dockerfile</code> does the following (build-time):</p>&#xA;&#xA;<ol>&#xA;<li>Derives from our ""golden"" base <code>Dockerfile</code> for Java applications</li>&#xA;<li>Grabs our tool for pulling configuration. (Grabs from an internal server available to any dev or Jenkins machine doing a build.) (You could also just use Linux tools like <code>wget</code> as well as DNS/convention-based naming for where to get it. You could also use AWS S3 and convention-based naming.)</li>&#xA;<li>Copy some things into the <code>Dockerfile</code>, like the JAR, <code>entrypoint.sh</code>...</li>&#xA;<li><code>ENTRYPOINT exec /app/entrypoint.sh</code></li>&#xA;</ol>&#xA;&#xA;<p>The <code>entrypoint.sh</code> does the following (run-time):</p>&#xA;&#xA;<ol>&#xA;<li>Uses our tool to pull configuration. (Some logic to understand that if <code>aws</code> profile is not active, the <code>aws</code> config file is not expected.) Dies immediately and loudly if there are any issues.</li>&#xA;<li><code>exec java $JAVA_OPTS -jar /app/app.jar</code> (picks up all the properties files, environment variables, etc.)</li>&#xA;</ol>&#xA;&#xA;<p>So we've covered that at application startup time, configuration is pulled from somewhere ... but where? To points from earlier, they could be in a git repository. You could pull down all profiles then use <code>SPRING_PROFILES_ACTIVE</code> to say which are active; but then you might pull down <code>application-prod.yml</code> onto a stage machine (not good). So instead, you could look at <code>SPRING_PROFILES_ACTIVE</code> (in your configuration-puller logic), and pull only what is needed.</p>&#xA;&#xA;<p>If you are using AWS, you could use S3 repository/ies instead of a git repository. This may allow for better access control. Instead of an <code>application-prod.yml</code> and <code>application-stage.yml</code> living in the same repo/bucket, you could make it so that <code>application-envspecific.yml</code> always has the required configuration, in the S3 bucket by some conventional name in the given AWS account. i.e. ""Get the config from <code>s3://ecs_config/$ENV_NAME/application-envspecific.yml</code>"" (where <code>$ENV_NAME</code> comes from <code>entrypoint.sh</code> script or ECS Task Definition).</p>&#xA;&#xA;<p>I mentioned that the <code>Dockerfile</code> works portably, and isn't coupled to certain deployment contexts. That is because <code>entrypoint.sh</code> is defined to check for config files in a flexible way; it just wants the config files. So if you use Docker's <code>--volume</code> option to mount a folder with config, the script will be happy, and it won't try to pull anything from an external server.</p>&#xA;&#xA;<p>I won't get into the deployment automation much ... but just mention quickly that we use terraform, boto3, and some custom Python wrapping code. jinja2 for templating (baking in those couple values that need to be baked in).</p>&#xA;&#xA;<p><strong>Here's serious limitation of this approach:</strong> the microservice process has to be killed/restarted to re-download and reload config. Now, with a cluster of stateless services, this does not necessarily represent downtime (given some things, like client-side load-balancing, <a href=""https://github.com/Netflix/ribbon"" rel=""noreferrer"">Ribbon</a> configured for retries, and horizontal scale so some instances are always running in the pool). So far it is working out, but the microservices still have pretty low load. Growth is coming. We shall see.</p>&#xA;&#xA;<p>There are many more ways to solve these challenges. Hopefully this exercise has got you thinking about what will work for your team. Just try to get some things going. Prototype rapidly and you'll shake out the details as you go.</p>&#xA;&#xA;<h1>Perhaps better: configuration servers</h1>&#xA;&#xA;<p>I think this is a more common solution: <strong>Configuration Servers.</strong> You mentioned <a href=""https://zookeeper.apache.org/"" rel=""noreferrer"">ZooKeeper</a>. There's also <a href=""https://www.consul.io/"" rel=""noreferrer"">Consul</a>. Both ZooKeeper and Consul offer <em>both</em> Configuration Management <em>and</em> Service Discovery. There's also <a href=""https://coreos.com/etcd/"" rel=""noreferrer"">etcd</a>.</p>&#xA;&#xA;<p>In our case, the Security team wasn't comfortable with a centralized Configuration Management server. We decided to use  NetflixOSS's <a href=""https://github.com/Netflix/eureka"" rel=""noreferrer"">Eureka</a> for Service Discovery, but hold off on a Configuration Server. If we wind up disliking the methods above, we may switch to <a href=""https://github.com/Netflix/archaius"" rel=""noreferrer"">Archaius</a> for Configuration Management. <a href=""http://cloud.spring.io/spring-cloud-netflix/"" rel=""noreferrer"">Spring Cloud Netflix</a> aims to make these integrations easy for Spring Boot users. Though I think it wants you to use <a href=""http://cloud.spring.io/spring-cloud-config/"" rel=""noreferrer"">Spring Cloud Config (Server/Client)</a> instead of Archaius. Haven't tried it yet.</p>&#xA;&#xA;<p>Configuration servers seem much easier to explain and think about. If you can, you should start off with a configuration server.</p>&#xA;&#xA;<pre><code>If the implementation is hard to explain, it's a bad idea.&#xA;If the implementation is easy to explain, it may be a good idea.&#xA;</code></pre>&#xA;&#xA;<h2>Comparisons of configuration servers</h2>&#xA;&#xA;<p>If you decide to try a config server, you'll need to do a research spike. Here are some good resources to start you off:</p>&#xA;&#xA;<ul>&#xA;<li><strong><a href=""http://technologyconversations.com/2015/09/08/service-discovery-zookeeper-vs-etcd-vs-consul/"" rel=""noreferrer"">Service Discovery: Zookeeper vs etcd vs Consul</a></strong></li>&#xA;<li><a href=""https://aphyr.com/posts/316-jepsen-etcd-and-consul"" rel=""noreferrer"">Aphyr's write-up on consistency in etcd and Consul</a> (the particular problems identified have now been fixed; but it's worth reading and thinking about)</li>&#xA;</ul>&#xA;&#xA;<p>If you try Consul, you should <strong><a href=""https://www.youtube.com/watch?v=2UHzc9ED8AA"" rel=""noreferrer"">watch this talk, ""Operating Consul as an Early Adopter""</a></strong>. Even if you try something else besides Consul, the talk has nuggets of advice and insight for you.</p>&#xA;&#xA;<p><strong>16/05/11 EDIT:</strong> The <em>ThoughtWorks Technology Radar</em> has now <a href=""https://www.thoughtworks.com/radar/tools"" rel=""noreferrer"">moved Consul into the ""Adopt"" category</a> (<a href=""https://www.thoughtworks.com/radar/tools/consul"" rel=""noreferrer"">history of their evaluation is here</a>).</p>&#xA;&#xA;<p><strong>17/06/01 EDIT:</strong>  We are considering moving to <a href=""http://kubernetesbyexample.com/"" rel=""noreferrer"">Kubernetes</a> for multiple reasons. If we do we will leverage the <a href=""https://github.com/coreos/etcd"" rel=""noreferrer"">etcd-powered</a> <a href=""https://kubernetes.io/docs/tasks/configure-pod-container/configmap/"" rel=""noreferrer"">ConfigMaps</a> feature that ships with K8S. That's all for now on this subject :-)</p>&#xA;&#xA;<h1>More resources</h1>&#xA;&#xA;<ul>&#xA;<li>My favorite one-stop shop on microservices: <strong><a href=""http://martinfowler.com/microservices/"" rel=""noreferrer"">Martin Fowler's ""Microservices Resource Guide""</a></strong></li>&#xA;<li>If you don't want to buy/read all of <em>Building Microservices</em> yet, there's <a href=""https://www.nginx.com/wp-content/uploads/2015/01/Building_Microservices_Nginx.pdf"" rel=""noreferrer"">a PDF of a few chapters, via NGINX</a>. Starting on page 86, it gets into ""Dynamic Service Registries"", covering ZooKeeper, Consul, Eureka, etc ... Newman covers these topics much better than I can.</li>&#xA;</ul>&#xA;"
47716739,47716674,1278880,2017-12-08T14:51:15,<p>Provide the path as an environment variable. Then read it from there in Node using:</p>&#xA;&#xA;<pre><code>process.env.ENV_VARIABLE_NAME&#xA;</code></pre>&#xA;&#xA;<p>This is standard microservice practice.</p>&#xA;
28706750,28607400,4567456,2015-02-24T21:33:09,"<p>I suggest that you look at token based authentication. </p>&#xA;&#xA;<p>In addition, JSON Web tokens could also be of interest to you.</p>&#xA;"
51889862,51149979,112544,2018-08-17T06:51:12,"<p>Am I correct that you're talking about the orchestration of your microservices/functions? </p>&#xA;&#xA;<p>If so have you looked at <a href=""https://aws.amazon.com/step-functions/"" rel=""nofollow noreferrer"">AWS Step Functions</a> or <a href=""https://docs.microsoft.com/en-us/azure/azure-functions/durable-functions-overview"" rel=""nofollow noreferrer"">Durable Functions on Azure</a>?</p>&#xA;&#xA;<h2>AWS Step Functions</h2>&#xA;&#xA;<blockquote>&#xA;  <p>AWS Step Functions lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. Using Step Functions, you can design and run workflows that stitch together services such as AWS Lambda and Amazon ECS into feature-rich applications. Workflows are made up of a series of steps, with the output of one step acting as input into the next. Application development is simpler and more intuitive using Step Functions, because it translates your workflow into a state machine diagram that is easy to understand, easy to explain to others, and easy to change. You can monitor each step of execution as it happens, which means you can identify and fix problems quickly. Step Functions automatically triggers and tracks each step, and retries when there are errors, so your application executes in order and as expected.</p>&#xA;</blockquote>&#xA;&#xA;<p>Source: <a href=""https://aws.amazon.com/step-functions/"" rel=""nofollow noreferrer"">https://aws.amazon.com/step-functions/</a></p>&#xA;&#xA;<h2>Azure Durable Functions</h2>&#xA;&#xA;<blockquote>&#xA;  <p>The primary use case for Durable Functions is simplifying complex, stateful coordination problems in serverless applications. The following sections describe some typical application patterns that can benefit from Durable Functions: Function Chaining, Fan-out/Fan-in, Async HTTP APIs, Monitoring.</p>&#xA;</blockquote>&#xA;&#xA;<p>Source: <a href=""https://docs.microsoft.com/en-us/azure/azure-functions/durable-functions-overview"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/azure-functions/durable-functions-overview</a></p>&#xA;"
51614476,51613104,9419680,2018-07-31T13:24:27,"<p>1) You can have profile specific <code>bootstrap-&lt;profile&gt;.properties</code> (like for <code>application-&lt;profile&gt;.properties</code>) for each supported profile to avoid rebuilding your application for each env. Then just pass application profile using to your application during start-up. Spring will load correct <code>bootstrap-&lt;profile&gt;.properties</code> and will connect to proper configuration server (or eureka, etc). Example:</p>&#xA;&#xA;<pre><code>java -jar your-app.jar --spring.profiles.active=dev&#xA;</code></pre>&#xA;&#xA;<p>2) You can pass your URLs externally as custom properties (same as with profile above) and have smth like this in <code>bootstrap.properties</code>. Example:</p>&#xA;&#xA;<pre><code>spring.cloud.config.uri=${config.server.url}&#xA;</code></pre>&#xA;&#xA;<p>then pass <code>--config.server.url= ...</code> during start-up.</p>&#xA;&#xA;<p>3) You can pass Spring properties in the same way during start-up. Example:</p>&#xA;&#xA;<pre><code>java -jar your-app.jar --spring.cloud.config.uri= ...&#xA;</code></pre>&#xA;&#xA;<p>4) You can use system env variables. Example:</p>&#xA;&#xA;<pre><code>spring.cloud.config.uri=${SYSTEM_ENV_CLOUD_CONFIG_URI}&#xA;</code></pre>&#xA;"
47889560,47889115,1951392,2017-12-19T14:52:36,"<p>According to your questions you have to achieve following flow :</p>&#xA;&#xA;<p>Client ----<strong>json</strong>-<strong>---></strong>  Server 1 --------<strong>Xml</strong> -----<strong>--></strong> Server2</p>&#xA;&#xA;<p>Client &lt;----<strong>json</strong>----  Server 1 &lt;--------<strong>Xml</strong> ------- Server2</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>You are able to accept JSON data into Java Models and now you have to hit another webservice with XML as your input. Below is method which can help you to achive it :</p>&#xA;&#xA;<pre><code>    private static String jaxbObjectToXML(Customer customer) {&#xA;    String xmlString = """";&#xA;    try {&#xA;        JAXBContext context = JAXBContext.newInstance(Customer.class);&#xA;        Marshaller m = context.createMarshaller();&#xA;&#xA;        m.setProperty(Marshaller.JAXB_FORMATTED_OUTPUT, Boolean.TRUE); // To format XML&#xA;&#xA;        StringWriter sw = new StringWriter();&#xA;        m.marshal(customer, sw);&#xA;        xmlString = sw.toString();&#xA;&#xA;    } catch (JAXBException e) {&#xA;        e.printStackTrace();&#xA;    }&#xA;&#xA;    return xmlString;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Pass this XML to webservice and it will return you XML.  UNmarshall it again into it's response object and return as JSON using Spring Boot Webservice.</p>&#xA;&#xA;<p><a href=""https://www.javatpoint.com/jaxb-marshalling-example"" rel=""nofollow noreferrer"">Reference</a></p>&#xA;"
49385378,49349235,3134112,2018-03-20T13:20:47,"<p>Consider also SAGA Pattern</p>&#xA;&#xA;<blockquote>&#xA;  <p>A Saga is a sequence of local transactions where each transaction updates data within a single service. The first transaction is initiated by an external request corresponding to the system operation, and then each subsequent step is triggered by the completion of the previous one.</p>&#xA;</blockquote>&#xA;&#xA;<p><br/>&#xA;<a href=""http://microservices.io/patterns/data/saga.html"" rel=""nofollow noreferrer"">http://microservices.io/patterns/data/saga.html</a>&#xA;<br/>&#xA;<a href=""https://dzone.com/articles/saga-pattern-how-to-implement-business-transaction"" rel=""nofollow noreferrer"">https://dzone.com/articles/saga-pattern-how-to-implement-business-transaction</a>&#xA;<br/>&#xA;<a href=""https://medium.com/@tomasz_96685/saga-pattern-and-microservices-architecture-d4b46071afcf"" rel=""nofollow noreferrer"">https://medium.com/@tomasz_96685/saga-pattern-and-microservices-architecture-d4b46071afcf</a></p>&#xA;"
41817211,38739188,3025825,2017-01-23T22:58:22,"<p>You can only debug this by seeing what happened during the container launch. Use <code>docker logs &lt;container_name&gt;</code> to view the logs from the container that failed to launch.</p>&#xA;&#xA;<p>Just at first glance, your <code>ports</code> directive should be wrapping ports in strings:</p>&#xA;&#xA;<pre><code>ports&#xA;  - ""1111:1111""&#xA;</code></pre>&#xA;&#xA;<p>Also, it's safer to use unreserved port numbers, typically above 2000 on most OSes.</p>&#xA;"
46998073,46997840,5271969,2017-10-29T08:06:52,"<blockquote>&#xA;  <p>I understand that is should work async but still, I’m afraid that for 10 or more request my app will be crushed</p>&#xA;</blockquote>&#xA;&#xA;<pre><code>I believe you have a bad idea on what a stateless server is.&#xA;</code></pre>&#xA;&#xA;<p>Stateless servers are powerful, scalable and extensible. Stateless servers can handle more than the requests mentioned by you without getting crushed until you've enough credits in your cloud billing account -- at least this is whole reason why we use stateless servers</p>&#xA;&#xA;<p>Go for it, your application can handle any number of requests :)</p>&#xA;&#xA;<p>If you're still worried, just try splitting up your application. Like - let one of your function do the extraction and once done, it can request to analysis function to start its job. You can communicate with your different stateless servers using pub/sub or API end-points (just check the docs)</p>&#xA;"
31792720,31764926,4101793,2015-08-03T16:51:05,"<p>What you are talking about is a Bounded Context (BC), which is a DDD pattern. DDD fits well with SOA if the services boundaries correspond to BCs. BCs are meant to be autonomous parts of the system, that do not share any common business logic with each others. When designing your BC, you should not consider any idea regarding the database or other cross-cutting concerns. You should keep in mind making your code as specific as possible, then you should not be tempted to reuse code. Keep Web Services for parts of the system that are reusable, in regard to business. And prefer Eventual Consistency if you can (then you should be able to work with notifications rather than direct WS calls).</p>&#xA;&#xA;<p>Regarding Eventual Consistency (if possible), I would choose the first option.</p>&#xA;"
26992479,25600580,43681,2014-11-18T10:51:27,"<p>The ""purest"" approach, i.e. the one that gives you the least amount of coupling, is to <em>not share any code</em>.</p>&#xA;&#xA;<p>If you find that two services (call them A and B) need the same functionality, your options are:</p>&#xA;&#xA;<ul>&#xA;<li>split if off as a separate service C, so A and B can use C</li>&#xA;<li>bite the bullet and duplicate the code</li>&#xA;</ul>&#xA;&#xA;<p>While this may sound awkward, you avoid the (not uncommon) problem of creating a ""utility"" or ""common"" or ""infrastructure"" library which everyone depends on, and which is then really hard to upgrade and change (i.e. which indirectly couples the services).</p>&#xA;&#xA;<p>In practice, as usual, it's a tradeoff.</p>&#xA;&#xA;<ul>&#xA;<li>If the shared functionality is substantial, I'd go for a seperate service.</li>&#xA;<li>If it's just constants, a shared library might be the best solution. You need to be very careful about backwards compatibility, though.</li>&#xA;<li>For configuration data, you could also implement a specific service, possibly using some existing technology such as LDAP.</li>&#xA;<li>Finally, for simple code that is likely to evolve independently, just duplicating might be the best solution.</li>&#xA;</ul>&#xA;&#xA;<p>However, what's best will depend on your specific situation and problem.</p>&#xA;"
45025263,45023334,7122593,2017-07-11T04:13:57,"<p>In case of spring boot application depending on spring cloud dependencies.&#xA;Zuul is the right option.</p>&#xA;&#xA;<p>Please go through below guide </p>&#xA;&#xA;<p><a href=""https://spring.io/guides/gs/routing-and-filtering/"" rel=""nofollow noreferrer"">https://spring.io/guides/gs/routing-and-filtering/</a></p>&#xA;&#xA;<p>you can find sample application here : </p>&#xA;&#xA;<p><a href=""https://github.com/BarathArivazhagan/Microservices-workshop"" rel=""nofollow noreferrer"">https://github.com/BarathArivazhagan/Microservices-workshop</a></p>&#xA;&#xA;<p>For Documentation Reference: <a href=""http://cloud.spring.io/spring-cloud-netflix/spring-cloud-netflix.html"" rel=""nofollow noreferrer"">http://cloud.spring.io/spring-cloud-netflix/spring-cloud-netflix.html</a></p>&#xA;"
46386504,46386326,7122593,2017-09-24T04:10:32,"<p>As stated in the below article: </p>&#xA;&#xA;<p><a href=""https://12factor.net/backing-services"" rel=""nofollow noreferrer"">https://12factor.net/backing-services</a></p>&#xA;&#xA;<p>It states that :</p>&#xA;&#xA;<blockquote>&#xA;  <p>A backing service is any service the app consumes over the network as part of its normal operation. Examples include datastores (such as MySQL or CouchDB), messaging/queueing systems (such as RabbitMQ or Beanstalkd), SMTP services for outbound email (such as Postfix), and caching systems (such as Memcached).</p>&#xA;</blockquote>&#xA;&#xA;<p>Microservice can connect to any backing service irrespective of the platform. In PCF, you are binding services to your microservices to connect. In other cloud environment, you can point to any backing service like AWS RDS or other services provided by the Platform.</p>&#xA;&#xA;<p>The real difference is this : </p>&#xA;&#xA;<blockquote>&#xA;  <p>Backing services like the database are traditionally managed by the same systems administrators as the app’s runtime deploy. In addition to these locally-managed services, the app may also have services provided and managed by third parties. Resources can be attached and detached to deploys at will. For example, if the app’s database is misbehaving due to a hardware issue, the app’s administrator might spin up a new database server restored from a recent backup. The current production database could be detached, and the new database attached – all without any code changes.</p>&#xA;</blockquote>&#xA;"
42460151,42458964,7122593,2017-02-25T18:45:54,"<p>Define GELF_ADDRESS property in application specific properties in config-server and try reading it using config client configuration in respective application </p>&#xA;&#xA;<p>Follow this tutorial to set up Config Server <a href=""https://spring.io/guides/gs/centralized-configuration/"" rel=""nofollow noreferrer"">https://spring.io/guides/gs/centralized-configuration/</a>&#xA;and to read the property in config client</p>&#xA;&#xA;<p>Define below property in app.properties:</p>&#xA;&#xA;<p>app.properties</p>&#xA;&#xA;<pre><code>GELF_ADDRESS=&lt;&lt;value to be set&gt;&gt;&#xA;</code></pre>&#xA;&#xA;<p>Update 1: </p>&#xA;&#xA;<p>I tried with sample. Check this out : <a href=""https://github.com/BarathArivazhagan/Spring-Task-Sample/tree/master/Logback-configuration-configserver"" rel=""nofollow noreferrer"">https://github.com/BarathArivazhagan/Spring-Task-Sample/tree/master/Logback-configuration-configserver</a></p>&#xA;"
30594638,30583466,1683873,2015-06-02T11:20:48,"<p>What you need is fail cover strategy. You should be posting events to the rabbit worker through a middlewar like apache mq or websphere mq. Also if u r not able to put a message on the queue, keep a backup of it in a db, which is polled every x seconds to check for undelivered messages for retrying</p>&#xA;"
43153293,43151368,1227937,2017-04-01T05:46:42,"<p>The TL;DR; is that <a href=""https://github.com/openzipkin/b3-propagation"" rel=""nofollow noreferrer"">B3 propagation</a> was initially designed for fixed size data: carrying data ancillary to tracing isn't in scope, and for this reason any solution that extends B3 in such a fashion wouldn't be compatible with existing code.</p>&#xA;&#xA;<p>So, that means any solution like this will be an extension which means custom handling in the <a href=""http://zipkin.io/pages/architecture.html"" rel=""nofollow noreferrer"">instrumented apps</a> which are the things passing headers around. The server won't care as it never sees these headers anyway.</p>&#xA;&#xA;<p>Ways people usually integrate other things like flags with zipkin is to add a tag aka binary annotation including its value (usually in the root span). This would allow you to query or retrieve these offline, but it doesn't address in-flight lookups from applications.</p>&#xA;&#xA;<p>Let's say that instead of using an intermediary like linkerd, or a platform-specific propagated context, we want to dispatch the responsibility to the tracing layer. Firstly, what sort of data could work alright? The easiest is something set-once (like zipkin's trace id). Anything set and propagated without mutating it is the least mechanics. Next in difficulty is appending new entries mid-stream, and most difficult is mutating/merging entries.</p>&#xA;&#xA;<p>Let's assume this is for inbound flags which never change through the request/trace tree. We see a header when processing trace data, we store it and forward it downstream. If this value doesn't need to be read by the tracing system, it is easiest, as it is largely a transport/propagation concern. For example, maybe other middleware read that header and it is only a ""side job"" we are adding to the tracer to remember certain things to pass along. If this was done in a single header, it would be less code than a pattern in each of the places this would be to added. It would be even less code if the flags could be encoded in a number, however unrealistic that may be.</p>&#xA;&#xA;<p>There are libraries with apis to manipulate the propagated context manually, for example, <a href=""https://github.com/JonathanMace/tracingplane"" rel=""nofollow noreferrer"">""baggage"" from brownsys</a> and OpenTracing (of which some libraries support zipkin). The former aims to be a generic layer for any instrumentation (ex monitoring, chargeback, tracing etc) and the latter is specific to tracing. OpenTracing has defines abstract types like <a href=""http://opentracing.io/documentation/pages/api/cross-process-tracing.html"" rel=""nofollow noreferrer"">injector and extractor</a> which could be customized to carry other fields. However, you still would need a concrete implementation (which knows your header format etc) in order to do this. Unless you want applications to read this data, it would need to be a secret detail of that implementation (specifically the trace context).</p>&#xA;&#xA;<p>Certain zipkin-specific libraries like Spring Cloud Sleuth and Brave have means to <a href=""https://github.com/openzipkin/brave/blob/master/brave/src/main/java/brave/propagation/Propagation.java"" rel=""nofollow noreferrer"">customize how headers are parsed</a>, to support variants of B3 or new or site-specific trace formats. Not all support this at the moment, but I would expect this type of feature to become more common. This means you may need to do some surgery in order to support all platforms you may need to support.</p>&#xA;&#xA;<p>So long story short is that there are some libraries which are pluggable with regards to propagation, and those will be easiest to modify to support this use case. Some code will be needed regardless as B3 doesn't currently define an expression like this.</p>&#xA;"
50592776,50506101,7193761,2018-05-29T20:53:32,"<p>Of course you can.&#xA;Microservices are just REST-Services.&#xA;You need to understand how REST-Services work.&#xA;After that just write 2 Microservices  (2 Rest-Services: producer-service and consumer-service) with Spring-boot, let them run under different server-ports, call the consumer-service from the other, and that's it: you have your Microservices.&#xA;Now this is the primitive way to write Microservices. </p>&#xA;&#xA;<p>To make them evolve, you need to add some ""magic"" (no rocket science), for example using Ribbon to distribute load between two instances of your ""producer-service"".</p>&#xA;&#xA;<p>You may use a discovery service which is just a spring-boot application with the annotation @EnableEurekaServer (You need to add the appropriate dependency in your pom)&#xA;Now add to your first (primitive) Microservices the annotation @EnableDiscoveryClient to the main classes and the defaultZone pointing to your eureka-service in the application.properties (or application.yml) of both,  start your eureka-service (discovery service) and the 2  Microservices: those will register on the discovery-service. Of course now you don't need to hard-code the http address of the producer-service in the consumer-service.<br>&#xA;Take a look at this <a href=""http://www.springboottutorial.com/creating-microservices-with-spring-boot-part-1-getting-started"" rel=""nofollow noreferrer"">tutorial </a></p>&#xA;"
46023092,46022865,183201,2017-09-03T11:42:38,"<p>Both approaches are valid, but probably the single nginx instance scenario is more flexible because in that case you can use the nginx as a gateway for your services, for things like SSL termination, authentication and so on. And by the way, if you are using ASP.NET Core 2.0, you can try to expose you services without nginx, because Kestrel is considered edge-ready in 2.0</p>&#xA;"
46855555,46850814,7791521,2017-10-20T18:52:08,"<p>You need to call <code>pm.disconnect()</code> at the end, otherwise you'll end up leaving all the created connections open. It says in the <a href=""http://pm2.keymetrics.io/docs/usage/pm2-api/"" rel=""nofollow noreferrer"">pm2 api documentation</a>:</p>&#xA;&#xA;<p><code>If your script does not exit by itself, make sure you called pm2.disconnect() at the end.</code></p>&#xA;"
51207103,51206924,1895073,2018-07-06T09:23:11,"<p>You can use <code>grafana + prometheus + cadvisor + node_exporter</code> to capture metrics of all containers with since how long it is running and nodes on which it is running.</p>&#xA;&#xA;<p>For more details you can follow this link : </p>&#xA;&#xA;<p><a href=""https://stefanprodan.com/2016/a-monitoring-solution-for-docker-hosts-containers-and-containerized-services/"" rel=""nofollow noreferrer"">https://stefanprodan.com/2016/a-monitoring-solution-for-docker-hosts-containers-and-containerized-services/</a></p>&#xA;&#xA;<p>Alternatively you can also use command <code>docker stats &lt;container-name/id&gt;</code> to check details about it.&#xA;Hope this will help.&#xA;Thank you!</p>&#xA;"
46751926,46696171,462242,2017-10-15T05:10:22,"<p>There are several techniques which can provide cross-service transactions (atomicity): 2PC, Percolator's transactions and Sagas.</p>&#xA;&#xA;<p><a href=""http://research.google.com/pubs/pub36726.html"" rel=""nofollow noreferrer"">Percolator's transactions</a> have serializable isolation level. They are known in the industry, see: Amazon's <a href=""https://aws.amazon.com/blogs/aws/dynamodb-transaction-library/"" rel=""nofollow noreferrer"">DynamoDB transaction library</a>, <a href=""https://www.cockroachlabs.com/blog/how-cockroachdb-distributes-atomic-transactions/"" rel=""nofollow noreferrer"">CockroachDB database</a>, and the Google's Pecolator system itself. A <a href=""http://rystsov.info/2016/03/02/cross-shard-txs.html"" rel=""nofollow noreferrer"">step-by-step visualization</a> of the Percolator's transactions may help you to understand how they work.</p>&#xA;&#xA;<p>The saga pattern was described in the late 80s in the <a href=""http://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf"" rel=""nofollow noreferrer"">Sagas</a> paper but became more relevant with the rise of microservices. Please see the <a href=""https://www.youtube.com/watch?v=xDuwrtwYHu8"" rel=""nofollow noreferrer"">Applying the Saga Pattern</a> talk for inspiration.</p>&#xA;&#xA;<p>But since you mentioned eventual consistency it's important to notice that all of the techniques require individual services to be linearizable (strong consistency) and to support compare and set.</p>&#xA;"
42241404,42230797,809122,2017-02-15T05:33:42,"<blockquote>&#xA;  <p>What happens if we don't execute acknowledgment.acknowledge(); Will the message be automatically resent by Kafka to this consumer?</p>&#xA;</blockquote>&#xA;&#xA;<p>No. A Kafka consumer reads messages sequentially for as long as a client is open. Kafka does not support more sophisticated acknowledgment modes, such as individual message acknowledgment, only updating the offset for a given consumer group and partition-topic. Spring Cloud Stream supports manual acknowledgment for messages in Spring Cloud Stream for scenarios where they are processed asynchronously (thus preventing message loss) - but the assumption is that once a message is acknowledged manually, its offset is saved, so all previous messages from the same partition-topic will be considered 'read'. If you want to single out failed messages, you can use DLQ support - and have a subsequent consumer receiving them. Restarting the consumer will resume reading from the last saved offset, so you have the option of not saving offsets for a series of unsuccessfully processed messages.  </p>&#xA;&#xA;<p>The Spring Cloud Stream consumers have built-in retry and DLQ support - see <code>enableDlq</code> in <a href=""http://docs.spring.io/spring-cloud-stream/docs/Brooklyn.SR2/reference/htmlsingle/#_kafka_consumer_properties"" rel=""nofollow noreferrer"">http://docs.spring.io/spring-cloud-stream/docs/Brooklyn.SR2/reference/htmlsingle/#_kafka_consumer_properties</a> as well as retry settings provided as part of the default consumer properties: <a href=""http://docs.spring.io/spring-cloud-stream/docs/Brooklyn.SR2/reference/htmlsingle/#_consumer_properties"" rel=""nofollow noreferrer"">http://docs.spring.io/spring-cloud-stream/docs/Brooklyn.SR2/reference/htmlsingle/#_consumer_properties</a></p>&#xA;"
42002586,42002512,2069922,2017-02-02T12:42:31,"<p>You could start each of the three instances with a different Spring profile. E.g. SPRING_PROFILES_ACTIVE=prod1 for the first instance, SPRING_PROFILES_ACTIVE=prod2 for the second, etc. </p>&#xA;&#xA;<p>Then you could set the port in application-prod1.properties, application-prod2.properties, etc (or in yaml files). </p>&#xA;&#xA;<p>You could also run the app in cloudfoundry. Then cloudfoundry will create the different containers for you. </p>&#xA;"
50216048,49944806,2069922,2018-05-07T13:56:51,"<p>Here are some thoughts:</p>&#xA;&#xA;<ul>&#xA;<li><p>Eureka Service discovery: in my opinion this is not strictly necessary when running on PCF. When you push an app on PCF usually a route is assigned to your app, and you can use this Route as a poor man's service discovery. Eureka would allow you to use client-side load balancing in the case of container-to-container networking, but usually you wouldn't need this. </p></li>&#xA;<li><p>Zuul: Can be very useful also on CloudFoundry in case you are doing things like writing frontend-for-backend services, providing frontends for different devices (mobiles, desktops, i-pads) that use the same backend services. Might also be useful for an authentication/authorization layer or rate-limiting. One native CloudFoundry alternative would be to use route-services for tasks such as rate limiting, authentication/authorization. </p></li>&#xA;<li><p>spring-cloud-config: makes sense if you want your configuration to be under version control for different environments. This is useful no matter if you are running on CloudFoundry or not. I don't know of any alternatives on plain CloudFoundry. </p></li>&#xA;<li><p>spring-cloud-feign: makes sense if you want use annotations such as <code>@RequestMapping</code> with your Feign client interfaces. This is independent on if you are running on CloudFoundry or not. AFAIK there are no alternatives for this in case you want to use Spring MVC annotations with Feign.</p></li>&#xA;<li><p>ribbon: makes sense if you want to use client side load balancing as opposed to let the CloudFoundry router to do the load balancing for you. </p></li>&#xA;</ul>&#xA;&#xA;<p>How developers can check locally if this works for them: </p>&#xA;&#xA;<ul>&#xA;<li><p>In general, I don't believe developers should need to check locally if their app is working fine together with zuul, cloud-config-service, and eureka. </p></li>&#xA;<li><p>They could check this in a dev or test space or environment though. </p></li>&#xA;<li><p>If they really want to check this on their local machine, they could download PCFDev and run these infrastructure components there.</p></li>&#xA;</ul>&#xA;&#xA;<p>Hope this helps.  </p>&#xA;"
37811586,37806128,1811106,2016-06-14T12:08:00,"<p>Docker Compose is mostly for defining different container, configure and using a single command make them available (also for sequencing). So it is best suited for local development, integration testing and use it as part of your Continuous Integration process.</p>&#xA;&#xA;<p>While not ruling out Docker compose can be used in production environment, I think it would be a good case of using <a href=""http://kubernetes.io/"" rel=""nofollow"">Kubernetes</a> which gives more control over scaling, managing multiple containers.</p>&#xA;&#xA;<p>This blog has some example scenarios to try out (and many other resources which can be helpful)</p>&#xA;&#xA;<p><a href=""https://renzedevries.wordpress.com/2016/05/31/deploying-a-docker-container-to-kubernetes-on-amazon-aws/comment-page-1/#comment-10"" rel=""nofollow"">https://renzedevries.wordpress.com/2016/05/31/deploying-a-docker-container-to-kubernetes-on-amazon-aws/comment-page-1/#comment-10</a></p>&#xA;"
51770887,48190148,7256738,2018-08-09T15:29:54,<p>Both rely on the services as the main component but a lot of differences there. Few are below</p>&#xA;&#xA;<p><strong>SOA:</strong> </p>&#xA;&#xA;<ul>&#xA;<li>Follows “share-as-much-as-possible” architecture approach    </li>&#xA;<li>Supports multiple message protocols  </li>&#xA;<li>Multi-threaded with more overheads to handle I/O  </li>&#xA;<li>Maximizes application service reusability </li>&#xA;<li>Not focussed fully into DevOps / Continuous Delivery</li>&#xA;</ul>&#xA;&#xA;<p><strong>MicroService:</strong> </p>&#xA;&#xA;<ul>&#xA;<li><p>Follows “share-as-little-as-possible” architecture approach</p></li>&#xA;<li><p>Uses lightweight protocols such as HTTP/REST &amp; AMQP</p></li>&#xA;<li><p>Single-threaded usually with use of Event Loop features for non-locking I/O handling</p></li>&#xA;<li><p>More focused on decoupling</p></li>&#xA;<li><p>Strong focus on DevOps / Continuous Delivery</p></li>&#xA;</ul>&#xA;
51785752,51726515,7256738,2018-08-10T11:41:40,"<p><strong>Synchronous</strong> = in synch</p>&#xA;&#xA;<ul>&#xA;<li><p>Sender wait for a response from the receiver to continue further.</p></li>&#xA;<li><p>Both Sender and Receiver should be in active state.</p></li>&#xA;<li><p>Sender send data to receiver as it requires an immediate response to continue processing. </p></li>&#xA;<li><p>When you execute something synchronously, you wait for it to finish before moving on to another task.</p></li>&#xA;</ul>&#xA;&#xA;<p><strong>Asynchronous</strong> = out of synch</p>&#xA;&#xA;<ul>&#xA;<li><p>Sender does not wait for a response from the receiver</p></li>&#xA;<li><p>Receiver can be inactive. </p></li>&#xA;<li><p>Once Receiver is active, it will receive and process.</p></li>&#xA;<li><p>Sender puts data in message queue and does not require an immediate response to continue processing. </p></li>&#xA;<li><p>When you execute something asynchronously, you can move on to another task before it finishes. </p></li>&#xA;</ul>&#xA;&#xA;<p><strong>In your case,</strong></p>&#xA;&#xA;<p>Catalog Service &lt;--  UI  -->  Order Service --> Shipment service</p>&#xA;&#xA;<p>1) UI has to fetch item details from Catalog Service (Synchronous because it needs item immedietly)</p>&#xA;&#xA;<p>2) Once all items selected, UI has to invoke Order service.(synchronous / asynchronous, depends upon user action)</p>&#xA;&#xA;<ul>&#xA;<li>User might add in shopping cart for future use (or) in favourites (or) to immediate process order.</li>&#xA;</ul>&#xA;&#xA;<p>3) Once all items exist in shopping cart collection , it has to invoke shipmentService. (asynchronous)</p>&#xA;&#xA;<ul>&#xA;<li><p>Payment should be synchronous. You need acknowledgement.</p></li>&#xA;<li><p>Assuming all payment and other stuff done, it calls shipment delivery service</p></li>&#xA;<li><p>Delivery is asynchronous because it cant get acknowledge immedietly. It may take 2 days delay etc.</p></li>&#xA;</ul>&#xA;"
34427737,34424706,2805154,2015-12-23T02:31:38,<p>Each <em>application</em> will have its own code but will share the same mongo db. You'll need to define the same collections (or a subset or even a superset) for the admin app. You can rsync certain directories between the two apps if that makes that process either but there isn't anything in Meteor that will do this for you afaik.</p>&#xA;
38514581,38511443,1695906,2016-07-21T21:22:40,"<p>Unless something has changed, the question of whether to it makes more sense to deploy SNS &rarr; Lambda, or SNS &rarr; SQS &rarr; Lambda, is based on a premise with a significant flaw.</p>&#xA;&#xA;<p>As is indicated in <a href=""http://docs.aws.amazon.com/lambda/latest/dg/invoking-lambda-function.html"" rel=""nofollow"">Supported Event Sources</a> in the Lambda documentation, Lambda events can be sourced from S3, DynamoDB, Kinesis, SNS, SES, Cognito, CloudFormation, CloudWatch &amp; Events (including Scheduled Events), AWS Config, Amazon Echo, and API Gateway.  </p>&#xA;&#xA;<p>And, of course, you can invoke them directly.</p>&#xA;&#xA;<p>But SQS is not a supported Lambda event source.</p>&#xA;"
48544421,48531937,1695906,2018-01-31T14:21:54,"<p>If all of the services are accessed under a single hostname then the DNS of course must point to exactly one place, so rerouting is fundamentally an all-or-nothing prospect.</p>&#xA;&#xA;<p>However, there's an effective workaround.  </p>&#xA;&#xA;<p>Configure a ""secret"" hostname for each service.  (""Secret"" in the sense that the client does not need to be aware of it.)  We'll call these ""service endpoints.""  The purpose of these hostnames is for routing requests to each service... svc1.api.example.com, svc2.api.example.com, etc.</p>&#xA;&#xA;<p>Configure each of these DNS records to point to the primary or failover load balancer, with Route 53 entries and a Route 53 health check that specifically checks that one service for health at each balancer.</p>&#xA;&#xA;<p>What you have at this point is a hostname for each service that will have a DNS answer that correctly points to the preferred, healthy endpoint.</p>&#xA;&#xA;<p>What you don't yet have is a way to ensure that client requests go to the right place.</p>&#xA;&#xA;<p>For this, create a CloudFront distribution, with your public API hostname as an Alternate Domain Name.  Define one CloudFront Origin for each of these service endpoints (leave ""Origin Path"" blank), then create a Cache Behavior for each service with the appropriate path pattern e.g. <code>/api/svc1*</code> and select the matching origin.  Whitelist any HTTP headers that your API needs to see.</p>&#xA;&#xA;<p>Finally, point DNS for your main hostname to CloudFront.</p>&#xA;&#xA;<p>The clients will automatically connect to their nearest CloudFront edge location, and CloudFront -- after matching the path pattern to discover where to send the request -- will check the DNS for that service-specific endpoint and forward the request to the appropriate balancer.</p>&#xA;&#xA;<p>CloudFront, in this application is not a ""CDN"" <em>per se</em>, but rather a globally-distributed reverse proxy -- logically, a <em>single</em> destination for all your traffic, so no failover configuration is required on the main hostname for the API... so no more all-or-nothing routing.    On the back-side of CloudFront, those service endpoint hostnames ensure that requests are routed to a healthy destination based on the Route 53 health checks.  CloudFront respects the TTL of these DNS records and will not cache DNS responses that it shouldn't.</p>&#xA;"
48666615,48660912,1695906,2018-02-07T14:46:55,"<p>It seems like you are looking for an <strong>API Gateway Private Integration</strong>.</p>&#xA;&#xA;<blockquote>&#xA;  <p>The API Gateway private integration makes it simple to expose your HTTP/HTTPS resources behind an Amazon VPC for access by clients outside of the VPC. To extend access to your private VPC resources beyond the VPC boundaries, you can create an API with private integration for open access or controlled access. You can do this by using IAM permissions, a custom authorizer, or an Amazon Cognito user pool.</p>&#xA;  &#xA;  <p><a href=""https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-private-integration.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-private-integration.html</a></p>&#xA;</blockquote>&#xA;&#xA;<p>Private integrations leverage other features of VPC --Network Load Balancer and VPC Link -- to allow API Gateway access to a ""backdoor"" connection to your instance-based services, so that they do not need to be exposed to the Internet at all.  The traffic flows from a (hidden) VPC owned by API Gateway, into your VPC, via a virtual private channel that has access to an NLB.</p>&#xA;"
50678348,50677506,5564578,2018-06-04T10:21:16,"<p>That course is so basic, that you can go ahead and take it. I mean, if you have at least very basic knowledge of VMs and infrastructure in general.</p>&#xA;"
50698178,50697247,5564578,2018-06-05T10:46:07,"<p>Your services are sending the requests to port 80 now. You need to specify parameter <code>targetPort</code>. Should look like this:</p>&#xA;&#xA;<pre><code>kind: Service&#xA;apiVersion: v1&#xA;metadata:&#xA;  name: lucky-server&#xA;spec:&#xA;  selector:&#xA;    app: lucky-server&#xA;  ports:&#xA;  - protocol: TCP&#xA;    targetPort: 8888  #this is your container port. where to send the requests&#xA;    port: 8888   #this is the service port. it is running on svc-ip:8888&#xA;  type: NodePort&#xA;</code></pre>&#xA;&#xA;<p>You should do the same with the other service. Also check the service port. Now it is on 8080 and 8888. You might be hitting them on port 80. </p>&#xA;&#xA;<p>There might be more issues, but for now, these for sure cause a problem.</p>&#xA;"
35766821,35743527,5437346,2016-03-03T08:26:13,"<p>This <strong>${#httpServletRequest.requestURL}</strong> did the trick...</p>&#xA;&#xA;<p><code>&lt;link th:href=""@{__${#httpServletRequest.requestURL}__webjars/bootstrap/?{bootstrap.version}/css/bootstrap.min.css}"" rel=""stylesheet"" /&gt;</code></p>&#xA;&#xA;<p>now all works fine.</p>&#xA;"
46748095,46744067,2375843,2017-10-14T18:41:16,"<p>One of the benefits of microservices is organisational scalability. You have several teams working on different microservices at the same time. Also, it should be possible to develop, build and run each microservice on its own. If you don't have those goals, you get all the complexities of microservices and very little of the advantages.</p>&#xA;&#xA;<p>In such a scenario, each microservice should live in its own version control. As long as this is given, if the IDE supports multiple version control sources per module, go for 2). Otherwise I guess you must stick with 1).</p>&#xA;"
48737054,48732814,2375843,2018-02-11T22:15:30,"<p>No one prevents you from building an asynchronous microservice architecture with Spring Boot :).</p>&#xA;&#xA;<p>Something along these lines:</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/gs55p.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gs55p.png"" alt=""enter image description here""></a> </p>&#xA;&#xA;<p>Instead of one service calling another synchronously, a service can put events to a queue (e.g. RabbitMQ). The events are delivered to services that subscribe to those events. </p>&#xA;&#xA;<p>Using RabbitMQ and its ""exchange"" concept, the event producing service doesn't even need to the consumers of its events.</p>&#xA;&#xA;<p>A blog post detailing this with Spring Boot code can be found here: <a href=""https://reflectoring.io/event-messaging-with-spring-boot-and-rabbitmq/"" rel=""nofollow noreferrer"">https://reflectoring.io/event-messaging-with-spring-boot-and-rabbitmq/</a></p>&#xA;"
35913782,35838016,4413603,2016-03-10T10:27:16,"<p>Apache Karaf is an OSGI container. OSGI with its modularity is an appealing solution to microservices. OSGI however can be quite complex with a steep learning curve. A better OSGI use case might be a plugin system / application.</p>&#xA;&#xA;<p><a href=""http://projects.spring.io/spring-boot/"" rel=""nofollow"">Spring Boot</a> is a great option. The benefit of Spring Boot would be all the additional features you get to manage your microservices once deployed, i.e. the actuator project for monitoring and <a href=""http://projects.spring.io/spring-cloud/"" rel=""nofollow"">Spring Cloud</a> which provide features such routing, load balancing, configuration, distributed messaging, etc. They have done all the legwork for you to get your microservices into production as quickly as possibly.</p>&#xA;&#xA;<p>You can also look at <a href=""http://wildfly-swarm.io/"" rel=""nofollow"">Wildfly Swarm</a> if you prefer plain JEE over Spring.</p>&#xA;&#xA;<p><a href=""http://www.dropwizard.io/0.9.2/docs/"" rel=""nofollow"">Dropwizard</a> is also a good option.</p>&#xA;&#xA;<p>These only scratch the surface, but are a great starting point.</p>&#xA;"
35913471,35913253,4413603,2016-03-10T10:12:54,"<p>For your project a microservice architecture seems like overkill. Microservices do have a lot of benefits, but they do have a lot of operational overhead and add a lot of complexity in terms of distributed services.</p>&#xA;&#xA;<p>It sounds like you also only have one primary requirement which is to pull news, in which case all the news-related services will form part of one microservice in any case.</p>&#xA;&#xA;<p>Once you start adding more distinct requirement domains you could look at building them as separate microservices. Even then it might be prudent to modularize the functionality into components within the same service, and as your application and system starts to grow split the functionality out into separate microservices.</p>&#xA;"
50831972,50830715,3511459,2018-06-13T07:58:58,"<p>As you describe it, it's probably not a docker or kubernetes issue. Instead, you should check if B is responding to A before A responds to B, and if so, check if A is not responding to the original call.</p>&#xA;&#xA;<p>You could set up logs to see if this is happening, or debug it if you can reproduce it in your machine.</p>&#xA;"
49903572,49890971,3122083,2018-04-18T15:28:12,"<p>This is what I refer to as changing a wheel while the car is moving.</p>&#xA;&#xA;<p>The whole point of micro services is to make them decoupled so you can release any given part on its own.</p>&#xA;&#xA;<p>You therefore have to make incremental changes that are backwards compatible with each other.</p>&#xA;&#xA;<p>This is similar to how you change your database. Say you want to remove a Boolean column and replace it with en enum. You do this not by changing the database and all the code at once, as that will have not only the deployment issue but also, what if something goes wrong and you have to roll back? Instead, you are better off first adding the new column, then changing some code to start writing to both columns, then changing all consumers and finally removing the old column. Each as separate, independent, backwards compatible changes.</p>&#xA;&#xA;<p>In short, I doubt there is some magic deployment method that you can use to deploy multiple independent systems with dependencies at the same time ( at least not without accepting down time ).</p>&#xA;&#xA;<p>The solution is to architect the changes to be broken down in small backwards compatible changes until you are done and can finally cleanup/remove the old code.</p>&#xA;&#xA;<p>This BTW, is why systems accumulate technical debt. As time passes and the system grows in complexity, a major upgrade may be too hard to break down and implement like this, so you are forced to make compromises that take time to cleanup.</p>&#xA;&#xA;<p>Anyway, maybe somebody else can describe some black magic but as I said, this is not a deployment issue but an architecture issue.</p>&#xA;&#xA;<p>Hope that helps</p>&#xA;"
34587208,34575783,1171619,2016-01-04T08:31:12,"<p>In Service Fabric terms, each actor type is mapped to a service type. There may be multiple instances of this actor type, but they are all configured, deployed and updated from this service. They will all use the same partitioning schema.</p>&#xA;&#xA;<p>Actors of different types are parts of different microservices.</p>&#xA;"
44641943,44564299,7668132,2017-06-20T00:34:41,"<p>This question was answered on the <a href=""https://discourse.linkerd.io/t/linkerd-not-displaying-count-of-requests-success-and-failure-rates/123/2"" rel=""nofollow noreferrer"">Linkerd community forum</a>. Adding the answer here as well for the sake of completeness:</p>&#xA;&#xA;<blockquote>&#xA;  <p>The dashboard gives a current snapshot of what's going on - it polls /admin/metrics.json every second and displays the metrics at that time (so at that instant, how many requests, retries, pending requests there are, so if there's nothing going through at that moment, those stats will be 0). For a longer term view of metrics, you'll need something else (see <a href=""https://linkerd.io/getting-started/admin/index.html#metrics"" rel=""nofollow noreferrer"">https://linkerd.io/getting-started/admin/index.html#metrics</a> for more info on collecting metrics).</p>&#xA;  &#xA;  <p>If you're on Kubernetes or DC/OS, you can also check out <a href=""https://github.com/linkerd/linkerd-viz"" rel=""nofollow noreferrer"">linkerd-viz</a>. Hope that helps!</p>&#xA;</blockquote>&#xA;"
49217282,49216360,5645988,2018-03-11T06:24:33,"<p>One option is using <a href=""https://www.npmjs.com/package/parallelshell"" rel=""nofollow noreferrer"">ParallelShell</a> module to run shell commands in parallel. You can find an example of how to use it <a href=""https://codeburst.io/headache-free-ssr-react-js-node-js-and-es6-app-boilerplate-tutorial-267f7be0b7b5"" rel=""nofollow noreferrer"">here</a></p>&#xA;"
52057460,52057408,3500196,2018-08-28T12:04:41,<p>You can use the amazon-cli if you want to test it.</p>&#xA;&#xA;<p>amazon-cli will help you to manually trigger your lamda via aws lambda invoke the command.</p>&#xA;&#xA;<p>Please install amazon-cli at your local from this link.</p>&#xA;&#xA;<p>After that you can invoke the amazon 1st lamda from your cli with aws lamda command. It comes with many options like you can pass payload (supposed to be pass from SNS in actual scenario).</p>&#xA;&#xA;<p>Executing AWS Lamda from Amazon-CLI Command details description</p>&#xA;&#xA;<p>Example command:</p>&#xA;&#xA;<pre><code>aws lambda invoke --function-name your_function_name --invocation-type RequestResponse outfile.txt --payload file:requestFile.txt&#xA;</code></pre>&#xA;&#xA;<p>I hope it helps.</p>&#xA;
39460014,39450504,2910628,2016-09-12T23:07:03,"<p>You can use Custom Domain Names : <a href=""http://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html"" rel=""nofollow"">http://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html</a></p>&#xA;&#xA;<p>setup the custom domains name (you'll need a SSL certificate) <code>http://myapi.com/</code></p>&#xA;&#xA;<p>then map your apis :</p>&#xA;&#xA;<pre><code>http://myapi.com/users&#xA;http://myapi.com/products &#xA;</code></pre>&#xA;&#xA;<p>Just call your functions like this :</p>&#xA;&#xA;<pre><code>http://myapi.com/users/create&#xA;http://myapi.com/users/read&#xA;http://myapi.com/products/whaterver&#xA;</code></pre>&#xA;"
42581361,41609091,1079460,2017-03-03T14:24:42,"<p>If you are approaching a microservice architecture, you should think of a microservice for managing files! Don't stream files around if you are microservice environment.&#xA;For example you might create a FileManagerService with an API exposed for with CRUD implementation and only use seneca act/add for serving the important data ... file-url, size, etc..</p>&#xA;"
44283214,30286443,429476,2017-05-31T11:10:55,"<p>We have not used submodules; What we have done is branching strategy</p>&#xA;&#xA;<p>Every micro-service has its own folder under base_folder folder.  </p>&#xA;&#xA;<p>There is a release branch  -->  currently one master ( this has everything)&#xA;There is an interface branch --> interfaces ( this has only interfaces like for example protobuffer/ grpc files for all services) . This branch is always merged to  master</p>&#xA;&#xA;<p>Each service  has a branch --> sprint_service_name  where  code is pushed  (for code review )  and a merge request created to master branch</p>&#xA;&#xA;<p><strong>Code flow</strong></p>&#xA;&#xA;<p>For new component</p>&#xA;&#xA;<pre><code>git checkout interfaces&#xA;git checkout -b sprint_service_name &#xA;(create branch from interface branch)&#xA;</code></pre>&#xA;&#xA;<p>For existing component</p>&#xA;&#xA;<pre><code>git checkout sprint_service_name&#xA;git fetch origin interfaces&#xA;git merge origin/interfaces (don't use git merge origin interfaces !!)&#xA;</code></pre>&#xA;&#xA;<p>(or for above two steps git pull origin interfaces)</p>&#xA;&#xA;<p>Here is a Developer flow</p>&#xA;&#xA;<pre><code>Push to sprint_service_name branch and create merge request to master branch&#xA;git push origin  sprint_service_name&#xA;</code></pre>&#xA;&#xA;<p><strong>Branch flow</strong></p>&#xA;&#xA;<pre><code>sprint_service_namexxx --&gt; Merge Request --&gt; master&#xA;sprint_interfaces --&gt;  Merge Request --&gt; Interfaces --&gt;master&#xA;interfaces --&gt; sprint_service_namexxx (by all, every day)&#xA;</code></pre>&#xA;&#xA;<p>All common parts will be in interfaces branch</p>&#xA;&#xA;<p>(you can have any number of private branches; But be careful not to merge of  master into  sprint_service_name or master into interfaces ; else unwanted files will be in your branch)</p>&#xA;&#xA;<p><strong>Pros</strong>&#xA;Each micro service will have only its code and interfaces folder</p>&#xA;&#xA;<p><strong>Cons</strong></p>&#xA;&#xA;<p>I have seen that the below flow does not always happen ideally, like </p>&#xA;&#xA;<pre><code>sprint_interfaces --&gt;  Merge Request --&gt; Interfaces --&gt;master&#xA;</code></pre>&#xA;&#xA;<p>It is more like</p>&#xA;&#xA;<pre><code>sprint_interfaces --&gt;  Merge Request --&gt; master&#xA;</code></pre>&#xA;&#xA;<p>Which means that someone has to manually take Interfaces <em>folder</em> from master and merge to Interfaces <em>branch</em>. But this is just a discipline thing and has not broken anything</p>&#xA;"
32071406,32071088,4691620,2015-08-18T11:39:48,"<p>They are what host a microservice since microservices is one application made up of many running processes.  This makes your application very modular allowing multiple languages, different frameworks, etc and the ability to add/remove microservices.  A container is basically a kernel level vm that is injected with all of the required dependencies required for your microservice to run.  Completely OS independent.  The has very very little overhead and self contains all the dependencies rather than mucking up a server running multiple services.</p>&#xA;"
45870338,45869766,717732,2017-08-24T20:26:50,"<p>With Docker Toolbox that's a little tricky, but actually the core-2.0 has nothing to do here. It's all about docker, docker-toolbox, and VS.</p>&#xA;&#xA;<p>First of all:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Is this the way one is supposed to start up that docker services? I have tried running this executable, and it seems to be working.</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes it is. If docker machine/services are running - that's good!</p>&#xA;&#xA;<p>Now, you have to realize that in docker, typically, the information about how/where the docker is running is kept in <strong>environment variables</strong>. The <code>quickstart</code> script not only starts the docker-machine for you and checks some basic things, it also sets up a couple of environmental variables so that later all commands like <code>docker</code>, <code>docker-compose</code> etc know where to look for the docker virtual machine. In your/our case that information mainly consists of an IP of the VM and a port number that Docker listens on.</p>&#xA;&#xA;<p>.. and your Visual Studio has no knowledge of that, because, I bet on that, you have ran the VisualStudio from StartMenu or from Desktop icon or by double-clicking on a solution file, so it had no chance of getting the environmental variables from <code>quickstart</code> console.</p>&#xA;&#xA;<p>The solution is quite simple: make sure that VS gets that information. That is, make sure it gets that environmental variables, <strong>and</strong> make sure that it gets the <strong>fresh</strong> state of them, because the IP/port may fluctuate sometimes. So don't just copy them to your OS settings, because nothing will ever automagically refresh them..</p>&#xA;&#xA;<p>The simplest way I found is to just close Visual Studio, run docker toolbox quickstart console, then <strong>run the VisualStudio from within that console</strong>, for example, for my VS2017 Community Edition:</p>&#xA;&#xA;<pre><code>Starting ""default""...&#xA;(default) Check network to re-create if needed...&#xA;(default) Waiting for an IP...&#xA;(.......snip..........)&#xA;&#xA;&#xA;                        ##         .&#xA;                  ## ## ##        ==&#xA;               ## ## ## ## ##    ===&#xA;           /""""""""""""""""""""""""""""""""""\___/ ===&#xA;      ~~~ {~~ ~~~~ ~~~ ~~~~ ~~~ ~ /  ===- ~~~&#xA;           \______ o           __/&#xA;             \    \         __/&#xA;              \____\_______/&#xA;&#xA;docker is configured to use the default machine with IP 192.168.99.100&#xA;For help getting started, check out the docs at https://docs.docker.com&#xA;&#xA;Start interactive shell&#xA;quetzalcoatl@LAP049 MINGW32 ~&#xA;$ /c/Program\ Files\ \(x86\)/Microsoft\ Visual\ Studio/2017/Community/Common7/IDE/devenv.exe C:\\PATH\\TO\\MY\\SOLUTION.sln&#xA;</code></pre>&#xA;&#xA;<p>The path is pretty long to write, even with TAB-completion, so usually make a tiny <code>.sh</code> script to run that for me.</p>&#xA;&#xA;<p>BTW! Notice that the path to DEVENV must be unix-like (<code>/c/Program\ Files...</code>), because the mingw shell has to understand that, while the path to SOLUTION must be a normal windows path (<code>c:\projects\foo\bar\..</code>) because VisualStudio will try to read that after starting up.</p>&#xA;"
37990326,37989016,3536548,2016-06-23T11:27:53,"<p>I managed to find this blog:</p>&#xA;&#xA;<p><a href=""https://www.igvita.com/2010/02/16/data-serialization-rpc-with-avro-ruby/"" rel=""nofollow"">https://www.igvita.com/2010/02/16/data-serialization-rpc-with-avro-ruby/</a></p>&#xA;&#xA;<p>which linked to the following samples:</p>&#xA;&#xA;<h1>client(send)</h1>&#xA;&#xA;<p><a href=""https://github.com/apache/avro/blob/trunk/lang/ruby/test/sample_ipc_client.rb"" rel=""nofollow"">https://github.com/apache/avro/blob/trunk/lang/ruby/test/sample_ipc_client.rb</a></p>&#xA;&#xA;<h1>server(receive)</h1>&#xA;&#xA;<p><a href=""https://github.com/apache/avro/blob/trunk/lang/ruby/test/sample_ipc_server.rb"" rel=""nofollow"">https://github.com/apache/avro/blob/trunk/lang/ruby/test/sample_ipc_server.rb</a></p>&#xA;"
47957268,47939451,9110949,2017-12-24T00:55:45,<p>It will be great if you add a unique identifier to relate both entities. so not only persisting the data it can be used for all the CRUD operation.</p>&#xA;
47957286,47939451,9110949,2017-12-24T01:00:34,"<p>In the other hand you can make use of PHP/MySQL as well. if you are going to use PHP then go with ORM (<strong>Object-relational mapping</strong>) framework. so you can handle the way how your entities will be saved, whether all child nodes or individual entities. </p>&#xA;"
38377456,38377156,135589,2016-07-14T14:52:58,"<p>If you have Spring Boot Actuator included in your project, you can enable a shutdown endpoint (by default it is not enabled). This means that if you make a request to: <a href=""http://yourserver.com/yourapp/shutdown"" rel=""nofollow"">http://yourserver.com/yourapp/shutdown</a>, the application will shutdown gracefully. An administrator could do such a request using a standard tool such as curl.</p>&#xA;&#xA;<p>See <a href=""http://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#production-ready-endpoints"" rel=""nofollow"">Endpoints</a> in the Spring Boot reference documentation. You can enable the shutdown endpoint by adding the following to your <code>application.properties</code>:</p>&#xA;&#xA;<pre><code>endpoints.shutdown.enabled=true&#xA;</code></pre>&#xA;&#xA;<p>Ofcourse, you'll want to restrict access to this endpoint, otherwise anyone who has access to the service could do a request and shutdown the application.</p>&#xA;"
37538872,37537605,6309,2016-05-31T06:58:20,"<p>Which tools will scale B?</p>&#xA;&#xA;<p>The two main ones are:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://docs.docker.com/swarm/"" rel=""nofollow""><strong>docker swarm</strong></a> (<a href=""https://github.com/docker/swarm"" rel=""nofollow"">GitHub repo</a>): see <a href=""https://github.com/docker/swarm-microservice-demo-v1"" rel=""nofollow"">docker/swarm-microservice-demo-v1</a></li>&#xA;<li><strong><a href=""http://kubernetes.io/"" rel=""nofollow"">Kubernetes</a></strong> (<a href=""https://github.com/kubernetes/kubernetes"" rel=""nofollow"">GitHub repo</a>): see for instance ""<a href=""http://getcloudify.org/2016/04/04/scaling-kubernetes-microservices-openstack-TOSCA-orchestration-cloud-open-source.html"" rel=""nofollow"">Scaling Kubernetes Microservices on OpenStack</a>"". </li>&#xA;</ul>&#xA;&#xA;<blockquote>&#xA;  <p>The only advantage of micro-service is scale it if required</p>&#xA;</blockquote>&#xA;&#xA;<p>Not ""the only advantage"": it is also isolation, and better use of the host resources.</p>&#xA;"
45004022,45002471,6309,2017-07-10T04:55:05,"<p>You can install <a href=""https://portainer.io/"" rel=""nofollow noreferrer""><strong>portainer.io</strong></a> (see <a href=""http://demo.portainer.io"" rel=""nofollow noreferrer"">its demo</a>, password <code>tryportainer</code>)</p>&#xA;&#xA;<p>But to truly isolate those third-party micro-services, you could run them in their own VM defined on your infrastructure.  That VM would  run a docker daemon and services. As long as the VM has access to the API, those micro-services containers will do fine, and won't lead/have access to anything directly from the infrastructure.<br>&#xA;You need to define/size your VM correctly to allocate enough resources for the containers to run, each one assuring their own <a href=""http://doger.io/"" rel=""nofollow noreferrer"">resource isolation</a>.</p>&#xA;"
37244059,37243939,6309,2016-05-15T21:54:12,"<p>One possibility is the one recommended by the Docker Postgres image:</p>&#xA;&#xA;<blockquote>&#xA;  <h2><a href=""https://github.com/docker-library/docs/tree/master/postgres#how-to-extend-this-image"" rel=""nofollow"">How to extend this image</a></h2>&#xA;  &#xA;  <p>If you would like to do additional initialization in an image derived from this one, add one or more <code>*.sql</code> or <code>*.sh</code> scripts under <code>/docker-entrypoint-initdb.d</code> (creating the directory if necessary). </p>&#xA;  &#xA;  <p>After the entrypoint calls <code>initdb</code> to create the default postgres user and database, it will run any <code>*.sql</code> files and source any <code>*.sh</code> scripts found in that directory to do further initialization before starting the service.</p>&#xA;</blockquote>&#xA;"
37178739,37176069,6309,2016-05-12T06:21:10,"<blockquote>&#xA;  <p>I dont see any other relevant logs to debug further as what is the reason so that i am container dies right after insert.</p>&#xA;</blockquote>&#xA;&#xA;<p>That is how a container is supposed to work: once its main command exits (or fail), the container exits as well and put itself in an ""Exited"" state.</p>&#xA;&#xA;<p>The main command (<a href=""https://docs.docker.com/engine/reference/builder/#entrypoint"" rel=""nofollow""><code>ENTRYPOINT</code></a> or <a href=""https://docs.docker.com/engine/reference/builder/#cmd"" rel=""nofollow""><code>CMD</code></a>) was for container B to insert query to a cassandra container. It does so and exit.</p>&#xA;"
51317693,51317424,6309,2018-07-13T04:32:38,"<pre><code>volumes: - ./src/ApiGateways/Web.Bff.Shopping/apigw:${ESHOP_OCELOT_VOLUME_SPEC:-/app/configuration}&#xA;</code></pre>&#xA;&#xA;<p>That means: </p>&#xA;&#xA;<p>Mount the <code>./src/ApiGateways/Web.Bff.Shopping/apigw</code> to the path mentioned by <code>$ESHOP_OCELOT_VOLUME_SPEC</code></p>&#xA;&#xA;<p>If <code>$ESHOP_OCELOT_VOLUME_SPEC</code> is <em>empty</em> (not defined), then use as a mount path <code>/app/configuration</code>.</p>&#xA;&#xA;<p>That gives the opportunity to a user to override the default path by a path of his/her choosing.</p>&#xA;&#xA;<pre><code>docker run -e ESHOP_OCELOT_VOLUME_SPEC=/my/path ...&#xA;</code></pre>&#xA;"
40068315,40068013,6309,2016-10-16T08:18:06,"<p>You can use <a href=""https://git-scm.com/docs/git-merge-base"" rel=""nofollow noreferrer""><code>git merge-base</code></a> to get the last merge:</p>&#xA;&#xA;<pre><code>git diff --name-only $(git merge-base --fork-point master myBranch)..myBranch&#xA;</code></pre>&#xA;&#xA;<p>On the fork-point part, see <a href=""https://stackoverflow.com/a/20423029/6309"">this answer</a>.</p>&#xA;&#xA;<p>From there, you can grep by pattern to filter out the result.</p>&#xA;&#xA;<p>As torek adds in the comments, if fork-point is not needed, and using <strong><a href=""http://kgrz.io/git/2015/07/27/git-intro-to-pathspec.html"" rel=""nofollow noreferrer"">a pathspec</a></strong></p>&#xA;&#xA;<pre><code>git diff --name-only master...myBranch -- requitements.txt&#xA;</code></pre>&#xA;&#xA;<p>See ""<a href=""https://stackoverflow.com/a/7256391/6309"">What are the differences between double-dot ""..."" and triple-dot ""<code>...</code>"" in Git diff commit ranges?</a>""</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/1eZaw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1eZaw.png"" alt=""http://mythic-beasts.com/~mark/git-diff-help.png""></a></p>&#xA;"
30286661,30286443,6309,2015-05-17T11:50:44,"<p>Unless those micro-services are tightly coupled (meaning it wouldn't make sense to download only some of them, and you would only work with <em>all</em> of them), keeping them each in a separate Git repo is recommended.</p>&#xA;&#xA;<p>But you can still reference them as <strong><a href=""http://git-scm.com/book/en/v2/Git-Tools-Submodules"">submodule</a></strong> in a parent repo in order to keep track of their state at any given time.</p>&#xA;"
38224343,38224152,6309,2016-07-06T12:44:11,"<p>You could consider using <a href=""https://git-scm.com/book/en/v2/Git-Tools-Submodules"" rel=""nofollow"">git submodules</a> with:</p>&#xA;&#xA;<pre><code>cd /path/to/WholeProject&#xA;git submodule -- /url/to/repo/A µS/A&#xA;git submodule -- /url/to/repo/B µS/B&#xA;</code></pre>&#xA;&#xA;<p>That way, you can clone WholeProject with a:</p>&#xA;&#xA;<pre><code>git clone --recursive&#xA;</code></pre>&#xA;&#xA;<p>And you get <code>A</code> and <code>B</code> at their last recorded SHA1.</p>&#xA;"
48134960,48134800,6309,2018-01-07T06:46:49,"<blockquote>&#xA;  <p>I'm wondering if go applications should or should not be deployed in containers (Docker)<br>&#xA;  Why should I deploy and run those binaries inside containers like Docker?</p>&#xA;</blockquote>&#xA;&#xA;<p>Of course, provided you separate the build from the actual final image (in order to not include in said final image build dependencies)<br>&#xA;See ""<a href=""https://made2591.github.io/posts/goa-docker-multistage"" rel=""nofollow noreferrer"">Golang, Docker and multistage build</a>"" from <strong><a href=""https://made2591.github.io/about/"" rel=""nofollow noreferrer"">Matteo Madeddu</a></strong>.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Can I deploy these servers together in one host running on different ports?</p>&#xA;</blockquote>&#xA;&#xA;<p>Actually, they could all run in their own container on their own port, even if that port is the same.<br>&#xA;Intra-container communication will work, using <strong><a href=""https://docs.docker.com/engine/reference/builder/#expose"" rel=""nofollow noreferrer"">EXPOSEd port</a></strong>.&#xA;However, if they are accessed from outside, then their <a href=""https://docs.docker.com/engine/reference/run/#expose-incoming-ports"" rel=""nofollow noreferrer""><em>published</em> port</a> need to be different indeed.</p>&#xA;&#xA;<blockquote>&#xA;  <p>What about scalibility and high availability without using Docker?<br>&#xA;  And what about the scalability and availability of using Docker?</p>&#xA;</blockquote>&#xA;&#xA;<p>As soon as you are talking about dynamic status, some kind of orchestration will be involved: see <a href=""https://docs.docker.com/engine/swarm/"" rel=""nofollow noreferrer"">Docker Swarm</a> or <a href=""https://kubernetes.io/"" rel=""nofollow noreferrer"">Kubernetes</a> for efficient cluster management.<br>&#xA;<a href=""https://blog.docker.com/2017/12/kubernetes-in-docker-ee/"" rel=""nofollow noreferrer"">Both are available with the latest docker</a>.</p>&#xA;&#xA;<p>Examples:</p>&#xA;&#xA;<ul>&#xA;<li>""<a href=""https://medium.com/wattpad-engineering/building-and-testing-go-apps-monorepo-speed-9e9ca4978e19"" rel=""nofollow noreferrer""><strong>Building and testing Go apps + monorepo + speed</strong></a>"": Or, how we test and build go code in a monorepo, with TravisCI, and deploy to Docker, quickly and easily. From <strong><a href=""https://twitter.com/jharlap"" rel=""nofollow noreferrer"">Jonathan Harlap</a></strong>, Principal Engineer @ Wattpad</li>&#xA;<li>""<a href=""https://blog.alexellis.io/introducing-functions-as-a-service/"" rel=""nofollow noreferrer""><strong>Introducing Functions as a Service (OpenFaaS)</strong></a>"", from <strong><a href=""https://twitter.com/alexellisuk"" rel=""nofollow noreferrer"">Alex Ellis</a></strong></li>&#xA;</ul>&#xA;"
49264962,49264474,152997,2018-03-13T20:04:37,"<p>Yes. Anything you compile targeting .NET Core will run on any of the platforms supported by .NET Core. However, there is no built-in publishing support, that would be a separate operation you'd have to set up yourself.</p>&#xA;&#xA;<p>You can find the list of currently supported platforms on the <a href=""https://docs.microsoft.com/en-us/dotnet/core/rid-catalog"" rel=""nofollow noreferrer"">Runtime Identifier</a> page. The current Ubuntu list is:</p>&#xA;&#xA;<blockquote>&#xA;  <ul>&#xA;  <li>ubuntu.14.04-x64</li>&#xA;  <li>ubuntu.14.10-x64</li>&#xA;  <li>ubuntu.15.04-x64</li>&#xA;  <li>ubuntu.15.10-x64</li>&#xA;  <li>ubuntu.16.04-x64</li>&#xA;  <li>ubuntu.16.10-x64</li>&#xA;  </ul>&#xA;</blockquote>&#xA;"
45533292,45528718,2979749,2017-08-06T15:02:17,"<p>This is a very good read - <a href=""https://martinfowler.com/bliki/CircuitBreaker.html"" rel=""nofollow noreferrer"">https://martinfowler.com/bliki/CircuitBreaker.html</a> :)</p>&#xA;"
45540395,45533748,2979749,2017-08-07T06:11:09,"<p>For backward compatibility, see  <a href=""https://stackoverflow.com/questions/33202053/product-versioning-microservices"">Product Versioning Microservices</a>, in general <a href=""http://semver.org/"" rel=""nofollow noreferrer"">Semantic Versioning</a> is advised by many...</p>&#xA;&#xA;<p>In the broader sense - there should be an agreed phase-out roadmap for major versions, that is communicated to API consumers (together with SLAs). This is why tracking who uses your APIs is important.</p>&#xA;"
45401416,45398103,2979749,2017-07-30T15:22:02,"<p><strong>Microservices as a tool</strong></p>&#xA;&#xA;<p>One thing you may want to think about is if microservices are an architecture for a small project with low traffic. </p>&#xA;&#xA;<p>Microservices architecture is a tool to solve e.g. high traffic challenges and with low traffic a monolith may be a more cost effective approach. Microservices come also at a cost (complexity all over the board - design, deployment, service discovery and relations). </p>&#xA;&#xA;<p>Keep in mind that your microservices shouldn't be too small and as per the best practices you should cover a single domain with them (<a href=""https://martinfowler.com/articles/microservices.html"" rel=""nofollow noreferrer"">https://martinfowler.com/articles/microservices.html</a>), not split a business domain into multiple microservices just for the sake of having microservices (unless this is a training project where you want to learn the tools for microservices architecture). </p>&#xA;&#xA;<p>I am not sure how large solution you would need to have to have a challenge of 100 microservices, but maybe you should review their design and make sure that they are not too small :)</p>&#xA;&#xA;<p>Nice and short article about this topic - <a href=""https://blog.newrelic.com/2016/02/08/microservices-what-they-are-why-to-use-them/"" rel=""nofollow noreferrer"">Microservice Architectures: What They Are and Why You Should Use Them</a>.</p>&#xA;&#xA;<p><strong>Lambda</strong></p>&#xA;&#xA;<p>Microservices aside, as @Ashan suggested, for low ongoing cost you may want to look at functional programming/lambda architecture and <a href=""https://serverless.com/"" rel=""nofollow noreferrer"">serverless framework</a>. Again - there is a complexity (since you go one level deeper in separating your deployment packages than with microservices) that is partially addressed by the serverless framework, but you have tools like <a href=""https://aws.amazon.com/lambda/"" rel=""nofollow noreferrer"">AWS Lambda</a>/Azure Functions/Google Functions to run your functions as a service and pay per use (real use, not reservation as in EC2). </p>&#xA;&#xA;<p><strong>Microservices with Docker and AWS ECS</strong></p>&#xA;&#xA;<p>If you want to stick to microservices, please look into <a href=""http://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-basics.html"" rel=""nofollow noreferrer"">Docker and Amazon EC2 Container Service</a>. This will allow you to effectively use AWS EC2 instances for running multiple microservices. You may want to put <a href=""https://aws.amazon.com/blogs/compute/microservice-delivery-with-amazon-ecs-and-application-load-balancers/"" rel=""nofollow noreferrer"">Application Gateway in front of AWS ECS</a> to manage the traffic.</p>&#xA;"
45429920,45396642,2979749,2017-08-01T05:54:57,"<p>Microservices is a very broad topic, for architecture questions I suggest you read e.g. <a href=""https://www.safaribooksonline.com/library/view/building-microservices/9781491950340/"" rel=""nofollow noreferrer"">Building Microservices</a> by Sam Newman and/or <a href=""http://shop.oreilly.com/product/0636920053675.do"" rel=""nofollow noreferrer"">Production-Ready Microservices</a> by Susan Fowler to learn more about them.</p>&#xA;&#xA;<p>Answering a question ""How to make sure my whole solution doesn't go down when service is not available?"" (which I assume is what you are looking for) on a space ship level - you have two options:</p>&#xA;&#xA;<ol>&#xA;<li><p>Retries when service you depend on fails - this is how Netflix does&#xA;it, you can do it manually, or if you are on Java stack, e.g.&#xA;<a href=""https://github.com/Netflix/Hystrix"" rel=""nofollow noreferrer"">Hystrix</a></p></li>&#xA;<li><p>Queues for communication between services preferably PaaS queue systems (AWS SQS, Azure ServiceBus), since managing highly available queue platform is a challenge on its own.</p></li>&#xA;</ol>&#xA;&#xA;<p>If this isn't the question you have on your mind, please be more specific :) Or read mentioned books first, they should answer many initial questions (or at least make you aware of potential challenges) in the microservices space.</p>&#xA;"
48610844,48604664,2979749,2018-02-04T17:14:39,"<p>First and most important - your microservice should be responsible for handling all data in a given business domain/bounded context. So the question is - 'Why do you need to share database connection between microservices and isn't this a sign you went too far with slicing your system?' Microservice is a tool and word 'micro' may be misleading a bit :)</p>&#xA;&#xA;<p>For more reading I would suggest e.g. <a href=""https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/architect-microservice-container-applications/identify-microservice-domain-model-boundaries"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/architect-microservice-container-applications/identify-microservice-domain-model-boundaries</a> (don' t worry, it's general enough to be applicable also to Spring).</p>&#xA;"
49305614,49284804,2979749,2018-03-15T17:22:38,"<p>Alternative opinion:</p>&#xA;&#xA;<ul>&#xA;<li>In 99% of real life cases you musnt have a single container that runs&#xA;database and the application, those should be separated, since one&#xA;(db) is keeping state, while the other (app) should be stateless. </li>&#xA;<li>You don't need a separate database for microservice, very often a separate schema is more than enough (e.g. you dont want to deploy a separate Exadata for each microservice :)). What is important is that only this microservice can read and write and make modifications to given tables others can operate on those tabls only through interfaces exposed by the microservice. </li>&#xA;</ul>&#xA;"
49306132,49302010,2979749,2018-03-15T17:50:43,"<p>If scalability isn't your concern, then I'd be pointing at the following benefits:</p>&#xA;&#xA;<ul>&#xA;<li>Increased change velocity - shorter time for a feature to get from the idea phase to production (lower complexity for developers)</li>&#xA;<li>Lower cost of testing (smaller scope to test)</li>&#xA;<li>Improved quality (again, smaller scope to test)</li>&#xA;</ul>&#xA;"
46582802,46566667,2979749,2017-10-05T09:56:28,"<p>First though that comes to me is that in general, microservice should handle given bounded context/domain. So I wonder what is this 'additional' data that you want to add to products and if it also shouldn't fall under products microservice. </p>&#xA;&#xA;<p>If you're sure it shouldn't, these are separate domains, then it's a question of your API design. If you predict to often query another microservice with 1000s of IDs, build an API that can handle 1000s of IDs (or preferably any length of the ID list).</p>&#xA;&#xA;<p>As for scenario 3 - if I would get such task, I would create an API that handles sorting and filtering, so that all those operations that databases are good in, will be handled by databases. </p>&#xA;&#xA;<p>In general, my pov is that API should be driven by your business needs and shouldnt constrain you in delivering high quality solution fast. So if simple CRUD API doesnt work for you (and it rarely does), just change it.</p>&#xA;"
46582515,46581254,2979749,2017-10-05T09:44:23,"<p>I would suggest checking the following two books - <a href=""http://shop.oreilly.com/product/0636920033158.do"" rel=""nofollow noreferrer"">Building Microservices</a>&#xA;  and <a href=""http://shop.oreilly.com/product/0636920053675.do"" rel=""nofollow noreferrer"">Production-Ready Microservices</a>. Great read for everyone who wants to start journey with microservices.</p>&#xA;"
45607236,45597395,2979749,2017-08-10T07:32:23,"<p>From the top of my head, these are the questions I would try to answer:</p>&#xA;&#xA;<ol>&#xA;<li>Does the feature operate on the same data (in the same domain) as existing service?</li>&#xA;<li>Will the feature be managed and maintained by team that already covers existing service?</li>&#xA;<li>Will the feature need to scale according to the scaling of existing service (vs will have completely different scaling needs)?</li>&#xA;<li>Will the feature require a technology stack that is the same (vs completely&#xA;different) as existing service?</li>&#xA;<li>Will the feature have the same change velocity as existing service?</li>&#xA;<li>Are all the security requirements the same in the new feature as in existing service? </li>&#xA;</ol>&#xA;&#xA;<p>The more 'Yes' answers, the more I would consider adding this feature to existing service.</p>&#xA;"
45630633,45629284,2979749,2017-08-11T08:40:45,"<p>However you call it, you will need a central place where services will need to register their host and port (and deregister when they shutdown gracefully), so that others know how to consume them. You can use a database (SQL/noSQL), Eureka, Consul, anything that will give you ""put data"" and ""get data"" capabilities.</p>&#xA;&#xA;<p>Benefit of solutions like Eureka and Consul are built-in health checks and removing service from the registry if it fails them.</p>&#xA;"
45654140,45642575,2979749,2017-08-12T19:28:34,"<p>API Gateways and queues are not really key here (this is your implementation detail for microservice). </p>&#xA;&#xA;<p>As @Ilya Bursov pointed out what you are looking for is either HTTP polling (repetitive checking if response for your request is already available) or <a href=""https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API"" rel=""nofollow noreferrer"">websockets</a> .</p>&#xA;&#xA;<p>You can find more details here <a href=""https://stackoverflow.com/questions/12555043/my-understanding-of-http-polling-long-polling-http-streaming-and-websockets"">My Understanding of HTTP Polling, Long Polling, HTTP Streaming and WebSockets</a></p>&#xA;"
45793223,45782033,2979749,2017-08-21T09:07:19,"<p>What should make you wonder is why multiple microservices will handle similar or identical data. This may mean that you are going too far in slicing your solution. Quoting <a href=""http://samnewman.io/books/building_microservices/"" rel=""nofollow noreferrer"">Sam Newman</a> - ""<em>bounded contexts represent autonomous business domains (i.e., distinct business capabilities), and therefore are the appropriate starting point for identifying the dividing lines for microservices.</em>"". </p>&#xA;&#xA;<p>So I would say - there should be a good business reason to go deeper than the business domain -> microservice split.</p>&#xA;&#xA;<p>One good comment about ""sharing libraries/components"" I've read recently is that soon this shared library/component becomes your bottleneck, any changes you make there will require a lot of regression testing across teams and the initial value of having it may be overshadowed by effort required to maintain it.</p>&#xA;&#xA;<p>So as you can see, if you go microservices, I'd vote for share-nothing approach ;-)</p>&#xA;"
45660610,45655728,2979749,2017-08-13T12:59:57,"<p>API = <a href=""https://www.quora.com/What-is-an-API-4"" rel=""noreferrer"">Application Programming Interface</a> </p>&#xA;&#xA;<p>Microservices = <a href=""https://martinfowler.com/articles/microservices.html"" rel=""noreferrer"">an architecture</a></p>&#xA;&#xA;<p>In short - microservice should expose a well-defined API. Microservice is the way you may want to architect your solution, API is what your consumers see. You can expose API without microservices in the backend (in fact, most non-training scenarios dont require microservices).</p>&#xA;&#xA;<p>You may want to read <a href=""http://samnewman.io/books/building_microservices/"" rel=""noreferrer"">http://samnewman.io/books/building_microservices/</a> before you decide on using microservices (unless this is for training purposes).</p>&#xA;"
45977484,45964545,2979749,2017-08-31T09:02:44,"<p>Option 2, so running all separate webapps on a single tomcat instance per node is a very good first step toward full-blown microservices architecture. In fact you anyway need this step to implement service discovery/registry later. As you write - it will already give you some level of isolation and ability to perform independent migrations (with separate wars).</p>&#xA;&#xA;<p>Just make sure you wrap the code for getting services URL into a class, so that you will make changes in one place only when adding Service Discovery (vs hardcoding URLs for services in multiple places in your app).</p>&#xA;&#xA;<p>Cons of stopping for some time on option 2 is that you will lose ability to scale on a per-service level. You will need to answer the question how critical it is in your specific scenario.</p>&#xA;&#xA;<p>What I don't see in your post is your approach for using queues. Honestly, if your solution can use asynchronous communication, I would strongly encourage it (note that Netflix stack is predominantly targeted at direct REST communication).</p>&#xA;"
41855411,41850142,818341,2017-01-25T15:29:24,<p>If I understand your question correctly it seems to me that there may be multiple ways to achieve this. </p>&#xA;&#xA;<p>One solution is to have a schema version somewhere in the database that your microservices periodically check. When your database schema changes you can increase the schema version. As a result of this if a service notices that the database schema version is higher than the current schema version of the service it can migrate the schema in the code which <code>gorm</code> allows.</p>&#xA;&#xA;<p>Other options could depend on how you run your microservices. For example if you run them using some orchestration platform (e.g. Kubernetes) you could put the migration code somewhere to run when your service initializes. Then once you update the schema you can force a rolling refresh of your containers which would in turn trigger the migration.</p>&#xA;
37945071,37944398,2594725,2016-06-21T12:54:58,"<p>If the output of the service is different for each channel id and API Key, then the API key needs to be part of the cache key.</p>&#xA;&#xA;<p>If the output is identical no matter what API key, then you can use any API key or the always the same API key.</p>&#xA;&#xA;<p>Passing down the API key from the caller, but caching the value does make no sense, or is essentially identically to randomly choose an API key. As soon as the value is in the cache, you will return a result that was retrieved with a different API key from the previous caller.</p>&#xA;"
50910478,48824086,958373,2018-06-18T13:19:06,<p>Use the Spring Boot <code>2</code> dependency of Keycloak as in:</p>&#xA;&#xA;<pre><code>&lt;dependency&gt;&#xA;  &lt;groupId&gt;org.keycloak&lt;/groupId&gt;&#xA;  &lt;artifactId&gt;keycloak-spring-boot-2-starter&lt;/artifactId&gt;&#xA;&lt;/dependency&gt;&#xA;</code></pre>&#xA;&#xA;<p>! Note the <code>-2-</code> in the artifact id.</p>&#xA;&#xA;<p>The Spring Boot parent being:</p>&#xA;&#xA;<pre><code>  &lt;parent&gt;&#xA;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&#xA;    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;&#xA;    &lt;version&gt;2.0.3.RELEASE&lt;/version&gt;&#xA;    &lt;relativePath/&gt;&#xA;  &lt;/parent&gt;&#xA;</code></pre>&#xA;&#xA;<p>With the Keycloak dependency being:</p>&#xA;&#xA;<pre><code>&lt;properties&gt;&#xA;  &lt;keycloak.version&gt;4.0.0.Final&lt;/keycloak.version&gt;&#xA;&lt;/properties&gt;&#xA;&lt;dependencyManagement&gt;&#xA;  &lt;dependencies&gt;&#xA;    &lt;dependency&gt;&#xA;      &lt;groupId&gt;org.keycloak.bom&lt;/groupId&gt;&#xA;      &lt;artifactId&gt;keycloak-adapter-bom&lt;/artifactId&gt;&#xA;      &lt;version&gt;${keycloak.version}&lt;/version&gt;&#xA;      &lt;type&gt;pom&lt;/type&gt;&#xA;      &lt;scope&gt;import&lt;/scope&gt;&#xA;    &lt;/dependency&gt;&#xA;  &lt;/dependencies&gt;&#xA;&lt;/dependencyManagement&gt;&#xA;</code></pre>&#xA;
33198375,33187526,1811877,2015-10-18T13:20:34,"<p>If you want to have content synchronized between all the microservices, in your case the header and footer, I'd suggest <a href=""https://zookeeper.apache.org/"" rel=""nofollow"">Zookeeper</a>, it's designed for distributed orchestration and has more of a push model - i.e. you'd update the header in Zookeeper and all of your services would receive that update almost instantly. </p>&#xA;&#xA;<p>I suggest the <a href=""http://curator.apache.org/"" rel=""nofollow"">Curator</a> library as it's much easier to work with than Zookeeper directly, the <a href=""https://git-wip-us.apache.org/repos/asf?p=curator.git;a=blob;f=curator-examples/src/main/java/cache/PathCacheExample.java;h=e121337c50ff3ad878720632e9f134a54f5149bb;hb=HEAD"" rel=""nofollow"">cache example</a> might be a useful starting point.</p>&#xA;"
38824948,38812779,6084005,2016-08-08T08:54:45,"<p>It depends of the application itself. If you want to use a application-aware database you can use a NoSQL database like cassandra. If you just want to model and store data on a ER basis you can use some relational database (e.g. Oracle).&#xA;Nevertheless, Cassandra should be deployed as a clustered database and so is not suitable to use as a local database.</p>&#xA;&#xA;<p>To advise you with more appropriate database to your context you have to give more info about your ""problem"", for instance, how many concurrent users you will support, how many requests per user, how much data you have? </p>&#xA;"
39610847,39457873,69798,2016-09-21T08:07:12,"<p>The correlation identifier should not be something that you add yourself, some framework that sends the messages should do this. This way a developer can't forget it and it has a consistent behaviour all over the place.</p>&#xA;&#xA;<p>Put a <code>MessageId</code> and <code>CorrelationId</code> in the headers of the message. On first message, both will be the same. On second message, <code>CorrelationId</code> is set to the <code>MessageId</code> of the previous one.</p>&#xA;&#xA;<p>You could also set a unique <code>ConversationId</code> which never changes, to track all messages spawning from one originator. Useful when using pub/sub and/or calling back to the originator of a message.</p>&#xA;"
38406666,38397672,69798,2016-07-16T00:42:35,"<p>Theoretically, the best option is to not share data, only unique identifiers (primary keys). </p>&#xA;&#xA;<p>In practice, you should not divide the data this way. Part of the user belongs to the game, other parts might be shared across different games. I'm assuming that's why you separated the two.</p>&#xA;&#xA;<p>Have a look at the DDD principle of Bounded Contexts and how you should/could create separate services this way. That being said, defining bounded contexts the proper way is the hardest thing to do in SOA and/or microservices.</p>&#xA;"
43505717,43502357,69798,2017-04-19T20:44:33,"<p>A service is the technical authority of a business capability.</p>&#xA;&#xA;<p>If you should be able to differentiate between an order from either system, but you can't, you're probably building a 'technical authority' for multiple business capabilities.</p>&#xA;&#xA;<p>Other than that, a service can have many components. Instead of focusing on technical issues, focus on the business issue and see if you can explain that. But a platform like Stackoverflow, with a 1-to-1 ratio on question &amp; answer probably isn't the correct medium for questions like this.</p>&#xA;"
37584831,37509121,69798,2016-06-02T07:03:30,"<p>A classic example is where an order comes in at sales and an event is published. Both Finance and Shipping are subscribed to the event, but shipping is also subscribed to the event coming from finance.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/3dS8k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3dS8k.png"" alt=""waiting for all events to come in""></a></p>&#xA;&#xA;<p>The funny thing is that you have no idea on the order in which the messages arrive. The event from sales might cause a technical error, because the database is offline. It might get queued again or end up in an error queue for operations to retry it. In the meantime the event from finance might arrive. So theoretically&#xA;the event from sales should arrive first and then the finance event, but in practice it can be the other way around.</p>&#xA;&#xA;<p>There are a number of solutions here, but I've never liked the graphical ones. As a .NET developer I've used K2 and Windows Workflow Foundation in the past, but the solutions most flexible are created in code, not via a graphical interface.</p>&#xA;&#xA;<p>I currently would use NServiceBus or MassTransit for this. On a sidenote, I currently work at Particular Software and we make NServiceBus. NServiceBus has Sagas for this kind of work (<a href=""http://docs.particular.net/nservicebus/sagas/"" rel=""nofollow noreferrer"">documentation</a>) and you can also read on my weblog about a <a href=""http://dennis.bloggingabout.net/2016/03/17/nservicebus-sagas-presentation/"" rel=""nofollow noreferrer"">presentation</a>, incl. code on GitHub.</p>&#xA;&#xA;<p>The term <code>saga</code> is kind of loaded, but it basically handles long running (business) processes. Gregor Hohpe calls it a <code>Process Manager</code> (<a href=""http://www.enterpriseintegrationpatterns.com/patterns/messaging/ProcessManager.html"" rel=""nofollow noreferrer"">link</a>).</p>&#xA;&#xA;<p>To summarize what sagas do : they are instantiated by incoming messages and have state. Incoming messages are bound/dispatched to a specific saga instance based on a correlationid, for example a <code>customer id</code> or <code>order id</code>. Once the message (event) is processed, state is stored until a new message arrives, or until the code marks the saga as completed and the state is removed from storage.</p>&#xA;&#xA;<p>As said, in the .NET world MassTransit and NServiceBus support this, but there are most likely alternatives in other environments.</p>&#xA;"
37876524,37864491,69798,2016-06-17T08:19:43,"<p>It really depends on what you're referring to.</p>&#xA;&#xA;<p><strong>Composite UI</strong></p>&#xA;&#xA;<p>In the user interface you should build a composite ui. A (micro)service should be responsible for a vertical, instead of a horizontal layer. For example, business layer or data layers should be replaced by verticals like finance, sales, etc. and within these you can build even smaller components. These components are technically responsible for a business problem, from storage up to user interface. I mostly use a framework like AngularJS for this, where a part of a UI requests some data and various services can add to the data. For example a list of recommended books in Amazon. You start with the url which maps to a single product-id. You retreive book information from ServiceA, price from ServiceB, shipping costs from ServiceC, discounts from ServiceD, etc. There's also a list of recommended books. The list contains several product-ids from ServiceB (for example) and those result in multiple requests to ServiceA again for book information, like name &amp; image url.</p>&#xA;&#xA;<p>Now an invoice can be kind of the same, or an email. Create it as if it was a composite ui.</p>&#xA;&#xA;<p><strong>Integration</strong></p>&#xA;&#xA;<p>When you want to retrieve data to be able to communicate to an external system, for example, there's no user interface. It doesn't belong to finance or sales or whatever. Create a new boundary, a bounded context if you will, called IT/Ops, for example. Its responsibility is integration with third parties, for example. It owns this problem.</p>&#xA;&#xA;<p>It can then define several interfaces, like <code>IProvideBookInformation</code> and <code>IProvideBookPrice</code>. The <code>IProvideBookInformation</code> can have a method like <code>BookInfo ProvideBook(Guid id)</code> where <code>BookInfo</code> is a DTO that is also owned by this IT/Ops service.</p>&#xA;&#xA;<p>Then <code>Sales</code> and <code>Finance</code> are responsible for implementing this interface. So they have a dependency on this interface. Then deploy this any way you like, in .NET world you could use NuGet, for example. Then upon deployment of this IT/Ops service you also deploy the components from other services that implement these interfaces. It's like the Composite UI example, where a website is deployed with several other components that provide the data for the user interface. But now it's a backend integration service, instead of something with a user interface. The IT/Ops service has no direct dependencies on the implementation. But when it needs implementations of the service, it loads up all components it can find and searches for implementations of its required interface. Once it finds the implementation, it executes it and gets data in return. The components might go directly to the database, which is simple and we all like simple. But it might also request data via some REST api or whatever you like best. It gathers all data this way, via the interfaces it is the owner of, but implementations provided by other services. Once all data is gathered, it calls out to the third party and does whatever it is supposed to do.</p>&#xA;"
37876587,37864255,69798,2016-06-17T08:24:10,"<p>Have another microservice that's not so much responsible for a business problem, but for a non-functional business problem : security.</p>&#xA;&#xA;<p>This microservice is logical (as all microservices should be logically responsible for a business problem) and isn't deployed on its own, but rather deployed with other microservices. Then build a proper API that both microservice A and B are aware of and are mandatory to execute, before accepting any calls or executing calls.</p>&#xA;&#xA;<p>Where other microservices should be part of some business related boundary (bounded context, if you will) the security microservice is within the boundary of a non-functional requirement. You could call this IT/Ops or Devops or something.</p>&#xA;"
37828764,37806275,69798,2016-06-15T07:27:30,"<p>Use as little layers as possible, so don't do a REST API on top of MySQL if it's not needed. Keep it as simple as possible.</p>&#xA;&#xA;<p>That being said, try to avoid RPC between microservices. Use messaging instead so that the services can be autonomous. We do not want them to be dependent on a service that might not be running or anything. Define bounded contexts and have microservices live within these bounded contexts, making them the owner of a specific business issue.</p>&#xA;&#xA;<p>From that point on you can choose whatever storage, protocols, security, etc. you require for every single microservice. It's probably easier for the messaging system to choose a single option. Although not required, it makes sure you do not need to implement gateways everywhere to convert from one to the other.</p>&#xA;"
40838311,40837178,6173350,2016-11-28T06:28:58,"<p>You can create a parent project then add the child projects as modules. You can easily comment or delete these modules before building the project.</p>&#xA;&#xA;<p>Here is a basic example.</p>&#xA;&#xA;<p><a href=""http://www.concretepage.com/build-tools/maven/parent-pom-child-pom-example"" rel=""nofollow noreferrer"">http://www.concretepage.com/build-tools/maven/parent-pom-child-pom-example</a></p>&#xA;&#xA;<p>You can get a good idea about parent child maven projects from the following thread as well.</p>&#xA;&#xA;<p><a href=""https://stackoverflow.com/questions/19395973/maven-adding-a-reference-to-a-parent-pom-project"">Maven: adding a reference to a parent pom project</a></p>&#xA;"
41673406,41672971,6173350,2017-01-16T09:44:27,"<p>Run the filter to add Header to request after <code>org.springframework.security.web.authentication.UsernamePasswordAuthenticationFilter</code> in your security config file.</p>&#xA;&#xA;<pre><code>@Override&#xA;protected void configure(HttpSecurity http) throws Exception {&#xA;    http.exceptionHandling().and().anonymous().and().servletApi().and().headers().and().authorizeRequests()&#xA;&#xA;    // Your existing security configurations&#xA;&#xA;    // Assuming UsernamePasswordAuthenticationFilter is the filter &#xA;    .addFilterAfter(new AddHeaderFilter(), UsernamePasswordAuthenticationFilter.class);&#xA;}&#xA;</code></pre>&#xA;&#xA;<p><code>UsernamePasswordAuthenticationFilter</code> is the enbuild spring filter which runs for spring security authentication.</p>&#xA;"
40764867,40760397,3950758,2016-11-23T12:41:29,"<p>I guess that is because of the changes made in strucutre of the jar file from Spring Boot 1.4: <a href=""https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-1.4-Release-Notes"" rel=""nofollow noreferrer"">Release Notes</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>The layout of executable jars has changed. If you are using Spring&#xA;  Boot’s Maven, Gradle, or Ant support to build your application this&#xA;  change will not affect you. If you are building an executable archive&#xA;  yourself, please be aware that an application’s dependencies are now&#xA;  packaged in BOOT-INF/lib rather than lib, and an application’s own&#xA;  classes are now packaged in BOOT-INF/classes rather than the root of&#xA;  the jar.</p>&#xA;</blockquote>&#xA;"
38577041,38576865,2116518,2016-07-25T20:39:13,"<p>When you render a template in Rails, nothing restricts you in terms of where the markup comes from. Regularly it would come from a locally found <code>*.html.erb</code> template.</p>&#xA;&#xA;<p>When a pre-rendered html piece comes from an external source you can do this:</p>&#xA;&#xA;<pre><code>render html: my_html_string.html_safe&#xA;</code></pre>&#xA;&#xA;<p>If you want a layout, then:</p>&#xA;&#xA;<pre><code>render html: my_html_string.html_safe, layout: 'my_layout'&#xA;</code></pre>&#xA;&#xA;<p>The <code>my_html_string</code> could be served by the external source either with html format renderer or as a string to be later serialized to e.g., json.</p>&#xA;&#xA;<pre><code>@my_html_string = render_to_string(&#xA;  template: 'templates/my_template.html.erb',&#xA;  layout: false&#xA;)&#xA;</code></pre>&#xA;"
49535308,49393058,2972326,2018-03-28T13:00:44,"<p>After some search I have found, that <a href=""https://github.com/EasyNetQ/EasyNetQ"" rel=""nofollow noreferrer"">EasyNetQ</a> (SubscribeAsync) does exactly what I need. This library makes using Rabbit MQ a lot easier.</p>&#xA;"
43756153,43753039,1020838,2017-05-03T09:28:39,"<p>Fabric8 is a tool that helps you building cloud ready applications whereas Openshift will help you deploying and managing those applications.&#xA;You can deploy Fabric8 locally on the cloud or on Openshift.</p>&#xA;&#xA;<p><a href=""https://fabric8.io/"" rel=""nofollow noreferrer"">Fabric8</a> is Open-Source and the correspondent supported product is <a href=""https://openshift.io/"" rel=""nofollow noreferrer"">Openshift.io</a>&#xA;<a href=""https://www.openshift.org/"" rel=""nofollow noreferrer"">Openshift Origin</a> is the Open-Source product, and <a href=""https://www.openshift.com/"" rel=""nofollow noreferrer"">Openshift Container Platform</a> is the supported product.</p>&#xA;&#xA;<p>Hope that helps</p>&#xA;"
40543462,40539447,1228454,2016-11-11T07:53:20,"<p>Go with option 1. Use <a href=""https://docs.docker.com/compose/"" rel=""nofollow noreferrer"">Docker Compose</a> for setting up your containers:</p>&#xA;&#xA;<ul>&#xA;<li>One ""service"" for your Kafka consumer.</li>&#xA;<li>One ""service"" for your REST API process.</li>&#xA;<li>If you want to containerize your database, add a Cassandra container for that as well.</li>&#xA;</ul>&#xA;&#xA;<p>Using Docker Compose will allow you to spin up things together with one command, you can have dependencies and links (DNS name resolution) between your containers, centralized logging, etc. - it's ideal for cases like yours.</p>&#xA;&#xA;<p>Separating the containers will allow you to scale, to control the lifecycle of your applications, and it will allow you start/stop/update each application individually. Also, you only need to run a single process per container, which is a proven and recommended best practice. It makes controlling the lifecycle of the container and the application easier, and it also keeps your container lean and easier to manage.</p>&#xA;&#xA;<p>Example: What do you do if your Kafka listener goes down and the REST API keeps running? To fix this, you have to restart the whole container (unless you want to SSH into the container and restart one of the processes). One process per container makes this trivial - you restart just that container.</p>&#xA;&#xA;<p>The fact that both are pointing to the same database is irrelevant - that is just something you'll have to live with if both services use the same data. The alternative would be to synchronize between two databases (one that the Kafka listener writes to, and one for the REST API). This would add more complexity and overhead. If you do a clean design, you can still add that later if you see a value in separating the data - I wouldn't worry about that initially.</p>&#xA;"
38614586,38614201,1558694,2016-07-27T13:37:55,"<p>Sounds like you think about splitting services according to entities, that's not really a good idea.</p>&#xA;&#xA;<p>In the example you gave you could go with either approach or store them separately from both, or duplicate. It depends on what you're trying to do, how do you access that information and how your services interact with each other. Fast solutions, without understanding full context can cause you trouble in the future, so better spend some time now and avoid it.</p>&#xA;&#xA;<p>Explaining what Microservices are and how to approach building them might take a whole book or two, so I encourage you to start with just that <a href=""http://shop.oreilly.com/product/0636920033158.do"" rel=""nofollow"">Microservices by Sam Newmann</a>.</p>&#xA;&#xA;<p>I've recommended that book before, it's a very good intro to the subject. It answers your questions in the first chapters.</p>&#xA;"
37783829,37715757,1558694,2016-06-13T07:19:40,"<p>The microservices should be built around the bounded contexts, which might align with what you call features here, but also might not ;) Finding the boundaries is a tricky and difficult task, and close to impossible to do correctly without having the full context of your system. Other people can offer tips or share insights from their own experience, but won't be able to give you the answer whether your model is correct or not. Also don't worry about ""correct"" too much, for sure there are a few ways you could structure your code and all of them could be useful. So focus on making the model useful and addressing any smells (e.g. code becoming hard to maintain, too chatty communication, etc.).</p>&#xA;&#xA;<p>Sam Newman in <a href=""http://shop.oreilly.com/product/0636920033158.do"" rel=""nofollow"">his book</a> advises to start with coarse-grained bounded contexts, which could be then split into smaller ones if needed in the future. He also has a very good chapter that talks about splitting monoliths into services. Even if you're working on a greenfield project, I recommend you have a look at his book, and other people's posts and talks on breaking monoliths into microservices. In my opinion this is the best way to get a feel how to go about finding those boundaries, what mistakes people make and how to get it right.</p>&#xA;&#xA;<p>I also recommend to see ""Modular monoliths"" presentation by Simon Brown. Microservices are not the only option for modularization and Simon shares useful insights there.</p>&#xA;"
48263018,45579511,4186022,2018-01-15T12:26:16,"<p>I have published a prototype on Github that shows how you could achieve the routing using a <code>Zuul Gateway</code>. This prototype just shows how you can route traffic based on a cookie to different instances of the same application. You can do the routing based on any other criteria.&#xA;You should also have a look at <code>Spring Cloud Gateway</code> as an alternative to <code>Zuul</code>. Seems to be very promising.&#xA;<a href=""https://github.com/adiesner/spring-boot-sample-ci-gateway"" rel=""nofollow noreferrer"">https://github.com/adiesner/spring-boot-sample-ci-gateway</a></p>&#xA;&#xA;<p>A more simple setup would be to just add nginx in front of your service and use the split_clients method.</p>&#xA;&#xA;<pre><code>http {&#xA;    # ...&#xA;    # application version 1a&#xA;    upstream version_1a {&#xA;        server 10.0.0.100:3001;&#xA;        server 10.0.0.101:3001;&#xA;    }&#xA;&#xA;    # application version 1b&#xA;    upstream version_1b {&#xA;        server 10.0.0.104:6002;&#xA;        server 10.0.0.105:6002;&#xA;    }&#xA;&#xA;    split_clients ""${arg_token}"" $appversion {&#xA;        95%     version_1a;&#xA;        *       version_1b;&#xA;    }&#xA;&#xA;    server {&#xA;        # ...&#xA;        listen 80;&#xA;        location / {&#xA;            proxy_set_header Host $host;&#xA;            proxy_pass http://$appversion;&#xA;        }&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p><a href=""https://www.nginx.com/blog/performing-a-b-testing-nginx-plus/"" rel=""nofollow noreferrer"">https://www.nginx.com/blog/performing-a-b-testing-nginx-plus/</a></p>&#xA;"
40934717,40932850,2683814,2016-12-02T14:39:13,"<p>You can set the last modified date millis to max milliseconds before comparsion This will technically negate the milliseconds offset.</p>&#xA;&#xA;<pre><code>public static Date setMaxMillis(Date day,Calendar cal) {&#xA;    cal.setTime(day);&#xA;    cal.set(Calendar.MILLISECOND, cal.getMaximum(Calendar.MILLISECOND));&#xA;    return cal.getTime();&#xA;}&#xA;&#xA;Query query = new Query(where(""id"").is(filter.getId()));&#xA;Criteria criteria = Criteria.where(""lastModified"").gt(setMaxMillis(filter.getLastModified(),Calendar.getInstance()));&#xA;query.addCriteria(criteria);&#xA;return mongoTemplate.findOne(query, MyDocument.class);&#xA;</code></pre>&#xA;"
50453226,43983286,2636199,2018-05-21T16:44:45,"<p>My company has been using GraphQL in production for about a year. Maintaining the schemas in our ""Platform API"" and also in our microservices became arduous. Developers kept asking us why they needed to do double work and what the benefit was. Especially since we required in-depth code reviews to change/update the production GraphQL schema</p>&#xA;&#xA;<p>Apollo GraphQL released <a href=""https://www.apollographql.com/docs/graphql-tools/schema-stitching.html"" rel=""nofollow noreferrer"">schema stitching</a> which has solved most of the problems we were having. Essentially individual microservices each maintain their own GraphQL endpoint, then our Node.js Platform API stitches them all together. The resulting API is a client developer's dream, and the backend developers get the level of autonomy about their code they're used to. I highly recommend trying schema stitching. We've been adopting it incrementally for a few months and it's been wonderful.</p>&#xA;&#xA;<p>As an added benefit, while defining our sub-schemas we started decoupling certain microservices, instead relying on the stitched data extensions to fill in holes in objects. Feels like the missing piece in DDD</p>&#xA;"
29058201,28581644,2140327,2015-03-15T07:16:39,<p>You need some kinda throttling on the service not to flood Kafka. One of the option would be to use Apigee. </p>&#xA;
44669114,44651737,520359,2017-06-21T07:10:36,"<p>There are many options here.</p>&#xA;&#xA;<p>One of these is that you may use <code>spring-security-oauth2</code> in combination with spring security. In this setup you will distinguish two kind of applications:</p>&#xA;&#xA;<ul>&#xA;<li>Authorization service (AS) - this service issues and verifies access tokens for authenticating clients (e.g. using password grants or authorization codes);</li>&#xA;<li>Resource service (RS) - it exposes a REST API for CRUD operations on resources, for example. The endpoints are protected. The RS communicates with the AS in order to grant access; </li>&#xA;</ul>&#xA;&#xA;<p>In a resource server you will use and configure the Spring Security as you would normally do in a stand alone application. Below is one very simple example.</p>&#xA;&#xA;<p>Here is the code in a REST controller with protected endpoints:</p>&#xA;&#xA;<pre><code>@PreAuthorize(""hasRole('READ_BOOK')"")&#xA;@GetMapping(value = ""/books/{id}"")&#xA;public ResponseEntity&lt;Book&gt; getBook(@PathVariable(""id"") String id) {&#xA;    ...&#xA;    //retrieves book details if you have role READ_BOOK&#xA;}&#xA;&#xA;@PreAuthorize(""hasRole('WRITE_BOOK')"")&#xA;@PostMapping(value = ""/books"")&#xA;public Book createBook(@RequestParam(""title"") String title) {&#xA;    ...&#xA;    //creates a book if you have role WRITE_BOOK&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>The security configuration will look just as if you are writing a monolithic application with the exception that:</p>&#xA;&#xA;<ul>&#xA;<li>you will add an <code>@EnableResourceServer</code> annotation on your configurations;</li>&#xA;<li>you will configure an instance of <code>RemoteTokenServices</code> - the task of this service is to verify the provided tokens against the authorization server and fetch user roles;</li>&#xA;</ul>&#xA;&#xA;<p>The AS will issue access tokens based on some <code>OAuth2</code> workflow. These tokens will be used by the client to access the protected endpoints.</p>&#xA;&#xA;<p>I've made a small PoC (proof of concept) setup in which I created two simple apps showing the whole thing in action. Please find it <a href=""https://github.com/luchob/oauth2-passwordgrant-example"" rel=""nofollow noreferrer"">here</a> and feel free to submit questions, proposals and PRs. The complete source code is included bundled with more explanations.</p>&#xA;&#xA;<p>Please note though that this setup in certain cases may cause too much inter-service communication. I have seen a more complicated setup in which the API gateway acts as a resource server and if the user has enough credentials it enriches the request with the necessary details and passes the request to downstream services which basically trust the gateway.</p>&#xA;"
44691651,44687892,520359,2017-06-22T06:23:52,"<p>In my understanding one of the basic purposes of the password grant is to provide a seamless migration path to OAuth2 for applications which store and collect user names and passwords. And there are plenty of those. </p>&#xA;&#xA;<p>Also as written in this <a href=""https://aaronparecki.com/oauth-2-simplified/"" rel=""nofollow noreferrer"">nice article</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Since this obviously requires the application to collect the user's&#xA;  password, it must only be used by apps created by the service itself.&#xA;  For example, the native Twitter app could use this grant type to log&#xA;  in on mobile or desktop apps.</p>&#xA;</blockquote>&#xA;&#xA;<p>In your case using OAuth2 and the password grant will still make some sense to me. That is mostly because there is lots of ready to use OAuth2 infrastructure (I mean the Spring OAuth2 here) in your tech stack. This infrastructure fits seamlessly in a microservice architecture and this will be the easiest and fastest way to wire all things together. But ideally web apps should use the Authentication Code grant and you should redirect the users to a special place where they authenticate (e.g. the Zuul gateway may redirect them to a page delivered by the AS). This is a bit more elaborate though and you may go for the password grant especially if you do not plan to connect other apps.</p>&#xA;&#xA;<p>As for your second question - FB, Google, LinkedIn - they remember the authorized apps. For example in Facebook - navigate to <code>Settings - Apps</code> and see which apps are authorized to access your data:</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/D874W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D874W.png"" alt=""Facebook""></a></p>&#xA;&#xA;<p>In LinkedIn - navigate to <code>Account - Parnters and Third parties - Permitted Services</code>. E.g. this setup gives access to user data to HackerRank:</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/FfN1m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FfN1m.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>If as a user you delete these apps you will be asked to authorize them once again.</p>&#xA;"
45222591,45212821,520359,2017-07-20T18:28:18,"<p>The microservice architecture is not a trivial area so I would suggest to you that you start with some theory first. One of the books which is often referred here and there is <a href=""http://shop.oreilly.com/product/0636920033158.do"" rel=""nofollow noreferrer"">Building Microservices</a> By Sam Neuman. I highly recommend reading it or at least a part of it. This is the theoretical part.</p>&#xA;&#xA;<p>Then for some hands on experiences you may want to clone/fork the <a href=""https://github.com/sqshq/PiggyMetrics"" rel=""nofollow noreferrer"">PiggyMetrics</a> project. This is an educational project but at the same time it contains quite a lot of patterns and advanced stuff. </p>&#xA;&#xA;<p>After that you will be able to answer your own questions yourself, albeit there will be much more to ask ;-)</p>&#xA;&#xA;<p>Good luck!</p>&#xA;"
43029054,30648096,393896,2017-03-26T13:01:24,"<p>Being in similar situation I am looking to adopt <a href=""https://readthedocs.org/"" rel=""nofollow noreferrer"">https://readthedocs.org/</a> with GIT backed.</p>&#xA;"
46569419,46564574,7750824,2017-10-04T16:06:31,"<p>@tarun lalwani, thanks for the update. I removed the -d --build option and the docker service ran successfully.</p>&#xA;"
45479696,44610425,17832,2017-08-03T09:08:17,"<p>Your diagram makes little sense. The incoming call is not 200 until it returns to the user successfully, after all internal services are called.</p>&#xA;&#xA;<p>If the authentication between B and C is internal (server to server auth), then you have an internal error, and 502 is a sane choice to return to A. Of course, you might decide to retry in server A, as you got a 502 from B, but it's pointless because it's an expired token. So you may decide as policy that internal 401s should be escalated back to A. Or you may find attaching metadata in the 502 error response body assists a retrying mechanism. Anyway, server-server auth shouldn't be failing where it is a valid call.</p>&#xA;&#xA;<p>So ... if C's authentication is working on the user's supplied token, then the user's authentication ran out during the call (rare, but happens) - in this case the token should have been extended elsewhere in the system prior to this call (probably in A's call to SSO). But it wasn't, so return 401 to where-ever in the application redirects to the login page.</p>&#xA;"
50428902,39615381,1335720,2018-05-19T19:47:47,"<p>Peter has done a great summary. And here are my additional points:</p>&#xA;&#xA;<ol>&#xA;<li><p>Cloud Service is not designed for micro service pattern, while Service Fabric is. If you want to enjoy the benefits brought by micros service, Service Fabric is your best choice.&#xA;With Cloud Service, if you want separate your application into autonomous services, you either </p>&#xA;&#xA;<ul>&#xA;<li>Create multiple cloud services. Which is difficult to monitor and manage since there is not a unified interface for a group of cloud services, Cloud Service is just not designed for this pattern. </li>&#xA;<li>Or add multiple roles into a single cloud service, this will lead to a) bloat of your cloud service configuration file, because all service configurations are in a single config file; and b) to upgrade a single role, you end up redeploy the whole cloud service! </li>&#xA;</ul></li>&#xA;<li><p>Cloud Service doesn't support cross region/DC deployment, while Service Fabric does. That means you can turn a DC level disaster recovery into a normal failover, which automatically handled by Service Fabric, see <a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-disaster-recovery#failures-of-a-datacenter-or-azure-region"" rel=""nofollow noreferrer"">this</a>.</p></li>&#xA;</ol>&#xA;"
35379538,35379246,5738152,2016-02-13T11:57:52,"<p>Django models is just a Python module, of course Django does a lot of 'magic' in other modules (i.e. forms, admin) using introspection, but the models itself could be used separately if desired.</p>&#xA;&#xA;<p>So there's no problem... If you use different ""Django projects"" for each project you named (on same machine or different), then just make your models as separate ""Django app"" and set each project to use it. If each project named by you is ""Django app"" (in this case on same machine obviously) then just import your models module in each ""project"" and use it. Even if you don't use Django, say your ""API project"" is based on Flask, you can still import Django models in Flask modules.</p>&#xA;&#xA;<p>Behind the scene of Django models there's the database, which doesn't care from where it is queried, and Django just provides a convenient way to use the DB via Python classes. So if the models from different apps or projects are set to use the same DB, then they will be 'shared'.</p>&#xA;"
51888028,51149979,6269864,2018-08-17T03:33:12,"<p>You should consider communicating using queues. When one function finishes, it puts the results into the Azure Storage Queue, which is picked up by another function. Therefore there is no direct communication between functions unless it's necessary to trigger the other function.</p>&#xA;&#xA;<p>In other words, it may look like this</p>&#xA;&#xA;<blockquote>&#xA;  <p>function1 ==> queue1 &lt;== function2 ==> queue2 &lt;== function 3 ==> somewhere else, e.g. storage</p>&#xA;</blockquote>&#xA;"
51752999,51734131,4589993,2018-08-08T18:02:09,"<p>not sure if i understand right.. but if u just want people to search the publications entity, then creating a view with the fields needed would be my first solution. &#xA;can u elaborate more why exactly u thought of elasticsearch ?</p>&#xA;&#xA;<p>second:&#xA;i dont think you should worry about duplications when it comes to elasticsearch.</p>&#xA;"
29889356,29888108,3674793,2015-04-27T07:10:41,"<p>I typically don't bother with extra client Libraries like RestSharp. I feel that the purpose of REST is to stay as close to gold old HTTP as possible, negating the need for anything other than HttpWebRequest/Response. Working with the request/responses directly gives great control and encourages you to think about what's actually going on instead of abstracting everything away like you would with a traditional WCF or ASMX service.</p>&#xA;&#xA;<p>For microservices I've built in the past I've kept the request and response objects within separate libraries and I've the distributed the source to other Developers within my organisation to give them a leg up in calling the service but it probably wouldn't be practical for external consumers; again I guess the point of going for a microservice over a full scale WCF service is that by their nature the request/responses being passed around are small and simple. I also felt a little uncomfortable with this practice at the start; however when I started getting really responsive web apps calling microservices with javascript (usually jquery) just as easily as traditional .NET ones I started seeing the potential for some really good integration of our internal systems. Eventually our intranets were providing actions and views into business applications that weren't possible previously.</p>&#xA;&#xA;<pre><code>HttpWebRequest webRequest = WebRequest.Create(""http://localhost:51467/api/email/send"") as HttpWebRequest;&#xA;webRequest.Method = ""POST"";&#xA;webRequest.Credentials = CredentialCache.DefaultCredentials; //or account you wish to connect as&#xA;webRequest.PreAuthenticate = true;&#xA;webRequest.ContentType = ""application/json""; // or xml if it's your preference&#xA;&#xA;string jsonData = Newtonsoft.Json.JsonConvert.SerializeObject(requestObject);&#xA;&#xA;using (StreamWriter streamWriter = new StreamWriter(webRequest.GetRequestStream()))&#xA;{&#xA;    streamWriter.Write(jsonData);&#xA;    streamWriter.Flush();&#xA;    streamWriter.Close();&#xA;}&#xA;&#xA;HttpWebResponse webResponse = webRequest.GetResponse() as HttpWebResponse;&#xA;&#xA;if (webResponse.StatusCode != HttpStatusCode.Accepted)&#xA;    throw new ApplicationException(""Unexpected Response Code. - "" + webResponse.StatusCode);&#xA;&#xA;string response;&#xA;using (System.IO.StreamReader readResponse = new System.IO.StreamReader(webResponse.GetResponseStream()))&#xA;{&#xA;    response = readResponse.ReadToEnd();&#xA;}&#xA;&#xA;//swap out for regular xml serializer if you've used xml&#xA;dynamic responseObject = Newtonsoft.Json.JsonConvert.DeserializeObject&lt;dynamic&gt;(response);&#xA;</code></pre>&#xA;&#xA;<p>Also another tip, if you're working with web api, I'd really suggest adding in the web api help pages and test client. You won't have the automatically generated wsdl you get with WCF and ASMX but you can get some really nice documentation about your microservice for other developers (even better in my opinion that auto generated proxy classes) and a test harness that lets to exercise the service from your browser</p>&#xA;&#xA;<p><a href=""https://github.com/wuchang/WebApiTestClient"" rel=""nofollow"">https://github.com/wuchang/WebApiTestClient</a>&#xA;<a href=""https://www.nuget.org/packages/Microsoft.AspNet.WebApi.HelpPage/"" rel=""nofollow"">https://www.nuget.org/packages/Microsoft.AspNet.WebApi.HelpPage/</a></p>&#xA;"
50400493,50400384,2267817,2018-05-17T21:05:51,"<p>The main issue is coupling. <a href=""https://samnewman.io/books/building_microservices/"" rel=""nofollow noreferrer"">Sam Newman, author of Building Microservices</a> puts it well:</p>&#xA;&#xA;<blockquote>&#xA;  <p>In general, I dislike code reuse across services, as it can easily&#xA;  become a source of coupling. Having a shared library for serialisation&#xA;  and de-serialisation of domain objects is a classic example of where&#xA;  the driver to code reuse can be a problem. What happens when you add a&#xA;  field to a domain entity? Do you have to ask all your clients to&#xA;  upgrade the version of the shared library they have? If you do, you&#xA;  loose independent deployability, the most important principle of&#xA;  microservices (IMHO).</p>&#xA;  &#xA;  <p>Code duplication does have some obvious downsides. But I think those&#xA;  downsides are better than the downsides of using shared code that ends&#xA;  up coupling services. If using shared libraries, be careful to monitor&#xA;  their use, and if you are unsure on whether or not they are a good&#xA;  idea, I'd strongly suggest you lean towards code duplication between&#xA;  services instead.</p>&#xA;</blockquote>&#xA;&#xA;<p><a href=""https://samnewman.io/blog/2015/06/22/answering-questions-from-devoxx-on-microservices/"" rel=""nofollow noreferrer"">https://samnewman.io/blog/2015/06/22/answering-questions-from-devoxx-on-microservices/</a></p>&#xA;"
39451607,39450504,1897897,2016-09-12T13:34:47,<p>You can put multiple functions in one serverless.yml</p>&#xA;&#xA;<pre><code>/src&#xA;-- event.json&#xA;-- users.handler.js&#xA;-- products.handler.js&#xA;-- serverless.yml&#xA;</code></pre>&#xA;
50329049,50328886,2422185,2018-05-14T11:20:30,"<p>In my opinion, running nginx in a separate pod is a better option because that way you can scale up and down application separately from a proxy.  Usually, we use one container with proxy and few with API. </p>&#xA;"
44315281,44274982,2498986,2017-06-01T19:06:48,"<p>The timeout can be set using the <code>connectionTimeout</code> property of Tomcat. </p>&#xA;&#xA;<p>Please refer this answer how to set it for Tomcat.</p>&#xA;&#xA;<p><a href=""https://stackoverflow.com/questions/42849209/configuring-maxkeepaliverequests-in-spring-boot-embedded-tomcat"">Configuring maxKeepAliveRequests in Spring Boot embedded Tomcat</a></p>&#xA;"
50402658,50401105,1196458,2018-05-18T01:43:52,"<p>If your micro services are not exposed to the web , then you can create Self Signed Certs for that purpose .The Self signed certs are used for intra calls.You can create them on sslshopper site.</p>&#xA;&#xA;<p>Also, dont use the same certs for all micro services.&#xA;I have implemented the similar solution , in our case we have set up Signed Service as another micro service which will cater to other micro services.This Self Signed Service will connect to HSM or Hardware Security modules.All the Certs are stored inside the HSMs belonging to the profiles.You can do some googling on HSM and it profile creation.</p>&#xA;&#xA;<p>Trust stores are used for keeping public certs and key stores are used for storing private keys .Thats the standard, not universal but in ideal scenario's.</p>&#xA;"
36629825,36627628,4343722,2016-04-14T17:18:24,"<p>This question is a little broad, but I think I can at least point you in the right direction from what I understand. First, let me start by stating your problem as I understand it.</p>&#xA;&#xA;<blockquote>&#xA;  <p>You have multiple databases populated by their own microservice each. Each database contains similar information that you want to be able to search over (i.e. author, body, title, etc.) You want an elasticsearch cluster that has access to the data in all of those databases and can return a result that includes the correct database and document that matches a search.</p>&#xA;</blockquote>&#xA;&#xA;<p>Elasticsearch is very powerful when it comes to handling complicated cases like this. Since all of your data has a similar structure and fields you can just use one index with additional fields on it to store which DB the document comes from and the document ID from that DB. This will allow you to perform searches such as 'Give me every post made by William Shatner across these 3 social networks.'</p>&#xA;&#xA;<p>You will need several additional pieces of functionality to make this work. First, you need a mechanism for getting the data from the database into the search index. On my team, we use a separate IndexingService that knows how to read event streams and send the live data to the ES index. You just need to decide on an indexing strategy (i.e. how often do you update the index with new entries?). Secondly, you will need some logic on the client side to take the raw search result and retrieve the relevant entry from the database.</p>&#xA;&#xA;<p>This is just one way to solve your problem. If you instead want an approach that allows you to maintain a different index for each social network, but still has a central place you can search across them all I suggest looking into using Elasticsearch Tribe Nodes. Basically, it is a single place to submit a search that knows about every search cluster and how to interact with them to return a unified search result.</p>&#xA;&#xA;<p>The best way to learn elasticsearch is to just get a cluster up and running and start experimenting! Good luck!</p>&#xA;"
46523340,46311488,6703530,2017-10-02T09:57:15,"<p>Microservices are not a replacement for ESB.</p>&#xA;&#xA;<p><strong>Microservices</strong>&#xA;is a concept for development of backend systems, including it's API. The opposite of microservices approach is a monolith. If the API is consumed directly by the consuming systems, we come to point-to-point (spaghetti) integration. If the consumer uses a middleware, often called API gateway we can have centralized visibility, security and tracing (as with ESB). API gateways are simpler than ESBs and thus are better suited for horizontal scaling. API gateways should not include additional business/integration logic.</p>&#xA;&#xA;<p><strong>ESB</strong> does the same as API gateway (acts as a proxy), but in addition allows to include business logic, compose multiple services into one and other advanced functions. ESBs often grew into burdensome solutions with a big overhead and only little added value, and that's why they became hated.</p>&#xA;&#xA;<p><strong>Conclusion</strong></p>&#xA;&#xA;<p>ESB can be used together with microservices architecture, there are many companies which keep ESB simple and it is almost equal to so called API gateways. </p>&#xA;&#xA;<p><strong>My opinion</strong></p>&#xA;&#xA;<p>API gateways are introducing new functionalities and are getting more complex, coming closer to ESB.</p>&#xA;"
46539990,45998405,6703530,2017-10-03T08:11:21,"<p>Microservices architecture pattern applies to development of backend systems/services, whereas ESB (e.g. Oracle SOA Suite) is intended as an intermediary layer between consumers and backend services. Backend services contain rich application logic, whereas ESB services provide only intermediary functions like routing, transformation, orchestration etc. </p>&#xA;&#xA;<p>ESB is not intended for rich application logic, though it's possible to do that.</p>&#xA;&#xA;<p>Using ESB (e.g. Oracle SOA Suite) to host microservices is achievable, but you will get a big overhead comparing to limited functions and scalability. If you are looking for centralized API management (tracing, security etc.), you can put an API gateway into your architecture instead of full scale ESB.</p>&#xA;"
42727326,42608033,953770,2017-03-10T20:38:55,"<p>Disclosure: I am the CTO of Conjur.</p>&#xA;&#xA;<p>Consider the workflow:</p>&#xA;&#xA;<ol>&#xA;<li>A container of Service A starts up.</li>&#xA;<li>It generates a new key pair.</li>&#xA;<li>It submits the public key to the public key store.</li>&#xA;<li>The public key store receives the public key.</li>&#xA;<li>The public key store associates the public key that it has received with the identity ""service-a"".</li>&#xA;<li>Service A signs a request to Service B.</li>&#xA;<li>Service B asks the key store for the known public key(s) of Service A.</li>&#xA;<li>The key store provides the public key(s).</li>&#xA;<li>Service B verifies the signature matches one of the keys.</li>&#xA;</ol>&#xA;&#xA;<p>The public key store must be sure to:</p>&#xA;&#xA;<ul>&#xA;<li>Store the public keys in a tamper-proof way.</li>&#xA;<li>Associate each public key to a service identity in a way that is also tamper-proof.</li>&#xA;<li>Provide some way for administrators to manipulate the key store (e.g. deroll keys of dead containers).</li>&#xA;<li>Keep an audit record of everything that's happened.</li>&#xA;</ul>&#xA;&#xA;<p>But there's also another bit that is pretty hard. In step (5), when the key store receives the key for Service A, it needs to verify that the key is actually coming from Service A, and not from an imposter. In other words, it has to authenticate the request. </p>&#xA;&#xA;<p>How you do this depends on the details of your infrastructure. If you are using raw Docker (as opposed to say, Kubernetes), you can use an agent on the server to correlate the IP address of the container to the container list (<code>docker ps</code>) on the machine. This will tell you the image of the container, which should tell you the identity of the service.</p>&#xA;&#xA;<p>There is a lot of subtlety to this problem, and the solution is somewhat different for each container environment.</p>&#xA;"
43678849,43655652,4234922,2017-04-28T11:13:07,"<p>For getting the AWS EC2's Ip addresses you can try the following command which will fetch the AWS output to a file you desire:</p>&#xA;&#xA;<pre><code> aws cloudformation describe-stacks --stack-name IngCassandraCluster --query 'Stacks[].Outputs[].OutputValue' --output text --&gt; kunaltest.text&#xA;</code></pre>&#xA;&#xA;<p>you can add this line in your playbook under :shell:"" to make it do what you want.</p>&#xA;"
47474261,47465212,4323754,2017-11-24T13:23:37,"<p>I would say that option 1 is favourable. You are then keeping the services decoupled/independent. It also scales, as other future services can then subscribe to the same events.</p>&#xA;&#xA;<p>I don't think data replication is such an issue. I prefer <a href=""https://en.wikipedia.org/wiki/Domain-driven_design"" rel=""nofollow noreferrer"">DDD</a> approach whereby individual services control their own data in their own context. Services can then made as simple as they need to be to serve their specific purpose.</p>&#xA;"
34797992,34660949,1319595,2016-01-14T19:25:15,"<p>i'm not using meteorhacks:cluster but i am running microservices for my meteor app.  it's on DO, so the setup may be different, but here is how i'm doing it.</p>&#xA;&#xA;<p>i'm using <a href=""https://github.com/peerlibrary/meteor-reactive-publish"" rel=""nofollow"">reactive-publish</a> to help with server side reactivity </p>&#xA;&#xA;<pre><code>// client ------&#xA;/server/lib/ddp-setup.js&#xA;ContentLibrary = DDP.connect('10.123.455.332:3000')&#xA;&#xA;/server/publications.js&#xA;Content = new Mongo.Collection('content', {connection: ContentLibrary})&#xA;Meteor.publish('content', function(opts){&#xA;  check(opts, Object)&#xA;  this.autorun(()=&gt;{&#xA;    let sub = ContentLibrary.subscribe('content', opts)&#xA;    if( sub.ready() ){&#xA;      return Content.find({})&#xA;    }&#xA;  })&#xA;})&#xA;&#xA;// server1 @ 10.123.455.332:3000 -------&#xA;&#xA;/server/publications.js&#xA;Meteor.publish('content', function(opts){&#xA;  check(opts, Object)&#xA;  // do something with opts...&#xA;  return Content.find({})&#xA;})&#xA;</code></pre>&#xA;&#xA;<p>the idea is that your client is only ever talking to it's own server, but the server then talks with all of your other microservices.  this affords you the increased security of allowing servers to speak to each other on a private network (as i have my setup with digital ocean).</p>&#xA;&#xA;<p>letting servers talk to each other over the private network is security at its best and the network latency almost zero between servers.  setting it up like this also means that you only have to worry about sticky sessions between your client browser and the web-facing app/service.</p>&#xA;&#xA;<p>this may or may not answer your question, but hopefully it will give you some insight into setting up the architecture.</p>&#xA;"
45539606,45538292,5384363,2017-08-07T04:50:45,"<p>Kafka has built-in support for this scenario.</p>&#xA;&#xA;<p>You can create two <a href=""https://kafka.apache.org/documentation/#intro_consumers"" rel=""nofollow noreferrer"">Consumer Groups</a>, one for <code>B</code>, and the other for <code>C</code>. Both <code>Consumer Groups</code> subscribe messages from <code>A</code>.</p>&#xA;&#xA;<p>Any message published by <code>A</code> will be sent to both groups. However, only one member of each group can receive the message.</p>&#xA;"
42080820,42079952,5384363,2017-02-07T02:46:48,"<p>There are two kinds of messaging system: <code>pub-sub</code> and <code>queue</code>. For a <code>pub-sub</code> system, every client processes all events, while for a <code>queue</code> system, each event is processed by only one client. It seems <strong>what you need is a <code>queue</code>, NOT <code>pub-sub</code></strong>.</p>&#xA;&#xA;<p>You can use a Redis <code>LIST</code> to implement a simple messaging <code>queue</code>:</p>&#xA;&#xA;<pre><code>// producers push events into a queue&#xA;RPUSH que event1&#xA;RPUSH que event2&#xA;// push other events...&#xA;&#xA;// consumers pop events from the queue&#xA;BLPOP que timeout&#xA;</code></pre>&#xA;&#xA;<p><strong>NOTE</strong>: this is <strong>NOT</strong> a perfect solution, it has some reliable problems. However, there're some open-source solutions based on Redis, you don't need to reinvent the wheel.</p>&#xA;"
32839956,31104540,5387947,2015-09-29T09:02:35,"<p>Even for distributed transactions you can get into ""transaction in doubt status"" if one of the participants crashes in the midst of the transaction. If you design the services as idempotent operation then life becomes a bit easier. One can write programs to fulfill business conditions without XA. Pat Helland has written excellent paper on this called ""Life Beyond XA"". Basically the approach is to make as minimum assumptions about remote entities as possible. He also illustrated an approach called Open Nested Transactions (<a href=""http://www.cidrdb.org/cidr2013/Papers/CIDR13_Paper142.pdf"" rel=""nofollow"">http://www.cidrdb.org/cidr2013/Papers/CIDR13_Paper142.pdf</a>) to model business processes. In this specific case, Purchase transaction would be top level flow and loyalty and order management will be next level flows. The trick is to crate granular services as idempotent services with compensation logic. So if any thing fails anywhere in the flow, individual services can compensate for it. So e.g. if order fails for some reason, loyalty can deduct the accrued point for that purchase.</p>&#xA;&#xA;<p>Other approach is to model using eventual consistency using CALM or CRDTs. I've written a blog to highlight using CALM in real life - <a href=""http://shripad-agashe.github.io/2015/08/Art-Of-Disorderly-Programming"" rel=""nofollow"">http://shripad-agashe.github.io/2015/08/Art-Of-Disorderly-Programming</a> May be it will help you.</p>&#xA;"
45469932,45467995,6896299,2017-08-02T20:00:16,"<p>Your problem here lies in the <code>with microservice gateway</code>. This is not doing what you think it does.</p>&#xA;&#xA;<p>The <code>microservice</code> keyword is actually used to hint the generator that back-end files should be generated on the microservice and the corresponding front-end files should be generated on the gateway with the correct ""URL path prefix"". This way you can use the same jdl file for both the microservice and the gateway. On the microservice the keyword is ignored but on the gateway it will prevent back-end files from being generated as well as correctly setup the entity front end to call <code>/microservice/api</code>.</p>&#xA;&#xA;<p>What you want is simply to generate a regular entity on your gateway, so just remove the <code>with microservice gateway</code> line.</p>&#xA;&#xA;<p>If you think that our docs should be improved in this area please submit a PR to JHipster/JHipster.github.io.</p>&#xA;"
40438762,40429027,6896299,2016-11-05T13:26:05,<p>This is normal. The gateway will only route requests to services that pass all consul healthchecks.</p>&#xA;&#xA;<p>Note that by default Spring Boot automatically configure a healtcheck for your database. It is located at the  /management/health endpoint of your microservice.</p>&#xA;&#xA;<p>You could configure Consul health checks with the spring.cloud.consul keys of your microservice. I don't remember the specific ones but if you use IDEA it will autocomplete the available properties...</p>&#xA;
42660372,42630302,6896299,2017-03-07T23:35:32,"<p>You should not write the URL part in your configuration. As we can see on your gateway screenshot, the gateway already knows the service's IP thanks to Eureka and the Registry. IP addresses should never go into the config as those are determined dynamically.</p>&#xA;"
40746214,40735590,6896299,2016-11-22T15:51:13,<p>It is actually easy to achieve what you want :</p>&#xA;&#xA;<ul>&#xA;<li><p>Deploy two versions of your service with different instanceIds for example <code>service-v1</code> and <code>service-v2</code>. This is easy to achieve with setting <code>eureka.instance.instanceId</code>.</p></li>&#xA;<li><p>Initialy your Zuul Gateway is is proxying requests to service-v1 with the following properties present in your config server's gateway.yml:</p></li>&#xA;</ul>&#xA;&#xA;<blockquote>&#xA;<pre><code>zuul:&#xA;  routes:&#xA;    service:&#xA;      path: /service/**&#xA;      serviceId: service-v1&#xA;</code></pre>&#xA;</blockquote>&#xA;&#xA;<ul>&#xA;<li>Change the config server zuul configuration to this:</li>&#xA;</ul>&#xA;&#xA;<blockquote>&#xA;<pre><code>zuul:&#xA;  routes:&#xA;    service:&#xA;      path: /service/**&#xA;      serviceId: service-v2&#xA;</code></pre>&#xA;</blockquote>&#xA;&#xA;<ul>&#xA;<li>Then trigger a refresh of the gateway's config: </li>&#xA;</ul>&#xA;&#xA;<blockquote>&#xA;<pre><code>curl -X POST http://localhost:8080/management/refresh&#xA;</code></pre>&#xA;</blockquote>&#xA;&#xA;<p>(note that this endpoint is secured so you will need to pass it a token to pass through)</p>&#xA;
50565607,50563413,6896299,2018-05-28T11:42:29,"<p>One of the points of doing that is to implement the <a href=""https://samnewman.io/patterns/architectural/bff/"" rel=""nofollow noreferrer"">Backend for Frontend</a> pattern.&#xA;Basically you proxy all calls from your front-end to your microservices through the gateway. For convenience reasons the frontend is packaged together with the gateway.&#xA;JHipster just adapted the exisiting entity generators from monoliths to let users generate the backend and front-end of an entity separately on microservices and gateways respectively. Of course this is an optional feature and you are free to use it the way you want.</p>&#xA;"
46258582,46247154,3427243,2017-09-16T21:29:48,"<p>I ran your docker file by copying a simple shell script which prints ""hello"" on to console. It ran successfully.</p>&#xA;&#xA;<p>Also, it does not seem like your docker container is exiting. Docker is running your shell script and exiting gracefully. To verify, check the exit code of your docker run command by running the below command as soon as your docker run is finished.</p>&#xA;&#xA;<p><code>echo $?</code></p>&#xA;&#xA;<p>If the above command prints ""0"" on to the screen then docker did not fail.</p>&#xA;&#xA;<p>If you are expecting your shell script to run as a background daemon you need to make sure that your script does not exit. The reason why docker container is exiting as soon as your shell scriptspecified in CMD line your Dockerfile  is finished executing is because Docker's design philosophy is to run <a href=""https://devops.stackexchange.com/questions/447/why-it-is-recommended-to-run-only-one-process-in-a-container"">one process per container</a>. In your case that single process is your shell script. So start whatever service you are starting in your shell script as a foreground process and the container will keep running as long as that service is running. As soon as that service dies your container dies which is what you might want. </p>&#xA;&#xA;<p>Hope that helps.</p>&#xA;"
50984325,50971334,4287915,2018-06-22T08:59:17,"<p><strong><em>I solved it by adding $jwt_user to User construct to skip 'fillable':</em></strong></p>&#xA;&#xA;<p>auth.php</p>&#xA;&#xA;<pre><code>'defaults' =&gt; [&#xA;    'guard' =&gt; 'api',&#xA;],&#xA;'guards' =&gt; [&#xA;    'api' =&gt; [&#xA;        'driver' =&gt; 'jwt',&#xA;    ],&#xA;],&#xA;</code></pre>&#xA;&#xA;<p>AuthServiceProvider.php</p>&#xA;&#xA;<pre><code>use App\User;&#xA;use \Firebase\JWT\JWT;&#xA;&#xA;public function boot()&#xA;    {&#xA;        $this-&gt;registerPolicies();&#xA;&#xA;        Auth::viaRequest('jwt', function ($request) {&#xA;            $publicKey = file_get_contents(storage_path('oauth-public.key'));&#xA;&#xA;            if(!$hasAuthHeader = $request-&gt;header('Authorization')?true:false){&#xA;                return null;&#xA;            }&#xA;&#xA;            preg_match('/Bearer\s((.*)\.(.*)\.(.*))/', $request-&gt;header('Authorization'), $jwt);&#xA;&#xA;            try {&#xA;                $res                        = JWT::decode($jwt[1], $publicKey, array('RS256'));&#xA;                $jwt_user                   = json_decode(json_encode($res-&gt;user), true);&#xA;                $local_user                 = User::find($jwt_user['id']);&#xA;                $jwt_user['local_profile']  = $local_user?$local_user:[];&#xA;                $user                       = new User([], $jwt_user);&#xA;                return $user;&#xA;            } catch (\Exception $e) {&#xA;                return null;&#xA;            }&#xA;        });&#xA;    }&#xA;</code></pre>&#xA;&#xA;<p>User.php</p>&#xA;&#xA;<pre><code>public function __construct(array $attributes = array(), $jwt_user = array())&#xA;    {&#xA;        parent::__construct($attributes);&#xA;&#xA;        foreach($jwt_user as $k=&gt;$v){&#xA;            $this-&gt;$k = $v;&#xA;        }&#xA;    }&#xA;</code></pre>&#xA;"
50275178,50217813,9603499,2018-05-10T14:23:12,"<p>Thanks for the answers they give a different perspectives, but since I have a scalla/play microservice, I added a special <a href=""https://www.playframework.com/documentation/2.5.x/AllowedHostsFilter"" rel=""nofollow noreferrer"">Playframework built-in http filter</a> in my application.conf and then allowing only the Kong gateway, now when trying to access my application by localhost:9000 I get denied, and that's absolutely what I was looking for.</p>&#xA;&#xA;<p>hope this answer gonna be helpful for future persons in this same situation.</p>&#xA;"
44815770,44803729,1035691,2017-06-29T03:11:30,"<p>The lifecycle of the server is decoupled from the apps it deploys, that was intentional.</p>&#xA;&#xA;<p>I'm not following your thoughts on how dataflow could connect the start of the queue, but from your description there's a few things you could do:</p>&#xA;&#xA;<p>You would need to modify the app in order to have it registered with eureka, but this is a very simple operation, no more than a few lines of code:</p>&#xA;&#xA;<ol>&#xA;<li><p>You can either start from a stream app perspective: <a href=""https://start-scs.cfapps.io/"" rel=""nofollow noreferrer"">https://start-scs.cfapps.io/</a> , select http source, your binder, and then add the spring-cloud-netflix library as well as <code>@EnableDiscoveryClient</code> at the Main boot class</p></li>&#xA;<li><p>Start with <a href=""http://start.spring.io"" rel=""nofollow noreferrer"">http://start.spring.io</a> Select Stream Rabbit or Stream Kafka, add Web and netflix libraries, then add the <code>@EnableDiscoveryClient</code> and <code>@EnableBinding</code> annotations and create a simple HTTP endpoint for your use case.</p></li>&#xA;</ol>&#xA;&#xA;<p>In any case should be a small addition.</p>&#xA;&#xA;<p>You can also open an issue at :<a href=""https://github.com/spring-cloud-stream-app-starters/http/issues"" rel=""nofollow noreferrer"">https://github.com/spring-cloud-stream-app-starters/http/issues</a> suggesting that we add @EnableDiscoveryClient to the http source app, we can take that in consideration on our next iteration as well.</p>&#xA;"
45186977,45174891,1035691,2017-07-19T09:51:40,"<p>It really depends, and I'm terrible sorry for starting an answer with that.</p>&#xA;&#xA;<p>Having a single channel with selectors is the simplest choice, but with the caveat that every single consumer will consume all messages from that destination. If this is your use case, then go for it.</p>&#xA;&#xA;<p>Another use case, would be an Event Sourcing type, where most of the consumers are only interested in a subset of events, and you perhaps would be better by placing Events (or better, aggregate roots) on each destination. This would allow you to scale better, and avoid unnecessary chattiness from the broker to consumers.</p>&#xA;&#xA;<p>In your example you could have something like this instead:</p>&#xA;&#xA;<pre><code>public interface Contracts {&#xA;&#xA;    @Output(""contract-creation"") MessageChannel creation();&#xA;    @Output(""contract-revogation"") MessageChannel revogation();&#xA;    @Output(""contract-termination"") MessageChannel termination();&#xA;&#xA;} &#xA;</code></pre>&#xA;&#xA;<p>That would create one Topic for each eventType, and perhaps is a bit overkill</p>&#xA;&#xA;<p>Perhaps you should create an interface <code>Event</code> with a <code>Type</code> and have your Events descend from it, and then have this instead:</p>&#xA;&#xA;<pre><code>public interface Events {&#xA;    @Output MessageChannel user();&#xA;    @Output MessageChannel contract();&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Now, all Contract events (creation,revogation,termination) would go to the same Destination. And on the receiving end you can create selectors to choose which one to apply:</p>&#xA;&#xA;<pre><code>@StreamListener(target = ""contract"", condition = ""payload.type=='created'"")&#xA;    public void contractCreated(@Payload ContractCreatedEvent){&#xA;&#xA;    }&#xA;&#xA;    @StreamListener(target = ""contract"", condition = ""payload.type=='terminated'"")&#xA;    public void contractTerminated(@Payload ContractTerminatedEvent){&#xA;&#xA;    }&#xA;</code></pre>&#xA;"
51178562,51173039,59563,2018-07-04T17:26:25,"<p>Assuming <code>OrdersAPI</code> is your Web API endpoint receiving requests from a browser, it would construct Azure Service Bus messages and send to a queue. Your Worker Role then would receive those messages and process. Processing would not be performed in Web API.</p>&#xA;"
50276745,50275156,59563,2018-05-10T15:46:56,"<blockquote>&#xA;  <p>Is it not the responsibility of the subscribing application to create it's own Subscription in Azure? e.g. on startup?</p>&#xA;</blockquote>&#xA;&#xA;<p>That's correct. The details depend on the underlying messaging service you're using. In case of Azure Service Bus, each service upon startup will subscribe to the events it's interested in. For example, <code>Ordering</code> will <a href=""https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.API/Startup.cs#L254"" rel=""nofollow noreferrer"">subscribe during startup to the events it handles</a>. The project has a <a href=""https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/BuildingBlocks/EventBus/EventBus/IEventBusSubscriptionsManager.cs"" rel=""nofollow noreferrer""><code>IEventBusSubscriptionsManager</code> contract</a> to be implemented specifically for each messaging service. For <a href=""https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/BuildingBlocks/EventBus/EventBusServiceBus/EventBusServiceBus.cs"" rel=""nofollow noreferrer"">Azure Service Bus implementation</a> each service has a physical subscription and each event its interested in is represented by a rule, filtering messages by the value of Service Bus message <code>Label</code> (<code>Label</code> contains event name).</p>&#xA;&#xA;<blockquote>&#xA;  <p>Conceptually, Topics represent different event stacks, correct? E.g. Customers, Ordering, etc? Or are they intended to be domain event boundaries? E.g. in this application, 'eShop' would be the topic</p>&#xA;</blockquote>&#xA;&#xA;<p>Topics are the points of fanning messages out. You could use a topic per service, but that would mean subscribers would need to know what service is publishing those events. Alternatively, and potentially better option is to have <em>a topic</em> that is known by all of your services, and publish events to that topic. Call it ""<code>Events</code>"" for now. Each service interested in various events would create a subscription. A subscription would be able to get <em>any</em> message (event) published to <code>Events</code> topic, but really should only ""catch"" and deliver events it needs (read ""subscribed to""). That's where filtering is coming in. By creating filters (<code>RuleDescription</code>s) a given subscription for each services declares on the broker what messages it will receive.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Azure deployments is a whole other topic, but related to the Service Bus configuration, are there any recommended techniques for managing that within source control?</p>&#xA;</blockquote>&#xA;&#xA;<p>A few options. </p>&#xA;&#xA;<ol>&#xA;<li>Code-based creation of entities at at run-time (topics, subscriptions with rules, queues).</li>&#xA;<li>Capture topology with ARM templates and version just like code in version control system. </li>&#xA;<li>Use <a href=""https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest"" rel=""nofollow noreferrer"">Azure CLI</a> and version control your scripts.</li>&#xA;</ol>&#xA;"
36731132,36705199,59563,2016-04-19T23:19:06,<p>Azure Service Bus is a broker with competing consumers. You can't have what you're asking with a general queue all instances of your service are using.</p>&#xA;
37719638,37662379,5265836,2016-06-09T07:35:57,"<p>I faced the same thing about a month back. Turned out be mistakes in the .jh file. Can you check/post your jh file here? Hope this helps.</p>&#xA;&#xA;<p>Especially for foreign key relationships, it is sufficient to define the column in the base entity alone. No need to define it in the target entity as well. </p>&#xA;&#xA;<p>Edit: Oops.. just noticed that you are using MongoDB. This answer is not applicable to you. Mods, please delete this post if irrelevant.</p>&#xA;"
30143656,28997963,1665452,2015-05-09T18:37:23,"<p>Why not have one microservice that deals with each email type. The idea with microservices is that they're very small and discrete. You could then either have the consuming microservice choose the email microservice to use, or create a gateway microservice than can do the choosing based on a parameter, method invocation etc.</p>&#xA;"
48756212,48753245,1143724,2018-02-12T21:59:38,"<p>You want to look at <a href=""https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html"" rel=""nofollow noreferrer"">AWS Security Groups</a>. </p>&#xA;&#xA;<blockquote>&#xA;  <p>A security group acts as a virtual firewall for your instance to control inbound and outbound traffic. </p>&#xA;  &#xA;  <p>For each security group, you add rules that control the inbound traffic to instances, and a separate set of rules that control the outbound traffic.</p>&#xA;</blockquote>&#xA;&#xA;<p>Even more specific to your use-case though might be their doc on <a href=""https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-security-groups.html"" rel=""nofollow noreferrer"">ELB Security Groups</a>. These are, as you may expect, security groups that are applied at the ELB level rather than the Instance level. </p>&#xA;&#xA;<p>Using security groups, you can specify who has access to which endpoints.</p>&#xA;"
45579384,45542936,33708,2017-08-08T23:19:00,"<p>I don't think the equivalent tooling is readily available, but we do continuously try to improve the state of tooling for gRPC and Protocol Buffers.</p>&#xA;&#xA;<p>In the meantime, there is <a href=""https://github.com/grpc/grpc/blob/master/doc/command_line_tool.md"" rel=""nofollow noreferrer"">grpc_cli</a> project that might be immediately helpful, and gives you an idea if you want to build your own similar tool.</p>&#xA;"
41263233,41262716,3966892,2016-12-21T12:51:56,"<p>We are using <strong>jHipster-Gateway</strong> for the same purpose:</p>&#xA;&#xA;<p>Look <a href=""https://jhipster.github.io/microservices-architecture/#gateway"" rel=""nofollow noreferrer""><strong>HERE</strong></a> for more detailed architecture. </p>&#xA;"
39703004,39672919,1177772,2016-09-26T12:35:38,"<p>I agree with the previous answer and I would add that Sails is a great candidate for clustering and in an environment where you may wish to scale horizontally to improve availability. I do not believe sails is the right candidate for the micro service architecture, however it is most likely the focus for an application which requires the usage of multiple services in its own right.</p>&#xA;&#xA;<p>I use a message service to glue together multiple applications, with sails consuming these messages in order to update a webpage. I probably see those applications as offering smaller services, with defined boundaries and my sails application as the front end, with the controller gluing what is necessary to satisfy the requirements of the end user.</p>&#xA;"
45491486,34722107,7145901,2017-08-03T18:00:57,"<p>I think it's not a good approach.&#xA;DB operation is critical in any process, so it must be in the DAO layer inside de microservice. Why you don't what to implement inside.&#xA;Using a service, you loose control, and if you have to change the process logic you have to change DAO service (Affecting to all the services).&#xA;In my opinion it is not good idea. </p>&#xA;"
51674964,51673608,527533,2018-08-03T14:36:29,"<p>Not sure about a recommended architecture, but i'll give my thoughts on it since i've been battling with similar architectural decisions.</p>&#xA;&#xA;<p>On the frontend, let's say you're handling 300k users, assuming a single server can handle 5k socket connections, you'll have 60 servers sitting behind a load balancer. Each of those 60 servers will roughly have 5k socket connections open, if the user refreshes his browser, he'll get a new socket connection to any of the 60 servers.</p>&#xA;&#xA;<p>Each of these 60 servers are connected to a Kafka cluster</p>&#xA;&#xA;<p>Upon connecting to any of these 60 servers, you would return some kind of an identification token, a GUID or something (<code>309245f8-05bd-4221-864f-1a0c42b82133&#xA;</code>), then that server would broadcast via Kafka to all other 59 servers that GUID <code>309245f8-05bd-4221-864f-1a0c42b82133</code> is connected to itself and each of those 59 servers would update their internal registry to take note that <code>309245f8-05bd-4221-864f-1a0c42b82133</code> belongs to Server1.</p>&#xA;&#xA;<p>You need to decide what happens when a user refresh, does he lose existing messages or do you want to retain those messages?&#xA;If the user should continue receiving message after refreshing even though the user is now connected to a new server, the browser needs to store that GUID in a Cookie or something, upon connecting to the new server, that new server will broadcast to all other 59 servers that <code>309245f8-05bd-4221-864f-1a0c42b82133</code> now belongs to Server2 and Server1 will update itself to take note of it.</p>&#xA;&#xA;<p>Storing the GUID in the frontend, you need to take security in account, if somebody hijacks that GUID, they can intercept your requests, so be sure to make Cookies HTTP Only, Secure and setup the relevant CORS settings.</p>&#xA;&#xA;<p>Your backend will be servers listening to messages from Kafka, you can have as many services as you want in this fashion, if one server struggles to keep up, simply spin up more instances, from 1 instance to 2 instances, your processing capacity doubles (as an example). Each of these backend instances will keep track of the same registry the frontend has, only, instead of tracking which socket is connected to which frontend instance via GUID, the backend will track which frontend instance handles which GUID.</p>&#xA;&#xA;<p>Upon receiving a message via the socket, Server2 will publish a message via Kafka where any number of backend instances can pick up the message and process it. Included with that message is the GUID, so if a response needs to come back, the backend will simply send back a message marked with that GUID and the correct frontend server will pick it up and send a response message via the socket back to the browser client.</p>&#xA;&#xA;<p>If any of the 60 frontend instances goes offline, the websocket should reconnect to any of the remaining instances, the backend should be notified that those 5k GUIDs have moved to other servers. In the event that messages reach the wrong server, the frontend instances should send back that message to the backend with re-routing instructions.</p>&#xA;&#xA;<p>Kafka being just one of many possible solutions, you can use RabbitMQ or any other queuing system or build one yourself. The messaging queue should be highly available and autoscale as needed and should at no point lose messages.</p>&#xA;&#xA;<p>So in short, many frontend instances behind a load balancer using a messaging queue to sync between themselves and to talk to backend instances which has access to databases and integrations.</p>&#xA;"
44086121,44083919,527533,2017-05-20T12:54:36,"<p>By server, I'm guessing you mean Tomcat / Jetty / other containers / etc ? You can either deploy each service on its own container (tomcat), or rename the generated WAR file to <code>a.war</code>, <code>b.war</code>, <code>c.war</code> in which case you can access the different services at <code>localhost:8080/a</code>, <code>localhost:8080/b</code>, etc</p>&#xA;&#xA;<p>If you're running JARs, you need to figure out how to run each service on its own port since you can only run one service per port, eg <code>localhost:8081</code>, <code>localhost:8082</code></p>&#xA;&#xA;<p>Have not used play myself</p>&#xA;"
44085976,44085454,527533,2017-05-20T12:39:16,"<p>You'll have to make a call to each microservice and do the join manually or pass in the relevant user ids to each service.</p>&#xA;&#xA;<p>UserMicroservice: </p>&#xA;&#xA;<pre><code>SELECT * FROM Users WHERE some condition is true&#xA;</code></pre>&#xA;&#xA;<p>Get list of Users back including their <code>id</code>s.</p>&#xA;&#xA;<p>ProductMicroserivce: </p>&#xA;&#xA;<pre><code>SELECT * FROM Products WHERE some condition is true AND userId IN (.........)&#xA;</code></pre>&#xA;&#xA;<p>So users and products can still be in two different databases, the products will just need to have the concept of a userId.</p>&#xA;&#xA;<p>The reverse can also be done, ProductMicroserivce: </p>&#xA;&#xA;<pre><code>SELECT * FROM Products WHERE some condition is true&#xA;</code></pre>&#xA;&#xA;<p>Extract all the UserIds then call the UserMicroservice: </p>&#xA;&#xA;<pre><code>SELECT * FROM Users WHERE some condition is true AND id IN (.........)&#xA;</code></pre>&#xA;"
50131419,37915326,9321133,2018-05-02T09:37:07,"<p>This can be achieved using the CQRS design pattern, which is separation of creation and viewing of entity by following asynchronous paradigm.</p>&#xA;&#xA;<p>While creation, we push the entity persistence to Kafka/RabbitMQ and push that to database asynchronously. Materialised views can be created on the DB which makes the retrieval faster.</p>&#xA;"
45850854,45847796,8218029,2017-08-23T23:46:40,"<p>I think of them as helper containers.  A pod can have 1 or more containers. A container should do only one thing, like a web server or load balancer.  So if you need some extra work to be done inside the pod, like github sync or data processing, you create an additional container AKA sidecar.</p>&#xA;"
37947082,37946486,3833773,2016-06-21T14:23:28,"<ol>&#xA;<li>What you described is not scaling. Making sure that a container spins up in place of a failed one is more commonly referred to as fault-tolerance or auto-healing. Amazon provides a number of platforms that can accommodate those needs. The simplest ones would be Elastic Beanstalk and Elastic Container Service.</li>&#xA;<li>Scaling your applications (which includes launching new containers when the existing ones are experiencing heavy loads) is also possible through Beanstalk and Container Service, and can be costumized to scale in response to different triggers (CPU, memory, network, latency and more)</li>&#xA;</ol>&#xA;&#xA;<p>There are a few other options available for launching and managing containers - the most popular ones would be Kubernetes, DC/OS and Docker Swarm, but these might complicate things rather than simplify them - so I would suggest starting out with the AWS built in tools I've mentioned.</p>&#xA;"
34920141,34889229,5299885,2016-01-21T09:44:07,"<p>Your option #2 is definitely preferable for a production environment. Ideally you should have some automation around this process, typically to perform something like a blue-green deploy where you replace containers based on the old image one by one with those from the new, testing as you go and then only when you are satisfied with the new deployment do you clean up the containers from the previous version and remove the image. That way you can quickly roll-back to the previous version if needed.</p>&#xA;&#xA;<p>In a development environment you may want to take a different approach where you bind mount the application in to the container at runtime allowing you to make updates dynamically without having to rebuild the image. There is a nice example in the <a href=""https://docs.docker.com/compose/extends/#different-environments"" rel=""nofollow"">Docker Compose docs</a> that illustrates how you can have a common base compose YML and then extend it so that you get different behavior in development and production scenarios.</p>&#xA;"
32394063,32334161,5299885,2015-09-04T08:59:47,"<p>Note that the two are not necessarily mutually exclusive. It is possible, for example, that you might still direct clients to a load balancer (which might perform other roles such as throttling) but have the load balancer use a service registry to locate instances.</p>&#xA;&#xA;<p>Also worth pointing out that service discovery enables client-side load balancing i.e. the client can invoke the service directly without the extra hop through the load balancer. My understanding is that this was one of the reasons that Netflix developed Eureka, to avoid inter-service calls having to go out and back through the external ELB for which they would have had to pay. Client-side load balancing also provides a means for the client to influence the load-balancing decision based on its own perspective of service availability.</p>&#xA;"
41944685,41942024,1572269,2017-01-30T20:18:50,"<p>There really isn't much of a drawback except in situations where a micro service cannot operate under <em>eventual consistency</em>.  And even in these cases, you can always add a dependency for your non-common micro service to know how to query the common micro service for relevant updates if necessary, although that's less than ideal.</p>&#xA;&#xA;<p>You will likely have to introduce some form of mediator mechanism for your use case though.  Something like a JMS broker is an ideal choice that would allow one micro service to inform other interested micro services that something occured so that they each can handle the event in their own way.</p>&#xA;&#xA;<p>For example, a <code>CustomerMessage</code> could be raised that contains the customer's id, name, address, and perhaps credit-limit and one micro service may only be concerned with the id and name while another may be interested also in the address and credit-limit.</p>&#xA;"
46031440,45992134,1572269,2017-09-04T06:31:52,"<p>It would seem the most logical based on your supplied code might be to simply add a boolean flag to your <code>UserContext</code> thread local variable and simply check that inside the listener.</p>&#xA;&#xA;<p>By default this flag would be false but for your special microservice or business use case, you could alter that state temporarily, run your process, and clear that state after you've finished, very much like a web filter chain works in web applications.</p>&#xA;"
51486849,48906817,523949,2018-07-23T20:34:49,"<p>In my understanding (not a big user of 2PC since I consider it limiting):</p>&#xA;&#xA;<ul>&#xA;<li>Typically, 2PC is for <em>immediate</em> transactions.</li>&#xA;<li>Typically, Sagas are for <em>long running</em> transactions.</li>&#xA;</ul>&#xA;&#xA;<p>Use cases are obvious afterwards:</p>&#xA;&#xA;<ul>&#xA;<li>2PC can allow you to commit the whole transaction in a request or so, spanning this request across systems and networks. Assuming each participating system and network follows the protocol, you can commit or rollback the entire transaction seamlessly.</li>&#xA;<li>Saga allows you split transaction into multiple steps, spanning long periods of times (not necessarily systems and networks).</li>&#xA;</ul>&#xA;&#xA;<p>Example:</p>&#xA;&#xA;<ul>&#xA;<li>2PC: Save Customer for every received Invoice request, while both are managed by 2 different systems.</li>&#xA;<li>Sagas: Book a flight itinerary consisting of several connecting flights, while each individual flight is operated by different airlines.</li>&#xA;</ul>&#xA;&#xA;<p>I personally consider Saga capable of doing what 2PC can do. Opposite is not accurate.</p>&#xA;&#xA;<p>I think Sagas are universal, while 2PC involves platform/vendor lockdown.</p>&#xA;"
51140741,50986816,523949,2018-07-02T17:03:05,"<p>As I was expecting - people try to fit everything into a concept even if it does not fit there. This is not a criticism, this is an observation from my experience and after reading your question and other answers.</p>&#xA;&#xA;<p>Yes, you are right that microservices architecture is based on asynchronous messaging patterns. However, when we talk about UI, there are 2 possible cases in my mind:</p>&#xA;&#xA;<ol>&#xA;<li><p>UI needs a response immediately (e.g. read operations or those commands on which user expects answer right away). <em>These don't have to be asynchronous</em>. Why would you add an overhead of messaging and asynchrony if the response is required on the screen right away? Does not make sense. Microservice architecture is supposed to solve problems rather than create new ones by adding an overhead.</p></li>&#xA;<li><p>UI can be restructured to tolerate delayed response (e.g. instead of waiting for the result, UI can just submit command, receive acknowledgement, and let the user do something else while response is being prepared). In this case, you can introduce asynchrony. The <em>gateway</em> service (with which UI interacts directly) can orchestrate the asynchronous processing (waits for complete events and so on), and when ready, it can communicate back to the UI. I have seen UI using SignalR in such cases, and the gateway service was an API which accepted socket connections. If the browser does not support sockets, it should fallback to the polling ideally. Anyway, important point is, this can only work with a contingency: <em>UI can tolerate delayed answers</em>.</p></li>&#xA;</ol>&#xA;&#xA;<p>If Microservices are indeed relevant in your situation (case 2), then structure UI flow accordingly, and there should not be a challenge in microservices on the back-end. In that case, your question comes down to applying event-driven architecture to the set of services (edge being the gateway microservice which connects the event-driven and UI interactions). This problem (event driven services) is solvable and you know that. You just need to decide if you can rethink how your UI works.</p>&#xA;"
50883140,46742274,523949,2018-06-15T21:56:30,"<p>I think you have the answer on the diagram itself:</p>&#xA;&#xA;<p>Initial Event step (red color) is the key. Every Event Processor produces an event, which is what gets into the Event Queue and then to the Event Mediator.</p>&#xA;&#xA;<p>The architecture is Event Driven and asynchronous. Single Event Queue handles path to the Event Mediator. And since this is the only way to get the event into the Event Mediator, obviously, anything wanting to send an event to the mediator would need to use this path.</p>&#xA;&#xA;<p>At some point, after certain event, the Event Mediator will declare the operation as successfully complete and will not dispatch more events to the Event Processors.</p>&#xA;&#xA;<p>Although, I must say, you are right, this is not clearly stated in the article. I assume this will be better clarified in the book they are previewing.</p>&#xA;"
51808788,51808718,10105518,2018-08-12T12:22:37,"<p>If I understand it right, you are trying to get data from various sources at a given frequency. Can't you use a spring batch for scheduling a timed job?</p>&#xA;&#xA;<p>If the data to be processed is going to become bigger/longer, you could submit the response data to be processed into a Queuing system like RabbitMQ/ Kafka and do it asynchronously. </p>&#xA;"
37156170,37148836,2705070,2016-05-11T07:44:02,"<p>Let's start by reviewing what service-discovery is - here's a good explanation: <a href=""https://www.nginx.com/blog/service-discovery-in-a-microservices-architecture/"" rel=""noreferrer"">https://www.nginx.com/blog/service-discovery-in-a-microservices-architecture/</a>&#xA;(this link should pretty much clarify the issue asked)</p>&#xA;&#xA;<p>And here's an example how it is used in practice:&#xA;Suppose you have service B which is used by service A. Service B (like most services in <a href=""https://en.wikipedia.org/wiki/Service-oriented_architecture"" rel=""noreferrer"">SOA</a>) is actually a cluster of applications of type B. Service A requires to use one of the nodes of cluster B, yet the cluster of B nodes is <strong>dynamic</strong>. i.e. B nodes are created and terminated, depending on the load on the overall B service. Now, we would like service A to communicate with a live B node every time it needs to use service B. In order to do so, we will use the service-discovery tool to provide us, <strong>at any given time</strong>, an address of one of the live B nodes.</p>&#xA;&#xA;<p>So, trying to answer your above questions, putting the end-point server information (specifically endpoint address) as static configuration in a config file which is read at the startup of service A, won't give you the <strong>dynamics</strong> you'd like when service B endpoints may constantly change. </p>&#xA;"
28686312,28664781,3689902,2015-02-24T00:36:41,"<p>You should use an external monitoring service like <a href=""http://pingometer.com"" rel=""nofollow"">Pingometer</a>.</p>&#xA;&#xA;<p>This lets you setup simple health checks (HTTP, HTTPS, Ping, etc.) at regular intervals and receive alerts if a node fails, is unavailable, or not responding with the correct content.</p>&#xA;&#xA;<p>In your contact, you can setup a webhook which is fired when a service goes down. You can use the webhook to trigger a failover, change DNS records, etc.</p>&#xA;&#xA;<p>We setup something similar and it's working quite well.</p>&#xA;&#xA;<p>You can also use something internally to monitor nGinX itself (e.g. cheaping workers + respawning them), but this doesn't let you know that a service is functioning externally (like a monitoring service would).</p>&#xA;"
37574051,37509121,6396846,2016-06-01T16:23:39,"<p>If i understand correctly, it looks like you need a CEP ( complex event processor), like <a href=""http://wso2.com/products/complex-event-processor/"" rel=""nofollow"">ws02 cep</a> or other , which does exactly that.&#xA;cep's can aggregate events and perform actions when certain conditions&#xA;have been met. </p>&#xA;"
48146303,48145352,2087854,2018-01-08T08:02:58,"<ul>&#xA;<li>If you have four different  Spring Boot applications, I suggest creating four different Dockerfiles, and building four different images from those files. Basically put one Dockerfile in each Spring application folder.</li>&#xA;<li>You <em>can</em> build PostgreSQL credentials (hostname, username &amp; password) into the application by writing it in the code. This is easiest.  </li>&#xA;</ul>&#xA;&#xA;<p>If you use AWS and ECS (Elastic Container Service) or EC2 to run your Docker containers you could store the credentials in the EC2 Parameter Store, and have your application fetch them at startup, however this takes a bit more AWS knowledge and you have to use the AWS SDK to fetch the credentials from the application. Here is a StackOverflow question about exactly this: <a href=""https://stackoverflow.com/questions/43211121/accessing-aws-parameter-store-values-with-custom-kms-key"">Accessing AWS parameter store values with custom KMS key&#xA;Ask Question</a></p>&#xA;"
32394209,32334161,1725871,2015-09-04T09:08:40,"<p>If you look at the tools from a completely different perspective, namely ITSM/ITIL, load balancing becomes ""just that"", whereas service discovery is a part of keeping your CMDB up to date, and ajour with all your services, and their interconnectivity, for better visibility of impact, in case of downtime, and an overview of areas that may need supplementing, in case of High availability applications.</p>&#xA;&#xA;<p>Furthermore, service-discovery only gives you a picture as of the last scan, and not near-real-time (of course dependent on which scanning interval you have set), whereas load balancing will keep an up-to-date picture of your application's health.</p>&#xA;"
51827794,51827636,5465007,2018-08-13T17:38:23,"<p>Depends heavily on your infrastructure. You could go for a service directory like consul.io, where you can request an endpoint (ip &amp; port) for an instance of a service. Another option would be to trust on DNS to resolve the services for you, for example via SRV records. Either way, this is not something that should be solved inside the microservice itself.</p>&#xA;"
31591837,30613574,429487,2015-07-23T15:29:59,"<p>Seems that there's no other way. Microservices architecture puts lots of stress in avoiding data sharing so as to not create dependencies. That means that each microservice will have some data duplicated. That also means that there must exist a way of getting data from other contexts. The preferred methods strive for eventual consistency, such as sending messages to event sourcing or AMQP systems and subscribing to them. You can also use synchronous methods (RPC calls, distributed transactions). That creates additional technologic dependencies, but if you cannot accept eventual consistency it could be the only way.</p>&#xA;"
34789450,34774290,236719,2016-01-14T12:18:49,"<p>Some of the things below might be a bit of a repetion based on what you know but let's see if parts of it can be useful for you still.</p>&#xA;&#xA;<p>The idea of microservices is to design each service around a single responsibility or quite common a single domain. This would mean that you will have different services handling Product APIs, Order APIs, Basket APIs and Customer APIs. As you already guessed, this would mean you will have all this data split between different databases and services collaborate with eachother, never access their own databases directly.</p>&#xA;&#xA;<p>This allows you to have all the business logic contained in one place and achieve that services are:</p>&#xA;&#xA;<ul>&#xA;<li>Released and Deployed independently</li>&#xA;<li>Replaceable and easier upgradable</li>&#xA;<li>Scaled independently</li>&#xA;<li>Easier &amp; faster releases</li>&#xA;<li>Easier to test</li>&#xA;</ul>&#xA;&#xA;<h1>Implications</h1>&#xA;&#xA;<h3>More things can go down</h3>&#xA;&#xA;<p>Implications of such approach is naturally a more complex deployment infrastructure and more things that can go wrong as services are communicating with each other typically over HTTP with REST APIs, but sometimes over pub-sub, Protocol Buffs or Thrift, or other light-weight messaging protocol. This leads to possible timeouts in communication, latency, etc.</p>&#xA;&#xA;<p>You will have to account for ""things can go down"" scenario much more than in a monolith and handle it according to your requirements. There are libraries and tools that help you with, for example Spring-Cloud-Netflix project wraps in Netflix Hystrix for circuit breaker support which helps you handle timeouts with remote service communications.</p>&#xA;&#xA;<h3>Resource usage</h3>&#xA;&#xA;<p>When it comes to resource usage it depends on how you look at it. Multiple services combined will most likely use more resources than monolith, especially if you account for multiple database instances, etc. However let me give you an example when for instance deploying to AWS and using Auto-scaling support that AWS provides. </p>&#xA;&#xA;<p>In a monolith you will scale the whole application. when the load on your application increases. This means that you will spin up more EC2 machines even if it's only a certain piece of the code that is being under pressure.</p>&#xA;&#xA;<p>In a microservice architecture you are able to scale each service on it's own depending on usage. This can lead to a much better resource utilization and being more cost-effective at the end as each service will use only a fraction of what a monolith does.</p>&#xA;&#xA;<h1>A few words about UI</h1>&#xA;&#xA;<p>I think the UI should be treated as a separate service as well. It should only provide the UI itself but not the business logic for certain operations - it's a responsibility of each microservice. So if you have a webshop, you may end up with the following services:</p>&#xA;&#xA;<ul>&#xA;<li>Frontend UI</li>&#xA;<li>Purchase service</li>&#xA;<li>Basket service</li>&#xA;<li>User service</li>&#xA;<li>Product service</li>&#xA;<li>Order service</li>&#xA;<li>Shipping service</li>&#xA;<li>... possible quite a few more.</li>&#xA;</ul>&#xA;&#xA;<p>The only logic your UI should have is how to display different type of data and then communicate with underlying services. However, UI can be combined by many different services too! As an example, Amazon has many different cervices that power each page you see and parts or the UI can be generated and shown by different UI services. It all depends on how small you want to make your services.</p>&#xA;&#xA;<p>There is a very good book ""Build Microservices"" by Sam Newman that I strongly would recommend you read - it covers a lot of these things and much much more. It should also answer a lot of different questions that may arise over time the more you dig into the beautiful world of microservices.</p>&#xA;"
47096181,47061556,4054447,2017-11-03T12:47:57,"<p>I ended up doing something like:</p>&#xA;&#xA;<p><code>&#xA;Parent-Folder&#xA;- docker-compose.yml (sym linked to Local-Development-Repo/docker-compose.yml)&#xA;- Local-Development-Repo&#xA;  - docker-compose.yml&#xA;- UI-Repo&#xA;- Server-Repo&#xA;</code></p>&#xA;&#xA;<p>This way, I can track docker-compose.yml and at the same time can start all the services via <code>docker-compose-up</code> from the Parent-Folder.</p>&#xA;"
44941653,44941218,1111215,2017-07-06T06:49:33,"<p>Seems like your credentials are not completely set. </p>&#xA;&#xA;<p><strong>Setting up your Azure credentials</strong></p>&#xA;&#xA;<p>Once the <code>serverless-azure-functions</code> plugin is installed, it expects to find your Azure credentials via a set of well-known environment variables. These will be used to actually authenticate with your Azure account, so that the Serverless CLI can generate the necessary Azure resources on your behalf when you request a deployment (see below).</p>&#xA;&#xA;<p>The following environment variables must be set, with their respective values:</p>&#xA;&#xA;<ul>&#xA;<li><em>azureSubId</em> - ID of the Azure subscription you want to create your service within</li>&#xA;<li><em>azureServicePrincipalTenantId</em> - ID of the tenant that your service principal was created within</li>&#xA;<li><em>azureServicePrincipalClientId</em> - ID of the service principal you want to use to authenticate with Azure</li>&#xA;<li><em>azureServicePrincipalPassword</em> - Password of the service principal you want to use to authenticate with Azure</li>&#xA;</ul>&#xA;&#xA;<p>For details on how to create a service principal and/or acquire your Azure account's subscription/tenant ID, refer to the <a href=""https://serverless.com/framework/docs/providers/azure/guide/credentials/"" rel=""nofollow noreferrer"">Azure credentials</a> documentation.</p>&#xA;"
52013836,51976057,1766831,2018-08-25T03:37:36,"<p>In MySQL, use <code>ORDER BY ... LIMIT 30, 10</code> to skip 30 rows and grab 10.</p>&#xA;&#xA;<p>Better yet remember where you left off (let's say $left_off), then do</p>&#xA;&#xA;<pre><code>WHERE id &gt; $left_off&#xA;ORDER BY id&#xA;LIMIT 10&#xA;</code></pre>&#xA;&#xA;<p>The last row you grab is the new 'left_off'.</p>&#xA;&#xA;<p>Even better is that, but with <code>LIMIT 11</code>.  Then you can show 10, but also discover whether there are more (by the existence of an 11th row being returned from the <code>SELECT</code>.</p>&#xA;"
44061772,44060464,1766831,2017-05-19T04:59:25,"<p>Since you are aiming for a 'huge' table, you need to shrink the datatypes as much as practical.  189M/year rows with your current schema might be 40GB/year</p>&#xA;&#xA;<pre><code>  `Id` varchar(255) COLLATE utf8_unicode_ci NOT NULL DEFAULT '',&#xA;  `SensorId` varchar(16) COLLATE utf8_unicode_ci NOT NULL,&#xA;</code></pre>&#xA;&#xA;<p>Do they need to be utf8?  Whether or not you need utf8, normalize each of <code>Id</code> and <code>SensorId</code>, or normalize the pair.  Probably <code>MEDIUMINT UNSIGNED</code> (3 bytes, 16M limit) would be sufficient.</p>&#xA;&#xA;<pre><code>  `Battery` double DEFAULT NULL,&#xA;  `Rain` double DEFAULT NULL,&#xA;  `Humidity` double DEFAULT NULL,&#xA;</code></pre>&#xA;&#xA;<p><code>DOUBLE</code> takes 8 bytes and gives you 16 significant digits.  I doubt if you can read Humidity to more than 3 significant digits.  <code>FLOAT</code> takes only 4 bytes and gives you 7 significant digits.  <code>DECIMAL(4,2)</code> might be worth considering -- values up to 99.99, taking only 2 bytes.  (Etc.)</p>&#xA;&#xA;<pre><code>  PRIMARY KEY (`Id`,`Time`)&#xA;</code></pre>&#xA;&#xA;<p>Without knowing the <code>SELECTs</code>, we cannot judge how useful this is.</p>&#xA;&#xA;<p>The above changes might get you down to 10GB/year.</p>&#xA;&#xA;<p>Get some of this stuff done, then let's talk about Summary Tables -- you do <em>not</em> want to scan 189M rows for anything!</p>&#xA;&#xA;<p>You have not yet said anything that would trigger using Partitioning.</p>&#xA;&#xA;<p>""which helps user to order records by"" -- What about filtering?  Are you really helping the user fetch 189M rows?</p>&#xA;"
42670371,42651456,3868848,2017-03-08T11:40:11,"<p>upon searching out more and looking into spring-cloud-netflix issue tracker, There is a wonderful discussion between <a href=""https://github.com/william-tran"" rel=""nofollow noreferrer"">william-tran</a> and <a href=""https://github.com/ryanjbaxter"" rel=""nofollow noreferrer"">ryanjbaxter</a> on the best practices. Thanks to both of you.</p>&#xA;&#xA;<p><a href=""https://github.com/spring-cloud/spring-cloud-netflix/issues/1290#issuecomment-242204614"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-netflix/issues/1290#issuecomment-242204614</a></p>&#xA;&#xA;<p><a href=""https://github.com/spring-cloud/spring-cloud-netflix/issues/1295"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-netflix/issues/1295</a></p>&#xA;&#xA;<p>In summary, Camden doesn't use the Ribbon HTTP Client(deprecated) so none of the ribbon.* properties will help you control the retry logic. Camden uses Apache HTTP client.</p>&#xA;&#xA;<p>So the solution would be to use Ribbon HTTP Client in camden version using below configuration</p>&#xA;&#xA;<pre><code>ribbon.restclient.enabled=true&#xA;</code></pre>&#xA;&#xA;<p>or </p>&#xA;&#xA;<p>Move to Camden.BUILD-SNAPSHOT or Dalston.BUILD-SNAPSHOT for using spring-retry (<a href=""https://github.com/spring-projects/spring-retry"" rel=""nofollow noreferrer"">https://github.com/spring-projects/spring-retry</a>)</p>&#xA;"
45411638,45400096,8392624,2017-07-31T09:06:59,"<p>What you have understood is perfectly good and you have found the right area where microservices are getting complex over monoliths (Distributed Transaction) but let me clear up some points about microservices.</p>&#xA;&#xA;<ol>&#xA;<li><p><strong>Microservice doesn't mean independent services exposed over HTTP</strong>: A microservice can communicate with other services either in a synchronous or asynchronous way, so REST is one of the solutions and it is applicable for synchronous communication but you can perform asynchronous communication too like message-driven using Kafka or hornetq etc. In synchronous communication an underlying service can call over Thrift protocol also.</p></li>&#xA;<li><p><strong>Microservice following SRP</strong>: The beauty of microservices is that each service concentrates over only one business domain use case, so it handles only one domain object's functionality. But utils module is for common methods so every microservice depends on it. So even a small change in the utils module needs to build all other microservices so it is a violation of the microservices 12 principles so dissolve the utils service and make it local with each service.</p></li>&#xA;<li><p><strong>Handling Authentication</strong>: To be specific a microservice can be one of three types:<br>&#xA;a. <strong><em>Core service</em></strong>: Just perform a domain operation (like account creation/updation/deletion)<br>&#xA;b. <strong><em>Aggregate service</em></strong>: Call one or more core service, gather results and perform some operation on it.<br>&#xA;c. <strong><em>Edge service</em></strong>: Exposed to a client (like Mobile/browser etc). We sometimes call it a gateway service; the crux of this service is take a user request and based on the URL forward it to an actual microservice. So it is the ideal place to put authentication if it is common for all microservices.</p></li>&#xA;<li><p><strong>Handling Distributed Transaction</strong>: Yes this is the hardest part of microservices but you can achieve it through an event-driven/message-driven way. Every action pops an event; a subscriber of this event receives the same and does some operation and generates another event. In case of failure it generates a reverse event which compensates the first event created.</p>&#xA;&#xA;<p>For example, say from micoservice A we debited 100 rupees so create an <code>AccountDebited</code> event. Now in microservice B we try to credit the account. If it is successful we create <code>AccountCredited</code> which is received by A and creates another event <code>AmountTransfered</code>. In case of failure we generate an <code>AccountCreditedFailed</code> event which is received by A and generates a reverse event - <code>AccountSpecialCredit</code> - which maintains the atomicity.</p></li>&#xA;</ol>&#xA;"
43168425,43142821,7804592,2017-04-02T12:40:25,"<p>right image is better option, easily scalable and managable</p>&#xA;"
41265652,41262716,5917104,2016-12-21T14:54:40,"<p>Assuming that you have a firewall in place, you could restrict inbound traffic to server to the ports that your Zuul endpoints are exposed on and disallow anyone from accessing the microservices' ports directly.</p>&#xA;&#xA;<p>If you want to avoid going the firewall route, you could force the endpoints to check for a specific HTTP header or something that is set by Zuul prior to forwarding a request, but that would be hacky and easy to circumvent. Based on my past experiences, the ""right"" way would be to do this via a firewall. Your app should be responsible for dealing with requests. Your firewall should be responsible for deciding who can hit specific endpoints.</p>&#xA;"
40988683,40988204,5917104,2016-12-06T05:52:52,"<p>I can comment on point #2 specifically.  If you're dealing with multiple client types (web, desktop, mobile), something like a Two Step View can help - define the logical structure of the page server-side, send that to the client in a format like JSON or XML, and then get the client to render the view in the appropriate format.  </p>&#xA;&#xA;<p><a href=""http://martinfowler.com/eaaCatalog/twoStepView.html"" rel=""nofollow noreferrer"">http://martinfowler.com/eaaCatalog/twoStepView.html</a></p>&#xA;"
41010663,41010290,5917104,2016-12-07T06:17:29,"<p>A standard that I've followed in the past is to use web services when the key requirement is speed (and data loss isn't critical) and messaging when the key requirement is reliability.  Like you've said, if the receiving system is down, a message will sit on a queue until the system comes back up to process it.  If it's a REST endpoint and it's down, requests will simply fail.</p>&#xA;"
48617034,48616949,5917104,2018-02-05T06:37:34,"<p>This is a very broad, and possibly, very opinionated question.  I would not advocate the use of micro services when you have a single application with many shared dependencies that is generally always deployed as a unit.</p>&#xA;&#xA;<p>Micro services are great, and they have their place - but if your application is of a such a nature that it's always deployed as a unit, micro services can seriously expand complexity - now you're deploying a multitude of separate artifacts, all at once, instead of just a single application.  The example that comes to mind for me is an ERP system.</p>&#xA;&#xA;<p>Edit: And to expand on this, micro services shouldn't be your default architecture for an application.  Keep it as simple as possible, and if you run into scalability issues, or something else that justifies going the micro services route, then do it.  Keep it as simple as it can possibly be.  So, IMHO, the answer to ""when should I not use micro services?"" is ""when you don't need them"".</p>&#xA;&#xA;<p>See <a href=""https://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it"" rel=""nofollow noreferrer"">YAGNI</a>.</p>&#xA;"
51066969,50986816,2713776,2018-06-27T15:52:53,"<p>What about using <a href=""https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise"" rel=""nofollow noreferrer"">Promises</a>?  Socket.io could also be a solution if you want realtime.</p>&#xA;&#xA;<p>Have a look at <a href=""https://martinfowler.com/bliki/CQRS.html"" rel=""nofollow noreferrer"">CQRS</a> also.  This architectural pattern fits the event driven model and microservice architecture.</p>&#xA;&#xA;<p>Even better.  Have a read of <a href=""http://dist-prog-book.com/chapter/2/futures.html"" rel=""nofollow noreferrer"">this</a>.</p>&#xA;"
28937375,28930710,99248,2015-03-09T07:58:58,"<p>If you want to always have the database up to date and always consistent with the already processed units then you have several updates on the same mutable entity. </p>&#xA;&#xA;<p>In order to comply with this you need to serialize the updates for the same entity. Either you do this by partitioning your data at producers, either you accumulate the events for the entity on the same queue, either you lock the entity in the worker using an distributed lock or a lock at the database level.</p>&#xA;&#xA;<p>You could use an actor model (in java/scala world using akka) that is creating a message queue for each entity or group of entities that process them serially.</p>&#xA;&#xA;<p>UPDATED&#xA;You can try an <a href=""https://stackoverflow.com/questions/5608078/equivalent-of-akka-but-for-net-concurrency-framework"">akka port to .net</a> and <a href=""http://www.infoq.com/news/2014/07/approaches-actor-model-net"" rel=""nofollow noreferrer"">here</a>.&#xA;Here you can find a nice tutorial with samples about using <a href=""http://danielwestheide.com/blog/2013/03/20/the-neophytes-guide-to-scala-part-15-dealing-with-failure-in-actor-systems.html"" rel=""nofollow noreferrer"">akka in scala</a>. &#xA;But for general principles you should search more about [actor model]. It has drawbacks nevertheless.</p>&#xA;&#xA;<p>In the end pertains to partition your data and ability to create a unique specialized worker(that could be reused and/or restarted in case of failure) for a specific entity.</p>&#xA;"
46953531,46934916,4623280,2017-10-26T11:48:13,"<p>There are similar cases in our project and we did it with spring components. As far as i understood, you want to open a REST endpoint which most probably accepts json object and you want to make a soap web service request within that request then you want to return a response containing information from the soap response. To make a soap request, you can use spring web services - WebServiceTemplate. It will marshall your object to xml and make soap request for you. Of course you can use some other web service frameworks like apache cxf which may best fit for some special cases, but i would first try to use a framework which is from spring family while using Spring. You should set <strong>timeout values</strong> on webservicetemplate object to not wait too long if the external system is not working well or you have some problems with the network. Because it directly affects your systems performance. Also here, i suggest you to implement <strong>circuit breaker pattern</strong> to make your system more robust. You should always isolate your systems performance from other systems that you integrate and in this scenario you can do this by doing the things explained above.</p>&#xA;"
46968648,46947956,4623280,2017-10-27T06:20:10,"<p>In this scenario you have 2 shared resources (database and queue) and you want them to be transacted together. You want your database to commit if the message sent to the queue. You want your database not to commit if it is not successfully sent and vice versa. This is simply global transaction mechanism like 2PC. However to implement a global transaction mechanism is not that easy and it is also very costly.</p>&#xA;&#xA;<p>I suggest you to implement at least one strategy on producer side and idempotency on consumer side to provide consistency. </p>&#xA;&#xA;<p>You should create a message table on producer side's database and persist messages to this table before sending to queue. Then with a scheduled thread (here there may have multiple threads to increase throughput but be careful if your messages needs to be consumed in the order they produced) or any other thing you can send them to queue and mark them as sent to ensure that the messages which are already sent will not be sent again. Even if you do that there might be some cases that your messages are sent more than once (e.g. you send a message to queue and your application crashed before marking the message as sent). But it is not a problem, because we already want to implement at least once strategy on producer side which means we want a message to be sent to queue at least once.</p>&#xA;&#xA;<p>To prevent a consumer to consume same messages which are produced more than once on producer side you should implement idempotent consumers. Simply, you can save id of consumed messages to a database table on consumer side and before processing messages coming from the queue, you may check if it is already consumed. If it is already consumed you should ignore it and get the next message. </p>&#xA;&#xA;<p>There are of course other options to provide consistency in microservices environment. You can find other solutions on this great blog - <a href=""https://www.nginx.com/blog/event-driven-data-management-microservices/"" rel=""nofollow noreferrer"">https://www.nginx.com/blog/event-driven-data-management-microservices/</a>. The solution i explained above also exists in this blog. You can find it in Publishing Events Using Local Transactions section.</p>&#xA;"
42206788,42204181,4623280,2017-02-13T14:45:25,"<p>We used spring-integration-kafka to produce and consume messages with Kafka in our microservices. In our case, we send org.springframework.messaging.Message objects to topics and get the same type from topics after deserialization from byte-array. In Message entity there are message-id, sent-time etc. headers values other than message payload which is the actual object that you want to transfer from one microservice to others. We use unique message-id value to implement idempotency. On producer side, you must implement some logic to ensure that, the message-id of the Message is the same when it is produced multiple times. This is actually related to your produce logic. In our case, we use Publishing Events Using Local Transactions which is very well described in the blog <a href=""https://www.nginx.com/blog/event-driven-data-management-microservices/"" rel=""nofollow noreferrer"">https://www.nginx.com/blog/event-driven-data-management-microservices/</a> by Chris Richardson. With this approach we can recrate Message object with the same message-id on producer side. On consumer side, we persist all the consumed message id values to database and check this ids before processing the received messages. If we see a message whose id is in our persistent store, we simply ignore it. </p>&#xA;&#xA;<p>In your case, To implement idempotency:</p>&#xA;&#xA;<ul>&#xA;<li>you should keep a unique identifier with the messages,</li>&#xA;<li>On producer side, you must generate the same identifier when it is produced multiple times,</li>&#xA;<li>On consumer side, you must check the received id to detect whether it is consumed before or not</li>&#xA;</ul>&#xA;&#xA;<p><strong>Regarding to Second Scenario Which is Described in UPDATE,</strong></p>&#xA;&#xA;<p>I think you should change your mind a little bit. If you want to implement publish-subscribe mechanism which is more suitable in microservices architecture, you shouldn't wait response on producer side. In this scenario, you wait other message to know whether the consumer consumed the message or not and if it is not consumed by the consumer, you send it again.</p>&#xA;&#xA;<p>How about the implementation below;&#xA;On producer side, you send messages to Kafka within a transaction in producer. You should provide a mechanism here to send messages to kafka only the transaction on producer side is committed. This is Atomicity issue and i give a link above which shows how to solve this issue. </p>&#xA;&#xA;<p>On Consumer side, you poll messages from kafka topic one by one in order and you get the next message only when the current message can be consumed. If it is not consumed, you shouldn't get the next message. Because the next message might be related to current message and if you consume the next message you may corrupt consistency of your data. Its not producer's concern when the message not consumed. On consumer side, you should provide retry and replay mechanisms to consume messages. </p>&#xA;&#xA;<p>I think you shouldn't wait response on producer side. Kafka is a very smart tool, and with its offset commit capability, as a consumer you don't have to consume messages when you poll messages from topic. If you have a problem while processing messages, you simply don't commit offset to get next message. </p>&#xA;&#xA;<p>With the implementation described above, you don't have a problem like ""<em>How could I distinguish when I should discard the message or reprocess it?</em>""</p>&#xA;&#xA;<p>Regards...</p>&#xA;"
47472033,47451190,4623280,2017-11-24T11:10:37,"<p>The short answer for this question is; It depends on your design.</p>&#xA;&#xA;<p>You can use only one topic for all your operations or you can use several topics for different operations. However you must know that;</p>&#xA;&#xA;<p>Your have to produce messages to kafka in the order that they created and you must consume the messages in the same order to provide consistency. Messages that are send to kafka are ordered within a topic partition. Messages in different topic partitions are not ordered by kafka. Lets say, you created an item then deleted that item. If you try to consume the message related to delete operation before the message related to create operation you get error. In this scenario, you must send these two messages to same topic partition to ensure that the delete message is consumed after create message.</p>&#xA;&#xA;<p>Please note that, there is always a trade of between consistency and throughput. In this scenario, if you use a single topic partition and send all your messages to the same topic partition you will provide consistency but you cannot consume messages fast. Because you will get messages from the same topic partition one by one and you will get next message when the previous message consumed. To increase throughput here, you can use multiple topics or you can divide the topic into partitions. For both of these solutions you must implement some logic on producer side to provide consistency. You must send related messages to same topic partition. For instance, you can partition the topic into the number of different entity types and you send the messages of same entity type crud operation to the same partition. I don't know whether it ensures consistency in your scenario or not but this can be an alternative. You should find the logic which provides consistency with multiple topics or topic partitions. It depends on your case. If you can find the logic, you provide both consistency and throughput. </p>&#xA;&#xA;<p>For your case, i would use a single topic with multiple partitions and on producer side i would send related messages to the same topic partition.</p>&#xA;&#xA;<p>--regards</p>&#xA;"
44873494,44870461,4623280,2017-07-02T17:36:28,"<p>It is possible to use a shared database for multiple microservices. You can find the patterns for data management of microservices in this link: <a href=""http://microservices.io/patterns/data/database-per-service.html"" rel=""noreferrer"">http://microservices.io/patterns/data/database-per-service.html</a>. By the way, it is a very useful blog for microservices architecture.</p>&#xA;&#xA;<p>In your case, you prefer to use database per service pattern. This make microservices more autonomous. In this situation, you should duplicate some of your data among multiple microservices. You can share the data with api calls between microservices or you can share it with async messaging. It depends on your infrastructure and frequency of change of the data. If it is not changing often, you should duplicate the data with async events. </p>&#xA;&#xA;<p>In your example, Delivery service can duplicate delivery locations and product information. Product service manage the products and locations. Then the required data is copied to Delivery service's database with async messages (for example you can use rabbit mq or apache kafka). Delivery service does not change the product and location data but it uses the data when it is doing its job. If the part of the product data which is used by Delivery service is changing often, data duplication with async messaging will be very costly. In this case you should make api calls between Product and Delivery service. Delivery service asks Product service to check whether a product is deliverable to a specific location or not. Delivery service asks Products service with an identifier (name, id etc.) of a product and location. These identifiers can be taken from end user or it is shared between microservices. Because the databases of microservices are different here, we cannot define foreign keys between the data of these microservices. </p>&#xA;&#xA;<p>Api calls maybe easier to implement but network cost is higher in this option. Also your services are less autonomous when you are doing api calls. Because, in your example when Product service is down, Delivery service cannot do its job. If you duplicate the data with async messaging, the required data to make delivery is located in the database of Delivery microservice. When Product service is not working you will be able to make delivery.</p>&#xA;"
41564363,37727820,4623280,2017-01-10T08:30:44,"<p>Spring Cloud Contract is another handy tool that i can suggest. It is well documented and the creators of the api answer your question fast in different environments(e.g. Gitter, Github). The framework works on both producer and consumer side. It ensures the api is compatible with the contracts defined by the consumer on the producer side. On producer side tests are generated automatically. The contracts are also shared with the consumer in a tricky way. They are pushed to your local artifact repository (e.g. Nexus, Artifactory) and consumer can access the contracts over the repo. After that, consumer can also check whether it works with the api written on producer or not. It can be used in also publish-subscribe mechanism. Great tool, use it.</p>&#xA;&#xA;<p>By the way; You can mock your beans and test contracts compatibility in a way that you it looks like unit test, but i dont understand why you insist on testing with unit test.</p>&#xA;&#xA;<p>Regards...</p>&#xA;"
48149646,48143008,4623280,2018-01-08T11:45:51,"<p>When you are using Kafka please be careful about offset management. Offset is the number which is kept for each TopicPartition - ConsumerGroup combination. You should take a look at the following parameters in Kafka:</p>&#xA;&#xA;<pre><code>offsets.retention.minutes&#xA;log.retention.hours&#xA;</code></pre>&#xA;&#xA;<p>If you are using high level java api;</p>&#xA;&#xA;<pre><code>auto.offset.reset&#xA;</code></pre>&#xA;&#xA;<p>There are so many cases that the offsets might get lost. So, i recommend you to implement idempotent consumers to have a chance to re-consume messages when things go wrong. In this way, you may get some messages multiple times but your consumer doesn't process them. It simply skips them to get next messages. </p>&#xA;"
44186908,44186109,7509168,2017-05-25T17:55:38,"<p>I recommend setting up your Visual Studio solution with the following projects:</p>&#xA;&#xA;<ol>&#xA;<li>WebAPI project</li>&#xA;<li>Application tier project</li>&#xA;<li>WebJob project&#xA;4 - N. [All other projects, like data access tier]</li>&#xA;</ol>&#xA;&#xA;<p>The WebAPI and WebJob projects both reference the Application tier project. </p>&#xA;&#xA;<p>Within the Web App, the WebAPI and WebJob projects each have their own copy of the Application tier DLL. There's nothing to gain by keeping one copy of the DLL on your Web App and sharing it between the two (Besides saving a few KB of disk space). But it will add complexity to your deployment. </p>&#xA;&#xA;<p>Also, you could at some point in the future want to move your WebJob to a different Web App. Sharing a single copy of the DLL between the two will force you to make changes to do so. </p>&#xA;"
26211361,25812816,459185,2014-10-06T07:09:02,<p>Dropwizard Jetty has shutdown hooks. So <code>kill -SIGINT &lt;pid&gt;</code> works out really well. </p>&#xA;
50080632,47050984,8128433,2018-04-28T20:11:18,"<p>I tried the solution mentioned above, however, it's also required to create a folder <code>storage/framework/sessions</code> if using the default settings.</p>&#xA;"
49000580,49000291,592817,2018-02-27T03:21:25,"<p>I personally never put the export feature as a separate service. </p>&#xA;&#xA;<p>Providing such a table based data, I provide a table view of the data with paging, and also give export function as an octet streamed data without paging limit. Export could be a type of a view. </p>&#xA;&#xA;<p>I've used the Apache POI library for report rendering but only for the small pages and complex shapes previously. POI also provides streaming version of workbook classes such as <a href=""https://poi.apache.org/apidocs/org/apache/poi/xssf/streaming/SXSSFWorkbook.html"" rel=""nofollow noreferrer""><code>SXSSFWorkbook</code></a>. </p>&#xA;&#xA;<p>To be a microservice, it should have a proper reason to be a external system. If the system only provides just export something, negative. It's too simple and overkill. If you're considering to add versioning, permission, distribution, folder zipping, or... storage management, well.. that could be an option. </p>&#xA;&#xA;<p>By the way, exporting such a big data into a file, Excel has max row limit to 1M size so you may hit the limit if your data size grow more. &#xA;Why don't use use just a CSV format? Easy to use, Easy to jump, Easy to process.</p>&#xA;"
36211765,36188740,6111921,2016-03-24T23:35:12,<p>Just do an <code>activator</code> instead of <code>activator run</code> then do a <code>runAll</code> as recommended above and in the getting started guide.</p>&#xA;
39019804,39014049,1074097,2016-08-18T13:42:34,"<p>Please see the netty examples which has everything you need:&#xA;<a href=""https://github.com/netty/netty/tree/4.1/example/src/main/java/io/netty/example/http/snoop"" rel=""nofollow"">https://github.com/netty/netty/tree/4.1/example/src/main/java/io/netty/example/http/snoop</a></p>&#xA;"
47465223,47461815,161457,2017-11-24T01:35:07,"<p>One option would be to use events.</p>&#xA;&#xA;<p>When Service B changes data in table T1, it raises a domain event (publishing a message to a message broker or service bus).</p>&#xA;&#xA;<p>Service A subscribes to that event. When it receives a message, it updates its local copy of table T1. It's not the system of record, but it will always be up to date (within the latency of the broker/bus and the time to process the message queue). Then when service A needs data, it serves it up from its local T1.</p>&#xA;&#xA;<p>For more detail on this and other techniques, see <a href=""https://www.infoq.com/presentations/microservices-data-centric"" rel=""nofollow noreferrer"">Managing Data in Microservices</a> by Randy Shoup, particularly the section on Microservice Techniques: Shared Data.</p>&#xA;"
43804186,35179495,5097393,2017-05-05T11:48:13,"<p>In cloud foundry, it's quite straightforward to scale an app to multiple instances once you deploy it.</p>&#xA;&#xA;<p><strong>For deploy -</strong></p>&#xA;&#xA;<pre><code>cf push -f &lt;deployment manifest file&gt;&#xA;</code></pre>&#xA;&#xA;<p><strong>For scaling to multiple instances -</strong></p>&#xA;&#xA;<pre><code>cf scale &lt;your_app_name&gt; -i &lt;number_of_instances&gt;&#xA;</code></pre>&#xA;"
43804547,39601492,5097393,2017-05-05T12:07:25,"<p>You can make use of service discovery pattern, which are mainly of two kinds -</p>&#xA;&#xA;<ol>&#xA;<li><p>Client-side discovery - This is where clients are responsible for figuring out available service instances. Example - Netflix OSS.</p></li>&#xA;<li><p>Server-side discovery - In this the service instances are registered on the server-side using a service registry. Example - AWS ELB.</p></li>&#xA;</ol>&#xA;&#xA;<p>You can read a lot about these on the internet. Just remember the keywords.&#xA;Hope this helps !</p>&#xA;"
47492029,47486390,5294769,2017-11-26T00:57:46,"<p>So here was my issue. The routing was working but I was returning 404's from my services. This was because the Location route was being passed along with the request. So in the end I found this answer: </p>&#xA;&#xA;<p><a href=""https://serverfault.com/questions/562756/how-to-remove-the-path-with-an-nginx-proxy-pass"">https://serverfault.com/questions/562756/how-to-remove-the-path-with-an-nginx-proxy-pass</a></p>&#xA;&#xA;<p>Where it points that you need a trailing slash in order to remove the location so my config file ended up looking something like this:</p>&#xA;&#xA;<pre><code>location /auth/ {&#xA;&#xA;            proxy_pass http://auth-target/;&#xA;&#xA;            proxy_redirect     off;&#xA;            proxy_set_header   Host $host;&#xA;            proxy_set_header   X-Real-IP $remote_addr;&#xA;            proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;&#xA;            proxy_set_header   X-Forwarded-Host $server_name;&#xA;        }&#xA;</code></pre>&#xA;"
30218830,30213456,775345,2015-05-13T15:08:59,"<p>If my wallet was just another bunch of records in the same sql database as the user then I would probably place the user and wallet creation code in the same service and handle that using the normal database transaction facilities.</p>&#xA;&#xA;<p>It sounds to me you are asking about what happens when the wallet creation code requires you touch another other system or systems?  Id say it all depends on how complex and or risky the creation process is.  </p>&#xA;&#xA;<p>If it's just a matter of touching another reliable datastore (say one that can't participate in your sql transactions), then depending on the overall system parameters, I might be willing to risk the vanishingly small chance that second write won't happen. I might do nothing, but raise an exception and deal with the inconsistent data via a compensating transaction or even some ad-hoc method. As I always tell my developers: ""if this sort of thing is happening in the app, it won't go unnoticed"".</p>&#xA;&#xA;<p>As the complexity and risk of wallet creation increases you must take steps to ameliorate the risks involved.  Let's say some of the steps require calling multiple partner apis.  </p>&#xA;&#xA;<p>At this point you might introduce a message queue along with the notion of partially constructed users and/or wallets.</p>&#xA;&#xA;<p>A simple and effective strategy for making sure your entities eventually get constructed properly is to have the jobs retry until they succeed, but a lot depends on the use cases for your application.</p>&#xA;&#xA;<p>I would also think long and hard about why I had a failure prone step in my provisioning process.</p>&#xA;"
34889455,34889229,2416049,2016-01-20T00:28:02,"<p>For production changes you'll want to deploy a new image for changes to the file. This ensures your process is repeatable.</p>&#xA;&#xA;<p>However, developing by making a new image every time you write a new line of code would be a nightmare. The best option is to run your docker container and mount the source directory of the container to your file system. That way, when you make changes in your editor, the code in the container updates too. </p>&#xA;&#xA;<p>You can achieve this like so:</p>&#xA;&#xA;<p><code>docker run -v /Users/me/myapp:/src myapp_image</code></p>&#xA;&#xA;<p>That way you only have to build <code>myapp_image</code> once and can easily make changes thereafter.</p>&#xA;&#xA;<p>Now, if you had a running container that was not mounted and you wanted to make changes to the file, you can do that too. It's not recommended, but it's easy to see why you might want to.</p>&#xA;&#xA;<p>If you run:</p>&#xA;&#xA;<p><code>docker exec -it &lt;my-container-id&gt; bash</code></p>&#xA;&#xA;<p>This will put you into the container and you can make changes in vim/nano/editor of your choice while you're inside.</p>&#xA;"
47624185,47622392,856498,2017-12-03T22:47:25,"<p>Well I found this nice tool <a href=""https://github.com/othiym23/node-continuation-local-storage"" rel=""nofollow noreferrer"">https://github.com/othiym23/node-continuation-local-storage</a></p>&#xA;&#xA;<pre><code>// Micro handler&#xA;const { createNamespace } = require('continuation-local-storage')&#xA;let namespace = createNamespace('foo')&#xA;&#xA;const handler = async (req, res) =&gt; {&#xA;  const clientId = // some header thing or host&#xA;&#xA;&#xA; namespace.run(function() {&#xA;  namespace.set('clientId', clientId)&#xA;    someCode()&#xA;  })&#xA;})&#xA;&#xA;&#xA;// Some other file&#xA;const { getNamespace } = require('continuation-local-storage')&#xA;&#xA;const someCode = () =&gt; {&#xA;  const namespace = getNamespace('foo')&#xA;  console.log(namespace.get('clientId'))&#xA;}&#xA;</code></pre>&#xA;"
49910496,49910421,2584754,2018-04-18T23:28:44,"<ol>&#xA;<li><p>Not at all. The use of microservices is an advanced architecture pattern that is hard to implement right, but that gives useful benefits in huge projects. This should not be of any concern to a small project, unless you want to test this particular architectural style.</p></li>&#xA;<li><p>Breaking an application in smaller chunk does increase its scalability, as resources can be increased on a smaller scale. However, statelesness, among other properties, are also key components to a scalable architecture.</p></li>&#xA;</ol>&#xA;"
49949187,49931495,4690866,2018-04-20T20:21:30,"<p>An <code>UNKNOWN</code> status generally means the server failed in some way. You probably want to check the server logs.</p>&#xA;&#xA;<p>There's no real limits to the number of connections and RPCs a server can have. It is possible to run out of file descriptors with too many connections. It is possible to reach RPC limits that would cause them to be queued until they could be sent. And as with anything, you may run into memory usage and similar limits.</p>&#xA;"
43060666,43055993,1045170,2017-03-28T04:30:39,"<p>I have a guess, that you're heard a reference on the talk of Eric Evans on GOTO 2015 about strategic design and microservices, so you may want to watch this capture at first:</p>&#xA;&#xA;<p><a href=""https://www.youtube.com/watch?v=yPvef9R3k-M"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=yPvef9R3k-M</a></p>&#xA;&#xA;<p>This topic also was covered by him in a further talk:</p>&#xA;&#xA;<p><a href=""https://www.infoq.com/presentations/ddd-microservices-2016"" rel=""nofollow noreferrer"">https://www.infoq.com/presentations/ddd-microservices-2016</a></p>&#xA;"
46233896,46190467,589490,2017-09-15T07:19:44,"<p>I think i have now understood it to a better extent.&#xA;The Aggregate would still be created, answer is that all the validations for any type of consistency should happen before my aggregate is constructed, it is in case of a failure beyond the purview of the code that a failure exists while updating the read side DB of the microservice which needs to be handled.&#xA;So in an ideal case aggregate would be created however the event associated would remain as undispatched unless all the read dependencies are updated, if not it remains as undispatched and that can be handled seperately.&#xA;The Event Store will still have all the event and the eventual consistency this way is maintained as is.</p>&#xA;"
34809745,34794630,540339,2016-01-15T11:10:13,"<p><strong>Option 1 (Preferred)</strong></p>&#xA;&#xA;<p>The easy way is the micro services should be behind the gateway, hence you would whitelist services to connect to them, meaning only authorized and trusted parties have access (i.e. the gateway only). Clients shouldn't have direct access to them. The Gateway is your night club bouncer.</p>&#xA;&#xA;<p><strong>Option 2</strong></p>&#xA;&#xA;<p>You can use a JWT or some form of token and share the secret key between the services. I use JWT Authorization Bearer tokens.</p>&#xA;&#xA;<p>The other services don't need to query the user service, they just need to know that the token is valid, then they have authorization to use the API. I get the JWT passed from the client to the gateway and inject it into the request that is sent to the other service behind, just a straight pass through.</p>&#xA;&#xA;<p>The micro service behind needs to have the same JWT consumption as the gateway for authorization but as I mentioned that is just determining a valid token, not querying a valid user.</p>&#xA;&#xA;<p>But this has an issue that once someone is authorized they can jump call upon other users data unless you include something like a claim in the token.</p>&#xA;&#xA;<p><strong>My Thoughts</strong></p>&#xA;&#xA;<p>The part that I found a challenge from Monolithic to Micro Services was that you needed to switch where you place your trust. In Monolithic you control everything you are in charge. The point of Micro Services is that other services are in complete control of their domain. You have to place your trust in that other service to fulfill its obligations and not want to recheck and reauthorize everything at every level beyond what is necessary.</p>&#xA;"
30915083,30908112,540339,2015-06-18T12:15:09,"<p>While @jlvaquero provided the idea above I just wanted to list what my actual solution was and why.</p>&#xA;&#xA;<p>It then comes down to this setup</p>&#xA;&#xA;<p><img src=""https://i.stack.imgur.com/wPivk.png"" alt=""enter image description here""></p>&#xA;&#xA;<p>Now the validation is done at a gateway level. The only thing I am having some degree of uncertainty of here is the fact I am now doing validation of an entity outside the service that is meant to be in charge of that domain. </p>&#xA;&#xA;<p>The inventory service is just accepting that the user is allowed to attach to that location. But considering that the location and user validation is outside of the services domain, it fits in that it shouldn't concern itself with that validation.</p>&#xA;"
47860379,47860278,6768051,2017-12-17T23:15:09,"<p>As far as I can see it seems that these two services need to share the same data and you are thinking to share also the library that is used to read/modify this data.</p>&#xA;&#xA;<p>They seem to belong to the same ""bounded context"" and so it would be ideal to consider them as a unique service.</p>&#xA;&#xA;<p>If you really can't separate  their data and their logic, it would be better to keep them together. </p>&#xA;&#xA;<p>I do not think that components in the same microservices have to follow the same deployment pattern: they will be the same microservice, managed by the same team, deployed always together, sharing the same source repository, but they will be deployed with a different strategy because only the second component needs high scalability.</p>&#xA;&#xA;<p>So same bounded context, same service, but different components.</p>&#xA;&#xA;<p>I do not have a lot of experience and so take this as my personal thought.</p>&#xA;"
50189847,50180928,87475,2018-05-05T13:12:51,"<p>Whatever the error might be, your httpclient request is causing your web server to crash. In order to investigate it, you need to go to to the Event Viewer of your AppService.</p>&#xA;&#xA;<p>Here are the steps to launch the event viewer:</p>&#xA;&#xA;<ol>&#xA;<li>Go to Kudu management site of your website (ie https://{sitename}.scm.azurewebsites.net</li>&#xA;<li>Open menu item: Tools => Support</li>&#xA;<li>Choose relevant Azure AD Directory of your website</li>&#xA;<li>Click on <strong>Analyze</strong> => Event Viewer.</li>&#xA;<li>Check the error messages</li>&#xA;</ol>&#xA;"
43518463,41809373,3301043,2017-04-20T11:37:14,"<p>It is the same concept even if it is another technology. </p>&#xA;&#xA;<p>The idea is to test for multiple vulnerabilities in the system. Usually you would want to test and control all the input in the application. The most severe vulnerabilities would be code injection attacks (SQL, Command, Client-Side code etc.), also not excluding many others.&#xA;You would also want to test for logical security vulnerabilities, like if some application feature is not implemented correctly (e.g. the Authentication/Authorization mechanism, including the user password recovery or account registration etc.)</p>&#xA;&#xA;<p>I would strongly recommend you to go through the <a href=""https://www.owasp.org/index.php/Top_10_2013-Top_10"" rel=""nofollow noreferrer"">OWASP Top 10</a> list and check for their guidelines for best security coding practices and how to avoid and prevent such attacks. Considering that you mentioned testing on Microservices, for which I suppose they are some kind of REST API's then focus more on API Security issues.</p>&#xA;"
46785825,46783912,7834803,2017-10-17T08:32:29,"<p>You should instantiate jetty Server class in init method, because it will be invoked on the target machine right after service deployment. Instantiating Server class in the constructor is useless - right after creating a Service instance(which could be performed on other nodes in most cases), this instance will be serialized and added to the internal cache and only after it will be started on the target machine. </p>&#xA;&#xA;<p>I think it's clear that jetty Server object can't be serialized properly. For example, serializing of ThreadPool can't be done, because Thread implementation contains blocks of native code.</p>&#xA;"
39767357,39763013,895667,2016-09-29T10:07:35,"<p>The powerShell script file that does the cluster setup magic is:</p>&#xA;&#xA;<pre><code>Program Files\Microsoft SDKs\Service Fabric\ClusterSetup\DevClusterSetup.ps1&#xA;</code></pre>&#xA;&#xA;<p>Looking inside, there is a function called <code>DeployNodeConfiguration</code> which sets the logs and data path using the PowerShell command <code>New-ServiceFabricNodeConfiguration</code>. Unfortunately, It does not seem that there is a way to limit the size of those folders.</p>&#xA;&#xA;<p>I believe that your slowness / freeze is due to insufficient space on the OS drive (happened to me too haha). A workaround can be to set the location of those folders to a non-OS drive with a limited amount of space.</p>&#xA;&#xA;<p>Hope this helps</p>&#xA;"
49355796,49352887,5028275,2018-03-19T04:26:24,"<p>The question is very broad, as authentication and authorization are big topics in themselves.</p>&#xA;&#xA;<p>I will try to keep the answer short here. If you have time and resources, please take a look at using OAuth. They are the industry-standard way of providing auth and access to REST APIs.</p>&#xA;&#xA;<p>You can define different access patterns and associate it with OAuth at the time of user login. The authentication can be a separate service and it just handles the case of which user has what privileges. It would be better here to not be service-specific here like user A can access API B and C. Instead be functionality driven, like user A is admin, user B is a privileged user, user C is a particular business user who has access to make payments, etc.</p>&#xA;&#xA;<p>Now that you have OAuth implemented and have a way of associating users to their access controls, pass these as headers to your actual micro-service. In your API, just check if the user has the right token and access and continue. If not, error out with 422. Heck, if you use good libraries, you can even do it outside of your API code (using filters, etc.)</p>&#xA;&#xA;<p>Now coming to the alternatives you looked at, all of them might work, but they will have cons. Example, whitelisted IP might mean that every time the IP of your client changes, you need to change the whitelist. Or make it wildcard match, which can expose other unwanted IPs as well if you are not careful. Internal vs external services can generally mean public vs private APIs. It means that some APIs are not even accessible in the public network and only those within the VPN or subnet can access. Comes with its own complexities and problems.</p>&#xA;&#xA;<p>My two cents: strive for cleaner and common patterns that everyone is using. </p>&#xA;"
39978519,39967784,5028275,2016-10-11T13:35:21,"<p>Adding to what Sean had said, microservices are what people started to call APIs when SOA had started to being put to use in many companies. The rise of Domain-driven design has also led to the increase in usage of the term. In the industry right now, there is absolutely no difference between the two, people call it as they seem fit.</p>&#xA;&#xA;<p>You are right when you said that you will end up with many micro-services when you follow the philosophy in principle. In my opinion, be it SOA or microservices, abstraction to independent services should depend only on the use-case, how the services are going to be deployed and how many teams are going to work on those in parallel. There is also an increasing cost to network bandwidth if the services are deployed across hosts (though containers and DC/OS frameworks are solving this problem now). If it is fast-changing service, having lots of moving parts, then breaking down a big service into microservices would make sense. Otherwise I would avoid premature optimisation and have the functionality packaged into a single (or a few big) services.</p>&#xA;"
48944891,47793065,7334835,2018-02-23T09:37:34,"<p>We asked ourselves the same question. Unfortunately, it seems that the answer to the question</p>&#xA;&#xA;<p><strong>Is there any solution other than ""iframes"" to get multiple Angular (5) Apps running on the same Page (edit: where each Angular app can use a different Angular version)?</strong></p>&#xA;&#xA;<p>currently is </p>&#xA;&#xA;<p><strong>No, unfortunately not, as long as you want to use Angular‘s change detection (which uses zone.js).</strong></p>&#xA;&#xA;<p>Due to zone.js Angular pollutes the global scope. Even worse zone.js patches an awful lot of browser APIs (<a href=""https://github.com/angular/zone.js/blob/master/STANDARD-APIS.md"" rel=""nofollow noreferrer"">https://github.com/angular/zone.js/blob/master/STANDARD-APIS.md</a>) to detect when a zone is completed.</p>&#xA;&#xA;<p>It’s only possible to use different versions of a framework in one page without side effects if the framework does not touch global scope (this seems to be true for React und Vue). Then you can bundle different framework versions with each application via Webpack (with separated scope and the downside that the size of each application bundle increases).</p>&#xA;&#xA;<p>So, if you want to build a web page with Angular where different applications/modules should be integrated on one page, the only feasible solution currently is to create a deployment monolith (e.g. bundle different modules from different teams in one Angular application by a CI/CD system as bhantol explained in his answer). </p>&#xA;&#xA;<p>It seems that the Angular team is also aware of the problem and that they might tackle it with following major versions. See robwormwald’s answer on the following Github issue regarding the Angular Elements roadmap: <a href=""https://github.com/angular/angular/issues/20891"" rel=""nofollow noreferrer"">https://github.com/angular/angular/issues/20891</a></p>&#xA;&#xA;<p>Hopefully there will be more information by the Angular team on that topic when the next major version Angular 6 is release at the end of march.</p>&#xA;"
49863828,49861169,983476,2018-04-16T18:17:32,"<p>Trying to implement cross service update through REST based synchronization is a bad idea because it is not scalable in a sense that if you add more microservices that needs to be aware of updates made on service A. You would have to modify the existing microservice that emits the change. This in fact introduces risk and additional maintenance cost.</p>&#xA;&#xA;<p>However, you can try to use <a href=""https://wiredcraft.com/blog/building-microservices-nsq-rabbitmq/"" rel=""nofollow noreferrer"">messaging queues</a> to emit events that indicates changes made on a service. This approach eliminates the need to modify any existing microservice (Thanks to pub/sub pattern) and just plug new consumers to your existing update emitting services in your ecosystem </p>&#xA;"
49863909,49843956,983476,2018-04-16T18:23:19,"<p>Please take a look at <a href=""https://msdn.microsoft.com/en-us/library/jj591559.aspx"" rel=""nofollow noreferrer"">Event Sourcing</a> and <a href=""https://msdn.microsoft.com/en-us/library/jj554200.aspx"" rel=""nofollow noreferrer"">CQRS</a>. You can also take a look at my personal coding project:</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/RONH7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RONH7.png"" alt=""enter image description here""></a></p>&#xA;"
49506728,49488573,983476,2018-03-27T07:26:33,"<ol>&#xA;<li>Don't generate your GUIDs on client apps. Its a bad idea because if this is generally business implementation that should be enclosed on your bounded context services. Client UIs should be thin and dumb as possible.</li>&#xA;<li>I would suggest you have a file server for both your layer 2 / bounded context APIs. This would make them independent of each other's existence. </li>&#xA;</ol>&#xA;&#xA;<p>There are also a few stuff that you can improve with this design:</p>&#xA;&#xA;<ol>&#xA;<li>Consider introducing an API Aggregator. An API aggregator would be in charge of implementing request routing from client applications to level two APIs. Doing so would help you encapsulate level 2 (Bounded context services) which gives you flexibility to swap level APIs. Client apps talking directly to your APIs dedicated for your bounded contexts disables you from making autonomous changes on these APIs</li>&#xA;<li>Consider implementing a <a href=""https://docs.microsoft.com/en-us/azure/architecture/patterns/compensating-transaction"" rel=""nofollow noreferrer"">Messaging Saga</a> and <a href=""https://docs.particular.net/nservicebus/sagas/"" rel=""nofollow noreferrer"">Compensating Transactions</a></li>&#xA;</ol>&#xA;&#xA;<p>To give you an idea how a usual microservice architecture looks like here is an architecture diagram from one of my designs:</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/GkP5V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GkP5V.png"" alt=""enter image description here""></a></p>&#xA;"
51223871,51175627,1573556,2018-07-07T13:54:20,"<p>For people who'll have this exact problem, it took me 5 days to find it. I was sure it was in JSON, because, when I copied/pasted the result, it was ok. In fact, it was in BSON (I didn't even know this format). So, just add this at the beginning of your file : </p>&#xA;&#xA;<pre><code>from bson.json_util import dumps&#xA;</code></pre>&#xA;&#xA;<p>Then, add this : </p>&#xA;&#xA;<pre><code># limit for the example&#xA;retrievedocs = someCollection.find({* something *}}, {'_id': 0}).limit(10) &#xA;docs = dumps(retrievedocs)&#xA;</code></pre>&#xA;&#xA;<p>Then, the serialization between Nameko and Flask will work.</p>&#xA;"
46133851,46131443,4326068,2017-09-09T18:28:40,<p>Your configuration is not setup properly. It's thinks the hostname is 'config'. </p>&#xA;
49302134,49302010,3019956,2018-03-15T14:30:54,"<p>We can't weigh advantages and disadvantages without looking what that application is all about, what are business rules, how you are dividing application and how two applications share business rules.&#xA;Dividing an application into two application is not just diving java classes into two groups. It require depth analysis from different perspective. Hope this helps.</p>&#xA;"
49994074,49992237,603316,2018-04-24T05:33:02,"<p>If you change Go code you have to recompile; that's not abnormal. Go code compiles quickly so that's usually not a big deal.</p>&#xA;&#xA;<p>I'm not sure why you are running govendor repeatedly. If you are working on a dependency it is often easier to unvendor it temporarily.</p>&#xA;&#xA;<p>To speed things up, don't use Docker during development; at least not for the Go code. Since Go doesn't have any dependencies beside itself there's no advantage to doing this. You can still run backing services (e.g. a database) in Docker, but you don't have to rebuild and restart that all the time.</p>&#xA;"
32997137,32996097,280244,2015-10-07T16:03:51,"<p>Think about <a href=""http://openid.net/connect/"" rel=""nofollow"">OpenID - Connect</a>. It is a SSO based on Oauth2.</p>&#xA;"
45383304,40234243,3834036,2017-07-28T22:35:13,"<p>First of all: OAuth 2.0 is not an authentication protocol, it is a delegated access protocol. It is clearly stated here: <a href=""https://oauth.net/articles/authentication/"" rel=""nofollow noreferrer"">https://oauth.net/articles/authentication/</a></p>&#xA;&#xA;<p>Although widely adopted, the OAuth 2.0 ""authentication framework"" left many details open for interpretations - which commonly leads to security flaws of the implementation.&#xA;Check here for the 10 most common implementation vulnerabilities in OAuth 2.0: <a href=""http://blog.intothesymmetry.com/2015/12/top-10-oauth-2-implementation.html"" rel=""nofollow noreferrer"">http://blog.intothesymmetry.com/2015/12/top-10-oauth-2-implementation.html</a></p>&#xA;&#xA;<p>Therefore, the actual best practice is to use OpenID Connect, a similar protocol (built on top of OAuth 2.0), well defined, that mitigate most of the shortcomings of OAuth 2.0.</p>&#xA;&#xA;<p>OpenID Connect is the best practice to authenticate end-users (mostly web).</p>&#xA;&#xA;<p>If you want to authenticate within the datacenter, the variety of used solutions is somewhat wider - but overall I think the most common best practices are:</p>&#xA;&#xA;<ul>&#xA;<li>""Leaner"" implementation: clear HTTP when you appropriate network security (e.g. well-configured VPC, so access from the internet to any of these servers is very unlikely)</li>&#xA;<li>""Safter"" implementation: Server to Server Basic Authentication (or similar) over HTTPS, while rotating the key every now and then. The keys should be stored in a secure storage, such as <a href=""https://www.vaultproject.io/"" rel=""nofollow noreferrer"">Vault</a></li>&#xA;</ul>&#xA;&#xA;<p>In any case, it is best the service will <em>delegate the request for the user</em> (i.e. by providing <code>user_id</code> as part of the request) - and permissions will be enforced for this user:</p>&#xA;&#xA;<ul>&#xA;<li>You probably don't want to allow a bug allowing one user to access the data of another user.</li>&#xA;<li>In any case, it is much better logs / audit will be done with link to user originating the request, and not some generic ""system user"".</li>&#xA;</ul>&#xA;"
45383478,45374744,3834036,2017-07-28T22:57:10,"<p>There is no inherent benefit of making health checks on different port over different URL path - or vice versa.</p>&#xA;&#xA;<p>At the end, the port is just an abstraction of the underlying Operating System, routing traffic to different processes.</p>&#xA;&#xA;<p>What can matter is the concrete network infrastructure / setup in use: Firewall, load balancers, service discovery, etc.</p>&#xA;&#xA;<p>Maybe different ports are easier to configure as rules in the Firewall / IPS / Web Application Firewall? - but this is very implementation specific.</p>&#xA;&#xA;<p>To recap: both are fine. Use what makes your life easier, unless you discover some concrete need by your infrastructure / network setup.</p>&#xA;"
45667795,45666983,3834036,2017-08-14T05:11:34,"<ul>&#xA;<li>Sharing large data set <em>may</em> be an indication for a suboptimal partitioning of the codebase into services. It is preferred all processing of the same domain will be done within a single service.</li>&#xA;<li>When multiple services do have meaningful processing to be done on the same data set - each should have its own copy of it. Sharing databases, is typically - a bad idea!</li>&#xA;<li>When heavyweight data is involved, cloning the data in a ""regular"" queueing system (such as RabbitMQ / SQS) is quite cumbersome and inefficient.&#xA;&#xA;<ul>&#xA;<li>A ""heavyweight"" queuing system such as Kafka / Kinesis - may be most efficient. One copy of the data will be persisted, and each service can read it from a ""shared"" stream.</li>&#xA;</ul></li>&#xA;</ul>&#xA;"
45657278,45655728,3834036,2017-08-13T05:05:17,"<p>The Microservices approach is about breaking your system (""pile of code"") into many small services, each typically has its own:</p>&#xA;&#xA;<ul>&#xA;<li>Clear business-related responsibility</li>&#xA;<li>Running process</li>&#xA;<li>Database</li>&#xA;<li>Code version control (e.g. git) repository</li>&#xA;<li>API</li>&#xA;<li>UI</li>&#xA;</ul>&#xA;&#xA;<p>The services themselves are kept <strong>small</strong> so as your system grow, there are more services - rather than larger services.</p>&#xA;&#xA;<p>Microservices can use REST, RPC, or any other method to communicate with one another, so REST is really orthogonal to the topic of microservices...</p>&#xA;"
20693597,20693516,495796,2013-12-19T23:18:07,"<blockquote>&#xA;  <p>I have read some articles and watched some videos, but did not find a concrete suggestion when it comes to serving those micro-services. My understanding is that they should be served with their own application server.</p>&#xA;</blockquote>&#xA;&#xA;<p>That is not really necessary. Frameworks such as Play and Spray do not need an application server.</p>&#xA;&#xA;<p>See <a href=""http://yobriefca.se/blog/2013/04/29/micro-service-architecture/"" rel=""nofollow"">here</a> - I quote:</p>&#xA;&#xA;<p><em>You're also not going to find any true micro service based architectures that are hosted in an application server, that kinds of defeats the point. To this end micro services self host, they grab a port and listen. This means you'll lose any benefits your typical enterprise application server may bring and your service will need to provide some of the more essential ones (instrumentation, monitoring etc.).</em></p>&#xA;&#xA;<hr>&#xA;&#xA;<blockquote>&#xA;  <p>My question is should they be deployed on different servers or it does not matter.</p>&#xA;</blockquote>&#xA;&#xA;<p>You definitely want to share servers to conserve hardware resources. Containerisation (see e.g. Docker and OpenShift) is quite a nice model.</p>&#xA;&#xA;<blockquote>&#xA;  <p>When they are served on the same server(computer) won't there be port conflicts?</p>&#xA;</blockquote>&#xA;&#xA;<p>Not if you use Virtual IP addresses and bind to the VIP.</p>&#xA;&#xA;<p>Alternatively, Docker has a different approach. It binds to a random port; then you could port forward that port on an ""agreed"" port for that service, to expose it to the outside world.</p>&#xA;"
50436006,50434941,495796,2018-05-20T14:46:55,"<p>That would depend on what system you were using for service registration. For service self-registration I would suggest Consul might be a good option. You would do the following:</p>&#xA;&#xA;<ol>&#xA;<li>Set up a Consul cluster with a consul client on each of your machines running a microservice, as covered in the Consul documentation.</li>&#xA;<li>In each microservice, open a HTTP connection to the Consul client running on localhost on the standard Consul HTTP client port (8500).</li>&#xA;<li>The microservice would self-register with the local Consul using the HTTP connection.</li>&#xA;<li>The microservice would discover the location of other services using either the same HTTP access to Consul, or using its built-in DNS server. Obviously there might be a bootstrapping problem here if you have service A that depends on service B and service B also depends on service A - in which case, you would need to implement some wait-and-retry logic.</li>&#xA;</ol>&#xA;"
30915091,30915043,364980,2015-06-18T12:15:36,"<p>Your <code>Dockerfile</code> is not correct.</p>&#xA;&#xA;<p>Try this instead:</p>&#xA;&#xA;<pre><code>FROM ubuntu&#xA;MAINTAINER Will Mayger&#xA;RUN echo ""deb http://archive.ubuntu.com/ubuntu/ $(lsb_release -sc) main universe"" &gt;&gt; /etc/apt/sources.list&#xA;RUN apt-get update&#xA;RUN apt-get install -y tar git curl nano wget dialog net-tools build-essential&#xA;RUN apt-get install -y python python-dev python-distribute python-pip&#xA;RUN git clone https://github.com/CanopyCloud/microservice-python&#xA;RUN pip install -r /microservice-python/requirements.txt&#xA;EXPOSE 80&#xA;WORKDIR /microservice-python/&#xA;RUN sudo rabbitmqctl add_user test testpass1&#xA;RUN sudo rabbitmqctl add_vhost myvhost&#xA;RUN sudo rabbitmqctl set_permissions -p myvhost test "".*"" "".*"" "".*""&#xA;RUN sudo rabbitmq-server&#xA;&#xA;CMD python /microservice-python/server.py&#xA;</code></pre>&#xA;&#xA;<p>The reason it isn't/wasn't correct was because you were defning multiple <code>CMD</code>(s) in your <code>Dockerfile</code>. I'm pretty sure docker will only set the last command in the resulting image and <code>CMD</code> does not ""run"" things as part of the image build process; <code>RUN</code> does.</p>&#xA;&#xA;<p><code>CMD</code> sets up the ""command"" that the image runs as part of <code>docker run &lt;image&gt;</code></p>&#xA;&#xA;<hr>&#xA;&#xA;<p>Also you seem to have combined RabbitMQ and your Python app into the one Docker Image/Container; which isn't really the <em>best</em> thing to do here.</p>&#xA;&#xA;<p>You <em>should</em> instead split this out into two images.</p>&#xA;&#xA;<ul>&#xA;<li>A RabbitMQ Image/Container</li>&#xA;<li>Your App Image/Container</li>&#xA;</ul>&#xA;&#xA;<p>And use ""Docker Links"" via <code>docker run --link</code> to link the containers together.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>You <em>can</em> build an Image for your Python APp quite easily by using something like this as a separate <code>Dockerfile</code> for your Python App:</p>&#xA;&#xA;<pre><code>FROM python:2.7-onbuild&#xA;&#xA;RUN pip install -r requirements.txt&#xA;&#xA;ADD server.py /app&#xA;&#xA;WORKDIR /app&#xA;CMD [""python"", ""./server.py""]&#xA;</code></pre>&#xA;"
27470942,27470721,171956,2014-12-14T15:44:17,"<p>Say for example you are using a Java EE 6 application.</p>&#xA;&#xA;<p>Your CSV batch updater could be nothing more than a Timer that every once in a while reads a CSV file dumped in a folder and for each user update encoded on that file pumps a message to a queue encoding the update you want to do.</p>&#xA;&#xA;<p>Somewhere else, you have a message driven bean that reacts to the update request message and triggers the update business logic for the user reported on the JMS message.</p>&#xA;&#xA;<p>After the transaction is committed successfuly, if you have ten differn application that are interested in knowing that the user was updated, you could post a message to, for example, a notification topic with - say - messageType='userUpdated'.&#xA;Each of your 10 applications that cares about this could be a consumer on this topic.&#xA;They would be informed that a user was updated and maybe internally publish a local event (e.g. a CDI event - a guava event - whatever - and the internal satke holders would now of it).</p>&#xA;&#xA;<p>Normally, there are always Java EE replacements in every technlogy stack.&#xA;Every decent technology stack offers ways to promote loose coupling between UI and business logic, precisely so that HTML / WEB is just viewed as one of many entry points to an application's business logic.</p>&#xA;&#xA;<p>In scala, i there is an AKKA framework that looks super interesting.</p>&#xA;&#xA;<p>The point is, as long as your business logic is not written in some place that only the web application can tap into, your fine. Otherwise, you've made the design decision to couple your business logic with UI.</p>&#xA;"
46680273,46575898,6550196,2017-10-11T05:10:02,"<p>The right word for microservice is right size service . Scope and size  Microservice should be based upon following </p>&#xA;&#xA;<ol>&#xA;<li>SRP - Microservice should have a single reason for change. It should do one thing and do it well. </li>&#xA;<li>Business capability - It should represent a business capability which should be relatively autonomous in nature and can be executed by a team with minimal interaction with other capabilities </li>&#xA;<li>BC/UL   It should be based upon <a href=""http://www.methodsandtools.com/archive/archive.php?id=97"" rel=""nofollow noreferrer"">BC and UL</a>. These are concepts from DDD. Please see the link for details. </li>&#xA;</ol>&#xA;"
51953177,51953109,3933365,2018-08-21T16:37:26,"<p>What you have asked could be one part of the job. Backend developer could also be working on the Model and Controller in case of MVC architecture, incase it is a single monolithic application. Better get it clarified. :)</p>&#xA;"
47643757,47630168,1181621,2017-12-04T23:37:49,"<p>Not an expert either, but </p>&#xA;&#xA;<blockquote>&#xA;  <p>If we do the token verification in API gateway only (not in microservices) then from the API gateway we need to send the username in every microservices. And microservices implementation needs to be changed to accept that param/header.</p>&#xA;</blockquote>&#xA;&#xA;<p>could be changed this way:</p>&#xA;&#xA;<ul>&#xA;<li>You make authentication/authorisation the problem of the gateway. </li>&#xA;<li>When gateway authorizes the client, it attaches JWT token to every microservice request its going to make on behalf of the client (instead of sending username). JWT will contain all information that microservices might need. If the microservice will need to call other microservice, it will need to pass that token further with the request.</li>&#xA;</ul>&#xA;&#xA;<p>So the idea is - for EVERY request that comes through the gateway, a NEW JWT is being attached to the request. Then you don't have expiry problem and tokens are easy to verify.</p>&#xA;"
48857114,48791411,1181621,2018-02-18T22:05:57,"<p>Try the following:</p>&#xA;&#xA;<ul>&#xA;<li>Create a <code>topic exchange</code> </li>&#xA;<li>Make your producer to send messages to it&#xA;with topic per resource: topic <code>A</code> for resource <code>A</code>, topic <code>B</code> for&#xA;resource <code>B</code> and so on.</li>&#xA;<li>Make 1 consumer to listen to 1 topic. Use durable queue for each&#xA;consumer, so the queue will survive consumer restarts. If you have&#xA;producer -> one exchange -> 1 queue -> 1 consumer message path, the&#xA;order of messages is guaranteed, see <a href=""https://www.rabbitmq.com/semantics.html"" rel=""nofollow noreferrer"">RabbitMQ broker semantics</a>&#xA;for details.</li>&#xA;<li>Make your consumer to get <a href=""https://stackoverflow.com/questions/29841690/how-to-consume-one-message"">1 message at a time</a>, and <code>ack</code> it only when&#xA;processing is done.</li>&#xA;</ul>&#xA;&#xA;<p>This will hopefully give you sequential processing that you need and you don't need to re-queue messages.</p>&#xA;&#xA;<p>Note that there is a possibility of receiving the same message twice - if your consumer died in between finishing processing task and <code>acking</code> the message.</p>&#xA;"
33837775,33814080,2246581,2015-11-20T23:29:31,"<p>This comes down to good your service location/detection technology is. All one service needs to know is where to send a request X in order to get a response Y from a particular service. If this is well implemented. you could very well have each developer run whatever subset of services he is directly working on locally and have the rest of the services run in a common environment (lets call it Remote)</p>&#xA;&#xA;<p>Remote will be configured to run all services in your platform and will have some way to get the latest code (based on cadence or based on regular intervals). Local environments could be configured to that all the services running locally will know what else is running locally and will know how to reach to the Remote services when they need any information. For developer environments, add conventions where each developer run minimum number of services required for productive development. This means if you are not working the code a service, run it remotely so you know you are running the latest code and are not out of date.</p>&#xA;&#xA;<p>Couple of gotchas with this approach</p>&#xA;&#xA;<ol>&#xA;<li>you could run service A locally and have service B running on remote. you test everything out locally and it seems to be working so you decide to push. While you are pushing, another developer also pushes changes to B and now your changes are not longer compatible. </li>&#xA;<li>If you are running services A and C locally and B remotely, it should be pretty straight forward to route requests to service B to the remote environment. You should however be careful that if A calls B and B calls C then the call to C needs to be routed from the remote environment to your local C service and not to your Remote C service.</li>&#xA;<li>Testing - You can get over a lot of issues related to testing with a complex environment like this by have two separate suite of tests. 1. unit tests- tests that test individual components in your service with all calls to other service being mocked. 2. environment integration tests- these tests validate the communication between different services. Suite 1 will check the internal code of your service and Suite 2 will run after you push your changes to remote and will continuously ensure that the inter-service communication is as expected.</li>&#xA;</ol>&#xA;&#xA;<p>Hope that helps</p>&#xA;"
49486253,49486062,5787829,2018-03-26T07:39:31,"<p>You can use <a href=""https://apacheignite.readme.io/docs/service-grid#section-affinity-key-based-deployment"" rel=""nofollow noreferrer"" title=""Affinity key based deployment"">Affinity key based deployment</a></p>&#xA;"
35762423,35743527,5787829,2016-03-03T03:06:44,"<p>I think you can't realize the behaviour you expect if you will use resources from localhost. Your test page can access  local resources for context 9090, e.g. when it trying to request for example &#xA;<code>&lt;script th:src=""@{webjars/angularjs/$angularjs.version$/angular.min.js}""&gt;&lt;/script&gt;</code> from 9090 request goes to 9090 but resources on 8080.&#xA;It will work if you will use static resources from CDN (from internet). </p>&#xA;&#xA;<p>Maybe <a href=""https://dzone.com/articles/thymeleaf-fragments-and"" rel=""nofollow"">this article</a> will be helpful for you.</p>&#xA;"
50792673,50790960,2060725,2018-06-11T07:43:59,"<p>You don't. What you describe is still having a single monolith and putting 100 microservices as a facade. That makes about as much sense as ordering a large pizza <em>and a dozen small salads as dessert</em> because you were told that having a small salad for a meal would help you lose weight. That's not how that works.</p>&#xA;&#xA;<p>Either look into microservices and what you gain by using that architecture, or go with your single service that does it all. Both might be valid choices, just don't pretend you are doing the other one too.</p>&#xA;"
36280486,36265833,619189,2016-03-29T09:34:32,"<p>It's hard to use versions with a timestamp or a git commit hash in everyday conversations with customers. So, you can consider the major part of the semantic version (v1 in v1.2.3) as an API version. You can create binary foo-v1.2.3.jar which exposes the API at foo/api/v1/. To introduce a new version of API you should change the version of binary accordingly. For example, for API version 2 you will create binary named foo-v2.0.1.jar.</p>&#xA;"
47607400,47607210,365237,2017-12-02T11:25:55,"<p>Of course it's possible. Like mentioned in the comments, ""pack it into a library and include it in all projects"" by using your dependency management. It's not a bad idea, though if the class is very simple, it might not be worth the effort.</p>&#xA;&#xA;<p>Since you're going with microservices, you could also change your architecture in a way that your authentication is a service in its own right, which other microservices use. Authentication usually isn't duplicated into all projects: there is a separate authentication service and different microservices either just check that authentication has already been done and has the permissions needed or call the authentication service when required.</p>&#xA;"
47036525,46782161,3479734,2017-10-31T13:22:04,"<p>You can use custom attributes (as suggested by @jarmod) if you only use Cognito userpools. But if you use other providers like Microsoft ADFS, Google, Facebook etc., you could look into Cognito Sync. Although Cognito Userpools now support some external providers, it may not be suited for your use case. So, you could integrate various Auth providers (including Userpool) in an Identity pool and use Cognito sync datasets to store preferences. In fact, that is the whole point of Sync, to provide cross device access to small datasets like user-preferences. This way if a user logs in with Userpool &amp; later with Facebook, you could give an option to link both accounts in your application &amp; merge the user preferences. It all depends on your use-case.</p>&#xA;"
29827127,29825744,3363254,2015-04-23T14:53:18,"<p>Web server and message broker have their own use cases. Web server used to host web services and the message broker are use to exchange messages between two points. If you need to deploy a web service then you have to use a web server, where you can process that message and send back a response. Now let's think that you need to have publisher/subscriber pattern or/and reliable messaging between any two nodes, between two servers, between client and server, or server and client, that's where the message broker comes into the picture where you can use a message broker in the middle of two nodes to achieve it. Using message broker gives you the reliability but you have to pay it with the performance. So the components you should use depends on your use case though there are multiple options available.</p>&#xA;"
37828818,37801454,2195638,2016-06-15T07:29:41,"<blockquote>&#xA;  <p>But the workflow of CAS is a little bit complicated: too many redirections, too many tickets transmitted among the CAS, web apps, and the user browser. I can't figure out why people deploy the Authentication Services in the center of all the web apps, rather than the front, like the following diagram.</p>&#xA;</blockquote>&#xA;&#xA;<p>You are thinking the old fashioned way, and thats why architects came up in the past with solutions like using portlet-servers for webapplications, session-shared-clusters, and lots of other heavy error-prone system-solutions which ate your cpu, memory or bandwith in ways which you could never predict until the first real life test. </p>&#xA;&#xA;<p>The CAS solution might look complicated on the first move, but you can predict how much logins, traffic and synchronisation data your system will generate and compared to the ""old solutions"", your system-to-user load will raise linear and not exponential. </p>&#xA;"
45093913,44977364,39094,2017-07-14T03:05:50,"<p>Technically and conceptually, a microservice is independent of other services (where in a monolith you'd have modules with inter-dependencies).</p>&#xA;&#xA;<p>Technically, a microservice built on modern microservices platforms (such as Node.JS, Spring Boot or .NetCore) will be more easily able to take advantages of containerization systems (such as Docker), perhaps supported by service registry and configuration management technologies (such as Kubernetes, ZooKeeper, Eureka and so on).</p>&#xA;&#xA;<p>The advantage of containerization is that it'll be easier to scale-out (add more containers).  Going further, the whole microservice / containerization concepts, and related technologies, also help enable things like CI/CD.</p>&#xA;"
47768881,47767993,6664886,2017-12-12T09:13:07,"<blockquote>&#xA;  <p>How to best eat an elephant?  One bite at a time...</p>&#xA;</blockquote>&#xA;&#xA;<p>Since this is a new world for you, I would start by peeling off one function of your application that isn't highly connected to anything else at first.  Pick on that reporting engine that doesn't use any actual business logic or that periodic maintenance task that queries the database directly or similar.  It need not be the most-in-need of performance enhancement (though IME reporting is almost always slower if it has to wade through business logic to get data).  The point is to peel something off that doesn't have huge prerequisites to get there so you gain confidence and understanding as to how the new environment will plug in.</p>&#xA;&#xA;<p>Once you have that, consider how much business logic you need and how deep your side effects are.  Try to find your second batch of work in things that don't have many side effects and are as close to a classical function (i.e. a bunch of inputs, one output) as possible.  If there's something in your RoR code that could be dispatched (like ""I got all this data"" ... then something magical happens ... then ""I output some processed data""), that's a good bet.</p>&#xA;&#xA;<p>There's a certain amount of latency added by the trips back and forth on HTTP (less if it's direct from the browser, but still...), so be careful to work in places where you know you can get gains.  A profiler against your old code would help bunches here--whatever's expensive would be your next candidate.</p>&#xA;&#xA;<p>If you still need optimizations after handling the low-hanging fruit, then you may need to consider splitting your business rules such that some live in the RoR world and others live in the Go one.  DO NOT MAINTAIN TWO IMPLEMENTATIONS OF THE SAME BUSINESS RULES--THAT WAY LIES MADNESS.  If you decide that orders are one system and supply chain is another, that's fine, but do not try to maintain implementations of the supply chain code in both environments.</p>&#xA;&#xA;<p>And for G-d's sake, if you migrate business logic from an old system to a new one, DON'T LEAVE THE OLD CODE IN PLACE BUT UNUSED!!!  You have version control--if someone needs to refer back to the old implementation, have them dig.  Otherwise you'll confuse your poor code maintainers with vestigial code that never gets run.  If you've deprecated code, REMOVE IT.</p>&#xA;&#xA;<p>Oh, and I'm sure I don't need to say this, but you need unit tests to verify bug-for-bug compatibility...</p>&#xA;&#xA;<p>Finally, keep in mind that RoR is highly MVC-oriented and Go is not <em>inherently</em> MVC (though there are a <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiZ_KuWi4TYAhUs0YMKHXIHDyAQFggnMAA&amp;url=https%3A%2F%2Fgithub.com%2Fgernest%2Futron&amp;usg=AOvVaw3VZd4unouRSN4BZ3fJbCAp"" rel=""nofollow noreferrer"">number</a> <a href=""https://github.com/kataras/iris"" rel=""nofollow noreferrer"">of</a> <a href=""https://revel.github.io/manual/concepts.html"" rel=""nofollow noreferrer"">frameworks</a> that are).  You'll definitely want to check out (and honestly probably discard) those frameworks in the vain attempt to do a direct port.  Go ahead: get it out of your system.  Then move on to doing small, incremental advances and slowly eat your elephant.</p>&#xA;"
51542467,51515839,2933686,2018-07-26T15:25:20,<p>Thank you all for your support and answers.</p>&#xA;&#xA;<p>seems the change I needed to do in my <code>app.yaml</code> file.<br>&#xA;<code>env: standard</code> does not support <strong>mongodb</strong>. you will have to use &#xA;    <code>env: flex</code> in your <strong>yaml</strong> file which will be used by <code>gcloud app deploy app.yaml</code>  command</p>&#xA;
30778271,30772774,53686,2015-06-11T10:27:44,"<p>As mentioned in the above comments, AWS will raise your limits if you have a legit use case - why wouldn't they? they are in the business of selling you services.</p>&#xA;&#xA;<p>But since you have asked for suggestion other than increasing those limits, and since you are in the early stages of designing your solution, you should consider basing part of your micro-services architecture on Docker or another container/container like service (my own preference would be for the AWS's container service). Depending on the nature of you solution, even within the limits of 20 EC2 instances (per region), if you had large enough instances running you could fit dozens (or even hundreds of lightweight) docker images running on each of those allocated 20 instances - so potentially hundres or thousands of walled off micro-services running on those 20 EC2 instances.</p>&#xA;&#xA;<p>Using an entire EC2 image for each of many micro-services you may have may end up being a lot more expensive than it needs to be.</p>&#xA;&#xA;<p>You should also consider the use of AWS Lamba for at least portions of your micro-service architecture - its the 'ultra-micro service' tool also offered by AWS.</p>&#xA;"
30298347,30296587,53686,2015-05-18T08:25:49,"<p>I think you are doing it wrong.</p>&#xA;&#xA;<p>It looks to me like you are using the same queue to do multiple different things. You are better of using a single queue for a single purpose.</p>&#xA;&#xA;<p>Instead of putting an event into the 'registration-new' queue and then having two different services poll that queue, and BOTH needing to read that message and both doing something different with it (and then needing a 3rd process that is supposed to delete that message after the other 2 have processed it).</p>&#xA;&#xA;<p>One queue should be used for one purpose. </p>&#xA;&#xA;<ul>&#xA;<li><p>Create a 'index-user-search' queue and a 'send to mixpanels' queue,&#xA;so the search service reads from the search queues, indexes the user&#xA;and immediately deletes the message.</p></li>&#xA;<li><p>The mixpanel-service reads from the mix-panels queue, processes the<br>&#xA;message and deletes the message.</p></li>&#xA;</ul>&#xA;&#xA;<p>The registration service, instead of emiting a 'registration-new' to a single queue, now emits it to two queues.</p>&#xA;&#xA;<p>To take it one step better, add SNS into the mix here and have the registration service emit an SNS message to the 'registration-new' topic (not queue), and then subscribe both of the queues I mentioned above, to that topic in a 'fan-out' pattern. </p>&#xA;&#xA;<p><a href=""https://aws.amazon.com/blogs/aws/queues-and-notifications-now-best-friends/"">https://aws.amazon.com/blogs/aws/queues-and-notifications-now-best-friends/</a></p>&#xA;&#xA;<p>Both queues will receive the message, but you only load it into SNS once - if down the road a 3rd unrelated service needs to also process 'registration-new' events, you create another queue and subscribe it to the topic as well - it can run with no dependencies or knowledge of what the other services are doing - that is the goal.</p>&#xA;"
46315343,46180957,7728822,2017-09-20T07:04:10,<p>Resolved this issue. This is when we compile the project by skipping the test classes by using -Dskiptests. So right after this when we run the pact:verify the test classpaths are not included in the search.</p>&#xA;&#xA;<p>Fixed it by compiling the test classes by using mvn test-compile compile and then running the pact:verify. Now the classes are getting picked up every single time.</p>&#xA;
44905910,44902847,6512567,2017-07-04T12:23:47,"<p>Using init containers and readyness checks can solve your issue: <a href=""https://blog.giantswarm.io/wait-for-it-using-readiness-probes-for-service-dependencies-in-kubernetes/"" rel=""nofollow noreferrer"">https://blog.giantswarm.io/wait-for-it-using-readiness-probes-for-service-dependencies-in-kubernetes/</a></p>&#xA;"
44906269,44905869,6512567,2017-07-04T12:39:55,"<p>First of all, you would need to clarify which communications are taking place. Is prometheus accessing blackbox exporter via port 9115, or is blackbox exporter accessing prometheus via port 80? Depending on which is right, the service would be different.</p>&#xA;&#xA;<p>In your service above, you are setting and endpoint which, when accessed via port 80, it will redirect traffic to port 9115 of your blackbox-exporter app. I will assume that prometheus is accessing the blackbox-exporter for the rest of the answer.</p>&#xA;&#xA;<p>Would prometheus access blackbox exporter via port 9115 or port 80. It looks to me that, in your initial set up, the prometheus would use port 9115 to access. Therefore, there is no reason to change the port in the service to 80. Could you try setting <code>port: 9115</code> in your service file instead? </p>&#xA;&#xA;<p>Plus, make sure you configure prometheus to use the correct address. I assume that it would previously use 127.0.0.1:9115, now it would need to be prometheus:9115 (as you named the service <code>prometheus</code>, which can be bit confusing).</p>&#xA;"
48278847,48277437,6512567,2018-01-16T10:15:14,"<p>When it comes to creating user, as you said, most charts and containers will have environment variables for creating a user at boot time. However, most of them do not consider the possibility of creating multiple users at boot time. </p>&#xA;&#xA;<p>What other containers do is, as you said, have the root credentials in k8s secrets so they access the database and create the proper schemas and users. This does not necessarily need to be done in the application logic but, for example, using an init container that sets up the proper database for your application to run.</p>&#xA;&#xA;<p><a href=""https://kubernetes.io/docs/concepts/workloads/pods/init-containers"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/concepts/workloads/pods/init-containers</a></p>&#xA;&#xA;<p>This way you would have a pod with two containers: one for your application and an init container for setting up the DB.</p>&#xA;"
41379078,40495213,6775223,2016-12-29T11:46:05,"<p>Reading your question, it sounds like your repository split is mainly for deployment concerns, so I will focus on that aspect.</p>&#xA;&#xA;<p>We are using one Git repository per Service Fabric application (which contains multiple services), this helps to simplify how Continuous Integration and Continuous deployment is done: if there is a change in the repo (code or config), the SF application needs to be build and deployed. </p>&#xA;&#xA;<p>If you are using the Build and Releases features of VSTS online, you can easily leverage the build Tasks available for Service Fabric in order to support differential upgrades.  Using the “Update Service Fabric App Versions” task (<a href=""https://www.visualstudio.com/en-us/docs/build/steps/utility/service-fabric-versioning"" rel=""nofollow noreferrer"">https://www.visualstudio.com/en-us/docs/build/steps/utility/service-fabric-versioning</a>), using the “Update only if changed” option with the ""deterministic compiler flag"" (<a href=""https://blogs.msdn.microsoft.com/dotnet/2016/04/02/whats-new-for-c-and-vb-in-visual-studio/"" rel=""nofollow noreferrer"">https://blogs.msdn.microsoft.com/dotnet/2016/04/02/whats-new-for-c-and-vb-in-visual-studio/</a>), to make sure that binaries are always the same if code is the same, you easily end-up with differential upgrades per SF application.</p>&#xA;"
43951220,43607751,2000823,2017-05-13T09:01:02,"<p>Flink has a REST API you can use to submit and control jobs -- it's used by the Flink Web UI. See the <a href=""https://ci.apache.org/projects/flink/flink-docs-release-1.2/monitoring/rest_api.html"" rel=""nofollow noreferrer"">docs here</a>. See also <a href=""https://stackoverflow.com/questions/41724269/apache-flink-rest-client-jar-upload-not-working"">this previous question</a>.</p>&#xA;"
51997342,51993175,4469944,2018-08-24T04:08:46,"<p>Make sure vault is running as per microservice configuration:</p>&#xA;&#xA;<pre><code># Vault&#xA;spring.cloud.vault.host=localhost&#xA;spring.cloud.vault.port=8200&#xA;</code></pre>&#xA;&#xA;<p>Usually, when the service is not running on a port, you can <code>Connection refused</code> error.</p>&#xA;"
52083290,52081388,4469944,2018-08-29T17:37:37,"<p>Third approach does the work and solves the problem without any extra effort. </p>&#xA;&#xA;<p>Second approach is not recommended as it will result in access traffic to your token service and thus will increase the response time.</p>&#xA;&#xA;<p>However JWT scoping rules ask every microservice to create its own token to talk to any other microservice. This has an advantage of more secure microservices and higher level of tracking but it comes with an overhead of complex Public Key Infrastructure and extra processing in creation of new JWT itself.</p>&#xA;&#xA;<p>In the end, your security requirements and request-response SLAs will decide which approach keeps a balance between them and is closest to solving your requirements.</p>&#xA;"
52099869,52099128,4469944,2018-08-30T14:46:18,"<p>You can use spring cloud config service to have a common properties file for common properties between microservices, you can arrange your configuration this way:</p>&#xA;&#xA;<pre><code>src/main/resources/&#xA;    bootstrap.yml&#xA;    application.yml&#xA;    service1.yml&#xA;    service2.yml&#xA;</code></pre>&#xA;&#xA;<p>Where application.yml in each profile folder will have default configurations for each service which can be overridden in configuration file of that service.</p>&#xA;"
51916054,51911797,4469944,2018-08-19T08:54:52,"<p>There are container services/container management systems available for example Amazon ECS, Azure container services, Kubernetes etc which take care of automated deployment by centralised repositories like Amazon ECR etc, automated scale up/down of microservice instances, take advantage of dynamic port allocations to run multiple instance of same service on a single instance/host and also give you a centralised dashboard to monitor resource usage and infrastructure events.</p>&#xA;&#xA;<p>You can make use of any one to get answers to all of your questions as all of them provide most of the functionalities needed for managing your microservices.</p>&#xA;"
51926752,51926575,4469944,2018-08-20T08:26:38,<p>Order Service can keep a shadow copy of limited user information (first name and last name in your case) in its database (using event sourcing) and can build Order object with limited user information all by itself.</p>&#xA;
51918172,51918125,4469944,2018-08-19T13:33:39,"<p>You have provided 'config' as the hostname of config service. Replace it with proper hostname. </p>&#xA;&#xA;<pre><code>spring:&#xA;  application:&#xA;    name: &lt;name&gt;&#xA;  cloud:&#xA;    config:&#xA;      uri: http://host:port&#xA;      fail-fast: true&#xA;      password: &lt;password&gt;&#xA;      username: &lt;user&gt;&#xA;</code></pre>&#xA;&#xA;<p>Since the application is not able to resolve 'config', it is giving you UnknownHost Exception.</p>&#xA;"
37939456,37906461,4469944,2016-06-21T08:39:04,"<p>Yes. Zuul acts as a gateway to your microservices, so that your web application doesn't need to be aware of each microservice. </p>&#xA;&#xA;<p>All the requests from web app will land on Zuul gateway which will call appropriate one/multiple microservice(s) to cater the request.</p>&#xA;"
39909391,39891218,4469944,2016-10-07T04:28:43,"<p>Difficulty to setup development environment for microservices is proportional to the number of microservices. I have a running setup on my laptop with 2 microservices along with eureka, zuul gateway with oAuth2, an authorization server and spring cloud config. Beyond this I have not tried as it will start becoming unmanageable.</p>&#xA;&#xA;<p>In this case it is better to have the development environment on cloud. You can choose any cloud provider and avail a free subscription for sometime. For eg. Amazon gives you 750 hours of linux time free for a month. You can create any number of instances as you need(with minimal cost after free usage).</p>&#xA;"
39199462,39134238,4469944,2016-08-29T06:03:37,"<p>I am implementing a similar solution. Not sure if it will address to your question completely, but, I hope it helps:</p>&#xA;&#xA;<ol>&#xA;<li><p>You can implement a new authentication micro-service to convert your oAuth2 access token to JWT token. This microservice will also sign this JWT token.</p></li>&#xA;<li><p>Your API gateway will route all client requests to authentication service, which will validate this token from IDM and will convert it to a signed JWT token.</p></li>&#xA;<li><p>API gateway will pass this JWT token to other microservices which will validate the signature from Authentication Service's public key. If the signature validates, roles can be extracted out of it for authorization.</p></li>&#xA;<li><p>Each microservice can have its own IDM credentials configured and when it wants to call any other microservice, it can generate an access token and call Authentication Service to get JWT which can be passed in call to other microservices.</p></li>&#xA;</ol>&#xA;"
40094592,40082248,4469944,2016-10-17T19:54:45,"<p>Instead of depending on message ordering in the queue, which depends on producer application as well as the queue itself, it would be better to handle it in the consumer application.</p>&#xA;&#xA;<p>While processing enrollment, you can check if a student exists. If not, just store it.</p>&#xA;&#xA;<p>Run a scheduler which will pick unprocessed enrollments and will see if a student exists. If yes, process it.</p>&#xA;"
40201250,40044128,4469944,2016-10-23T09:05:11,<p>The problem was solved after I imported CA certificate in JRE certificate store:</p>&#xA;&#xA;<pre><code>keytool -importcert -alias startssl -keystore $JAVA_HOME/jre/lib/security/cacerts -storepass changeit -file ca.der&#xA;</code></pre>&#xA;
40231193,40208887,4469944,2016-10-25T04:04:34,"<p>Yes, this design does defeat the separation of concerns and can become bottleneck in future when your microservices will grow and database will undergo frequent changes.(for eg. changes needed in one microservice's database will need a down time for all three.)</p>&#xA;&#xA;<p>Better to have separate database instance for each microservice and use caching for common tables/data.</p>&#xA;"
40125167,40108015,4469944,2016-10-19T07:46:00,<p>Not if all microservices are deployed in Highly Available(HA) configuration which is always the case for production environments.</p>&#xA;&#xA;<p>Multiple Load Balanced instances of every microservice will ensure higher availability as well as higher load tolerance.</p>&#xA;
47985987,34402406,4469944,2017-12-27T03:28:05,"<p>There can be various approaches to do this:</p>&#xA;&#xA;<ol>&#xA;<li><p>Third service can make a call to order service to get the order information and then based on account id, make a call to account service to fetch account information and build the order object.</p></li>&#xA;<li><p>Order Service can make a call to account service based on account number and set it to order object before responding to the third service.</p></li>&#xA;<li><p>Order Service can keep a shadow copy of account information in its database (using event sourcing, for example) and can build Order object with account information all by itself.</p></li>&#xA;</ol>&#xA;"
47977313,47975940,8470712,2017-12-26T10:36:56,"<p>IntelliJ might be providing a separate IDE for node js could you look at it, I use Atom or VS code(sometimes) for node related stuff. Maybe take a look at <a href=""https://www.jetbrains.com/help/idea/node-js.html"" rel=""nofollow noreferrer"">this</a> as well.</p>&#xA;"
41642420,41640621,2776504,2017-01-13T19:50:03,"<p>It's difficult to judge the viability of any solution without looking ""inside"" the boxes, however:</p>&#xA;&#xA;<ul>&#xA;<li><p>If the only thing you care about here is stopping the frontend from potentially tampering with the data, you can create a sort of ""signature"" of the packet of data sent by MS2 to the frontend and propagate the signature to MS1 together with the packet. The signature can be a hash of the packet contatenated with a pseudorandom number generated in a deterministic way from a seed shared by the microservices (so MS1 can recreate the same pseudorandom number as MS2 without the need for an additional B2B HTTP request, and then verify the integrity of the packet).</p></li>&#xA;<li><p>The first idea that comes to my mind is to verify whether the ownership of the data could be modified. If MS1 must frequently access a subset of data from MS2 it MAY be possible to move the ownership of that subset from MS2 to MS1. </p></li>&#xA;<li><p>In an ideal world the microservices should be completely standalone, each one with it's own persistence layer and a replication system in place. You say that a broker is not a viable solution, so what about a shared data layer?</p></li>&#xA;</ul>&#xA;&#xA;<p>Hope it helps!</p>&#xA;"
51410943,51386591,1008568,2018-07-18T21:03:39,"<p>The core message of the Functional vs Contract discussion in your case is this: those permutations of unhappy paths belong in your Provider code, because that is where the logic is.</p>&#xA;&#xA;<p>As @fabricio-lemos puts it, you just need to encapsulate a single example of a 400 bad request to prove that your code can handle that case.</p>&#xA;&#xA;<p>As for the 500 error, that is possibly because you are not setting up a single expected interaction (via the Pact DSL) per test. If you hit the Pact mock server before the expected interaction has been registered, you get the 500.</p>&#xA;&#xA;<p>In pseudo-code your test structure might look like</p>&#xA;&#xA;<pre><code>Describe ""User API""&#xA;  BeforeAll&#xA;     setup Pact mock server&#xA;  AfterEach&#xA;     verify and clear interactions &#xA;  With valid request&#xA;     setup 201 interaction&#xA;     run tests&#xA;  With invalid request&#xA;     setup 400 interaction&#xA;     run tests &#xA;</code></pre>&#xA;"
52066874,52057408,1008568,2018-08-28T22:12:45,"<p>Yes, you can use Pact to do contract testing with Lambda.</p>&#xA;&#xA;<p>For example, Pact JS has support for asynchronous programming and has an example for Lambda using the serverless framework [1].</p>&#xA;&#xA;<p>There is a good intro article [2] on the matter also.</p>&#xA;&#xA;<ul>&#xA;<li>[1] <a href=""https://github.com/pact-foundation/pact-js/#asynchronous-api-testing"" rel=""nofollow noreferrer"">https://github.com/pact-foundation/pact-js/#asynchronous-api-testing</a></li>&#xA;<li>[2] <a href=""https://dius.com.au/2017/09/22/contract-testing-serverless-and-asynchronous-applications/"" rel=""nofollow noreferrer"">https://dius.com.au/2017/09/22/contract-testing-serverless-and-asynchronous-applications/</a></li>&#xA;</ul>&#xA;"
50251437,50248129,1008568,2018-05-09T10:42:35,"<p>Short answer - no.</p>&#xA;&#xA;<p>Calling the mock API in the test independent of your actual consumer code is worthless (as you imply), because it is a self-fulfilling prophecy. Pact is designed to test the collaborating service on the Consumer side; the adapter code that makes the call to the Provider.</p>&#xA;&#xA;<p>Typically, this call will pass through things like data-access layers and other intermediates. Your Pact tests would use a service that uses these, and the benefit is that the contract gets defined through this process, that is guaranteed to be up-to-date with consumer needs, because it is generated via your code.</p>&#xA;&#xA;<p>We've just updated the <a href=""https://docs.pact.io"" rel=""nofollow noreferrer"">docs</a> today, perhaps that helps.</p>&#xA;"
44351769,44331178,1008568,2017-06-04T07:29:01,"<p>Assume you're using something like Mocha, you should separate these interactions into individual tests - e.g. call <code>addInteraction</code> in each <code>describe</code> block that's contextual to the test case you're running (possibly in the <code>before</code> to make your tests clearer).</p>&#xA;&#xA;<p>Your general structure might look like the below (pseudo-code):</p>&#xA;&#xA;<pre><code>context(""Contacts exist"")&#xA;  describe(""call some API"")&#xA;    before()&#xA;      provider.addInteraction(interactionWithContacts)&#xA;&#xA;    it(""Returns a list of contact objects"")&#xA;      # your test code here&#xA;      # Verify - this will also clear interactions so&#xA;      # your next test won't conflict &#xA;      provider.verify()&#xA;&#xA;context(""No contacts exist"")&#xA;  describe(""call some API"")&#xA;    before()&#xA;      provider.addInteraction(interactionWithNoContacts)&#xA;&#xA;    it(""Returns an empty list of contacts"")&#xA;      # your test code here&#xA;      # Verify - this will also clear interactions so&#xA;      # your next test won't conflict &#xA;      provider.verify()       &#xA;</code></pre>&#xA;"
50300803,50284055,1667256,2018-05-11T21:58:56,"<p>Did you look at <a href=""https://cloud.google.com/pubsub/docs/overview"" rel=""nofollow noreferrer"">Cloud Pub/Sub</a> (topic subscription/publication service).</p>&#xA;&#xA;<blockquote>&#xA;  <p>Cloud Pub/Sub brings the scalability, flexibility, and reliability of enterprise message-oriented middleware to the cloud. By providing many-to-many, asynchronous messaging that decouples senders and receivers, it allows for secure and highly available communication between independently written applications. </p>&#xA;</blockquote>&#xA;&#xA;<p>I believe Pub/Sub can mostly substitute Kafka or RabitMQ in your case.</p>&#xA;&#xA;<blockquote>&#xA;  <p>How are front-end clients expected know when a file has completed      processed and ready? Do they need to poll data from somewhere?</p>&#xA;</blockquote>&#xA;&#xA;<p>For example, if you are using dataflow API to process the file, Cloud dataflow can publish the progress and send the status to a <strong>topic</strong>. Your front end (app engine) just needs to subscribe to that <strong>topic</strong> and receive update.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/WJsXg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WJsXg.png"" alt=""enter image description here""></a></p>&#xA;"
45053331,45052215,7840440,2017-07-12T09:10:46,"<p>""Sure up"" your netstat output with awk:</p>&#xA;&#xA;<pre><code>netstat -antp | awk '$6 == ""ESTABLISHED"" { split ($5,prt,"":"");if ( prt[2] == ""8585"" ) { cnt++ } } END { print cnt }'&#xA;</code></pre>&#xA;&#xA;<p>This will take the netstat output and then check the 6th delimited piece of data (with the default space) and check it against ESTABLISHED. If condition is met, the fourth delimited piece of data is split using : into an array port. The second element of this will have the connecting port and so this is checked against 8585. If this condition is met, the connection count is incremented. At the end of the awk script, the final cnt variable is printed. </p>&#xA;"
47509733,38931315,771768,2017-11-27T11:18:26,"<p>Yes, a <code>GET</code> request to <code>/favicon.ico</code> just needs to return your favicon file.</p>&#xA;&#xA;<p>I haven't set up Katana routing before, but you could try the <code>Microsoft.Owin.StaticFiles</code> package, and you can set up <a href=""https://odetocode.com/blogs/scott/archive/2014/02/10/building-a-simple-file-server-with-owin-and-katana.aspx"" rel=""nofollow noreferrer"">something like this</a>.</p>&#xA;"
48899206,48438747,1954744,2018-02-21T06:11:13,"<p>Why don't you try some containerization (<a href=""https://www.docker.com/"" rel=""nofollow noreferrer"">docker</a> or <a href=""https://github.com/rkt/rkt/"" rel=""nofollow noreferrer"">rkt</a>) to deploy microservice.&#xA;This will give you flexibility of deployment, because in containerization each and every container have their own IP, so port and IP does not conflict.</p>&#xA;&#xA;<p>Definitely after this you required some service discovery to discover your microservices to call from other microservice.</p>&#xA;"
48899653,48030343,1954744,2018-02-21T06:43:38,"<p>Definitely Docker. Using containerization gives you max flexibility. </p>&#xA;&#xA;<p>In your first approach, you jar is dependent on Java. Whenever you create new VM, you need to install fix set of software to support you application.</p>&#xA;&#xA;<p>Benefits in second approach, </p>&#xA;&#xA;<p><strong>First, everything is going to be in single container.</strong> </p>&#xA;&#xA;<p>You can install all required software in container and that container can be user in any VM. You have flexibility to use java of your choice for each microservices. Only install docker and everything is going to be worked. </p>&#xA;&#xA;<p><strong>Second, Dev Prod Parity</strong></p>&#xA;&#xA;<p>If you thing very much of microservice architecture and <a href=""https://12factor.net/"" rel=""nofollow noreferrer"">12-factor</a> apps. Then docker helps to support lots of factors. &#xA;Your java and other software are going to be unique in all your environment. That means you are never going to get surprise whether it is working in QA and not in Prod due to some version mismatch of runtime environment.</p>&#xA;&#xA;<p><strong>Third, Flexibility</strong></p>&#xA;&#xA;<p>If you go into microservice architecture, then why only java. You can also go with GO, Python or other languages. At this time, rather installing runtime environment for each platform on each VM it is very useful to have microservice in containers.</p>&#xA;&#xA;<p><strong>Last, Deployment Easiness</strong></p>&#xA;&#xA;<p>You can use docker-compose or docker swarm to run 100s of mivroservice in single command.</p>&#xA;"
43358215,40936597,252344,2017-04-12T00:17:16,"<p>For me, the issue was that I had an empty folder, src/main/resources/templates. When this folder exists, FreeMarkerView cannot see the embedded templates contained in spring-cloud-netflix-eureka-server. I don't remember where this folder originated but I suspect it is in the online sample.</p>&#xA;"
46133689,46131196,252344,2017-09-09T18:10:59,<p>This particular message is just a warning. Your application is attempting to register with Eureka but Eureka is not responding. You can either start an instance of Eureka or disable the automatic registration by adding the following to your application.yml.</p>&#xA;&#xA;<pre><code>eureka:&#xA;  client:&#xA;    register-with-eureka: false&#xA;</code></pre>&#xA;
50401159,50401105,6368697,2018-05-17T22:06:51,"<p>One way to do it (your question is broad, and without code it may be more on-topic on SoftwareEngineering than here) is the following:</p>&#xA;&#xA;<ol>&#xA;<li>create your own CA</li>&#xA;<li>(optional, create a sub-CA only for your microservices)</li>&#xA;<li>generate all certificates by this CA</li>&#xA;<li>make your code trust all certificates from this CA, instead of checking the certificate itself (but you should still check the dates, the correct signing, etc.)</li>&#xA;</ol>&#xA;&#xA;<p>In that way, when you introduce a new microservice you have only one certificate to generate for it, and nothing to change in the other microservices, which is a goal you need to reach otherwise it can not be managed.</p>&#xA;&#xA;<p>By doing it like that you can even move some of your microservices outside of our systems, as needed. They just need to be shipped with the CA public key.</p>&#xA;&#xA;<p>I would advise against reusing the same certificate for different microservices as it would only lead to problems.</p>&#xA;&#xA;<p>Also, loosely related but make sure to read this:&#xA;<a href=""https://www.cs.utexas.edu/~shmat/shmat_ccs12.pdf"" rel=""nofollow noreferrer"">https://www.cs.utexas.edu/~shmat/shmat_ccs12.pdf</a>&#xA;it shows various pitfalls when using TLS outside of the web. It is an eye opening. While being targeted only at TLS specifically and not PKI problems, this Internet-Draft (<a href=""https://datatracker.ietf.org/doc/draft-gutmann-tls-lts/"" rel=""nofollow noreferrer"">https://datatracker.ietf.org/doc/draft-gutmann-tls-lts/</a>) can provide useful insights on how to properly implement TLS1.2 with some secure default choices. The ""LTS"" part specifically means ""Long Term Support"".</p>&#xA;&#xA;<p>Have a look also at this related question: <a href=""https://security.stackexchange.com/questions/175627/securing-internal-micro-services-letsencrypt-vs-self-signed-certificates-be"">https://security.stackexchange.com/questions/175627/securing-internal-micro-services-letsencrypt-vs-self-signed-certificates-be</a> and the answer giving you other ideas, like using a vault.</p>&#xA;"
47325102,47324935,471481,2017-11-16T08:43:36,"<p>Eureka Server will let you add as many microservices (modules of spring boot projects like you said).</p>&#xA;&#xA;<p>From Spring Cloud <a href=""https://cloud.spring.io/spring-cloud-netflix/"" rel=""nofollow noreferrer"">landing page</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p>As long as Spring Cloud Netflix and Eureka Core are on the classpath&#xA;  any Spring Boot application with @EnableEurekaClient will try to&#xA;  contact a Eureka server on <a href=""http://localhost:8761"" rel=""nofollow noreferrer"">http://localhost:8761</a> (the default value of&#xA;  eureka.client.serviceUrl.defaultZone):</p>&#xA;</blockquote>&#xA;&#xA;<p>That means that you can use a single eureka server for multiple microservices which are registered as clients to the eureka server. </p>&#xA;&#xA;<p>So yes, it's a One-To-Many relationship.</p>&#xA;&#xA;<p>You will want at one point to look into multiple Eureka servers used in load balancing, for redundancy purposes, but for now you will be fine.</p>&#xA;"
48625485,48624757,1193657,2018-02-05T15:06:13,"<p>In a service architecture proper way to make your code independent out of constraints of communication channel is by having properly modeled self-sufficient messages. Historic examples would be WSDL in document mode, EDIFACT, HATEOAS etc. From this point of view microservices with spring-boot and kafka are just different implementation of same old thing done since mainframes ruled the world.</p>&#xA;&#xA;<p>Essentially if you take a view of your app as blackbox asynchronous server; everything app does is receives events and produces new ones. It should not matter how events are raised within app. Http requests, xml within jms messages, json in kafka, whatever - all those things are just a way to pass events and business layer of application should respond only to a content of events. </p>&#xA;&#xA;<p>So business layer is usually structured around some custom model/domain which are delivered as payload. Business layer is invoked/triggered by listener/producer layer which talks to communcation channel (kafka listener, http listener etc..). Aside from logging and enforcing security you should not have communication channel logic in app. I have seen unfortunate examples of business logic driven by by originating jms connection or parsing url of request. If you ever have this in your code you have failed to properly structure your code.</p>&#xA;&#xA;<p>However that is easier to say than to implement. Some people are good at this level of modeling, and some never learn. </p>&#xA;&#xA;<p>And there is no other way to learn but to try and fail.</p>&#xA;"
37819991,37793364,1200057,2016-06-14T18:54:37,<p>Since it's not changeable it means you need a new task definition.</p>&#xA;&#xA;<p><code>Task definition max containers</code> means you can have 10 per task definition. Just clone the definition if you need that many more containers.</p>&#xA;&#xA;<p>Take it with a grain of salt but if you're setting your tasks on a per instance basis could you not work with creating AMIs instead or possibly let multiple tasks go on the same container instance?</p>&#xA;
35652303,35600997,559289,2016-02-26T12:54:42,"<p>Generally, the most common XMPP extension use to build messaging systems for machines-to-machines, internet of things, microservices, etc is PubSub, as defined in <a href=""http://xmpp.org/extensions/xep-0060.html"" rel=""nofollow"">XEP-0060</a>.</p>&#xA;&#xA;<p>This is a module you can enable in ejabberd. It is API based, so you can even customize the behaviour of that module to your application specific.</p>&#xA;&#xA;<p>Pubsub basically allows to decouple senders and receivers and is especially designed for that use case.</p>&#xA;"
51044658,50552293,3903417,2018-06-26T13:55:37,"<p>I didn't find any good sample for microservice based on Java EE 8. So myself did a sample on wildfly swarm.</p>&#xA;&#xA;<p>1- hibernate as JPA (eclipselink in pom.xml is also available)</p>&#xA;&#xA;<p>2- EJB 3 , JAX-RS , JSON-B , JSON-P , JSF 2 </p>&#xA;&#xA;<ul>&#xA;<li>here is the link : <a href=""https://github.com/omidhaghighatgoo/JavaEE-8-Microservice"" rel=""nofollow noreferrer"">https://github.com/omidhaghighatgoo/JavaEE-8-Microservice</a></li>&#xA;</ul>&#xA;"
52105751,52105225,10169530,2018-08-30T21:43:13,"<p>There's no right or wrong in this case. Architecture is all about pros and cons.</p>&#xA;&#xA;<p>Some pros of sharing database:</p>&#xA;&#xA;<ul>&#xA;<li><p>ACID transactions enforce data consistency.</p></li>&#xA;<li><p>Single database is easier to operate and maintain.</p></li>&#xA;</ul>&#xA;&#xA;<p>Some drawbacks:</p>&#xA;&#xA;<ul>&#xA;<li><p>Possible schema change conflicts among teams need to be coordinated.</p></li>&#xA;<li><p>Locks may affect performance.</p></li>&#xA;</ul>&#xA;&#xA;<p>Reference: <a href=""https://microservices.io/patterns/data/shared-database.html"" rel=""nofollow noreferrer"">https://microservices.io/patterns/data/shared-database.html</a></p>&#xA;"
52098305,52093232,10169530,2018-08-30T13:28:50,"<p>More important than separating codebase is to create a separate CI/CD pipeline per microservice (running its own unit and integration tests), and another pipeline to integrate all microservices and run end-to-end tests.</p>&#xA;&#xA;<p>Since you plan to release your microservices as Docker containers, consider a Docker Registry for versioning and distribution of your images.</p>&#xA;&#xA;<p>Here are some references that follow this idea: <a href=""https://dzone.com/articles/cicd-for-containerised-microservices"" rel=""nofollow noreferrer"">DZone</a>, <a href=""https://docs.microsoft.com/en-us/azure/architecture/microservices/ci-cd"" rel=""nofollow noreferrer"">Microsoft</a></p>&#xA;"
51650000,51623293,10169530,2018-08-02T09:21:40,"<p>If you are looking for a fully managed service, AWS provides ""Step Functions"" to satisfy your stateful requirements: <a href=""https://stackoverflow.com/questions/tagged/aws-step-functions"">https://stackoverflow.com/questions/tagged/aws-step-functions</a></p>&#xA;"
51650113,51615385,10169530,2018-08-02T09:27:40,"<p>You are probably looking for a proxy server. Take a look at Zuul and its security filters:</p>&#xA;&#xA;<p><a href=""https://github.com/spring-cloud/spring-cloud-netflix/issues/796"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-netflix/issues/796</a></p>&#xA;&#xA;<p><a href=""https://github.com/spring-cloud/spring-cloud-security/issues/88"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-security/issues/88</a></p>&#xA;"
51650291,51602388,10169530,2018-08-02T09:37:40,"<p>In a cloud architecture, when you have a RESTful interface, you usually have one API Gateway redirecting your requests to a load balancer, serverless function, queue or whatever you want/need: <a href=""https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started.html</a></p>&#xA;&#xA;<p>For communication between microservices, I would suggest a messaging queue for decoupling purposes. Your merge idea could become hard to maintain after some time. Check out SQS: <a href=""https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-tutorials.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-tutorials.html</a></p>&#xA;"
51650446,51584078,10169530,2018-08-02T09:46:32,"<p>This looks more like a problem to be solved by an ETL tool: <a href=""https://en.m.wikipedia.org/wiki/Extract,_transform,_load"" rel=""nofollow noreferrer"">https://en.m.wikipedia.org/wiki/Extract,_transform,_load</a> </p>&#xA;&#xA;<p>I know that AWS provides an ETL service called ""Glue"": <a href=""https://docs.aws.amazon.com/glue/latest/dg/getting-started.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/glue/latest/dg/getting-started.html</a></p>&#xA;"
51654489,51651248,10169530,2018-08-02T13:08:03,"<p>It seems that you are missing the @EnableDiscoveryClient client-side annotation. It usually goes with @SpringBootApplication.</p>&#xA;&#xA;<p>""The @EnableDiscoveryClient activates the Netflix Eureka  DiscoveryClient implementation. There are other implementations for other service registries like Hashicorp’s Consul or Apache Zookeeper.""</p>&#xA;&#xA;<p><a href=""https://spring.io/guides/gs/service-registration-and-discovery/"" rel=""nofollow noreferrer"">https://spring.io/guides/gs/service-registration-and-discovery/</a></p>&#xA;"
51654920,51512075,10169530,2018-08-02T13:28:56,"<p>Have you tried @MockBean?</p>&#xA;&#xA;<p><a href=""https://docs.spring.io/spring-boot/docs/current/api/org/springframework/boot/test/mock/mockito/MockBean.html"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-boot/docs/current/api/org/springframework/boot/test/mock/mockito/MockBean.html</a></p>&#xA;&#xA;<p>""When @MockBean is used on a field, as well as being registered in the application context, the mock will also be injected into the field. Typical usage might be:""</p>&#xA;&#xA;<p>@RunWith(SpringRunner.class)&#xA; public class ExampleTests {</p>&#xA;&#xA;<pre><code> @MockBean&#xA; private ExampleService service;&#xA;&#xA; @Autowired&#xA; private UserOfService userOfService;&#xA;&#xA; @Test&#xA; public void testUserOfService() {&#xA;     given(this.service.greet()).willReturn(""Hello"");&#xA;     String actual = this.userOfService.makeUse();&#xA;     assertEquals(""Was: Hello"", actual);&#xA; }&#xA;&#xA; @Configuration&#xA; @Import(UserOfService.class) // A @Component injected with ExampleService&#xA; static class Config {&#xA; }&#xA;</code></pre>&#xA;&#xA;<p>}</p>&#xA;"
51649939,51634486,10169530,2018-08-02T09:18:38,"<p>What about Kafka? It uses ZooKeeper under the covers: <a href=""https://kafka.apache.org/"" rel=""nofollow noreferrer"">https://kafka.apache.org/</a></p>&#xA;&#xA;<p>You are probably looking for leader election: <a href=""https://stackoverflow.com/questions/43146546/when-does-kafka-leader-election-happen"">When does Kafka Leader Election happen?</a></p>&#xA;"
42928771,42928059,1847482,2017-03-21T13:36:59,"<p>Knowing the number of users is useful, but it is not enough. <br/>&#xA;At the very least you should be able to estimate the expected number of requests per second in the peek time and the required response time. <br/></p>&#xA;&#xA;<p>Once you know this the safest way to know the limits is to performance test it, either from a simple Java program or from something like JMeter. <br/>&#xA;For a typical tomcat performance take a look e.g. on this: <a href=""https://stackoverflow.com/questions/7193012/how-to-handle-2000-requests-sec-on-tomcat"">how to handle 2000+ requests/sec on tomcat?</a> <br/></p>&#xA;&#xA;<p>In general, Spring REST is not going to add much performance penalty and you also can have a cluster of tomcat instances, so the bottleneck might be somewhere else, e.g. in the DB. <br/></p>&#xA;&#xA;<p>As to the microservices question, start with the simplest approach (the monolith) and just make sure you follow standard SW development practices, i.e. have modules and classes etc. It is possible to move to the microservices later, but you should have a real reason for this, since it is more complex.</p>&#xA;"
50631003,50629814,962746,2018-05-31T18:55:08,"<p>I like @RiverTam solution <a href=""https://stackoverflow.com/a/50629965/962746"">https://stackoverflow.com/a/50629965/962746</a>. The only thing to fix is: multiple objects in source array can have the same tagId, so i'm index response objects instead of source objects:</p>&#xA;&#xA;<pre><code>const lodashUniq = require('lodash.uniq');&#xA;const lodashMap = require('lodash.map');&#xA;&#xA;const source = [&#xA;    {id: 1, name: ""test"", tagId: 1},&#xA;    {id: 2, name: ""test"", tagId: 15},&#xA;    {id: 3, name: ""test"", tagId: 5},&#xA;];&#xA;&#xA;const uniqueIds = lodashUniq(lodashMap(source, 'tagId'));&#xA;const tags = await axios.get('http://apihost/tag', { id: uniqueIds });&#xA;&#xA;const tagsIndex = new Map(tags.map(tag =&gt; [tag.id, tag]));&#xA;const result = source.map(s =&gt; (&#xA;    {... s, tag: tagsIndex.get(s.tagId)}&#xA;));&#xA;</code></pre>&#xA;"
35422602,35267071,3977402,2016-02-16T02:26:48,"<p>From my understanding, Mantl is a collection of tools/applications that ties together to create a cohesive docker-based application platform. Mantl is ideally deployed on virtualized/cloud environments (AWS, OpenStack, GCE), but I have just recently able to deploy it on bare-metal. </p>&#xA;&#xA;<p>The main component in Mantl is Mesos, which manages dockers, handles scheduling and task isolation. Marathon is a mesos framework that manages long running tasks, such as web services, this is where most application reside. The combination of mesos-marathon handles application high-availability, resiliency and load-balancing. Tying everything together is consul, which handles service discovery. I use consul to do lookups for each application to communication to each other. Mantl also includes the ELK stack for logging, but I haven't had any success in monitoring any of my applications, yet. There is also Chronos, where scheduled tasks are handles ala cron. Traefik acts as a reverse-proxy, where application/service endpoints are mapped to URLs for external services to communicate.</p>&#xA;&#xA;<p>Basically, your microservices should be self-contained in docker images, initiate communications via consul lookup and logs into standard io. Then you deploy your app, using the Marathon API, and monitor it in Marathon UI. When deploying your dockerized-app, marathon will register you docker image names in consul, along with its' exposed port. Scheduled tasks should be deployed in Chronos, where you will be able to monitor running tasks and pending scheduled tasks.</p>&#xA;"
36587015,36586934,390330,2016-04-13T01:35:56,<blockquote>&#xA;  <p>Is there a known practice for doing this with Node</p>&#xA;</blockquote>&#xA;&#xA;<p>Within NodeJs you move <code>request</code> around whereever you need <em>request context stuff</em>. </p>&#xA;&#xA;<p>With every other system you need to carry the stuff around <em>in the request system format for that thing</em>. E.g. for Event store we store it in the event metadata.</p>&#xA;&#xA;<p>For thrift I recommend just adding it as a property in every query that is echoed back in every response. </p>&#xA;
41469762,37150273,6428091,2017-01-04T17:22:44,"<p>I've implemented a solution similar De Zhang posted using the IP address set by rancher on the IPSec managed network with this mini script as the entrypoint of the image:</p>&#xA;&#xA;<pre><code>echo ""Setting rancher managed ip address on MANAGED_IP environment variable""&#xA;export MANAGED_IP=$(curl --retry 5 --connect-timeout 3 -s 169.254.169.250/latest/self/container/primary_ip)&#xA;echo ""Rancher IP is ${MANAGED_IP}""&#xA;java -Djava.security.egd=file:/dev/./urandom -jar /app.jar&#xA;</code></pre>&#xA;&#xA;<p>Before the application bootstrap this set the IP address obtained from the metadata service and store it in the environment variable MANAGED_IP, then print it.</p>&#xA;&#xA;<p>My config repo (I'm using Spring Cloud Config Server) looks like this</p>&#xA;&#xA;<pre><code>eureka:&#xA;  {...}&#xA;  shouldUseDns: false&#xA;  instance:&#xA;    preferIpAddress: true&#xA;    ip-address: ${MANAGED_IP}&#xA;</code></pre>&#xA;"
40371531,36832219,4963003,2016-11-02T02:23:48,"<ol>&#xA;<li><p>Can all microservices data(cached) be kept under a single instance of redis server?&#xA;In microservice architecture it's prefirible ""elastic scale SaaS"". You can think your <strong>Cache service</strong> is perse a <strong>microservice</strong> (that will response on demand)   Then you have multiple options here. The recommended practice on data storage is <strong><em>sharding</em></strong> <a href=""https://azure.microsoft.com/en-us/documentation/articles/best-practices-caching/#partitioning-a-redis-cache"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/documentation/articles/best-practices-caching/#partitioning-a-redis-cache</a> .See the diagram below for book <a href=""http://www.apress.com/us/book/9781484212769"" rel=""nofollow noreferrer"" title=""Microservices, IoT and Azure"">Microservices, IoT and Azure</a></p></li>&#xA;<li><p>Should every microservice have its own cache database in redis? It's possible to still thinking ""vertical partition"" but you should consider ""horizontal partitions"" so again consider sharding; additionally It's not a bad idea to have ""local cache"" specialy to avoid DoS &#xA;""Be careful not to introduce critical dependencies on the availability of a shared cache service into your solutions. An application should be able to continue functioning if the service that provides the shared cache is unavailable. The application should not hang or fail while waiting for the cache service to resume.""</p></li>&#xA;<li><p>How to refresh cache data without setting EXPIRE? Since it would consume more memory.&#xA;You can define your synch polices; I think cache is suitable for things that have few changes.&#xA;""It might also be appropriate to have a <strong>background process</strong> that periodically updates reference data in the cache to ensure it is up to date, or that refreshes the cache when reference data changes.""</p></li>&#xA;</ol>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/ymTlq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ymTlq.png"" alt=""Microservice Architecture""></a>&#xA;For cahe best practices check&#xA;<a href=""https://azure.microsoft.com/en-us/documentation/articles/best-practices-caching/"" rel=""nofollow noreferrer"" title=""Caching Best Practices"">Caching Best Practices</a></p>&#xA;"
43906384,43892712,6998715,2017-05-11T04:10:33,"<p>I understand what you want to do but in essence that's not what microservices are about as far as I understand. It's the service bit in the name that makes a difference. When you assemble a front end, a backend and a db together you're more or less building a complete application. You gain much more freedom in changing the UI by decoupling it.</p>&#xA;&#xA;<p>Lars</p>&#xA;"
44427134,44420585,6998715,2017-06-08T05:12:57,"<p>Don't restrict your design decision to those 2 options you've outlined. The hardest thing about microservices is to get your head around what a service is and how to cut your application into chunks/services that make sense to be implemented as a 'microservice'. </p>&#xA;&#xA;<p>Just because you have those 3 entities (user, video &amp; message) doesn't mean you have to implement 3 services. If your actual use case shows that these services (or entities) depend heavily on each other to fulfil a simple request from the front-end than that's a clear signal that your cutting was not right. </p>&#xA;&#xA;<p>From what I see from your example I'd design 1 microservice that fulfills the request. Remember that one of the design fundamentals of a microservice is to be as independent as possible.&#xA;There's no need to over complicate services, it's not SOA.</p>&#xA;&#xA;<p><a href=""https://martinfowler.com/articles/microservices.html"" rel=""nofollow noreferrer"">https://martinfowler.com/articles/microservices.html</a> -> great read!</p>&#xA;&#xA;<p>Regards,&#xA;Lars</p>&#xA;"
44061512,44060464,6998715,2017-05-19T04:31:41,<p>May be a complete <strong>different solution</strong> could be looking at what you actually want to store over the long-term. Surely there just so many questions you want answered by the results of your data collection. </p>&#xA;&#xA;<p>Could a solution be to run on some regular basis that generates some key insights that will be stored for the long run and then prunes the table to create more space - or archives the 'old' data?</p>&#xA;&#xA;<p>Just a though&#xA;- Lars</p>&#xA;
44147382,44131588,6998715,2017-05-24T01:13:55,<p>there might be cases when it's o.k. to share those models. In general I'd keep them separate. Models on the client don't have to worry about where data comes from and how often those models (service side) change and if that change affects them directly. Decoupling the 2 sides could make your build/deploy cycles more stable and not forcing clients to have to be rebuild whenever a service model changes etc. Also different parts of the client model may be sourcing their data from different microservices. All a matter of your overall model complexity I guess.</p>&#xA;
44147529,44115310,6998715,2017-05-24T01:30:39,<p>I don't know the exact answer to your question but in terms of your design I'd question if you really want to make your microservice1 depending on microsevice2. A microservice should be autonomous in the way it works and being able to be deployed on it's own (in theory anyway!). May be you could have an orchestrating microservice that receives your session information and then calls the 2 other microservices to pass that information on via 'standard' attributes.</p>&#xA;
44104451,44085454,6998715,2017-05-22T03:58:37,"<p>While I think there's nothing wrong with doing it the way Jan suggested I would like to add that the difference microservices should add to your system is of a different nature. </p>&#xA;&#xA;<p>The above segregation of services is what we've seen a lot in the SOA world and it turned out to become too complex very quickly without offering much value.</p>&#xA;&#xA;<p>If you, and I understand it's just an example, have the need to query user connected to products - why split the service up? You end up designing a service per db-entity instead of looking at what the requirements are for a given, what seems to me bounded context.</p>&#xA;&#xA;<p>-Lars</p>&#xA;"
41577104,41548676,7400635,2017-01-10T19:46:07,"<p>You can get started with ConductR in dev mode immediately, for free, without contacting sales. Instructions are at: <a href=""https://www.lightbend.com/product/conductr/developer"" rel=""noreferrer"">https://www.lightbend.com/product/conductr/developer</a>&#xA;You do need to register (read: provide a valid email) and accept TnC to access that page. The sandbox is free to use for dev mode today so you can see if ConductR is right for you quickly and easily. </p>&#xA;&#xA;<p>For production, I'm thrilled to say that soon you'll be able to deploy up to 3 nodes in production if you register w/Lightbend.com (same as above) and generate a 'free tier' license key.  </p>&#xA;&#xA;<p>Lagom is opinionated about microservices. There's always Akka and Play if those opinions aren't shared by a project. Part of that opinion is that deployment should be easy. Good tools feel 'right' in the hand. You are of course free to deploy the app as you like, but be prepared to produce more polyfill the further from the marked trails you go. </p>&#xA;&#xA;<p>Regarding service lookup, ConductR provides redirection for HTTP service lookups for use with 'withFollowRedirects' on Play WS [1]&#xA;Regarding <code>sbt dist</code>, each sub-project service will be a package. You can see this in the Chirper example [2] on which <code>sbt dist</code> generates chirp-impl.zip, friend-impl.zip, activity-stream-impl, etc as seen in the Chirper top level build.sbt file. &#xA;As that ConductR is the clean and lighted path, you can reference how it does things in order to better understand how to replace Lagom's deployment poly w/ your own. That's the interface Lagom knows best. Much of ConductR except the core is already OSS so can try github if the docs don't cover something. </p>&#xA;&#xA;<p>Disclosure: I am a ConductR-ing Lightbender.</p>&#xA;&#xA;<ol>&#xA;<li><p><a href=""http://conductr.lightbend.com/docs/1.1.x/ResolvingServices"" rel=""noreferrer"">http://conductr.lightbend.com/docs/1.1.x/ResolvingServices</a></p></li>&#xA;<li><p>git@github.com:lagom/activator-lagom-java-chirper.git</p></li>&#xA;</ol>&#xA;"
41783572,41783283,1240763,2017-01-21T19:29:38,"<p>There's no such thing as an ""embedded RabbitMQ broker"".</p>&#xA;&#xA;<p>You have to install and start it separately. It is not written in Java, it's Erlang.</p>&#xA;&#xA;<p>What leads you to believe Boot embeds a broker?</p>&#xA;"
49336501,49334373,1240763,2018-03-17T13:22:49,"<p>It simply means you either haven't installed RabbitMQ, or it's not running on the default (localhost:5672). Same thing with Kafka, you need to install and run a broker.</p>&#xA;&#xA;<p>I see Josh provides a docker-compose.yml to run RabbitMQ in a docker image, or you can just install RabbitMQ as a service.</p>&#xA;"
51594625,51579626,1240763,2018-07-30T13:04:13,"<p>You seem to have some confusion:</p>&#xA;&#xA;<pre><code>properties.setHeader(""x-delayed-type"",""direct"");&#xA;properties.setHeader(""x-delayed-message"",true);&#xA;</code></pre>&#xA;&#xA;<p>Those properties are exchange properties, not message properties.</p>&#xA;&#xA;<p>To set the <code>x-delay</code> property, simply use <code>properties.setDelay(15000)</code>.</p>&#xA;"
50279094,50278841,1240763,2018-05-10T18:14:05,"<p>Each application can be visualized using the <a href=""https://docs.spring.io/spring-integration/reference/html/system-management-chapter.html#integration-graph"" rel=""nofollow noreferrer"">spring integration runtime graph</a> together with a viewer application.</p>&#xA;&#xA;<p>See the <a href=""https://github.com/spring-projects/spring-integration-samples/tree/master/applications/file-split-ftp"" rel=""nofollow noreferrer"">file-split-ftp sample</a> for an example. The viewer mentioned on the README page is in the the angular 1.x branch of the referenced spring-flo project.</p>&#xA;&#xA;<p>Also see <a href=""https://ordina-jworks.github.io/architecture/2018/01/27/Visualizing-your-Spring-Integration-components-and-flows.html"" rel=""nofollow noreferrer"">Tim Ysewyn's blog post here</a> ""VISUALIZING YOUR SPRING INTEGRATION COMPONENTS &amp; FLOWS"".</p>&#xA;&#xA;<p>If you create your microservices using Spring Cloud Stream and deploy/orchestrate them using Spring Cloud Dataflow, you can visualize them at a higher level; examples are in the <a href=""http://docs.spring.io/spring-cloud-dataflow/docs/1.4.1.RELEASE/reference/htmlsingle/#_creating_fan_in_fan_out_streams"" rel=""nofollow noreferrer"">reference manual</a>.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/Mm3sn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Mm3sn.png"" alt=""enter image description here""></a></p>&#xA;"
29777051,29775643,1240763,2015-04-21T15:45:53,"<p>Why not use a <code>Queue</code> instead of a <code>Topic</code>? Then your instances will compete for messages rather than all get a copy.</p>&#xA;&#xA;<p><strong>EDIT</strong></p>&#xA;&#xA;<p>rabbitmq might be a better fit for you - publish to a fanout exchange and have any number of queues bound to it, with each queue having any number of competing consumers.</p>&#xA;&#xA;<p>I have also seen JMS topics used where competing clients connect with the same client id. Some (all?) brokers will only allow one such client to consume. The others keep trying to reconnect until the current consumer dies.</p>&#xA;"
44370371,44361901,1240763,2017-06-05T13:43:13,<p>It's not part of the spec; it depends on the broker configuration how many times it will be delivered; many brokers can be configured to send the message to a dead-letter queue after some number of attempts.</p>&#xA;&#xA;<p>There is no guarantee the redelivery will go to the same instance.</p>&#xA;
49559845,49559239,1240763,2018-03-29T15:27:09,"<p>Your bindings don't make sense; the first one will match all keys with the form <code>foo.bar</code>, <code>baz.qux</code> etc, so the second one is irrelevant.</p>&#xA;&#xA;<p>You should probably just use a fanout exchange for the events and each service has 2 queues, one on the fanout for events and one on the topic exchange for jobs (with a narrow binding for just its own jobs).</p>&#xA;"
45176024,45174891,1240763,2017-07-18T20:01:24,"<p>The <code>condition</code> expression property mentioned in David's blog post is baked into the <code>@StreamListener</code> annotation, so you can route different event types from the same destination to different listeners.</p>&#xA;"
28115445,28114758,1240763,2015-01-23T17:29:41,"<p>A number of the <a href=""https://github.com/spring-projects/spring-integration-samples"" rel=""nofollow"">sample apps</a> including those under <code>dsl</code> and the <code>stomp-chat</code> app are Spring Boot/Spring Integration apps.</p>&#xA;"
37151158,36734091,2944955,2016-05-11T00:34:19,"<p>With some trial and errors, I found following</p>&#xA;&#xA;<ol>&#xA;<li>The scope for the user is nothing but his group association</li>&#xA;<li>When one wants to add the scope, first create a group</li>&#xA;</ol>&#xA;&#xA;<p><code>uaac group add custom.report</code></p>&#xA;&#xA;<ol start=""3"">&#xA;<li>Add member to this group</li>&#xA;</ol>&#xA;&#xA;<p><code>uaac member add custom.report xyz@abc.com</code></p>&#xA;&#xA;<ol start=""4"">&#xA;<li>But if you are trying to get the token for the user with this value in the scope using Token Issuer,</li>&#xA;</ol>&#xA;&#xA;<p><code>ti = CF::UAA::TokenIssuer.new(""http://localhost:8080/uaa"", ""reportclient"")&#xA; token = ti.owner_password_credentials_grant({ :username =&gt; request[:username],&#xA;                                               :password =&gt; request[:password]})</code></p>&#xA;&#xA;<p>Then you must have the 'reportclient' with authorities - 'custom.report'.</p>&#xA;&#xA;<p>You can add the authority to reportclient with following command</p>&#xA;&#xA;<pre><code>uaac client update reportclient --authorities ""&lt;existing authorities list separated by space&gt; custom.report""&#xA;</code></pre>&#xA;&#xA;<p>Only then the token received will have custom.report scope.</p>&#xA;&#xA;<p>Hope this helps someone.&#xA;And a better way if someone knows, is welcome.</p>&#xA;"
47039564,47020319,329660,2017-10-31T15:54:34,"<p>Event sourcing is not event-driven architecture. Theoretically, you can have an internally event sourced Bounded Context/microservice in an ecosystem that doesn't use events for integration. You can also have non-event sourced BC's integrating via events.</p>&#xA;&#xA;<p>Event-driven is one kind of <a href=""https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/architect-microservice-container-applications/communication-in-microservice-architecture"" rel=""nofollow noreferrer"">asynchronous microservice integration</a>. Synchronous integration is also possible. I don't know if that's what you implicitly contrast event-based integration with in your question, but the kind of dependency you have to manage is very similar in both cases.</p>&#xA;&#xA;<p>So, no <em>dependency nightmare</em> that I can think of, at least no more than what you typically have when a subsystem A depends on a subsystem B.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Now, if I am developing Service B, i need to know all of the events&#xA;  that Service A can generate</p>&#xA;</blockquote>&#xA;&#xA;<p>No, you only subscribe to the ones you're interested in.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Also, if service A adds any new event, Service B also needs to change&#xA;  in order to handle that new event.</p>&#xA;</blockquote>&#xA;&#xA;<p>Again, not if you're not interested in it.</p>&#xA;&#xA;<blockquote>&#xA;  <p>All of this seems to create a dependency nightmare, and seems like you&#xA;  cannot truly develop each service 'independently'.</p>&#xA;</blockquote>&#xA;&#xA;<p>As soon as one service depends on another, you obviously can't develop each service independently. You might have overinterpreted the kind of ""independence"" that loose coupling via events allows.</p>&#xA;"
47558018,47554214,329660,2017-11-29T17:02:32,"<p>I will start with the same premise as @ConstantinGalbenu but follow with a different proposition ;)</p>&#xA;&#xA;<blockquote>&#xA;  <p>Eventual consistency means that the whole system will eventually&#xA;  <em>converge</em> to a consistent state.</p>&#xA;</blockquote>&#xA;&#xA;<p>If you add to that <em>""no matter the order in which messages are received""</em>, you've got a very strong statement by which your system will naturally tend to an ultimate coherent state without the help of an external process manager/saga.</p>&#xA;&#xA;<p>If you make a maximum number of operations commutative from the receiver's perspective, e.g. it doesn't matter if <code>link A to B</code> arrives before or after <code>create A</code> (they both lead to the same resulting state), you're pretty much there. That's basically the first bullet point of Solution 2 generalized to a maximum of events, but not the second bullet point.</p>&#xA;&#xA;<blockquote>&#xA;  <ul>&#xA;  <li>Microservice B listens for ""A linked to B"" events and, upon receiving&#xA;  such an event, verifies that B exists. If it doesn't, it emits a ""link&#xA;  to B refused"" event.</li>&#xA;  </ul>&#xA;</blockquote>&#xA;&#xA;<p>You don't need to do this in a nominal case. You'd do it in the case where you know that A didn't receive a <code>B deleted</code> message. But then it shouldn't be part of your normal business process, that's delivery failure management at the messaging platform level. I wouldn't put this kind of systematic double-check of everything by the microservice where the original data came from, because things get way too complex. It looks as if you're trying to put some immediate consistency back into an eventually consistent setup.</p>&#xA;&#xA;<p>That solution might not always be feasible, but at least from the point of view of a passive read model that doesn't emit events in response to other events, I can't think of a case where you couldn't manage to handle all events in a commutative way.</p>&#xA;"
47532868,47516458,329660,2017-11-28T13:33:03,"<p>I don't think the problem you raise is exclusive to the projection part of your system - it can also happen between microservices A, B and C.</p>&#xA;&#xA;<p>Normally, the projector gets <code>C created</code> at the same time as B does. Only then can B bind itself to C, which makes it impossible for the specific order you mentioned to happen to the projector.</p>&#xA;&#xA;<p>However, you're right to say that the messages could arrive in the wrong order if for instance, the network communication between B and C is considerably faster than between C and the projector.</p>&#xA;&#xA;<p>I've never come across such a problem, but a few options come to mind :</p>&#xA;&#xA;<ul>&#xA;<li><p>Don't enforce ""foreign keys"" at the read model level. Store B with its C reference even if you know very little about C for now. In other words, make <code>B bound to C</code> and <code>C created</code> commutative.</p></li>&#xA;<li><p>Add a <a href=""https://www.infoq.com/news/2017/11/event-sourcing-microservices"" rel=""nofollow noreferrer"">causation ID</a> to your events. This allows a client to recognize and deal with out of order messages. You can choose your own policy - reject, wait for causation event to arrive, try to process anyway, etc. That is not trivial to implement, though.</p></li>&#xA;<li><p>Messaging platforms can guarantee ordering under certain conditions. You mentioned Kafka, under the same topic and partition. RabbitMQ, I think, has even stronger prerequisites. </p>&#xA;&#xA;<p>I'm not a messaging expert but it looks like the inter-microservice communication scenarios where it would be feasible are limited though. It also seems to go against the current trend in eventual consistency, where we tend to favor commutative operations (see CRDTs) over ensuring total order.</p></li>&#xA;</ul>&#xA;"
52112877,52105225,329660,2018-08-31T09:53:32,"<p>You would probably benefit from analyzing your microservices through the lens of DDD <a href=""https://martinfowler.com/bliki/BoundedContext.html"" rel=""nofollow noreferrer"">Bounded Contexts</a> and their relationships in terms of <a href=""https://www.slideshare.net/StijnVolders/context-mapping"" rel=""nofollow noreferrer"">Context Mapping</a>.</p>&#xA;&#xA;<p>Since you said that service A <em>exposes</em> the data and handles CRUD on it, we can assume that it acts as the master of that data - in a customer-supplier relationship.</p>&#xA;&#xA;<p>In that case, having service B access the database directly looks like the poorest option, because (especially unstructured) DB's make for a weak integration mechanism. Unless you're using stored procedures, you can't manage versions, have custom errors, monitor usage, etc. It also prevents the supplier from having a server side cache, and you have to set privileges carefully so that service B doesn't <em>write</em> to the DB accidentally.</p>&#xA;&#xA;<p>Note that there are other ways of integrating beyond synchronous HTTP API calls. For instance, service B could subscribe to events published by service A and store the data it needs locally.</p>&#xA;"
51653674,51542197,329660,2018-08-02T12:28:19,"<p>If you want to provide a synchronous API, I only see two options:</p>&#xA;&#xA;<ul>&#xA;<li>Design your domain model so that <code>Lead</code> creation logic, the ""10 leads max"" rule and the list of leads for a user are co-located in the same Aggregate root (hint: an <a href=""http://udidahan.com/2009/06/29/dont-create-aggregate-roots/"" rel=""nofollow noreferrer"">AR can spawn another AR</a>).</li>&#xA;<li>Accept to involve more than one non-new Aggregate in the same transaction.</li>&#xA;</ul>&#xA;&#xA;<p>The tradeoff depends on transactional analysis about the aggregates in question - will reading from them in the same transaction lead to a lot of locking and race conditions?</p>&#xA;"
34724740,34722107,329660,2016-01-11T15:09:04,"<p>Each microservice should be a full-fledged application with all necessary layers (which doesn't mean there cannot be shared <em>code</em> between microservices, but they have to run in separate processes).</p>&#xA;&#xA;<p>Besides, it is often recommended that each microservice have its own database. See <a href=""http://microservices.io/patterns/data/database-per-service.html"" rel=""nofollow"">http://microservices.io/patterns/data/database-per-service.html</a> <a href=""https://www.nginx.com/blog/microservices-at-netflix-architectural-best-practices/"" rel=""nofollow"">https://www.nginx.com/blog/microservices-at-netflix-architectural-best-practices/</a> Therefore, I don't really see the point of a web service that would only act as a data access facade.</p>&#xA;"
50761003,50759872,329660,2018-06-08T12:46:45,"<p>A few thoughts:</p>&#xA;&#xA;<ul>&#xA;<li><p>The blog post you linked to is confusing because it equates validation with invariants. </p>&#xA;&#xA;<p>In DDD literature, invariants refer most of the time to domain rules that are enforced in the root entity and nowhere else. Regarding this <em>domain</em> validity, not ""all of the guidance"", as you put it, is for enforcing them on the command side - much the contrary. Popular <a href=""http://codebetter.com/gregyoung/2009/05/22/always-valid/"" rel=""nofollow noreferrer"">schools of thought</a> consider that an entity should always be valid and therefore should take care of its own rules (aka invariants).</p>&#xA;&#xA;<p>The kind of validity that the samples in Jimmy Bogard's post speak to, on the other hand, is on the borderline between domain validity and user input validity. I wouldn't make it a stone-set rule to put that kind of validation on one side or the other. While it's legitimate to consider that it's really a job for the Command, with some typesystems you can perfectly encode that kind of non-null constraints in the entity property types and it would be a shame not to take advantage of that free extra correctness.</p></li>&#xA;<li><p>As has been said in the comments, putting an interface on top of a command seems strange. </p></li>&#xA;<li><p>Thinking about what could happen if someone subclassed the command in a malicious way is probably pointlessly defensive. Within a team, you have total control over what is implemented and I can't think of a good reason for command programmers to be on a different team than the ones who write the corresponding entities.</p></li>&#xA;</ul>&#xA;"
39727929,39721791,329660,2016-09-27T14:56:18,"<p>Using a Repository inside a Factory is generally not a good idea since it <em>hides away</em> from consuming code (i.e. the Application Service) the fact that another Aggregate is brought to the table and used in the current transaction.</p>&#xA;&#xA;<p>But that doesn't mean bringing in another aggregate is always bad. You have to make an educated decision :</p>&#xA;&#xA;<ul>&#xA;<li><p>You could actually need <code>Company</code> to be part of the business transaction that creates a new <code>Employee</code>. You could want that because <code>Company</code> holds domain invariants about employees (such as a list of unique employee emails) or because creating an <code>Employee</code> changes things inside the <code>Company</code> itself and you want to prevent concurrent Employee creations to mess things up in the company.</p>&#xA;&#xA;<p>There are a few ways to do this but it might be a good idea to have a <code>createEmployee()</code> method in the Company itself since the two are tied together.</p></li>&#xA;<li><p>Or, you don't care about the <code>Company</code> (other than its ID) when creating an <code>Employee</code>. You could consider that you don't need immediate consistency between an existing company and the employee. Or, if you have foreign keys in your database, that they already provide enough security to prevent orphan Employees from being created.</p>&#xA;&#xA;<p>In that case, just keep companyID as the only pointer to the company all along, from UI to domain. You don't even need to use <code>CompanyRepository</code> to load the company because it is not involved in the process.</p></li>&#xA;</ul>&#xA;"
29645880,29636899,329660,2015-04-15T08:57:54,"<p>Microservices are not supposed to ""break"" Bounded Contexts, they are complementary since there's a <a href=""http://martinfowler.com/articles/microservices.html#DecentralizedDataManagement"" rel=""noreferrer"">natural correlation between microservice and BC boundaries</a>.</p>&#xA;&#xA;<blockquote>&#xA;  <p>we want a microservice to garantee that data persisted in tables is&#xA;  consistent, in a centralized manner</p>&#xA;</blockquote>&#xA;&#xA;<p>Microservices are not here for facading purposes. And they are all about <em>de</em>centralisation, not centralization. They are modular units organized around business capabilities. They will usually act as gateways to your Bounded Contexts, <em>in front of the Domain</em>, not proxies to a persistent store <em>behind the Domain</em>.</p>&#xA;&#xA;<p>It seems you're trying to apply the wrong solution to your problem -- <strong>using microservices as convergence points to a data-centric monolith</strong>, which defeats their whole purpose.</p>&#xA;&#xA;<p>Maybe you should elaborate on what you mean by ""guarantee data persisted is consistent"" and ""persist data conforming to specific rules"". It might just be implemented in the persistence layer or in services that are not microservices.</p>&#xA;"
34020707,33869866,329660,2015-12-01T12:41:38,"<p>I'm not sure how your question really relates to DDD and how much you want to embrace DDD, but it has this important notion of ubiquitous language. That language contains very precise and unambiguous domain terms which must be the same in the domain expert's mouth, in all the models and ultimately in the code.</p>&#xA;&#xA;<p>A Contact is a Contact. An Employee is an Employee. They have different names for a reason. They reflect different domain concepts.</p>&#xA;&#xA;<p>Unless a Contact can somehow turn out to also be an Employee (which only an expert in your domain can tell and certainly not someone on StackOverflow), <em>there's no reason for them to share anything</em> - whether it be a database table, a base class, etc., and even less so if they are in different Bounded Contexts.</p>&#xA;"
49851267,49849813,8726716,2018-04-16T07:00:22,"<p><a href=""http://senecajs.org"" rel=""nofollow noreferrer"">seneca</a> is not based on spring but can quick build restful API</p>&#xA;"
44662091,42836979,670908,2017-06-20T20:04:45,"<p>There's <a href=""https://github.com/danp/heroku-buildpack-runit"" rel=""nofollow noreferrer"">RUNIT buildpack</a> which makes it easy to combine multiple processes within a single dyno - as long as they all fit within your dyno memory limit (512M for a Hobby dyno).</p>&#xA;&#xA;<p>You still have only one HTTP endpoint per Heroku app, which means your microservices will need to communicate via queues, pub/sub or some RPC hub (like deepstream.io).</p>&#xA;"
49117068,49116339,3765180,2018-03-05T18:23:49,"<p>My suggestion would be to use view engines, such as EJS etc.</p>&#xA;&#xA;<p>In <em>node.js</em> you can send HTML files to the user, so for example when some one visits <em>www.domain.com/register</em> you send him an HTML file, by using something like this:</p>&#xA;&#xA;<pre><code>var path = require('path'); // Core Module in Node JS&#xA;res.sendFile( path.join( __dirname, ""register.html"" ) ); // Send the register HTML file&#xA;</code></pre>&#xA;&#xA;<p>Although it wouldn't be the cleanest solution, <em>you could</em> also create <em>multiple</em> servers in the code, each using a different <em>port</em>.</p>&#xA;&#xA;<p>Since you want a <em>Front End</em> and a <em>Back End</em> on the same server, you could make one server for the front end on port 80 ( and 443 if you are using SSL ) <em>and another server on a different port</em>, such as 3000, or whatever your heart desires.</p>&#xA;&#xA;<p>You can then get info from the server with Ajax, or something similar, that is entirely up to you.</p>&#xA;&#xA;<p>Here is an example:</p>&#xA;&#xA;<pre><code>const http = require('http');&#xA;&#xA;http.createServer(function (req, res) {&#xA;    res.writeHead(200, {'Content-Type': 'text/plain'});&#xA;    res.write('Front End!');&#xA;    res.end();&#xA;  }).listen(80);&#xA;&#xA;http.createServer(function (req, res) {&#xA;    res.writeHead(200, {'Content-Type': 'text/plain'});&#xA;    res.write('Back End!');&#xA;    res.end();&#xA;}).listen(3000);&#xA;</code></pre>&#xA;&#xA;<p>I would personally always go with the first example, as it is what most people opt for, and it is much easier to implement.</p>&#xA;&#xA;<p>Also, view engines allow you to embed variables from your <em>node.js</em> application into the HTML files, so there won't be any need for Ajax etc.</p>&#xA;"
40620387,40518729,883644,2016-11-15T21:53:20,<p>In the <code>TestConsole.csproj</code> project I reuse the same <code>IBusControl</code> object to send the command and to consume the event. Once I created two separate bus objects it worked as expected.</p>&#xA;
41009097,40918134,2194364,2016-12-07T03:50:02,<p>Even though it was throwing NPE the actual issue was with the missing jar files. Here is the list of jar files </p>&#xA;&#xA;<pre><code>          &lt;dependencies&gt;&#xA;                &lt;dependency&gt;&#xA;                        &lt;groupId&gt;com.microsoft.azure&lt;/groupId&gt;&#xA;                        &lt;artifactId&gt;azure-svc-mgmt&lt;/artifactId&gt;&#xA;                        &lt;version&gt;0.9.7&lt;/version&gt;&#xA;                &lt;/dependency&gt;&#xA;                &lt;dependency&gt;&#xA;                        &lt;groupId&gt;com.microsoft.azure&lt;/groupId&gt;&#xA;                        &lt;artifactId&gt;azure-svc-mgmt-compute&lt;/artifactId&gt;&#xA;                        &lt;version&gt;0.9.7&lt;/version&gt;&#xA;                &lt;/dependency&gt;&#xA;                &lt;dependency&gt;&#xA;                        &lt;groupId&gt;com.microsoft.azure&lt;/groupId&gt;&#xA;                        &lt;artifactId&gt;azure-svc-mgmt-network&lt;/artifactId&gt;&#xA;                        &lt;version&gt;0.9.7&lt;/version&gt;&#xA;                &lt;/dependency&gt;&#xA;                &lt;dependency&gt;&#xA;                        &lt;groupId&gt;com.microsoft.azure&lt;/groupId&gt;&#xA;                        &lt;artifactId&gt;azure-svc-mgmt-sql&lt;/artifactId&gt;&#xA;                        &lt;version&gt;0.9.7&lt;/version&gt;&#xA;                &lt;/dependency&gt;&#xA;                &lt;dependency&gt;&#xA;                        &lt;groupId&gt;com.microsoft.azure&lt;/groupId&gt;&#xA;                        &lt;artifactId&gt;azure-svc-mgmt-storage&lt;/artifactId&gt;&#xA;                        &lt;version&gt;0.9.7&lt;/version&gt;&#xA;                &lt;/dependency&gt;&#xA;                &lt;dependency&gt;&#xA;                        &lt;groupId&gt;com.microsoft.azure&lt;/groupId&gt;&#xA;                        &lt;artifactId&gt;azure-svc-mgmt-websites&lt;/artifactId&gt;&#xA;                        &lt;version&gt;0.9.7&lt;/version&gt;&#xA;                &lt;/dependency&gt;&#xA;                &lt;dependency&gt;&#xA;                        &lt;groupId&gt;com.microsoft.azure&lt;/groupId&gt;&#xA;                        &lt;artifactId&gt;azure-svc-mgmt-media&lt;/artifactId&gt;&#xA;                        &lt;version&gt;0.9.7&lt;/version&gt;&#xA;                &lt;/dependency&gt;&#xA;                &lt;dependency&gt;&#xA;                        &lt;groupId&gt;com.microsoft.azure&lt;/groupId&gt;&#xA;                        &lt;artifactId&gt;azure-servicebus&lt;/artifactId&gt;&#xA;                        &lt;version&gt;0.9.7&lt;/version&gt;&#xA;                &lt;/dependency&gt;&#xA;                &lt;dependency&gt;&#xA;                        &lt;groupId&gt;com.microsoft.azure&lt;/groupId&gt;&#xA;                        &lt;artifactId&gt;azure-serviceruntime&lt;/artifactId&gt;&#xA;                        &lt;version&gt;0.9.7&lt;/version&gt;&#xA;                &lt;/dependency&gt;&#xA;        &lt;/dependencies&gt;&#xA;</code></pre>&#xA;
49418409,39763013,2853820,2018-03-21T23:56:32,"<p>This turned out to be a bug in Service Fabric, upgrade your local cluster to the latest version <code>6.1.472.9494</code> which will fix the issue. <a href=""https://github.com/Azure/service-fabric-issues/issues/677#issuecomment-356124446"" rel=""nofollow noreferrer"">more details here</a></p>&#xA;"
51320923,51314434,4696508,2018-07-13T08:31:26,<p>You can try using hostname/ipaddress instead of localhost. The 'localhost' represents the current container.</p>&#xA;
49005935,47017875,2817980,2018-02-27T10:02:41,"<p>There are multiple options I can think of now:</p>&#xA;&#xA;<ol>&#xA;<li>If there is a single instance of micro-service deployed, you can use something like quartz to time the job.</li>&#xA;<li>Create a RESTful API for cleanup, invoke it using a script, please refer to <a href=""https://stackoverflow.com/a/15090893/2817980"">https://stackoverflow.com/a/15090893/2817980</a> for example. This will make sure that only one instance of the service works on cleanup.</li>&#xA;<li>If there is a master-slave replica, ask the master to allocate to only 1 instance</li>&#xA;<li>Create a scheduled job using something like quartz and then check if the job already taken up by some other scheduler in zookeeper/redis/db or any other storage.</li>&#xA;</ol>&#xA;&#xA;<p>I can discuss more on this.</p>&#xA;"
40312438,39932500,6750653,2016-10-28T20:02:01,"<p>Just to close the loop on this, I'm using option 1. While having multiple repositories may seem like a lot, implementing a microservices architecture precludes a good devops process to make this release management process simpler.</p>&#xA;"
48358985,48344922,9244861,2018-01-20T17:20:09,"<p>JDL documentation is not clear about this. You can use one JDL file for this and specify a microservice name in it. Remove skipServer option, because JHipster automatically does it for you. Follow <a href=""https://github.com/jhipster/generator-jhipster/issues/7018"" rel=""nofollow noreferrer"">this discussion on Github</a>.</p>&#xA;&#xA;<p>Updated JDl file:</p>&#xA;&#xA;<pre><code>entity Address&#xA;{&#xA;    streetName  String required,&#xA;    apartmentOrHouseNumber  String,&#xA;    city    String  required,&#xA;    zipcode Long    required,&#xA;    state   String,&#xA;    country String&#xA;}&#xA;&#xA;entity BookCoverType&#xA;{&#xA;    coverType   String  required&#xA;}&#xA;&#xA;&#xA;entity Author&#xA;{&#xA;    firstName   String  required,&#xA;    lastName    String  required,&#xA;    middleName  String,&#xA;}&#xA;&#xA;entity Book &#xA;{&#xA;    bookName String required,&#xA;    bookTitle String    required,&#xA;    numberOfPages Integer   required,&#xA;}&#xA;&#xA;&#xA;&#xA;&#xA;relationship OneToOne &#xA;{&#xA;    //Book{bookCoverType(coverType)} to BookCoverType&#xA;    Author{address(streetName)} to Address&#xA;}&#xA;&#xA;relationship OneToMany &#xA;{&#xA;    BookCoverType{book(bookTitle)} to Book&#xA;    Book{author(firstName)} to Author&#xA;    Author{book(bookTitle)} to Book&#xA;&#xA;}&#xA;&#xA;&#xA;microservice * with books&#xA;&#xA;paginate Book with pagination&#xA;paginate Author with pagination&#xA;</code></pre>&#xA;"
40961203,40960054,5850301,2016-12-04T17:06:23,"<p>You may use any. Stateless or Statefull. Is should not really matter.&#xA;In my view, you can do below:</p>&#xA;&#xA;<ol>&#xA;<li>Create a custom listener say ""ServiceBusCommunicationListener"" derived from ICommunicationListener. In the ""public Task OpenAsync(CancellationToken cancellationToken)"" method of ICommunicationListener, you can write code to access service bus queue. </li>&#xA;<li>For Service Bus Queue read, you can use ""Microsoft.ServiceBus.Messaging.SubscriptionClient"" and use its ""OnMessageAsync"" method to continuously receive message.</li>&#xA;<li><p>Once you have this, inside your service code, you may use StatefulService's ""CreateServiceReplicaListeners"" override or StatelessService's ""CreateServiceInstanceListeners"".    </p>&#xA;&#xA;<pre><code>    protected override IEnumerable&lt;ServiceReplicaListener&gt; CreateServiceReplicaListeners()&#xA;        {&#xA;            return new[] { new ServiceReplicaListener(context =&gt; new ServiceBusCommunicationListener(context)) };&#xA;        }&#xA;</code></pre></li>&#xA;</ol>&#xA;"
36841798,36840448,2448829,2016-04-25T13:20:47,"<p>Absolutely, the search engine should be encapsulated just like any other data store.  Think about as if it were a database.</p>&#xA;"
45421618,45420766,2448829,2017-07-31T17:05:12,"<p>I would consider wrapping the queue in a service.  Being that it is a third-party, you may not have control if/when you need to change things.  For example, if you need to add security, or you need to introduce throttling.</p>&#xA;&#xA;<p>If it were purely internal, the additional abstraction is probably not worth it, but externally exposed resources probably need more control</p>&#xA;"
45484423,45468971,2448829,2017-08-03T12:37:15,"<p>Reporting often has little to do with your application domain model.  Usually it is a separate, cross-cutting concern, and is implemented at the back-end data store for performance reasons.</p>&#xA;&#xA;<p>Doing a common domain model just for reporting will couple all of your services to an unacceptable degree, you will create a monolith.  </p>&#xA;&#xA;<p>It seems like you have created a compromise, where a report service is decoupled by making it it's own service.  This seems reasonable, but like you said, it is causing a lot of communication overhead.  It also imposes a run-time overhead that may or may not be justifiable.</p>&#xA;&#xA;<p>Regardless, like I said, reporting is often cross-cutting, and as such, usually does not fit into a domain model particularly well.  The decision to either move it into a back-end process vs making it a run-time process like you have depends on the operational requirements of your system.</p>&#xA;&#xA;<p>I would highly encourage you <em>not</em> to try and make all of your microservice domains fit a common model.  This will create a monolith, and will make changes to that domain extraordinarily expensive.  You will have all the overheads of a monolith, plus all the overheads of microservices...</p>&#xA;"
32831549,32831192,2448829,2015-09-28T20:48:04,"<p>If you are going to need to retrieve reviews by the first letter of the movie title, put an attribute called ""movie review key"" or even the movie title itself on the review service.</p>&#xA;&#xA;<p>I've had to learn the hard way that denormalization is a way of life in microservices.  If you try to strictly normalize your services, you will end up with FAR too much chattiness.  Things that change rarely (like a movie title) can definitely get copied to a separate store.</p>&#xA;"
34776126,34774290,2448829,2016-01-13T20:11:39,"<p>This is a big list, but I'll give it a shot:</p>&#xA;&#xA;<p>Databases:&#xA;Yes, they should have their own database.  That does not necessarily mean their own database SERVER, that is a scaling concern, but you certainly don't want to couple them together.  The absolutely cleanest way is to separate them physically, but for a licensing standpoint, separate schemas that don't have permission to each other is probably sufficient.</p>&#xA;&#xA;<p>RAM:&#xA;Yes, this will definitely require more space for the program area.  This is usually irrelevant, as the data for programs usually far outstrips the program/framework memory.  I believe the spring boot framework needs around <a href=""https://spring.io/blog/2015/12/10/spring-boot-memory-performance"" rel=""nofollow"">18 mb</a> to run.  Unless you have a very small service, you are going to need an order of magnitude (or two) more RAM to run your service, at which point the spring framework is a rounding error.  Even the JVM itself is probably overwhelming this.  You can do some testing, but please don't spend any time optimizing away 2-3 mb of RAM... Optimize when you hit a gig or more.</p>&#xA;&#xA;<p>Licensing:  Other than the above comment on database instances, this is too broad to address.  If you are licensing individual products/libraries, then you will have to consult your license agreement to see how it will break out.  Some license by the ""server"", some by the ""application"", some by the process.  </p>&#xA;&#xA;<p>UI:  I don't really understand this.  Microservices are assembled into an application.  That application has a UI.  The services themselves don't usually have a UI.  This application is usually either a separate application server, or sometimes just a HTML page with javascript to invoke the services.  </p>&#xA;"
29077723,29071226,2448829,2015-03-16T13:12:20,"<p>These decisions will be driven by two concerns: state and security (which is a specific case of state).<br>&#xA;State:  How are you going to keep transient state (i.e. the stuff you would have shoved into session data)?  If you want to try and keep them in the UI, then you can think about keeping pure microservices, otherwise you will need a coordinating ""service"" to hold that state, and that, by necessity, will become the dispatching center as well.</p>&#xA;&#xA;<p>Security:  Authentication is a specific case of state which usually cannot be stored in the client.  It is usually the thing that drives people to a stateful application.  It will also be the driving factor for the API layer vs the Web Application layer.  API layers need to be secured, probably through some sort of Auth token scheme (look at OAuth or similar).  The validation of this token, and the retrieval of user credentials can be fairly slow (comparatively).  This is usually done most efficiently at a centralized service, not on each microservice call.</p>&#xA;"
49256664,49252691,2448829,2018-03-13T12:49:06,"<p>This one is really opinion based, there are no right or wrong answers here (actually not true, lots of wrong answers, but also lots of right ones).  It really depends on your use-case.  Personally, I would tend toward looser coupling until I need stronger coupling.  </p>&#xA;&#xA;<p>My favorite pattern for this has been to put in a URL for the Bank ID instead of a numeric.  That gives you a pointer to the bank, without having to know the specifics of another service's internal structure.</p>&#xA;&#xA;<p>If, at a later date, you decide you need the ID, you can always resolve all the URLs, and gather the IDs, or you can do that at run-time.  The whole point is, you don't have to know this yet, until you have consuming use-cases for this data, don't couple it.</p>&#xA;"
49177177,49171571,2448829,2018-03-08T15:50:21,"<p>You should stop worrying about normalizing your microservice so much.  This is one of the harder bits to microservices, is to stop thinking about them like they are fully normalized database tables.</p>&#xA;&#xA;<p>I'd store your author names along with your books.  You can have extended author attributes in the author service, and you can still have an author ID to link the two, but keep enough denormalized author information to perform your search in that book service.  </p>&#xA;&#xA;<p>Then the call to search is simple... the potential downside, that author information is out-of-date on the books, is probably ok... if someone changes an author name, you will have to have a process to update it from the author service, but that's a FAR more scalable process than looking up authors for each call.</p>&#xA;"
51451942,51421205,2448829,2018-07-21T00:58:48,"<p>Any time you have multiple input sources to a particular piece of state, you have the potential for a race condition.  This is why we have locks.  You MUST use a lock of some sort to handle the race condition.  </p>&#xA;&#xA;<p>This may be implemented at the database layer, or using a locking library, or a shared piece of memory (please use a locking library unless you have way more understanding of memory / CPU architecture than this posting implies).</p>&#xA;&#xA;<p>Most languages implement some form of mutex locking to allow this problem to be solved.  If you are using multiple processes / machines / etc, you will need some form of external mutex locking (often implemented by your database provider).  If you lack this, there are distributed lock systems that also can be used.</p>&#xA;"
36574652,36573857,2448829,2016-04-12T13:22:39,<p>This is usually resolved with an external data store that is atomic in nature.  Use a transactional data store like a SQL database to store user names/ids.  This will allow you to do things like create unique constraints to enforce the uniqueness of these user names.</p>&#xA;
45013246,45007694,2448829,2017-07-10T13:23:37,"<p>This is one of the more complicated topics in services (not just microservices).  I have done this two different ways.</p>&#xA;&#xA;<p>1)  Put in a backwards compatibility layer into the new service (thus allowing v2 to continue to service v1 client calls)</p>&#xA;&#xA;<p>2)  Keep the database backwards compatible, putting in intelligent defaults for new columns, only deleting columns once all relevant versions are removed.</p>&#xA;&#xA;<p>Ultimately #1 has been the easiest to maintain over time, just because we rarely get everyone off the v1 services, leading to a messy db schema.  An implementation where the api conversions happen as a chain, with each version converting to the next highest, until ultimately it reaches the current version.</p>&#xA;&#xA;<p>I would avoid db-level synchronization if at all possible.  It's rare that everyone gets off a given version, and once you get to a half-dozen of these sitting around you will be very sad.</p>&#xA;"
48211078,48209566,2448829,2018-01-11T15:55:08,"<p>You are creating a redundant queue inside your database.  If you queue is unreliable, you could create several of them, but engineering your own queue using a database is duplicating effort.  If you really need two queues for redundancy, create two queues.  It can double as a hospital queue, as there is likely a need to handle processing failures from your primary queue (with the same retry semantics).</p>&#xA;"
48216539,48208376,2448829,2018-01-11T22:03:44,"<p>The key requirement here needs to be not coupling these services together.  Otherwise you might as well have created a monolith, and skipped all the complex plumbing you are putting in place.</p>&#xA;&#xA;<p>This excludes #1 &amp; #2</p>&#xA;&#xA;<p>Option #3 would be best handled by something along the lines of an <a href=""https://en.wikipedia.org/wiki/Enterprise_service_bus"" rel=""nofollow noreferrer"">ESB</a> like MuleSoft.  </p>&#xA;&#xA;<p>4 could work, as long as you are OK if something goes down for a few days, and those events are lost to the ether.</p>&#xA;&#xA;<p>Ultimately, I think an ESB or a configurable event bus would be in your best interests.  #4 could work, and would be simplest, but has the potential downside of dropping events.</p>&#xA;&#xA;<p>Options #1 &amp; #2 will make a giant mess, and you should avoid them.</p>&#xA;"
50077880,50076257,2448829,2018-04-28T15:02:15,"<p>We currently have a similar architecture that we run today at my organization. It is not quite as flexible as the one you are proposing, but strongly separating the business logic services from data/domain services has been very successful over a long period of time.  I don't know if there is a formal architectural name for it, but it definitely allowed a stable domain to be reused in lots of different applications.  </p>&#xA;&#xA;<p>I know this doesn't perfectly answer your question, but it is an example of a successful architecture that closely approximates your proposed one.</p>&#xA;"
37865112,37864491,2448829,2016-06-16T16:52:29,"<p>You are looking for something along the lines of <a href=""https://en.wikipedia.org/wiki/Business_Process_Execution_Language"" rel=""nofollow"">BPEL</a> to abstract your business logic.  Unless you have an explicit need to externalize this, I highly recommend that you do not.  It is much harder to test, and adds significant complexity to your service.</p>&#xA;&#xA;<p>That being said, you probably want to wrap your other services with a facade so that you are insulated from the details of the calls.  This allows your logic to be testable, and allows those services implementations to change independently from the rest of your application.</p>&#xA;"
32375812,32373324,2448829,2015-09-03T12:35:42,"<p>You almost definitely want a public/private split in your microservices architecture.  The public side should be authenticating the token, and the private side is used to service calls from other API calls.  This way you are only authenticating once per call.</p>&#xA;&#xA;<p>You can accomplish this by, as you said, creating a gateway service, which dispatches those calls to the private services.  This is a very common pattern.  We have found it useful to authenticate the gateway side to the private API with client certificate authentication, sometimes referred to as two-way SSL.  This is a little more secure than a shared-secret (which can easily leak).</p>&#xA;"
30213907,30213456,2448829,2015-05-13T11:47:30,"<p>All distributed systems have trouble with transactional consistency.  The best way to do this is like you said, have a two-phase commit.  Have the wallet and the user be created in a pending state.  After it is created, make a separate call to activate the user.</p>&#xA;&#xA;<p>This last call should be safely repeatable (in case your connection drops). </p>&#xA;&#xA;<p>This will necessitate that the last call know about both tables (so that it can be done in a single JDBC transaction).  </p>&#xA;&#xA;<p>Alternatively, you might want to think about why you are so worried about a user without a wallet.  Do you believe this will cause a problem?  If so, maybe having those as separate rest calls are a bad idea.  If a user shouldn't exist without a wallet, then you should probably add the wallet to the user (in the original POST call to create the user).</p>&#xA;"
33677717,33669733,2448829,2015-11-12T17:34:47,"<p>If the point of the document consumer is to just hide the API of the document service, then it is almost definitely a waste of time.  If the problem is that the document service is too big, and is difficult to scale the way you need to for asynchronous document processing, that is a different problem.</p>&#xA;&#xA;<p>I have gotten around this in the past by taking the same binary, and creating multiple interfaces to it, allow me to deploy it in ""service mode"" or ""worker mode"".  In ""service mode"" it serves public and synchronous requests.  In worker mode, a subset of the system is spun up just to service messages from a queue.</p>&#xA;"
33811647,33805449,2448829,2015-11-19T18:29:09,"<p>If you truly need a multi-master consistent database, then you will almost definitely need to implement this at the database layer.  </p>&#xA;&#xA;<p>I would not cache things that are transactionally sensitive.  If you truly need to do this, and cannot specify a reasonable TTL in which content can be stale, then you will need to set up a pub/sub sort of mechanism to expire modified entities.  A lot of this really depends on your data, how often it changes, can you separate cacheable vs non-cacheable data?  These questions strongly influence your caching decisions.</p>&#xA;"
33882585,33869866,2448829,2015-11-23T23:15:37,"<p>A couple of things here:</p>&#xA;&#xA;<p>1) CRM and HRM are <em>not</em> microservices.  There is nothing micro about these domains.</p>&#xA;&#xA;<p>2) In general, you want as little coupling between services as possible.  That doesn't mean you can't use compositional patterns, but you should think about why you are using them in any given situation.  If you are doing it just to avoid code-duplication or database duplication, you should really think skeptically about it.</p>&#xA;&#xA;<p>What do you want to accomplish by putting the employees into the CRM database? Is it just that you already have a ""person"" domain/database in CRM?  If so, your case for keeping employees in your CRM is very weak.  If, on the other hand, your goal is to treat employees as customers, and internally market to them, then putting employees in the CRM is probably valuable.</p>&#xA;&#xA;<p>I would really suggest you look at designing these as separate stores.  It will allow these systems to evolve independently, scale independently, etc.   </p>&#xA;&#xA;<p>If you really want all contacts in a single system, consider a new service (maybe something more ""micro""), that stores and retrieves contacts.  This could be used by both services, but will be independent of each.</p>&#xA;"
36646714,36642718,2448829,2016-04-15T12:07:02,"<p>Realistically, unless you are doing something computationally intensive on reading or writing, your database IO will likely be your point of contention.  I would strongly consider building your system ""perfect"", and then running capacity tests to see where the choke points are.  Do not forget the words of Donald Knuth:  ""<a href=""http://c2.com/cgi/wiki?PrematureOptimization"" rel=""nofollow noreferrer"">Premature optimization is the root of all evil</a>"".  </p>&#xA;&#xA;<p>If you decide that your service needs to be scaled, see if you can scale both reading and writing horizontally (make more instances of the combined service).</p>&#xA;&#xA;<p>If this fails to get you the performance you need, THEN look at much more complex scaling requirements like another answerer has <a href=""https://stackoverflow.com/a/36645418/569662"">proposed</a>.</p>&#xA;"
47166862,47162798,2448829,2017-11-07T20:28:13,"<p>I'd say you should have one more complex service, and three simple ones.  Your category service should just be CRUD for categories, your user service should be CRUD for users, and your products should be more complicated, call it a productlisting service, and then still have a simple product service.</p>&#xA;&#xA;<p>Your productlisting service should have all the complexity, but probably be denormalized.</p>&#xA;&#xA;<pre><code>GET/POST/PUT/DELETE /product&#xA;GET/POST/PUT/DELETE /category&#xA;GET/POST/PUT/DELETE /user&#xA;&#xA;POST/PUT /productlisting/usercategory/&lt;userid&gt;  &lt;list of categories&gt;&#xA;POST/PUT /productlisting/productcategory/&lt;productid&gt;  &lt;list of categories&gt;&#xA;GET /productlisting&#xA;</code></pre>&#xA;&#xA;<p>Either this, or combine them all into one monolith...  They aren't separate concerns, and coupling them by ID will just make you sad in the long-run, as your coupling will be brittle.  I never make one service aware of another service's entity ID.  Such relational constraints in a distributed system is just asking for trouble.</p>&#xA;"
35068936,35039565,420797,2016-01-28T18:13:37,"<p>I suggest using as many operation references as possible:</p>&#xA;&#xA;<pre><code>paths:&#xA;  health:&#xA;    $ref: ""path/to/your/operation.json""&#xA;</code></pre>&#xA;&#xA;<p>Many people build simple routers to combine the specs into single ones, which can be quite powerful:</p>&#xA;&#xA;<pre><code>https://router/{service}/{all-operations-from-service}&#xA;</code></pre>&#xA;&#xA;<p>If you have a valid spec file, then swagger-ui will be fine.  There are some known issues for extremely large specs loading slowly, but consider that a bug not a limitation of swagger-ui (meaning it'll be fixed).</p>&#xA;"
48692252,47793065,2196157,2018-02-08T18:18:29,"<p><strong>Yes you can.</strong></p>&#xA;&#xA;<p><strong>However</strong> you don't really want to get stuck in a framework when you are writing web components, because the whole point of a web component is to be reused possibly anywhere.</p>&#xA;&#xA;<p>I share your vision, it's definitely better for the quality of the software as well as the productivity of developers to work on focused features rather than large apps when it comes to implementation.</p>&#xA;&#xA;<p><strong>Finally</strong>, what you are looking for right now is not Angular, but <strong><a href=""https://stenciljs.com/"" rel=""nofollow noreferrer"">StencilJS</a></strong>. Stencil is a compiler for web components who also generate a virtual DOM inside your component to improve it's UI performances. Check it out ;-)</p>&#xA;"
41162375,41036545,166589,2016-12-15T10:53:34,"<p>As it was said earlier, <code>@JsonAnyGetter</code> and <code>@JsonAnySetter</code> can be the best choice for you.&#xA;I think that you can do it as much flexible as much type-safe it can be.</p>&#xA;&#xA;<p>The very first thing that comes to my mind is accurate separating the properties you need and the whole rest.</p>&#xA;&#xA;<h2>The core</h2>&#xA;&#xA;<h3>Calculation.java</h3>&#xA;&#xA;<p>A simple immutable ""calculation"" object.&#xA;Of course, it can be designed in any other way, but immutability makes it simpler and more reliable, I believe.</p>&#xA;&#xA;<pre class=""lang-java prettyprint-override""><code>final class Calculation {&#xA;&#xA;    private final double a;&#xA;    private final double b;&#xA;    private final Operation operation;&#xA;    private final Double result;&#xA;&#xA;    private Calculation(final double a, final double b, final Operation operation, final Double result) {&#xA;        this.a = a;&#xA;        this.b = b;&#xA;        this.operation = operation;&#xA;        this.result = result;&#xA;    }&#xA;&#xA;    static Calculation calculation(final double a, final double b, final Operation operation, final Double result) {&#xA;        return new Calculation(a, b, operation, result);&#xA;    }&#xA;&#xA;    Calculation calculate() {&#xA;        return new Calculation(a, b, operation, operation.applyAsDouble(a, b));&#xA;    }&#xA;&#xA;    double getA() {&#xA;        return a;&#xA;    }&#xA;&#xA;    double getB() {&#xA;        return b;&#xA;    }&#xA;&#xA;    Operation getOperation() {&#xA;        return operation;&#xA;    }&#xA;&#xA;    Double getResult() {&#xA;        return result;&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<h3>Operation.java</h3>&#xA;&#xA;<p>A simple calculation strategy defined in an enumeration since Jackson works with enumerations really good.</p>&#xA;&#xA;<pre class=""lang-java prettyprint-override""><code>enum Operation&#xA;        implements DoubleBinaryOperator {&#xA;&#xA;    ADD {&#xA;        @Override&#xA;        public double applyAsDouble(final double a, final double b) {&#xA;            return a + b;&#xA;        }&#xA;    },&#xA;&#xA;    SUBTRACT {&#xA;        @Override&#xA;        public double applyAsDouble(final double a, final double b) {&#xA;            return a - b;&#xA;        }&#xA;    },&#xA;&#xA;    MULTIPLY {&#xA;        @Override&#xA;        public double applyAsDouble(final double a, final double b) {&#xA;            return a * b;&#xA;        }&#xA;    },&#xA;&#xA;    DIVIDE {&#xA;        @Override&#xA;        public double applyAsDouble(final double a, final double b) {&#xA;            return a / b;&#xA;        }&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<h2>Jackson mappings</h2>&#xA;&#xA;<h3>AbstractTask.java</h3>&#xA;&#xA;<p>Note that this class is intended to supply a value, but collect the rest into a satellite map managed by the methods annotated with <code>@JsonAnySetter</code> and <code>@JsonAnyGetter</code>.&#xA;The map and the methods can be safely declared <code>private</code> since Jackson does not really care the protection level (and this is great).&#xA;Also, it's designed in immutable manner except of underlying map that can be just shallow-copied to a new value.</p>&#xA;&#xA;<pre class=""lang-java prettyprint-override""><code>abstract class AbstractTask&lt;V&gt;&#xA;        implements Supplier&lt;V&gt; {&#xA;&#xA;    @JsonIgnore&#xA;    private final Map&lt;String, Object&gt; rest = new LinkedHashMap&lt;&gt;();&#xA;&#xA;    protected abstract AbstractTask&lt;V&gt; toTask(V value);&#xA;&#xA;    final &lt;T extends AbstractTask&lt;V&gt;&gt; T with(final V value) {&#xA;        final AbstractTask&lt;V&gt; dto = toTask(value);&#xA;        dto.rest.putAll(rest);&#xA;        @SuppressWarnings(""unchecked"")&#xA;        final T castDto = (T) dto;&#xA;        return castDto;&#xA;    }&#xA;&#xA;    @JsonAnySetter&#xA;    @SuppressWarnings(""unused"")&#xA;    private void set(final String name, final Object value) {&#xA;        rest.put(name, value);&#xA;    }&#xA;&#xA;    @JsonAnyGetter&#xA;    @SuppressWarnings(""unused"")&#xA;    private Map&lt;String, Object&gt; getRest() {&#xA;        return rest;&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<h3>CalculationTask.java</h3>&#xA;&#xA;<p>Here is a class the defines a concrete calculation task.&#xA;Again, Jackson works perfectly with private fields and methods, so the entire complexity can be encapsulated.&#xA;One disadvantage I can see is that JSON properties are declared both for serializing and deserializing, but it can be also considered an advantage as well.&#xA;Note that <code>@JsonGetter</code> arguments are not necessary here as such, but I just doubled the property names both for in- and out- operations.&#xA;No tasks are intended to be instantiated manually - let just Jackson do it.</p>&#xA;&#xA;<pre class=""lang-java prettyprint-override""><code>final class CalculationTask&#xA;        extends AbstractTask&lt;Calculation&gt; {&#xA;&#xA;    private final Calculation calculation;&#xA;&#xA;    private CalculationTask(final Calculation calculation) {&#xA;        this.calculation = calculation;&#xA;    }&#xA;&#xA;    @JsonCreator&#xA;    @SuppressWarnings(""unused"")&#xA;    private static CalculationTask calculationTask(&#xA;            @JsonProperty(""a"") final double a,&#xA;            @JsonProperty(""b"") final double b,&#xA;            @JsonProperty(""operation"") final Operation operation,&#xA;            @JsonProperty(""result"") final Double result&#xA;    ) {&#xA;        return new CalculationTask(calculation(a, b, operation, result));&#xA;    }&#xA;&#xA;    @Override&#xA;    public Calculation get() {&#xA;        return calculation;&#xA;    }&#xA;&#xA;    @Override&#xA;    protected AbstractTask&lt;Calculation&gt; toTask(final Calculation calculation) {&#xA;        return new CalculationTask(calculation);&#xA;    }&#xA;&#xA;    @JsonGetter(""a"")&#xA;    @SuppressWarnings(""unused"")&#xA;    private double getA() {&#xA;        return calculation.getA();&#xA;    }&#xA;&#xA;    @JsonGetter(""b"")&#xA;    @SuppressWarnings(""unused"")&#xA;    private double getB() {&#xA;        return calculation.getB();&#xA;    }&#xA;&#xA;    @JsonGetter(""operation"")&#xA;    @SuppressWarnings(""unused"")&#xA;    private Operation getOperation() {&#xA;        return calculation.getOperation();&#xA;    }&#xA;&#xA;    @JsonGetter(""result"")&#xA;    @SuppressWarnings(""unused"")&#xA;    private Double getResult() {&#xA;        return calculation.getResult();&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<h2>Client-server interaction</h2>&#xA;&#xA;<h3>CalculationController.java</h3>&#xA;&#xA;<p>Here is a simple GET/PUT/DELETE controller for integration testing, or just to be manually tested with <code>curl</code>.</p>&#xA;&#xA;<pre class=""lang-java prettyprint-override""><code>@RestController&#xA;@RequestMapping(""/"")&#xA;final class CalculationController {&#xA;&#xA;    private final CalculationService processService;&#xA;&#xA;    @Autowired&#xA;    @SuppressWarnings(""unused"")&#xA;    CalculationController(final CalculationService processService) {&#xA;        this.processService = processService;&#xA;    }&#xA;&#xA;    @RequestMapping(method = GET, value = ""{id}"")&#xA;    @ResponseStatus(OK)&#xA;    @SuppressWarnings(""unused"")&#xA;    CalculationTask get(@PathVariable(""id"") final String id) {&#xA;        return processService.get(id);&#xA;    }&#xA;&#xA;    @RequestMapping(method = PUT, value = ""{id}"")&#xA;    @ResponseStatus(NO_CONTENT)&#xA;    @SuppressWarnings(""unused"")&#xA;    void put(@PathVariable(""id"") final String id, @RequestBody final CalculationTask task) {&#xA;        processService.put(id, task);&#xA;    }&#xA;&#xA;    @RequestMapping(method = DELETE, value = ""{id}"")&#xA;    @ResponseStatus(NO_CONTENT)&#xA;    @SuppressWarnings(""unused"")&#xA;    void delete(@PathVariable(""id"") final String id) {&#xA;        processService.delete(id);&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<h3>ControllerExceptionHandler.java</h3>&#xA;&#xA;<p>Since the <code>get</code> and <code>delete</code> methods declared below in the DAO class throw <code>NoSuchElementException</code>, the exception can be easily mapped to HTTP 404.</p>&#xA;&#xA;<pre><code>@ControllerAdvice&#xA;final class ControllerExceptionHandler {&#xA;&#xA;    @ResponseStatus(NOT_FOUND)&#xA;    @ExceptionHandler(NoSuchElementException.class)&#xA;    @SuppressWarnings(""unused"")&#xA;    void handleNotFound() {&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<h2>The application itself</h2>&#xA;&#xA;<h3>CalculationService.java</h3>&#xA;&#xA;<p>Just a simple service that contains some ""business"" logic.</p>&#xA;&#xA;<pre class=""lang-java prettyprint-override""><code>@Service&#xA;final class CalculationService {&#xA;&#xA;    private final CalculationDao processDao;&#xA;&#xA;    @Autowired&#xA;    CalculationService(final CalculationDao processDao) {&#xA;        this.processDao = processDao;&#xA;    }&#xA;&#xA;    CalculationTask get(final String id) {&#xA;        return processDao.get(id);&#xA;    }&#xA;&#xA;    void put(final String id, final CalculationTask task) {&#xA;        processDao.put(id, task.with(task.get().calculate()));&#xA;    }&#xA;&#xA;    void delete(final String id) {&#xA;        processDao.delete(id);&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<h2>The data layer</h2>&#xA;&#xA;<h3>CalculationMapping.java</h3>&#xA;&#xA;<p>Just a holder class in order to work with MongoDB repositories in Spring Data specifying the target MongoDB document collection name.</p>&#xA;&#xA;<pre class=""lang-java prettyprint-override""><code>@Document(collection = ""calculations"")&#xA;public final class CalculationTaskMapping&#xA;        extends org.bson.Document {&#xA;&#xA;    @Id&#xA;    @SuppressWarnings(""unused"")&#xA;    private String id;&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<h3>ICalculationRepository.java</h3>&#xA;&#xA;<p>A Spring Data MongoDB CRUD repository for the <code>CalculationMapping</code> class.&#xA;This repository is used below.</p>&#xA;&#xA;<pre class=""lang-java prettyprint-override""><code>@Repository&#xA;interface ICalculationRepository&#xA;        extends MongoRepository&lt;CalculationTaskMapping, String&gt; {&#xA;}&#xA;</code></pre>&#xA;&#xA;<h3>CalculationDao.java</h3>&#xA;&#xA;<p>The DAO component does not make much work in the demo itself, and it's more about delegating the persistence job to its super class and being easy to find by Spring Framework.</p>&#xA;&#xA;<pre class=""lang-java prettyprint-override""><code>@Component&#xA;final class CalculationDao&#xA;        extends AbstractDao&lt;CalculationTask, CalculationTaskMapping, String&gt; {&#xA;&#xA;    @Autowired&#xA;    CalculationDao(@SuppressWarnings(""TypeMayBeWeakened"") final ICalculationRepository calculationRepository, final ObjectMapper objectMapper) {&#xA;        super(CalculationTaskMapping.class, calculationRepository, objectMapper);&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<h3>AbstractDao.java</h3>&#xA;&#xA;<p>This is the heart of persisting the whole original object.&#xA;The <code>ObjectMapper</code> instance is used to convert tasks to their respective task mappings (see the <code>convertValue</code> method) according to the serialization rules specified with the Jackson annotations.&#xA;Since the demo uses Spring Data MongoDB, the mapping classes are effectively <code>Map&lt;String, Object&gt;</code> and inherit the <code>Document</code> class.&#xA;Unfortunately, <code>Map</code>-oriented mappings do not seem to work with Spring Data MongoDB annotations like <code>@Id</code>, <code>@Field</code>, etc (see more at <a href=""https://stackoverflow.com/questions/41148449/how-do-i-combine-java-util-map-based-mappings-with-the-spring-data-mongodb-annot"">How do I combine java.util.Map-based mappings with the Spring Data MongoDB annotations (@Id, @Field, ...)?</a>).&#xA;However, it can be justified as long as you do not want to map arbitrary documents.</p>&#xA;&#xA;<pre><code>abstract class AbstractDao&lt;T, M extends Document, ID extends Serializable&gt; {&#xA;&#xA;    private final Class&lt;M&gt; mappingClass;&#xA;    private final CrudRepository&lt;M, ID&gt; crudRepository;&#xA;    private final ObjectMapper objectMapper;&#xA;&#xA;    protected AbstractDao(final Class&lt;M&gt; mappingClass, final CrudRepository&lt;M, ID&gt; crudRepository, final ObjectMapper objectMapper) {&#xA;        this.mappingClass = mappingClass;&#xA;        this.crudRepository = crudRepository;&#xA;        this.objectMapper = objectMapper;&#xA;    }&#xA;&#xA;    final void put(final ID id, final T task) {&#xA;        final M taskMapping = objectMapper.convertValue(task, mappingClass);&#xA;        taskMapping.put(ID_FIELD_NAME, id);&#xA;        if ( crudRepository.exists(id) ) {&#xA;            crudRepository.delete(id);&#xA;        }&#xA;        crudRepository.save(taskMapping);&#xA;    }&#xA;&#xA;    final CalculationTask get(final ID id) {&#xA;        final Map&lt;String, Object&gt; rawTask = crudRepository.findOne(id);&#xA;        if ( rawTask == null ) {&#xA;            throw new NoSuchElementException();&#xA;        }&#xA;        rawTask.remove(ID_FIELD_NAME);&#xA;        return objectMapper.convertValue(rawTask, CalculationTask.class);&#xA;    }&#xA;&#xA;    final void delete(final ID id) {&#xA;        final M taskMapping = crudRepository.findOne(id);&#xA;        if ( taskMapping == null ) {&#xA;            throw new NoSuchElementException();&#xA;        }&#xA;        crudRepository.delete(id);&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<h2>The Spring Boot application</h2>&#xA;&#xA;<h3>EntryPoint.class</h3>&#xA;&#xA;<p>And a Spring Boot demo that runs all of it as a single HTTP application listening to port 9000. </p>&#xA;&#xA;<pre><code>@SpringBootApplication&#xA;@Configuration&#xA;@EnableWebMvc&#xA;public class EntryPoint&#xA;        extends SpringBootServletInitializer {&#xA;&#xA;    @Override&#xA;    protected SpringApplicationBuilder configure(final SpringApplicationBuilder builder) {&#xA;        return builder.sources(EntryPoint.class);&#xA;    }&#xA;&#xA;    @Bean&#xA;    EmbeddedServletContainerCustomizer embeddedServletContainerCustomizer() {&#xA;        return c -&gt; c.setPort(9000);&#xA;    }&#xA;&#xA;    @Bean&#xA;    ObjectMapper objectMapper() {&#xA;        return new ObjectMapper()&#xA;                .setSerializationInclusion(NON_NULL)&#xA;                .configure(FAIL_ON_UNKNOWN_PROPERTIES, false);&#xA;    }&#xA;&#xA;    @SuppressWarnings(""resource"")&#xA;    public static void main(final String... args) {&#xA;        SpringApplication.run(EntryPoint.class, args);&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<h2>Testing the application with <code>curl</code></h2>&#xA;&#xA;<pre><code>(mongodb-shell)&#xA;&gt; use test&#xA;</code></pre>&#xA;&#xA;<blockquote>&#xA;  <p>switched to db local</p>&#xA;</blockquote>&#xA;&#xA;<pre><code>(bash)&#xA;$ curl -v -X GET http://localhost:9000/foo&#xA;</code></pre>&#xA;&#xA;<blockquote>&#xA;  <p>> GET /foo HTTP/1.1<br>&#xA;  > User-Agent: curl/7.35.0<br>&#xA;  > Host: localhost:9000<br>&#xA;  > Accept: <em>/</em><br>&#xA;  ><br>&#xA;  &lt; HTTP/1.1 404<br>&#xA;  &lt; Content-Length: 0<br>&#xA;  &lt; Date: Thu, 15 Dec 2016 10:07:40 GMT<br>&#xA;  &lt;  </p>&#xA;</blockquote>&#xA;&#xA;<pre><code>(mongodb-shell)&#xA;&gt; db.calculations.find()&#xA;</code></pre>&#xA;&#xA;<p>(empty)</p>&#xA;&#xA;<pre><code>(bash)&#xA;$ curl -v -X PUT -H 'Content-Type: application/json' \&#xA;    --data '{""a"":3,""b"":4,""operation"":""MULTIPLY"",""result"":12,""foo"":""FOO"",""bar"":""BAR""}' \&#xA;    http://localhost:9000/foo&#xA;</code></pre>&#xA;&#xA;<blockquote>&#xA;  <p>> PUT /foo HTTP/1.1<br>&#xA;  > User-Agent: curl/7.35.0<br>&#xA;  > Host: localhost:9000<br>&#xA;  > Accept: <em>/</em><br>&#xA;  > Content-Type: application/json<br>&#xA;  > Content-Length: 72<br>&#xA;  ><br>&#xA;  &lt; HTTP/1.1 204<br>&#xA;  &lt; Date: Thu, 15 Dec 2016 10:11:13 GMT<br>&#xA;  &lt;</p>&#xA;</blockquote>&#xA;&#xA;<pre><code>(mongodb-shell)&#xA;&gt; db.calculations.find()&#xA;</code></pre>&#xA;&#xA;<blockquote>&#xA;  <p>{ ""_id"" : ""foo"", ""_class"" : ""q41036545.CalculationTaskMapping"", ""a"" : 3, ""b"" : 4, ""operation"" : ""MULTIPLY"", ""result"" : 12, ""foo"" : ""FOO"", ""bar"" : ""BAR"" }</p>&#xA;</blockquote>&#xA;&#xA;<pre><code>(bash)&#xA;$ curl -v -X GET http://localhost:9000/foo&#xA;</code></pre>&#xA;&#xA;<blockquote>&#xA;  <p>> GET /foo HTTP/1.1<br>&#xA;  > User-Agent: curl/7.35.0<br>&#xA;  > Host: localhost:9000<br>&#xA;  > Accept: <em>/</em><br>&#xA;  ><br>&#xA;  &lt; HTTP/1.1 200<br>&#xA;  &lt; Content-Type: application/json;charset=UTF-8<br>&#xA;  &lt; Transfer-Encoding: chunked<br>&#xA;  &lt; Date: Thu, 15 Dec 2016 10:16:33 GMT<br>&#xA;  &lt;<br>&#xA;  {""a"":3.0,""b"":4.0,""operation"":""MULTIPLY"",""result"":12.0,""foo"":""FOO"",""bar"":""BAR""}</p>&#xA;</blockquote>&#xA;&#xA;<pre><code>(bash)&#xA;curl -v -X DELETE http://localhost:9000/foo&#xA;</code></pre>&#xA;&#xA;<blockquote>&#xA;  <p>> DELETE /foo HTTP/1.1<br>&#xA;  > User-Agent: curl/7.35.0<br>&#xA;  > Host: localhost:9000<br>&#xA;  > Accept: <em>/</em><br>&#xA;  ><br>&#xA;  &lt; HTTP/1.1 204<br>&#xA;  &lt; Date: Thu, 15 Dec 2016 11:51:26 GMT<br>&#xA;  &lt;</p>&#xA;</blockquote>&#xA;&#xA;<pre><code>(mongodb-shell)&#xA;&gt; db.calculations.find()&#xA;</code></pre>&#xA;&#xA;<p>(empty)</p>&#xA;&#xA;<p>The source code can be found at <a href=""https://github.com/lyubomyr-shaydariv/q41036545"" rel=""nofollow noreferrer"">https://github.com/lyubomyr-shaydariv/q41036545</a></p>&#xA;"
47271551,47271002,3381181,2017-11-13T18:55:23,"<p>You can use <a href=""http://gemfire82.docs.pivotal.io/docs-gemfire/latest/developing/transactions/chapter_overview.html"" rel=""nofollow noreferrer"">transactions</a>. Something like this should work:</p>&#xA;&#xA;<pre><code>txMgr = cache.getTransactionManager();&#xA;txMgr.begin();&#xA;boxReceive.put();&#xA;...&#xA;boxtransfer.put();&#xA;txMgr.commit();&#xA;</code></pre>&#xA;&#xA;<p>This will work provided you <a href=""http://gemfire82.docs.pivotal.io/docs-gemfire/latest/developing/partitioned_regions/colocating_partitioned_region_data.html"" rel=""nofollow noreferrer"">co-locate</a> the box-receive and the box-transfer region and use the same key, or use a PartitionResolver to colocate the data.</p>&#xA;"
46451037,46434569,327101,2017-09-27T14:52:54,"<p>In your microservice you can do whatever you want. Let's say that we have two microservice <strong>A</strong> and <strong>B</strong> and we want to monitor their health check:</p>&#xA;&#xA;<p><strong>Microservice A</strong></p>&#xA;&#xA;<p>This microservice uses SQL Server, so we're going to check SQL connection.</p>&#xA;&#xA;<pre><code>services.AddHealthChecks(checks =&gt;&#xA;{&#xA;    checks.AddSqlCheck(""ServiceA_DB"", Configuration[""ConnectionString""]);&#xA;});&#xA;</code></pre>&#xA;&#xA;<p><strong>Microservice B</strong></p>&#xA;&#xA;<p>This microservice uses SQL server too, but it also uses some other service (for example REST API), so we're going to check SQL connection and the REST API</p>&#xA;&#xA;<pre><code>services.AddHealthChecks(checks =&gt;&#xA;{&#xA;    checks.AddUrlCheck(Configuration[""RequiredServiceUrl""]);&#xA;    checks.AddSqlCheck(""ServiceB_DB"", Configuration[""ConnectionString""]);&#xA;});&#xA;</code></pre>&#xA;&#xA;<p><strong>Web Status</strong></p>&#xA;&#xA;<p>Finally we have some web application that monitors these two microservices</p>&#xA;&#xA;<pre><code>services.AddHealthChecks(checks =&gt;&#xA;{&#xA;    checks.AddUrlCheck(Configuration[""ServiceAUrl""]);&#xA;    checks.AddUrlCheck(Configuration[""ServiceBUrl""]);&#xA;});&#xA;</code></pre>&#xA;&#xA;<p>It means that if I navigate to <code>http://webstatus/hc</code> (health check page), the system checks <code>http://serviceA/hc</code> (it checks db), and <code>http://serviceB/hc</code> (it checks db and rest api).</p>&#xA;&#xA;<p>Or you can visualize health check of each microservice as shown <a href=""https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/implement-resilient-applications/monitor-app-health"" rel=""nofollow noreferrer"">there</a> (Figure 10-8)</p>&#xA;"
50680903,50647694,1638590,2018-06-04T12:41:20,"<p>I conclude and the top highlights for the reason why using <code>Eureka</code> and/or <code>Kubernetes</code> can be listed as:</p>&#xA;&#xA;<ul>&#xA;<li>Java only</li>&#xA;<li>developer friendly</li>&#xA;<li>lower server costs for the complete stack</li>&#xA;<li>less OPS dependent</li>&#xA;<li>more resources on developer communities and tutorials</li>&#xA;<li>gradual learning curve</li>&#xA;</ul>&#xA;&#xA;<p>So,</p>&#xA;&#xA;<blockquote>&#xA;  <p>If you need some of your microservices in an other language or you&#xA;  can rely on your developers knowledge on <code>Kubernetes</code> are not afraid&#xA;  to spend a bit more time and money investing in your tech stack to&#xA;  have a wider and less dependent system then <code>Kubernetes</code> is the way to&#xA;  go.</p>&#xA;</blockquote>&#xA;&#xA;<p>On the other hand</p>&#xA;&#xA;<blockquote>&#xA;  <p>If you need a fast development well integrated with <a href=""https://spring.io/projects/spring-boot"" rel=""nofollow noreferrer"">spring-boot</a> stack&#xA;  with easy to use Java annotations without large involvement of DevOps then and less resource to train your developers then go for Eureka and <a href=""http://projects.spring.io/spring-cloud/"" rel=""nofollow noreferrer"">spring-cloud</a> stack.</p>&#xA;</blockquote>&#xA;&#xA;<p>for more details and comparison charts and features list please refer to <a href=""http://www.ofbizian.com/2016/12/spring-cloud-compared-kubernetes.html"" rel=""nofollow noreferrer"">this article</a>.</p>&#xA;"
41183772,41036545,1785022,2016-12-16T11:51:44,"<p>So if I understand what you want correctly:</p>&#xA;&#xA;<p>Service A -> send json string -> Service B -> send same json string -> Service C</p>&#xA;&#xA;<ul>&#xA;<li>Service A creates the JSON String</li>&#xA;<li>Service B cares only for a few parameters of the JSON String</li>&#xA;<li>Service C cares only for a few parameters of the JSON String</li>&#xA;</ul>&#xA;&#xA;<p>If this is what you want, I would suggest a design where you store the entire json string, and just pass that through, and by using IgnoreUnknownProperties in both Service B and C, you can have your objects parsed as you like. This however requires you to parse it yourself, but that is easily done using ObjectMapper, you can create it and put it in your spring context and use it anywhere.</p>&#xA;&#xA;<p>Example:</p>&#xA;&#xA;<pre><code>// Reading data&#xA;String jsonString restTemplate.getForObject(request, String.class);&#xA;MyClass actualObj = new ObjectMapper().readValue(jsonString, MyClass.class);&#xA;actualObj.setOriginalJson(jsonString);&#xA;&#xA;// Writing data&#xA;String jsonString = actualObj.getOriginalJson();&#xA;restTemplate.postForLocation(URI.create(url), jsonString);&#xA;&#xA;@JsonIgnoreProperties(ignoreUnknown = true)&#xA;public class MyClass {&#xA;    private String property1;&#xA;    private String property2;&#xA;&#xA;    @JsonIgnore&#xA;    private String originalJson;&#xA;&#xA;    public String getOriginalJson() {&#xA;        return originalJson;&#xA;    }&#xA;&#xA;    public void setOriginalJson(String originalJson) {&#xA;        this.originalJson = originalJson;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>You can easily just wrap the string + MyClass in one object, or create a variable inside MyClass that holds the string for future use (as my example shows).</p>&#xA;&#xA;<p>This is the easiest way I see you can keep the data and yet not having to modify your objects to contain it. </p>&#xA;&#xA;<p>Another note, don't make new ObjectMapper() everytime, create it once, put it into the spring context, and then get it by Autowired where you want to use this.</p>&#xA;"
45208903,45208766,1271230,2017-07-20T08:05:06,"<p>In Microservices architecture you have two ways to communicate between the microservices:</p>&#xA;&#xA;<ul>&#xA;<li>Synchronous - that is, each service calls directly the other microservice , which results in dependency between the services</li>&#xA;<li>Asynchronous - you have some central hub (or message queue) where you place all requests between the microservices and the corresponding service takes the request, process it and return the result to the caller. This is what RabbitMQ (or any other message queue - MSMQ and Apache Kafka are good alternatives) is used for. In this case all microservices know only about the existance of the hub.</li>&#xA;</ul>&#xA;&#xA;<p><a href=""http://microservices.io/"" rel=""noreferrer"">microservices.io</a> has some very nice articles about using microservices</p>&#xA;"
47147203,46934916,6070462,2017-11-06T22:54:06,"<p>As per my knowledge , you should use Spring boot application with Maven build.&#xA;In order to send a REST call to SOAP web service and get back a JSON Response ,you need to follow all of this steps in order :</p>&#xA;&#xA;<ol>&#xA;<li><p>Business Logic  : Mapping the JSON fields such as headers, query-param , body variables to XML Request mandatory fields (either using pojo classes or object-mapper) Reason : Since the SOAP end point will only accept XML Request.</p></li>&#xA;<li><p>Service Broker Logic : Import ""org.springframework.web.client.RestTemplate"" and use </p>&#xA;&#xA;<pre><code>**ResponseEntity&lt;String&gt; responseEntity=RestTemplate.exchange(endPointURL, HttpMethod.GET/POST/PUT/DELETE, HttpEntity/headers, uriVariables)**&#xA;&#xA;**endpointURL** -- SOAP End point URL ,that the REST service has to consume.&#xA;&#xA; **HTTPMethod** -- Method Type such as GET ,PUT ,POST ,DELETE etc.&#xA;&#xA; **HTTPEntity** -- Soap requires for mandatory sender/headers/{a}.Make sure that you set your header name and value as key-Valye pair in HTTP headers.&#xA;&#xA; **uriVariables** -- (Object... urivariables) such as String.class ,Integer.class&#xA;&#xA; You should also put the **connectTimeout** ,**isSSLDisabled**,**responseCached** elements while generating request to restTemplate.&#xA;</code></pre></li>&#xA;<li><p>responseEntity.getBody() is the XML response after un-marshalling.It can be extracted by using mapper. </p>&#xA;&#xA;<pre><code>     XML_BaseResponse response=mapper.readValue(responseEntity.getBody(), XML_BaseResponse.class);&#xA;</code></pre></li>&#xA;<li><p>Business Logic : Extract necessary field from XML_BaseResponse and using setter's or getter's functions,set the mandatory fields in the response.</p>&#xA;&#xA;<pre><code>     baseResponse.setName(xml_baseResponse.getPersonsName());&#xA;     baseResponse.setAddress(xml_baseResponse.getAddress());&#xA;     baseResponse.setCity(xml_baseResponse.getcityName());&#xA;</code></pre></li>&#xA;</ol>&#xA;"
47147286,34503547,6070462,2017-11-06T23:00:50,"<p>Use RestTemplate.exchange mentioned below :</p>&#xA;&#xA;<p><strong>ResponseEntity responseEntity=RestTemplate.exchange(endPointURL, HttpMethod.GET/POST/PUT/DELETE, HttpEntity/headers, uriVariables)</strong></p>&#xA;&#xA;<p><strong>endpointURL</strong> -- SOAP End point URL ,that the REST service has to consume.</p>&#xA;&#xA;<p><strong>HTTPMethod</strong> -- Method Type such as GET ,PUT ,POST ,DELETE etc.</p>&#xA;&#xA;<p><strong>HTTPEntity</strong> -- Soap requires for mandatory sender/headers/{a}.Make sure that you set your header name and value as key-Valye pair in HTTP headers.</p>&#xA;&#xA;<p><strong>uriVariables</strong> -- (Object... urivariables) such as String.class ,Integer.class</p>&#xA;&#xA;<p>You should also put the <strong>connectTimeout</strong> ,<strong>isSSLDisabled</strong>,<strong>responseCached</strong> in the constructor while generating request to restTemplate.</p>&#xA;"
51293499,51292640,7341759,2018-07-11T20:08:45,"<p>It is recommend for microservices to use different databases and maintain their own data (<a href=""https://www.martinfowler.com/bliki/PolyglotPersistence.html"" rel=""nofollow noreferrer"">PolyglotPersistence</a>).</p>&#xA;&#xA;<p>Sharing of data across the API's should happen via API's.</p>&#xA;&#xA;<p>In this case, if the payment service requires booking details, the details should be retrieved by making a call to the Booking service API.</p>&#xA;&#xA;<p>For more details, here is the <a href=""https://www.martinfowler.com/articles/microservices.html"" rel=""nofollow noreferrer"">link</a> to microservices article by Martin Fowler.</p>&#xA;"
42063602,42062199,854207,2017-02-06T08:47:52,"<p>Apart what they already response regarding no blocking features, another great feature to use Reactive programing, is the important use of backpressure. Normally is used in situations where your publisher emit more information than your consumer can process.</p>&#xA;&#xA;<p>So having this mechanism you can control the flow of traffic between both and avoid the nasty out of memory problems.</p>&#xA;&#xA;<p>You can see some practicle examples of Reactive programing here <a href=""https://github.com/politrons/reactive"" rel=""nofollow noreferrer"">https://github.com/politrons/reactive</a></p>&#xA;&#xA;<p>And about back pressure here <a href=""https://github.com/politrons/Akka/blob/master/src/main/scala/stream/BackPressure.scala"" rel=""nofollow noreferrer"">https://github.com/politrons/Akka/blob/master/src/main/scala/stream/BackPressure.scala</a></p>&#xA;&#xA;<p>By the way, the only disadvantage about reactive programing, is the curve of learning because you´re changing paradigm of programing. But nowadays all important companies respect and follow the reactive manifesto <a href=""http://www.reactivemanifesto.org/"" rel=""nofollow noreferrer"">http://www.reactivemanifesto.org/</a></p>&#xA;"
38165449,38164006,854207,2016-07-02T23:48:53,"<p>In order to delay a particular step you can use zip and combine that every item emitted in your first Observable.from go with an interval of X time.</p>&#xA;&#xA;<pre><code>   /**&#xA; * If we want to delay the every single item emitted in the pipeline we will need a hack,&#xA; * one possible hack is use zip operator and combine every item emitted with an interval so every item emitted has to wait until interval emit the item.&#xA; */&#xA;@Test&#xA;public void delay() {&#xA;    long start = System.currentTimeMillis();&#xA;    Subscription subscription = Observable.zip(Observable.from(Arrays.asList(1, 2, 3)), Observable.interval(200, TimeUnit.MILLISECONDS), (i, t) -&gt; i)&#xA;                                          .subscribe(n -&gt; System.out.println(""time:"" + (System.currentTimeMillis() - start)));&#xA;    new TestSubscriber((Observer) subscription).awaitTerminalEvent(3000, TimeUnit.MILLISECONDS);&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>This will print</p>&#xA;&#xA;<pre><code>   time:537&#xA;   time:738&#xA;   time:936&#xA;</code></pre>&#xA;&#xA;<p>More practicle examples here <a href=""https://github.com/politrons/reactive"" rel=""nofollow"">https://github.com/politrons/reactive</a></p>&#xA;"
46425459,45847796,3092159,2017-09-26T11:36:44,"<p>The best (original?) description of the ""Sidecar""-pattern I know of is from Brendan Burns and David Oppenheimer in their publications on ""Container Patterns for Distributed Systems"". &#xA;Check out the paper + slides here:&#xA;<a href=""https://www.usenix.org/conference/hotcloud16/workshop-program/presentation/burns"" rel=""nofollow noreferrer"">https://www.usenix.org/conference/hotcloud16/workshop-program/presentation/burns</a></p>&#xA;&#xA;<p>There are other design patterns too, like ""Ambassador"" or ""Adapter"". I'm not really sure whether the istio implementation is really a sidecar in the way they describe it there, but anyway I think that's where the term originates from.</p>&#xA;"
51742089,51726683,4077247,2018-08-08T08:30:46,"<p>I've had the same question on my mind for a while now and still cannot find a good generic solutions... For what it's worth...</p>&#xA;&#xA;<p>If the communication is one way and the ""caller"" does not need to wait for a result, I find Kinesis streams very powerful - just post a ""task"" onto the stream and have the stream trigger a lambda to process it. But obviously, this works in very limited cases...</p>&#xA;&#xA;<p>For the response-reply world, I call the API Gateway endpoints just like an end user would (with the added overhead of marshaling and unmarshaling data to ""fit"" in the HTTP world, and unnecessary multiple authentications).</p>&#xA;&#xA;<p>In rare cases, I may have a single backend lambda function which gets invoked by both the Gateway API lambda and other microservices directly. This adds an extra ""hop"" for ""end users"" (instead of [UI -> Gateway API -> GatewayAPI lambda], now I have [UI -> Gateway API -> GatewayAPI lambda -> Backend lambda]), but makes microservice originated calls faster (since the call and all associated data no longer need to be ""tunneled"" through an HTTP request). Plus, this makes the architecture more complicated (I no longer have a single official API, but now have a ""back channel"" direct dependencies).</p>&#xA;"
45853867,45853546,478399,2017-08-24T05:50:10,"<p>Sprin is injection framework and itself does not have such functionality.</p>&#xA;&#xA;<p>I'd re-write your question as following: how to implement cross-machine Boolean flag? </p>&#xA;&#xA;<p>There are a lot of ways to do this. Te most straight forward ways are:&#xA;1. Via database. You can create table with single column and single line where you will store tor flag. You can even lock the table prior accessing it, so inter process communication will be guaranteed.&#xA;2. Use other tool that provides such functionality.  For example I'd recommend you Haselcast. It is a java grid that among other features has atomic variables that share state among different processes and machines. BTW they have also a spring integration. </p>&#xA;"
51853499,51726683,624831,2018-08-15T05:41:49,"<p>I'm going to <strong>assume Lambdas for the solution</strong> but they could just as well be ECS instances or ELB's.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Current problem&#xA;  <a href=""https://i.stack.imgur.com/6BebS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6BebS.png"" alt=""original problem""></a></p>&#xA;</blockquote>&#xA;&#xA;<p>One important concept to understand about lambdas before jumping into the solution is the decoupling of your <em>application code</em> and an <code>event_source</code>. </p>&#xA;&#xA;<p>An event source is a different way to invoke your <em>application code</em>. You mentioned API Gateway, that is only one method of invoking your lambda (an HTTP REQUEST). Other interesting <code>event sources</code> relevant for your solution are:</p>&#xA;&#xA;<ul>&#xA;<li>Api Gateway (As noticed, not effective for inter service communication)</li>&#xA;<li><a href=""https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html"" rel=""nofollow noreferrer"">Direct invocation</a> (via AWS Sdk, can be sync or async)</li>&#xA;<li><a href=""https://aws.amazon.com/sns/"" rel=""nofollow noreferrer"">SNS</a> (pub/sub, eventbus)</li>&#xA;<li>There are over 20+ different ways of invoking a lambda. <a href=""https://docs.aws.amazon.com/lambda/latest/dg/invoking-lambda-function.html"" rel=""nofollow noreferrer"">documentation</a></li>&#xA;</ul>&#xA;&#xA;<h2>Use case #1 Sync</h2>&#xA;&#xA;<p>So, if your HTTP_RESPONSE depends on one lambda calling another and on that 2nd lambdas result. A direct <a href=""https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html"" rel=""nofollow noreferrer"">invoke</a> might be a good enough solution to use, this way you can <code>invoke</code> the lambda in a synchronous way. It also means, that lambda should be subscribed to an API Gateway as an event source and have code to normalize the 2 different types of events. (This is why lambda documentation usually has <code>event</code> as one of the parameters)</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/GyVEp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GyVEp.png"" alt=""Sync use case""></a></p>&#xA;&#xA;<h2>Use case #2 Async</h2>&#xA;&#xA;<p>If your HTTP response doesn't depend on the other micro services (lambdas) execution. I would highly recommend SNS for this use case, as your original lambda publishes a single event and you can have more than 1 lambda subscribed to that event execute in parallel.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/JMyzq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JMyzq.png"" alt=""Async/parallel execution""></a></p>&#xA;&#xA;<h2>More complicated use cases</h2>&#xA;&#xA;<p>For more complicated use cases:</p>&#xA;&#xA;<ul>&#xA;<li>Batch processing, fan-out pattern <a href=""https://aws.amazon.com/blogs/compute/messaging-fanout-pattern-for-serverless-architectures-using-amazon-sns/"" rel=""nofollow noreferrer"">example #1</a> <a href=""https://theburningmonk.com/2018/04/how-to-do-fan-out-and-fan-in-with-aws-lambda/"" rel=""nofollow noreferrer"">example #2</a></li>&#xA;<li>Concurrent execution (one lambda calls next, calls next ...etc) AWS <a href=""https://aws.amazon.com/step-functions/?sc_channel=PS&amp;sc_campaign=acquisition_US&amp;sc_publisher=google&amp;sc_medium=ACQ-P%7CPS-GO%7CBrand%7CSU%7CApplication%20Integration%7CStep%20Functions%7CUS%7CEN%7CText&amp;sc_content=step_function_e&amp;sc_detail=aws%20step%20function&amp;sc_category=step_function&amp;sc_segment=277809106507&amp;sc_matchtype=e&amp;sc_country=US&amp;s_kwcid=AL!4422!3!277809106507!e!!g!!aws%20step%20function&amp;ef_id=V4j0KwAABRGQZnJs:20180815054032:s"" rel=""nofollow noreferrer"">Step functions</a></li>&#xA;</ul>&#xA;"
40928401,40927853,2133695,2016-12-02T09:09:05,"<p>If you're working with the Hazelcast cluster manager, you can create <a href=""http://docs.hazelcast.org/docs/3.6.3/manual/html-single/index.html#creating-cluster-groups"" rel=""nofollow noreferrer"">separate cluster groups</a>.</p>&#xA;&#xA;<p>Check out the documentation for <a href=""http://vertx.io/docs/vertx-hazelcast/java/#_configuring_this_cluster_manager"" rel=""nofollow noreferrer"">configuring the cluster manager</a>.</p>&#xA;"
45861114,45855433,8494783,2017-08-24T12:02:17,"<p>I believe that you can find the answer for your question in the official spring cloud config server documentation:&#xA;Here: <a href=""http://cloud.spring.io/spring-cloud-config/spring-cloud-config.html#_spring_cloud_config_client"" rel=""nofollow noreferrer"">http://cloud.spring.io/spring-cloud-config/spring-cloud-config.html#_spring_cloud_config_client</a></p>&#xA;&#xA;<p>Basically you have to choose between a ""Config First Bootstrap"" or ""Discovery First Bootstrap"".</p>&#xA;&#xA;<p>From the docs:&#xA;""If you are using a `DiscoveryClient implementation, such as Spring Cloud Netflix and Eureka Service Discovery or Spring Cloud Consul (Spring Cloud Zookeeper does not support this yet), then you can have the Config Server register with the Discovery Service if you want to, but in the default ""Config First"" mode, clients won’t be able to take advantage of the registration.</p>&#xA;&#xA;<p>If you prefer to use DiscoveryClient to locate the Config Server, you can do that by setting spring.cloud.config.discovery.enabled=true (default ""false""). The net result of that is that client apps all need a bootstrap.yml (or an environment variable) with the appropriate discovery configuration. (...)""</p>&#xA;"
49823487,49823345,1957567,2018-04-13T18:52:44,"<p>Just gonna give you my experience on this, not necessarily a 'complete answer':</p>&#xA;&#xA;<p>If you create a .NET Core WEB API in Visual Studio and 'Add Docker support', you can directly deploy them to regular AppServices (specify 'Linux' as the operating system when you CREATE the AppService).</p>&#xA;&#xA;<p>The Visual Studio wizard will automatically setup the container registry for your images, and configure the appservice to deploy whenever there's a new tag for your image.</p>&#xA;&#xA;<p>I've also migrated this same Web API to deploy to a Windows AppService without docker, we didn't need to change any of the code, just deploy it without docker as a regular web app. </p>&#xA;&#xA;<p>Which means if you build this as a .NET CORE 2 Web API you can deploy the same code to either a Linux Appservice using Docker or to a regular Appservice.</p>&#xA;&#xA;<p>If you're planning to use anything different than AppServices tho, you might want to check Container services or Service Fabric, which IIRC is more expensive (would make sense if you need to scale this massively)</p>&#xA;"
50940568,50930291,5313728,2018-06-20T05:06:54,"<p>Hi Yogendra,&#xA;              The problem was with pom.xml indeed, i was explicitly adding ribbon dependency in pom.xml while it's already been part of eureka client because of this i was getting this problem. after removing the ribbon client from pom.xml everything is fine.</p>&#xA;"
37075329,37074642,5670592,2016-05-06T14:46:40,"<p>You need to add the following to your application.yml file:</p>&#xA;&#xA;<pre><code>spring:&#xA;    cloud:&#xA;        config:&#xA;            enabled: true&#xA;</code></pre>&#xA;&#xA;<p>Per comment chain, you also need to add the properties to bootstrap.yml instead of application.yml . The reason is that the former is loaded before the latter in the spring startup cycle. Here is another SO post answered by user Michael Isvy explaining why, and copied below for posterity: <a href=""https://stackoverflow.com/questions/32997352/what-is-the-diference-between-putting-a-property-on-application-yml-or-bootstrap"">What is the diference between putting a property on application.yml or bootstrap.yml in spring boot?</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>I have just asked the <code>Spring Cloud</code> guys and thought I should share the info I have here.</p>&#xA;  &#xA;  <p><strong>bootstrap.yml is loaded before application.yml.</strong></p>&#xA;  &#xA;  <p>It is typically used for the following:</p>&#xA;  &#xA;  <ul>&#xA;  <li>when using Spring Cloud Config Server, you should specify <code>spring.application.name</code> and <code>spring.cloud.config.server.git.uri</code> inside <code>bootstrap.yml</code></li>&#xA;  <li>some <code>encryption/decryption</code> information</li>&#xA;  </ul>&#xA;  &#xA;  <p>Technically, <code>bootstrap.yml</code> is loaded by a parent Spring ApplicationContext. That parent ApplicationContext is loaded before the one that uses application.yml.</p>&#xA;</blockquote>&#xA;"
37203783,37180556,2394816,2016-05-13T07:26:20,"<p>It seems that v1 of Pact is all about testing providers where you can control the state and put the system in a deterministic state.</p>&#xA;&#xA;<p>v2 is aiming to work with the scenarios you mention.</p>&#xA;&#xA;<p>At this time PactNet does not support v2 of the <a href=""https://github.com/pact-foundation/pact-specification"" rel=""nofollow"">Pact specification</a>.</p>&#xA;"
51747958,51454764,3773888,2018-08-08T13:28:09,"<p>In angular projects, we have modules and components. let's imagine that we have 2 modules which are <strong>X</strong> and <strong>Y</strong>, and each of them has 2 components,<strong>X1</strong> and <strong>X2</strong> and <strong>Y1</strong> and <strong>Y2</strong>.</p>&#xA;&#xA;<p>the project structure is as shown below.</p>&#xA;&#xA;<ol>&#xA;<li>AppModule&#xA;&#xA;<ul>&#xA;<li>X ( import )</li>&#xA;<li>Y ( import )</li>&#xA;</ul></li>&#xA;<li>X&#xA;&#xA;<ul>&#xA;<li>X1 ( declare )</li>&#xA;<li>X2 ( declare )</li>&#xA;</ul></li>&#xA;<li>Y&#xA;&#xA;<ul>&#xA;<li>Y1 ( declare )</li>&#xA;<li>Y2 ( declare )</li>&#xA;</ul></li>&#xA;</ol>&#xA;&#xA;<p>Now as you have mentioned in the question, we want to use X1 component into Y2 component.&#xA;Generally, in the angular application, this situation is not a common one. we will reach to these positions rarely, but what if we get one? &#xA;Angular has mentioned that each component should use in its own module and if a component needs to be used in several components, so it does not belong to a single module,<strong>it belongs to the whole application or multiple modules</strong> and it's common.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p><br/>&#xA;<strong>Solution</strong>&#xA;We can imagine that X1 wants to be used in Y2 and X2 components. We have to create another module and named it 'ShareModule' and declare, export all common components into it, then we can simply import our ShareModule into any module which we want to use X1 into it. so our project structure should be like below.</p>&#xA;&#xA;<ol>&#xA;<li>AppModule&#xA;&#xA;<ul>&#xA;<li>X ( import )</li>&#xA;<li>Y ( import )</li>&#xA;</ul></li>&#xA;<li>X&#xA;&#xA;<ul>&#xA;<li>ShareModule ( import )</li>&#xA;<li>X2 ( declare )</li>&#xA;</ul></li>&#xA;<li>Y&#xA;&#xA;<ul>&#xA;<li>ShareModule ( import )</li>&#xA;<li>Y1 ( declare )</li>&#xA;<li>Y2 ( declare )</li>&#xA;</ul></li>&#xA;<li>ShareModule&#xA;&#xA;<ul>&#xA;<li>X1 ( Declare and Export )</li>&#xA;</ul></li>&#xA;</ol>&#xA;&#xA;<p><a href=""https://stackblitz.com/edit/angular-iejgfw?file=src/app/share.module.ts"" rel=""nofollow noreferrer"">Have a look at demo.</a></p>&#xA;"
47780640,47774291,1813696,2017-12-12T20:01:23,<p>THere are two ways to do this but both the cases UI/client call the services Asynchronously</p>&#xA;&#xA;<ol>&#xA;<li>Implement RestController which can be accessed in Synchronously but Asynchronous nature added from the browser. This is more traditional way</li>&#xA;<li>Make the Controller method reactive. UI call the service and notified when the data is available. This is new and supported by SPring 5.0</li>&#xA;</ol>&#xA;&#xA;<p>Both are good but #1 approach is old but #2 requires more handle both in UI and server. Adding asynchronous behavior will add better customer experience i.e. they can do other stuff in the UI without waiting for the data.</p>&#xA;
41944503,41942024,3192768,2017-01-30T20:06:13,<p>Depends a lot on your deployment strategy. If you going to bundle/package both the APIs into one then it's ok if both share the same entities(infact you should not duplicate entities). I would prefer having all the entities and repositories/DAO into one common bundle/package just to expose various APIs for crud operations(without any other business logic). And then my other components will consume these APIs and will have the business logic.</p>&#xA;
50892100,50891973,5235392,2018-06-16T21:58:08,"<p>in a very very abstract way, every process that run indefinitely, like a server that listen to requests is basically a while loop like</p>&#xA;&#xA;<pre><code>while flag:&#xA;  listen and process request &#xA;</code></pre>&#xA;&#xA;<p><code>shutdown</code> is just an api effecting that <code>flag</code> and making the program to exist from the loop and resume a direct flow, one that will result in ending that program </p>&#xA;&#xA;<p>so to shutdown a service that is not a spring boot, you need to create an api that can manipulate the code flow like above</p>&#xA;&#xA;<p>also, <code>spring boot</code> is a very robust and minimal wrapper, so you can incorporate spring boot to almost any service and program in java</p>&#xA;&#xA;<p>note that <code>Eureka</code> support manual overriding of services state, so you can tell Eureka to ignore and deregister one or more services regardless to their actual state</p>&#xA;&#xA;<p>with most discovery services that work in passive mode (don't actively initiate contact with the services) like <code>eureka</code> each service send an health check every x time, the discovery server will know the service is offline only after x time has passed and an health check is due (there are some other parameters like thresholds and special operation modes that taken into consideration also) </p>&#xA;&#xA;<p>I refer you to this <a href=""https://stackoverflow.com/questions/39127889/eureka-detect-service-status"">post</a> for more information about controlling the eviction rate of <code>""spring-boot-eureka""</code> (especially the last comment) </p>&#xA;"
50066509,50066231,5235392,2018-04-27T16:16:42,"<p>a service is called a micro service only if he can be as a standalone process, meaning it can communicate with other services via socket, fd, pipes etc.. (common and most easy use is socket, often as a higher protocol aka http)  </p>&#xA;&#xA;<p>if your services are meeting this criteria than each one of them should be in a different docker container, you can expose any port on any container and because docker maintain a host and dns system you can access each one via the <code>name_of_container:port</code> you should look in the <a href=""https://docs.docker.com/compose/gettingstarted/#prerequisites"" rel=""nofollow noreferrer"">docker compose</a> docs for more info</p>&#xA;"
35391820,35314963,5902017,2016-02-14T12:35:46,"<p>According to: <a href=""https://facebook.github.io/flux/docs/overview.html#content"" rel=""nofollow"">https://facebook.github.io/flux/docs/overview.html#content</a> </p>&#xA;&#xA;<blockquote>&#xA;  <p>Occasionally we may need to add additional controller-views deeper in the &#xA;  hierarchy to keep components simple. This might help us to better encapsulate a &#xA;  section of the hierarchy related to a specific data domain.</p>&#xA;</blockquote>&#xA;&#xA;<p>And this is what I am thinking about responsibility of particulary component's domain (three of them was described). So could it be reliable to make three controller views (or stores) that can access dependent API to manage resource's data?</p>&#xA;"
47570822,47552159,5381825,2017-11-30T10:10:42,"<p>the solution is to implement ErrorDecoder interface and override</p>&#xA;&#xA;<pre><code>@Override&#xA;Exception decode(String methodKey, Response response) {&#xA;    // response has getBody() method to get all data from a request &#xA;}&#xA;</code></pre>&#xA;"
44697966,44697767,8012737,2017-06-22T11:22:54,<p>You should add a reference to the <code>IService</code> interface in your interface. So change your <code>interface IPushMessageService</code> to <code>interface IPushMessageService : IService</code>.</p>&#xA;&#xA;<p>Side node 1: You should probably make your interface public.</p>&#xA;&#xA;<p>Side node 2: It is better to put your interface in a separate project to prevent any circular dependencies when working with remoting.</p>&#xA;
43974835,43636544,8012737,2017-05-15T08:37:55,"<p>An endpoint would be useless without a port. So even if you could have one, you shouldn't want it. You are probably looking for a way to call the service without knowing its port number. This can be achieved by using a <a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-reverseproxy"" rel=""nofollow noreferrer"">reverse proxy</a>. With a reverse proxy you can call a service by providing the port of the reverse proxy.</p>&#xA;"
44405759,44404941,8012737,2017-06-07T07:04:05,"<p>In your <code>CreateServiceInstanceListeners</code> method you are returning an array of listeners. This means that it is possible to create multiple listeners. Just add it like you would with any other array:</p>&#xA;&#xA;<pre><code>protected override IEnumerable&lt;ServiceInstanceListener&gt; CreateServiceInstanceListeners()&#xA;{&#xA;    return new ServiceInstanceListener[]&#xA;    {&#xA;        new ServiceInstanceListener(serviceContext =&gt; this.CreateServiceRemotingListener(serviceContext), ""RemotingListener""),&#xA;        new ServiceInstanceListener(serviceContext =&gt; new OwinCommunicationListener(Startup.ConfigureApp, serviceContext, ServiceEventSource.Current, ""ServiceEndpoint""))&#xA;    };&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Note that even though the listener name is an optional parameter, you <strong>have to</strong> give your listeners a name. I've also experienced some problems with the service proxy trying to connect to the other endpoint. In order to solve this declare the remoting listener first and your other listeners second.</p>&#xA;"
50292946,50281965,1794497,2018-05-11T13:02:10,"<p>Usually, the way an API Gateway communicates with the internal microservices is more of a implementation detail. Nothing prevents you to use a message broker to realize that.</p>&#xA;&#xA;<p>The main drawback of de-coupling the API Gateway with the actual microservices is that it complicates a bit the synchronous handling of API calls. What you usually end up implementing is some sort of long polling mechanism.</p>&#xA;&#xA;<p>Therefore, it just depends on your use case. If I am dealing with asynchronous APIs then I usually rely on message brokers for de-coupling the front-end with the back-end. </p>&#xA;"
43477602,43476908,1599681,2017-04-18T16:31:05,"<p>Take a look on Docker.</p>&#xA;&#xA;<p>With <code>docker-compose</code> you can play with several services with an easy integration without worrying about the IP addresses to connect them.</p>&#xA;&#xA;<p>Also if you add <code>nginx</code> to your stack, it's gonna be very easy to scale those services, there are several videos and tutorials that you can lookup to help you get started.</p>&#xA;&#xA;<p>I've heard about<code>seneca</code>, but I haven't used, I think you shouldn't depend on a specific framework because one of the ideas behind of Microservices is the low coupling.</p>&#xA;"
50365277,50361821,4816065,2018-05-16T07:56:37,"<p>Kubernetes is a container management tool whereas Spring Cloud is a collection of tools to build microservices architectures. There are overlap, like service discovery, gateway or configuration services. </p>&#xA;&#xA;<p>You can use Spring Cloud to build your microservices architecture by deploying the jars yourself. You don't need kubernetes as such for this.</p>&#xA;&#xA;<p>Difference :&#xA;<a href=""https://developers.redhat.com/blog/2016/12/09/spring-cloud-for-microservices-compared-to-kubernetes/"" rel=""nofollow noreferrer"">https://developers.redhat.com/blog/2016/12/09/spring-cloud-for-microservices-compared-to-kubernetes/</a></p>&#xA;"
39312634,39305118,6720449,2016-09-04T00:53:20,"<h2>Off the Shelf Suggestions</h2>&#xA;&#xA;<p>Rather than rolling your own, I recommend looking into off the shelf products, as there is a lot of complexity here that will not be apparent out the outset, e.g.</p>&#xA;&#xA;<ul>&#xA;<li>Managing event subscriber list - an SQS queue is more appropriately paired with an event consumer, rather than with an event producer as when a message is consumed it is no longer available on the queue - so if you want to support multiple subscribers for a given event (which is a massive benefit of event driven architectures), how do you know which SQS queues you push the event message onto when it is first raised?</li>&#xA;<li>Retry semantics, error forwarding queues - handling temporary errors due to ephemeral infrastructure issues vs permanent errors due to business logic semantic issues</li>&#xA;<li>Audit trails of which messages were raised when and sent where</li>&#xA;<li>Security of messages sent via SQS (does your business case require them to be encrypted? SQS is an application service offered by Amazon which doesn't provide storage level encryption</li>&#xA;<li>Size of messages - SQS has a message size limit so you may eventually need to handle out-of-band transmission of large messages</li>&#xA;</ul>&#xA;&#xA;<p>And that's just off the top of my head...</p>&#xA;&#xA;<p>A few off the shelf systems that would assist:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://particular.net/nservicebus"" rel=""nofollow"">NServiceBus</a> provides a framework for managing command and event messaging, and it has a plugin framework permitting flexible transport types - <a href=""https://github.com/ahofman/NServiceBus.AmazonSQS"" rel=""nofollow"">NServiceBus.SQS</a> offers SQS as a transport.&#xA;&#xA;<ul>&#xA;<li>Offers comprehensive and flexible retry, audit and error handling</li>&#xA;<li>Opinionated use of commands vs events (command messages say ""Do this"" and are sent to a single service for processing, event messages say ""Something happened"" and are sent to an arbitrary number of flexible subscribers)</li>&#xA;<li>Outbox pattern provides transactionally consistent messaging even with non-transactionally consistent transports, such as SQS</li>&#xA;<li>Currently the SQS plugin uses default NServiceBus subscriber persistence, which requires an SQL Server for storing the event subscriber list (see below for an option that leverages SNS)</li>&#xA;<li>Built in support for sagas, offering a framework to ensure multi transaction eventual consistency with rollback via compensating actions</li>&#xA;<li>Timeouts supporting scheduled message handling</li>&#xA;<li>Commercial offering, so not free, but many plugins/extensions are open source</li>&#xA;</ul></li>&#xA;<li><a href=""http://masstransit-project.com/"" rel=""nofollow"">Mass Transit</a>&#xA;&#xA;<ul>&#xA;<li>Doesn't support SQS off the shelf, but does support Azure Service Bus and RabbitMq, so could be an alternative for you if that is an option</li>&#xA;<li>Similar offering to NServiceBus, but not 100% the same - <a href=""http://looselycoupledlabs.com/2014/11/masstransit-versus-nservicebus-fight/"" rel=""nofollow"">NServiceBus vs MassTransit</a> offers a comprehensive comparison</li>&#xA;<li>Fully open source/free</li>&#xA;</ul></li>&#xA;<li><a href=""https://github.com/justeat/JustSaying"" rel=""nofollow"">Just Saying</a>&#xA;&#xA;<ul>&#xA;<li>A light-weight open source messaging framework designed specifically for SQS/SNS based</li>&#xA;<li>SNS topic per event, SQS queue per microservice, use native SNS SQS Queue subcription to achieve fanout</li>&#xA;<li>Open Source Free</li>&#xA;</ul></li>&#xA;</ul>&#xA;&#xA;<p>There may be others, and I've most personal experience with NServiceBus, but I strongly recommend looking into the off the shelf solutions - they will free you up to start designing your system in terms of business events, rather than worrying about the mechanics of event transmission.</p>&#xA;&#xA;<p>Even if you do want to build your own as a learning exercise, reviewing how the above work may give you some tips on what's needed for reliable event driven messaging.</p>&#xA;&#xA;<h2>Transactional Consistency and the Outbox Pattern</h2>&#xA;&#xA;<p>The question has been edited to ask about the what happens if parts of the operation succeed, but the publish operation fails.  I've seen this referred to as the transactional consistency of the messaging, and it generally means that within a transaction, all business side-effects are committed, or none.  Business side effects may mean:</p>&#xA;&#xA;<ul>&#xA;<li>Database record updated</li>&#xA;<li>Another database record deleted</li>&#xA;<li>Message published to a message queue</li>&#xA;<li>Email sent</li>&#xA;</ul>&#xA;&#xA;<p>You generally don't want an email sent or a message published, if the database operation failed, and likewise, you don't want the database operation committed if the message publish failed.</p>&#xA;&#xA;<p><strong>So how to ensure consistency of messaging?</strong></p>&#xA;&#xA;<p>NServiceBus handles this in one of two ways:</p>&#xA;&#xA;<ol>&#xA;<li>Use a transactionally consistent message transport, such as MSMQ.&#xA;&#xA;<ol>&#xA;<li>MSMQ is able to make use of <a href=""https://blogs.msdn.microsoft.com/florinlazar/2004/03/04/what-is-msdtc-and-why-do-i-need-to-care-about-it/"" rel=""nofollow"">Microsoft's DTC (Distributed Transaction Coordinator)</a> and DTC can enroll the publishing of messages in a distributed transaction with SQL server updates - this means that if your business transaction fails, your publish operation will be rolled back and visa versa</li>&#xA;</ol></li>&#xA;<li>The <a href=""http://docs.particular.net/nservicebus/outbox/"" rel=""nofollow"">Outbox Pattern</a>&#xA;&#xA;<ol>&#xA;<li>With the outbox pattern, messages are not dispatched immediately - they are added to an Outbox table in a database, ideally the same database as your business data, as part of the same transaction</li>&#xA;<li>AFTER the transaction is committed, it attempts to dispatch each message, and only removes it from the outbox on successful dispatch</li>&#xA;<li>In the event of a failure of the system after dispatch but before delete, the message will be transmitted a second time.  To compensate for this, when Outbox is enabled, NServiceBus will also do de-duplication of inbound messages, by maintaining a record of all inbound messages and discarding duplicates.</li>&#xA;<li>De-duplication is especially useful with Amazon SQS, as it is itself eventually consistent, and the same messages may be received twice.</li>&#xA;<li>This is the not far from the original concept in your question, but there are differences:&#xA;&#xA;<ol>&#xA;<li>You were concepting a background timed process to scan the events table (aka Outbox table) and publish events to SQS</li>&#xA;<li>NServiceBus executes handlers within a <a href=""http://docs.particular.net/nservicebus/pipeline/"" rel=""nofollow"">pipeline</a> - with Outbox, the dispatch of messages to the transport (aka pushing messages into an SQS queue) is simply one of the last steps in the pipeline.  So - whenever a message is handled, any outbound messages generated during the handling will be dispatched immediately after the business transaction is committed - no need for a timed scan of the events table.</li>&#xA;</ol></li>&#xA;<li>Note: Outbox is only successful when there is an ambient NServiceBus Handler transaction - i.e. when you are handling a message within the NServiceBus pipeline.  This will NOT be the case in some contexts, e.g. a WebAPI Request pipeline.  For this reason, <a href=""http://docs.particular.net/nservicebus/hosting/publishing-from-web-applications"" rel=""nofollow"">NServiceBus recommends using your API request to send a single Command message only</a>, and then combining business data operations with further messaging within a transactionally consistent command handler in a backend endpoint service. Although point 3 in their doc is more relevant to the MSMQ than SQS transport.</li>&#xA;</ol></li>&#xA;</ol>&#xA;&#xA;<h2>Handler Semantics</h2>&#xA;&#xA;<p>One more comment about your proposal - by convention, <code>UserChangedEmailHandler</code> would more commonly be associated with the service that does something in response to the email being changed, rather than simply participating in the propagation of the information that the email has changed.  When you have 50 events being published by your system, do you want 50 different handlers just to push those messages onto different queues?</p>&#xA;&#xA;<p>The systems above use a generic framework to propagate messages via the transport, so you can reserve <code>UserChangedEmailHandler</code> for the subscribing system and include in it the business logic that should happen whenever a user changes their email.</p>&#xA;"
45770673,45688730,6720449,2017-08-19T10:54:41,"<p>The commenters are right - there are some subjective definitions at play here.  But there are some principles and concepts that can help reason about the different approaches.</p>&#xA;&#xA;<h2>Conway's Law</h2>&#xA;&#xA;<p>It's not strictly the original definition, but I think the distinction can be better understood with reference to <a href=""http://www.melconway.com/Home/Conways_Law.html"" rel=""noreferrer"">Conway's Law</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization's communication structure.</p>&#xA;</blockquote>&#xA;&#xA;<h2>The Inverse Conway Manouvre</h2>&#xA;&#xA;<p>Following from that thinking, the <a href=""https://www.thoughtworks.com/radar/techniques/inverse-conway-maneuver"" rel=""noreferrer"">Inverse Conway Maneuver</a> evolved:</p>&#xA;&#xA;<blockquote>&#xA;  <p>The 'Inverse Conway Maneuver' recommends evolving your team and organizational structure to promote your desired architecture. Ideally your technology architecture will display isomorphism with your business architecture.</p>&#xA;</blockquote>&#xA;&#xA;<p>The Inverse Conway Maneuver is an attempt to structure your organisation to harness Conway's Law to achieve a better system design.</p>&#xA;&#xA;<h2>Decomposition By Business Capability</h2>&#xA;&#xA;<p>With an understanding of these concepts, we can consider decomposition by Business Capability to be guiding the system design according to the way the business is structured. This echos Conway's law.</p>&#xA;&#xA;<p>The pro of this approach is it helps to ensure alignment between development teams and business structural units.  The con is it may bake business inefficiencies that arose before an automated system was considered, into the design of your system.</p>&#xA;&#xA;<h2>Decomposition By Domain</h2>&#xA;&#xA;<p>Domain Driven Design (DDD) provides a suite of tools and methodologies to reason about the underlying domain at hand, to reflect the best available understanding of the domain in the software design and to evolve the software design as understanding of the domain grows and changes. DDD Strategic Patterns guide the creation of a <a href=""https://www.infoq.com/articles/ddd-contextmapping"" rel=""noreferrer"">Context Map</a> which can form the foundation of your microservices decomposition.</p>&#xA;&#xA;<p>From this, we can consider decomposition by Domain to be guiding the system design according to an analysis of the processes and information flows .</p>&#xA;&#xA;<p>The pro of this approach is that it can lead to a system design that closely models the reality of what is happening (or needs to happen).  Hopefully the business structure already aligns with this - but where it doesn't, it can reveal inefficiencies in the existing business organisational structure.</p>&#xA;&#xA;<p>If you have influence over the organisational structure, this can be a foundation for utilising the Inverse Conway Maneuver and to allow you to evolve the software, the dev teams and the business units to achieve alignment.</p>&#xA;&#xA;<p>If you don't, you may end up introducing friction points where the system design becomes misaligned with the business capabilities.</p>&#xA;&#xA;<h2>Conclusion</h2>&#xA;&#xA;<p>The reality is, neither approach is mutually exclusive - you will probably end up with a compromise that attempts to balance alignment with business capabilities as they are already understood and problem domains as they are revealed through a DDD process.</p>&#xA;"
41153429,41123857,6720449,2016-12-14T22:35:50,"<p>There are definitely a few options here:</p>&#xA;&#xA;<h2>AWS Native Way</h2>&#xA;&#xA;<p>Not sure why you consider Topics far from ideal?</p>&#xA;&#xA;<p>SNS topics are exactly designed for eventing:</p>&#xA;&#xA;<ol>&#xA;<li>Create one SNS topic per event type</li>&#xA;<li>Create one SQS queue per consumer</li>&#xA;<li>Subscribe the SQS queue to the SNS topics (events) that the consumer is interested in subscribing to</li>&#xA;</ol>&#xA;&#xA;<p>And that's about it.</p>&#xA;&#xA;<p>Pros:</p>&#xA;&#xA;<ul>&#xA;<li>Subscriptions are managed outside of either the publisher or consumer - very loosely coupled</li>&#xA;<li>Cons - you can hook up other subscribers (e.g. email or api invocations or lambda functions) to trigger specific actions for specific events - quite extensible</li>&#xA;</ul>&#xA;&#xA;<p>Cons:</p>&#xA;&#xA;<ul>&#xA;<li>No native way to replay event history - e.g. for a new consumer to obtain info about events that occurred before it subscribed</li>&#xA;</ul>&#xA;&#xA;<h2>Publisher Makes Event History available for Consumers to Poll</h2>&#xA;&#xA;<p>This is not AWS specific, but another approach is for each publisher to expose an API that permits consumers to poll for new events.  Usually an event store database stores the events, and the API permits both caching and 'retrieve new events since X' semantics.</p>&#xA;&#xA;<p>The <a href=""https://en.wikipedia.org/wiki/Atom_(standard)"" rel=""nofollow noreferrer"">Atom</a> protocol is frequently used to publish events in this way.</p>&#xA;&#xA;<p>Pros:</p>&#xA;&#xA;<ul>&#xA;<li>Full event history is available for replaying events and 'catching up' new consumers</li>&#xA;<li>Infrastructure agnostic</li>&#xA;</ul>&#xA;&#xA;<p>Cons:</p>&#xA;&#xA;<ul>&#xA;<li>Have to run/maintain an event store and event publishing mechanism</li>&#xA;</ul>&#xA;&#xA;<h2>Service Bus Approach</h2>&#xA;&#xA;<p>I'm not as familiar with other languages, but in the .NET world, there are two big projects that offer a service bus semantic - <a href=""https://particular.net/nservicebus"" rel=""nofollow noreferrer"">NServiceBus</a> and <a href=""http://masstransit-project.com/"" rel=""nofollow noreferrer"">Mass Transit</a></p>&#xA;&#xA;<p>Both offer a range of 'transports' for the event communication.  NServiceBus has an open source AWS SQS plugin to use SQS queues as the transit, but at time of writing the subscriptions are maintained by NServiceBus in local persistence per publisher (who then forwards each event when it is published to each subscriber in the locally persisted list of subscriptions).</p>&#xA;&#xA;<h2>Concerns?</h2>&#xA;&#xA;<p>If you can clarify your concerns about SNS Topics, I'd be happy to expand the answer.</p>&#xA;"
33336404,33335786,980929,2015-10-25T23:43:26,"<p>If you don't need to persist your data, you could also take a look at <a href=""http://redis.io/"" rel=""nofollow"" title=""Redis"">Redis</a> and its pubsub features. It is mature, really simple to configure and use, great documentation and a big community.</p>&#xA;&#xA;<p>Here's a list of available client libraries (5 Erlang libs for example)&#xA;<a href=""http://redis.io/clients"" rel=""nofollow"">http://redis.io/clients</a> </p>&#xA;"
40309948,40304324,1309377,2016-10-28T17:06:50,"<p>By main service I assume that application is the one that needs to talk to the other 4. </p>&#xA;&#xA;<p>Conceptually, what you will do in this case is have acmeair-as, acmeair-bs, acmeair-cs, acmeair-fs register with a unique name and url with service discovery.</p>&#xA;&#xA;<p>acmeair-mainapp will then request the name of the microservice it needs to talk to and service discovery will return to you the URL of that microservice.</p>&#xA;&#xA;<p>You can use this Java code I have as an <a href=""https://github.com/alohr51/Microservices-Orders/blob/master/src/com/ibm/ws/msdemo/service/disovery/ServiceDiscovery.java"" rel=""nofollow"">example</a> to register a Java microservice with service discovery (which is what your 4 microservice apps should do) It additionally does the ""heartbeat"" needed to keep your application alive in SD's eyes.</p>&#xA;&#xA;<p>You can also view the <a href=""https://console.ng.bluemix.net/apidocs/129?&amp;language=java"" rel=""nofollow"">Service Discovery API Docs</a>. They provide Java samples for their requests there. These Docs can help you craft a request for your main application to use to get the URLs it needs from the other 4 microservices</p>&#xA;"
44050081,43144337,3347394,2017-05-18T14:07:21,"<p>In the API Designer you can edit the Base Path and the specific API Paths:</p>&#xA;&#xA;<p><strong>Base Path:</strong>&#xA;<a href=""https://i.stack.imgur.com/GMDxR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GMDxR.png"" alt=""Edit Base Path""></a></p>&#xA;&#xA;<p><strong>API Paths</strong>&#xA;<a href=""https://i.stack.imgur.com/iqtpi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iqtpi.png"" alt=""Edit API Path""></a></p>&#xA;&#xA;<p>To get to these Design settings complete the following steps:</p>&#xA;&#xA;<ol>&#xA;<li>In your API Connect service, click the <strong>Drafts</strong> section in the Navigation pane and then click the <strong>APIs</strong> tab:</li>&#xA;</ol>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/WrDmt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WrDmt.png"" alt=""API Manager""></a></p>&#xA;&#xA;<ol start=""2"">&#xA;<li>Click the corresponding API you want to edit and the API Designer will be displayed. Here you can edit the Base Path and API Paths as seen in the sections below:</li>&#xA;</ol>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/ir4FY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ir4FY.png"" alt=""API Designer""></a></p>&#xA;&#xA;<p>For more information, please see the following documentation &#xA;<a href=""https://www.ibm.com/support/knowledgecenter/en/SSMNED_5.0.0/com.ibm.apic.toolkit.doc/task_APIonPrem_defineapiresources.html"" rel=""nofollow noreferrer"">Defining Paths for a REST API</a>.</p>&#xA;"
38372151,38328727,209602,2016-07-14T10:46:09,"<p>You can use task queues. Take a look at following article. Although it is for python, concepts can be adopted for other languages. </p>&#xA;&#xA;<p><a href=""https://www.fullstackpython.com/task-queues.html"" rel=""nofollow"">https://www.fullstackpython.com/task-queues.html</a></p>&#xA;"
48415574,48370641,1987266,2018-01-24T05:46:01,"<p>For your problem , first lets understand the ways in which one service interacts with each other , lets create two service order service and customer service :</p>&#xA;&#xA;<ol>&#xA;<li>One service need some data from other service to process its requests, eg: lets say you want to place an order , so from ui you must hit http request to order service (which can be rest , or have a api gateway in between n that use some other protocol like protobu ) to call order service for placing an order , now assume order service needs to check the validity of the customer , so order service needs to call customer service in sync — most probably you will hit rest or create a protobuf or have a persistent websocket between the service  and then after response from customer service , order service move ahead.</li>&#xA;</ol>&#xA;&#xA;<p>In this case sync communication needs to be imitated , one direct approach is rest or protobuff , or imitate it through messaging</p>&#xA;&#xA;<ol start=""2"">&#xA;<li>Based on one service events , you want to update other service : in this generally the preferred style is messaging bus , where one service emits out event and multiple other services have a listener on the messaging bus and react accordingly . Eg : lets say on update of customer name in customer service , you want to update the cached name of the customer in order service , in this when name is updated in customer service , it emits customer name updated event , order service subscribe to it , then react to it</li>&#xA;</ol>&#xA;&#xA;<p>Your third question is , is it possible to have service with no rest endpoint ::: yes its possible , but every service needs to be reachable . So use rest or other form is needed</p>&#xA;&#xA;<p>Above mentioned link : <a href=""http://microservices.io"" rel=""nofollow noreferrer"">microservices.io</a> is very good , go through it once again </p>&#xA;"
48443868,48441868,1987266,2018-01-25T13:21:40,"<p>You should generally consider almost zero sync communication between microservices(if still you want sync comminucation try considering circuit breakers which allow your service to be able to respond but with logical error message , if no circuit breaking used dependent services will also go down completly).This could be achieved by questioning the consistency requirement of the micorservice.</p>&#xA;&#xA;<p>Sometimes these things are not directly visible , for eg: lets say there are two services order service and customer service and order service expose a api which say place a order for customer id. and business say you cannot place a order for a unknown customer</p>&#xA;&#xA;<p>one implementation is from the order service you call the customer service in sync ---- in this case customer service down will impact your service, now lets question do we really need this.</p>&#xA;&#xA;<p>Because a scenario could happen where customer just placed an order and somebody deleted that customer from customer service, now we have a order which dosen't belong to customer.Consistency cannot be guaranteed. </p>&#xA;&#xA;<p>In the new sol. we are saying allow the order service to place the order without checking the customer id and do one of the following:</p>&#xA;&#xA;<ol>&#xA;<li>Using ProcessManager check the customer validity and update the status of the order as invalid and when customer get deleted using ProcessManager update the order status as invalid or perform business logic</li>&#xA;<li>Do not check at all , because placing a order dosen't count a thing, when this order will be in the process of  dispatch that service will anyway check the customer status</li>&#xA;</ol>&#xA;&#xA;<p>Statement here is try to achieve more async communication between microservices , mostly you will be able find the sol. in the consistency required by the business. But in case your business wants to check it 100% you have to call other service and if other service is down , your service will give logical errors.</p>&#xA;"
48495582,48493849,1987266,2018-01-29T06:32:59,"<p>We prefer them to be in the same service as they logically are doing work on same objects.</p>&#xA;&#xA;<p>This also highly depends upon how you write your business logic. Like we prefer here to write our business logic in Aggregates(Domain Driven Design) and that's why writing consumer in the same service makes sense. </p>&#xA;&#xA;<p>In some case where they are just updating data for searching kind of things , you may write them in separate service.</p>&#xA;&#xA;<p>You can also look at <a href=""https://www.lagomframework.com/"" rel=""nofollow noreferrer"">Lagom</a> (microservices framework for java)</p>&#xA;"
48515814,42691892,1987266,2018-01-30T07:11:51,"<p>Microservices are logical block for any application , combining them at sql level dosen't make any sense.</p>&#xA;&#xA;<p>For eg: let's consider you create an order service , which allow customer to place order.</p>&#xA;&#xA;<p>Now a order contain order items as well and may have a reference of customer object , for all these you might end up creating multiple tables. So don't just think sql table and microservices together</p>&#xA;&#xA;<p>If you still have doubts post a more exact question , will help :)</p>&#xA;"
48506986,48505946,1987266,2018-01-29T17:38:41,"<p>Let's break your problem and see if this is good idea or not:</p>&#xA;&#xA;<ol>&#xA;<li><p>Having major components as diff. services and on diff. address --- good idea, this is what microservices say</p></li>&#xA;<li><p>Single Database Cluster: -- This is not a preferred approach, as diff. services generally require diff. type of database need one may require cassandra, one may elastic search and one may be postgres. If in your case if all services are ok with same database type, still make sure that data tables of each service are completely separate and you do not  take joins   , only way to access other service data is through service call</p></li>&#xA;</ol>&#xA;"
48472324,48460986,1987266,2018-01-27T03:35:17,"<p>your identification of the problem is correct.</p>&#xA;&#xA;<p>But the solution to your problem will depend on use case to use case.</p>&#xA;&#xA;<p>In your example of search service , product service and customer service should publish their events on kafka or similar messaging and search service listen to them and updates it.</p>&#xA;&#xA;<p>In case of lets say in order service while creating an order for a customer , you want to check customer exists , then you might do it by calling the sync api of customer service , but for that also there are variour other approaches , i have answered here <a href=""https://stackoverflow.com/questions/48441868/linking-microservices-and-allowing-for-one-to-be-unavailable"">linking Microservices and allowing for one to be unavailable</a></p>&#xA;&#xA;<p>From my perspective sync communication between services should be avoided , and there are way around for this , above link would help</p>&#xA;&#xA;<p>You can use domain driven design philosophy to correctly break your services and their contract</p>&#xA;"
49250000,49226141,1987266,2018-03-13T06:56:48,"<p>i think even in this case, what i would have done is create a consumer of OrderPlaced event in Order Microservice Only. That event processor will read all the details from order create a MailToBeSent event and write it on a Topic or Queue , which CommunicationService should listen and send the email.</p>&#xA;&#xA;<p>Communication Service should not understand , how to create a email based on order(as core purpose of cummunication service is to send emails). </p>&#xA;&#xA;<p>Design wise also communication service should not require to change every time you add a new service which want a mail sending functionality.</p>&#xA;"
49314108,49284804,1987266,2018-03-16T05:58:40,<p>First of all each Microservice should have its own database.</p>&#xA;&#xA;<p>Secondly it's not necessary and also not recommended to have the Microservice and its database on the same container.</p>&#xA;&#xA;<p>Generally a single Microservice will have multiple deployments for scaling and they all connect to a single Database instance which should be a diff. container and if using things like NoSql DB's its a database cluster. </p>&#xA;
49408475,49349235,1987266,2018-03-21T14:04:08,"<p>If you would like to code the workflow:</p>&#xA;&#xA;<p>Micorservice A which accepts the Job and command for update the job&#xA;Micorservice B which provide read model for the Job</p>&#xA;&#xA;<p>Based on JobCreatedEvents use some messaging queue and process and update the job through queue pipelines and keep updating JobStatus through every node in pipeline.</p>&#xA;&#xA;<p>I am assuming you know things about queues and consumers.</p>&#xA;&#xA;<p>Myself new to Camunda(workflow engine), that might be used not completely sure</p>&#xA;"
49450233,45400096,1987266,2018-03-23T12:58:36,"<p>In one way you are correct, in Microservices from outside it looks like this. When you go inside as you rightly mention about two complex concern :</p>&#xA;&#xA;<p>Authentication:- For each module I need to ensure it authenticates the request which is not the case right now</p>&#xA;&#xA;<p>Transaction:- I can not maintain the transaction atomicity across different services which I could do very easily at present</p>&#xA;&#xA;<p>Apart from this there are various things which one need to understand otherwise doing and deploying microservices would be very tough:</p>&#xA;&#xA;<p>I am mentioning some of them here, complete list you can see from my post: </p>&#xA;&#xA;<ul>&#xA;<li>What exactly is a microservice? Some said it should not exceed 1,000 lines of code.Some say it should fit one bounded context (if you don't know what a bounded context is, don't bother with it right now; keep reading).</li>&#xA;<li>Even before deciding on what the ""micro""service will be, what exactly is a service?</li>&#xA;<li>Microservices do not allow updating multiple entities at once; how will I maintain consistency between entities? </li>&#xA;<li>Should I have a single database cluster for all my microservices?</li>&#xA;<li>What is this eventual consistency thing everyone is talking about?</li>&#xA;<li>How will I collate data which is composed of multiple entities residing in different services?</li>&#xA;<li>What would happen if one service goes down? How would the dependent services behave?</li>&#xA;<li>Should I make a sync invocation between microservices to always get consistent data?</li>&#xA;<li>How will I manage version upgrades to a few or all microservices? Is it always possible to do it without downtime?</li>&#xA;<li>And the last unavoidable question - how do I test the entire application as an integrated application?&#xA;&#xA;<ul>&#xA;<li>How to do circuit breaking? (if one service down should not impact other)</li>&#xA;<li>CI/CD pipelines and .......</li>&#xA;</ul></li>&#xA;</ul>&#xA;&#xA;<p>What we understood while starting our journey in microservices are: </p>&#xA;&#xA;<ol>&#xA;<li>Design pattern for breaking business problem in microservices is Domain Driven Design</li>&#xA;<li>Platform which support microservices development. (we used Lagom for this) which address some of the above concern out of the box</li>&#xA;</ol>&#xA;&#xA;<p>So in all while moving towards multiple process arch. communicating using Rest or some other methods, new considerations needs to be taken care which are not directly visible in Monolithic, and people want to whether you know about those considerations or not.</p>&#xA;"
49270517,49252691,1987266,2018-03-14T05:48:32,"<p>First of all the option that you choose will depend on your business needs.</p>&#xA;&#xA;<ol>&#xA;<li>Ignore Completly: in your Bank and Account case i would not like to use this approach as this might result in a lost account. This type of approach you could take in cases where the business flow would ensure that sonner or later it would get resolved.</li>&#xA;<li>Validate Always: I would not use this as such as this would make service dependent. What i would do i use a variation of this:</li>&#xA;</ol>&#xA;&#xA;<blockquote>&#xA;  <p>** First i would allow account to be created without the validation, the initial state of the account is CREATED</p>&#xA;  &#xA;  <p>** Now in AccountService create a ProcessManager which listen to AccountCreated Event and in async validates it with Bank Service to&#xA;  check whether Bank Id is valid, if yes , update the account state to&#xA;  VERIFIED , or if invalid bank id , update the state to INVALID BANK ID&#xA;  and take the appropriate action</p>&#xA;</blockquote>&#xA;&#xA;<ol start=""3"">&#xA;<li>Keep Read Only Copy: This i will not also do, because first need to duplicate lot of data, second the copy can be stale. Eg: the Bank Deleted but not known to this service or Bank Just created and not known to this service, in both the cases extra checks and async validation are required.</li>&#xA;</ol>&#xA;&#xA;<p>Now actually all approach are valid, might depends on your critical business needs.Sometimes real time validation is required even if it creates a dependency</p>&#xA;"
49044245,49036468,1987266,2018-03-01T07:00:54,"<p>You should query the device service only.</p>&#xA;&#xA;<p>And treat the user id like a filter in the device service. For eg: you should search on userid similar to how you would search device based on device type. Just another filter</p>&#xA;&#xA;<p>Eg : /devices?userid= </p>&#xA;&#xA;<p>Also you could cache some basic information of user in device service, to save round trips on getting user data</p>&#xA;"
49104413,37213471,1987266,2018-03-05T06:06:23,"<p>We also started our journey some time back and i started writing a blog series for exactly the same thing: <a href=""https://dzone.com/articles/how-i-started-my-journey-in-micro-services-and-how"" rel=""nofollow noreferrer"">https://dzone.com/articles/how-i-started-my-journey-in-micro-services-and-how</a></p>&#xA;&#xA;<p>Basically what i understood is to break my problem in diff. microservices, i need a design framework which Domain Driven Design gives(Domain Driven Design Distilled Book by Vaugh Vernon).</p>&#xA;&#xA;<p>Then to implement the design (using CQRS and Event Sourcing and ...) i need a framework which provides all the above support.</p>&#xA;&#xA;<p>I found Lagom good for this.(Eventuate , Spring Microservices are some other choices).</p>&#xA;&#xA;<p>Sample Microservices Domain analysis using Domain Driven Design by Microsoft: <a href=""https://docs.microsoft.com/en-us/azure/architecture/microservices/domain-analysis"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/architecture/microservices/domain-analysis</a></p>&#xA;&#xA;<p>One more analysis is: <a href=""http://cqrs.nu/tutorial/cs/01-design"" rel=""nofollow noreferrer"">http://cqrs.nu/tutorial/cs/01-design</a></p>&#xA;&#xA;<p>After reading on Domain Driven Design i think lagom and above links will help you to build a end to end application. If still any doubts , please raise :)</p>&#xA;"
49166730,48921774,1987266,2018-03-08T06:27:15,"<p>First of all the above answer is correct in suggesting that you need to breaup your microservice in a better way.</p>&#xA;&#xA;<p>Now If scalability is your concern(lots of api calls between microservice).</p>&#xA;&#xA;<p>I strongly suggest you to validate that how many of the constraints are really required at the first level, and how many of them you could do in async way. With that what i mean is in distributed enviornment we actually do not need to validate all the things at the same time.</p>&#xA;&#xA;<p>Sometimes these things are not directly visible , for eg: lets say there are two services order service and customer service and order service expose a api which say place a order for customer id. and business say you cannot place a order for a unknown customer</p>&#xA;&#xA;<p>one implementation is from the order service you call the customer service in sync ---- in this case customer service down will impact your service, now lets question do we really need this.</p>&#xA;&#xA;<p>Because a scenario could happen where customer just placed an order and somebody deleted that customer from customer service, now we have a order which dosen't belong to customer.Consistency cannot be guaranteed.</p>&#xA;&#xA;<p>In the new sol. we are saying allow the order service to place the order without checking the customer id and do one of the following:</p>&#xA;&#xA;<p>Using ProcessManager check the customer validity and update the status of the order as invalid and when customer get deleted using ProcessManager update the order status as invalid or perform business logic&#xA;Do not check at all , because placing a order dosen't count a thing, when this order will be in the process of dispatch that service will anyway check the customer status</p>&#xA;&#xA;<p>In this way your API hits are reduced and better independent services are produced</p>&#xA;"
49147286,49113488,1987266,2018-03-07T08:31:12,"<blockquote>&#xA;  <p>Can I continue with my second approach - Horizontal partitioning of&#xA;  databases according to domain object?</p>&#xA;</blockquote>&#xA;&#xA;<p>Temporarily yes, if based on that you are able to scale your current system to meet your needs.</p>&#xA;&#xA;<p>Now lets think about why on the first place you want to move to Microserices as a development style.</p>&#xA;&#xA;<ol>&#xA;<li>Small Components - easier to manager</li>&#xA;<li>Independently Deployable - Continous Delivery</li>&#xA;<li>Multiple Languages </li>&#xA;<li>The code is organized around business capabilities</li>&#xA;<li>and .....</li>&#xA;</ol>&#xA;&#xA;<p>When moving to Microservices, you should not have multiple services reading directly from each other databases, which will make them tightly coupled.</p>&#xA;&#xA;<p>One service should be completely ignorant on how the other service designed its internal structure.</p>&#xA;&#xA;<p>Now if you want to move towards microservices and take complete advantage of that, you should have vertical partition as you say and services talk to each other. </p>&#xA;&#xA;<p>Also while moving towards microservices your will get lots and lots of other problems. I tried compiling on how one should start on microservices on this <a href=""https://stackoverflow.com/questions/48971368/microservices-complete-case-study-example/49003694#49003694"">link</a> .</p>&#xA;&#xA;<blockquote>&#xA;  <p>How to separate services which are reading data from same table:</p>&#xA;</blockquote>&#xA;&#xA;<p>Now lets first create a dummy example: we have three services Order , Shipping , Customer all are three different microservices.</p>&#xA;&#xA;<p>Following are the ways in which multiple services require data from same table:</p>&#xA;&#xA;<ol>&#xA;<li>Service one needs to read data from other service for things like validation.</li>&#xA;</ol>&#xA;&#xA;<p>Order and shipping service might need some data from customer service to complete their operation.</p>&#xA;&#xA;<p>Eg: While placing a order one will call Order Service API with customer id , now as Order Service might need to validate whether its a valid customer or not. </p>&#xA;&#xA;<p><strong>One approach Database level exposure -- not recommened --</strong> use the same customer table -- which binds order service to customer service Impl</p>&#xA;&#xA;<p><strong>Another approach, Call another service to get data</strong></p>&#xA;&#xA;<p>Variation - 1 Call Customer service to check whether customer exists and get some customer data like name , and save this in order service</p>&#xA;&#xA;<p>Variation - 2 do not validate while placing the order, on OrderPlaced event check in async from Customer Service and validate and update state of order if required</p>&#xA;&#xA;<p>I recommend Call another service to get data based on the consistency you want. </p>&#xA;&#xA;<ol start=""2"">&#xA;<li>In some use cases you want a single transaction between data from multiple services.</li>&#xA;</ol>&#xA;&#xA;<p>For eg: Delete a customer. you might want that all order of the customer also should get deleted. </p>&#xA;&#xA;<p>In this case you need to deal with eventual consistency, service one will raise an event and then service 2 will react accordingly.</p>&#xA;&#xA;<p>Now if this answers your question than ok, else specify in what kind of scenario multiple service require to call another service.</p>&#xA;&#xA;<p>If still not solved, you could email me on puneetjindal.11@gmail.com, will answer you  </p>&#xA;"
48396610,30213456,1987266,2018-01-23T07:48:46,"<p>One simple Solution is you create user using the User Service and use a messaging bus where user service emits its events , and Wallet Service registers on the messaging bus, listens on User Created event and create Wallet for the User. In the mean time , if user goes on Wallet UI to see his Wallet, check if user was just created and show your wallet creation is in progress, please check in some time</p>&#xA;"
48396755,48374452,1987266,2018-01-23T07:59:58,"<p>I will try to clear some of the confusion.</p>&#xA;&#xA;<p>You are building MONOLITHIC application using Spring Boot but want it to be microservice ready, RIGHT..</p>&#xA;&#xA;<p>First break your application logically into diff. services and define what data(sql tables) is owned by each service  (every service should have its own DB URL instance , they all might point to same physical DB). Then from one service code you should not access any DB Entity which other service own, when you need data owned by other service then call them their REST API.</p>&#xA;&#xA;<p>Now while doing this you will get stuck in things like where you want to do operations which span multiple services but you want transactional consistency. For eg: on creation of user , user wallet should be created, but user and user wallet should be diff. service.For this you will need to use messaging bus and create user wallet based on the event of user creation.</p>&#xA;&#xA;<p>The solution i currently provided is a very very abstracted and basic view for implementing microservices.This is just for understanding. </p>&#xA;&#xA;<p>I think lots of your questions will be un-answered still. Now ask more specific questions and i will help</p>&#xA;"
51593422,51593335,1987266,2018-07-30T11:54:55,<p>It is best to call the edit of the pages microservices directly. </p>&#xA;
51297738,51292640,1987266,2018-07-12T05:14:12,"<p>In microservices there is nothing called a relational consistency, your system should become eventual consistent.</p>&#xA;&#xA;<p>Lets take your case, you have two microservices with diff. DB</p>&#xA;&#xA;<ul>&#xA;<li>Booking</li>&#xA;<li>Payment</li>&#xA;</ul>&#xA;&#xA;<p>Now lets say someone created a booking, now wants to pay for the booking using the payment service. </p>&#xA;&#xA;<p>You must be thinking that is Payment Service i should check whether the booking exist or not or is the booking cancelled or not. </p>&#xA;&#xA;<p>The above validations could be achieved but not 100% without creating a very high level dependency between the services and that is against the principles of microservices.</p>&#xA;&#xA;<p>Let me give you a example:</p>&#xA;&#xA;<ol>&#xA;<li>Someone cancelled a booking while payment in progress - assume you processing a payment in Payment Service after checking booking is Ok from Booking Service , while payment is in progress someone cancelled the booking</li>&#xA;</ol>&#xA;&#xA;<p>Now as in this case temporarily system is in a inconsistent state, what you should do is from events on Booking Service like BookingCancelled, check if we took a payment , yes then start the payment reverse process , rather than checking all things at once.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Go for a eventual consistent system in which both service listen to&#xA;  events to one other and tries to be consistent.</p>&#xA;</blockquote>&#xA;"
50798242,50796780,1987266,2018-06-11T12:52:59,"<p>just for the tenancy check , this can be done using the JWT token (token which can store tenancy information and other metadata).</p>&#xA;&#xA;<p>Let me provide another example of the same scenario which can't be solved with JWT.</p>&#xA;&#xA;<p>Assume one Customer wants to create a Order and our system wants to check whether the customer exist or not while creating the order.</p>&#xA;&#xA;<p>As Order and Customer service are separate, and we want minimal dependencies between them, there are multiple sol. to above problems:</p>&#xA;&#xA;<ul>&#xA;<li>create Order in ""validating state"" and on OrderCreated event check for customer validity and update customer state to ""Valid""</li>&#xA;<li>another one before creating order check for the customer (which is not the right way as it creates dependency, untill and unless very critical do not do it)</li>&#xA;<li>last way is the let the order be created , somebody who will final check the order for delivery will verify customer will remove</li>&#xA;</ul>&#xA;"
50021390,37711051,1987266,2018-04-25T11:40:24,"<p>this <a href=""https://github.com/lagom/online-auction-java"" rel=""nofollow noreferrer"">lagom application example</a> is a microservices application written in <a href=""https://www.lagomframework.com/"" rel=""nofollow noreferrer"">Lagom</a> . It is a akka based framework (DDD for design). </p>&#xA;&#xA;<p>Application is complete and working. See if that serve your purpose.</p>&#xA;"
50023027,50006750,1987266,2018-04-25T12:58:52,"<p>My arch. for the above probem would be like:</p>&#xA;&#xA;<p>Invoice Agg&#xA;    status&#xA;    paid&#xA;    InvoiceInfo</p>&#xA;&#xA;<p>InvoiceInfo&#xA;    CompanyInvoiceInfo&#xA;    DebtInvoiceInfo&#xA;    EmployeeInvoiceInfo</p>&#xA;&#xA;<p>InvoiceCreated&#xA;InvoiceInReview&#xA;InvoiceBilled</p>&#xA;&#xA;<p>will be the event review</p>&#xA;&#xA;<p>Basically all api on agg. should require invoice_id to take the process further:</p>&#xA;&#xA;<p>How user will get it and call API?</p>&#xA;&#xA;<ol>&#xA;<li>While the invoice is generated and receipt given to user or on Mail where invoice id is written?</li>&#xA;<li>List all invoices of an Employee , or Company or Dept ? I need a table(EmployeeInvoice) which have  (EmployeeId and InvoiceId and InvoiceStatus) , a table (CompanyInvoice) which have (CompanyId, InvoiceId , InvoiceStatus) and DeptInvoice(DeptId, CompanyId , InvoiceStatus)</li>&#xA;</ol>&#xA;&#xA;<p>From this one can click the Api for InvoiceUpdate , here also InvoiceId is present.</p>&#xA;&#xA;<ol start=""3"">&#xA;<li>The case where one needs to Expose an API to outside world like /updateInvoice?employeeId= or /updateInvoice?companyId= or /updateInvoice?deptId= here one already knows from the API structure which kind of table one needs to query. </li>&#xA;</ol>&#xA;&#xA;<p>Now to answer your question:</p>&#xA;&#xA;<ol>&#xA;<li>You will need some table to query back the invoice id in case in API InvoiceId is not mandated. That table can be flattened like i have done above or can be like you have there. As in Aggregate or EventStore model eventstore is based on a single ID always and API many times require to query it from diff. business ids.</li>&#xA;</ol>&#xA;"
48618204,48271493,1987266,2018-02-05T08:13:28,"<p>I think microservices and DDD have different origin but we need both. See how: </p>&#xA;&#xA;<p>DDD says how to break and design your software around the business domain</p>&#xA;&#xA;<p>Microservices says break and design your business domain in smaller sort of independent services to scale</p>&#xA;&#xA;<p>Now if you look closely both talk about break and design your software into smaller pieces, microservices in term of services(for scaling) and DDD in terms of Bounded Context(for design). </p>&#xA;&#xA;<p>Also microservices need consistency which can be eventual only and DDD tactical strategy helps in designing eventual consistent system </p>&#xA;&#xA;<p>To solve a big business problem we need both DDD and Microservices, and need to find a good cor-relation between them.</p>&#xA;&#xA;<p>From my perspective no microservice can span a bounded context , also one bounded context may have multiple micorservice .And the dependecny between microservices is majorly governed by mapping between bounded context</p>&#xA;"
48656440,48624757,1987266,2018-02-07T05:13:35,"<p>You should create a Interface lets say ""Queue"" which provide all functionalities which you want from Kafka or RabbitMQ, the create diff. impl like KafkaQueue and RabbitMQQueue of the Queue interface and inject the right impl which you want to use in your system.</p>&#xA;&#xA;<p>In this your if new queue system is used , your existing code will not be changed </p>&#xA;&#xA;<p>Creating another microservice is an extra overhead in this case</p>&#xA;"
48678678,48667874,1987266,2018-02-08T06:07:23,"<p>There are two approaches you could take based on your services.</p>&#xA;&#xA;<p>If a service is called very frequently let him cache the token (not in the core code, but some layer above) and listen for user login logout updates, in this case worst could happen is even after logout user can call this service for a very small period of time(time between syncing)</p>&#xA;&#xA;<p>Also in this case multiple services could use the same cache on (memchahe or like)</p>&#xA;&#xA;<p>If a service is called less frequently or require very high consistency call every time for consistency check.</p>&#xA;"
48703998,29761872,1987266,2018-02-09T10:40:19,"<p>In Microservices you create diff. read models, so for eg: if you have two diff. bounded context and somebody wants to search on both the data then somebody needs to listen to events from both bounded context and create a view specific for the application.</p>&#xA;&#xA;<p>In this case there will be more space needed, but no joins will be needed and no joins.</p>&#xA;"
48821320,48792602,1987266,2018-02-16T06:37:49,"<p>To answer your question first of all lets understand : </p>&#xA;&#xA;<blockquote>&#xA;  <p>it is mandatory that each micro service has its own database. In my&#xA;  case it may cost very expensive.</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes it is said that every microservice should have its own database. </p>&#xA;&#xA;<p>What they mean is tables/collection of each microservice should be separate (you could use a single scalable database instance) and one microservice should only access the data of other microservices only through API calls</p>&#xA;&#xA;<p>Benefits of having a separate model are:</p>&#xA;&#xA;<ol>&#xA;<li><p>Model will be clean. Eg: In E-Commerce Customer have diff. meaning for Shipping Microservice, Order Microservice, Customer Management Microservice and so on. If we put all data required by multiple microserives Customer Object will become very big</p></li>&#xA;<li><p>Microservices could evolve independently. In this case if we have a single Customer object and one microservice lets say Order one want to add something to the schema, all microservices needs to change</p></li>&#xA;</ol>&#xA;&#xA;<p>If we have a single Database Schema we will be getting into a big mess.</p>&#xA;&#xA;<blockquote>&#xA;  <p>In my case it may cost very expensive.</p>&#xA;</blockquote>&#xA;&#xA;<p>If expensive means read model actually require data from multiple microservices. then its better to listen to events from multiple microservices and create a single read model , little duplication of data is ok.</p>&#xA;&#xA;<p>If anything else, ask more specific question.</p>&#xA;"
49622918,49606124,1987266,2018-04-03T05:34:57,"<p>We do not prefer sharing database across multiple services, that makes their deployments and upgrades difficult.</p>&#xA;&#xA;<p>Generally, you can take event stream from multiple Microservices and one or multiple Microservice use neo4j to create graph datastructure specific for their use case.</p>&#xA;&#xA;<p>In this data duplication will happen so you have to take a judicious call on when to duplicate the data.</p>&#xA;"
49643105,49640387,1987266,2018-04-04T04:58:36,<p>One of the way would be that timer job will put the data into message queue and there are multiple listeners on the queue (which is partitioned based on some id) and you scale by increasing the listeners.</p>&#xA;&#xA;<p>Mostly this should suffice.</p>&#xA;
48860178,48833778,1987266,2018-02-19T05:47:41,"<p>Copying complete data generally never required, most of times for purposes of scale or making microservices more independent, people tend to copy some of the information which is more or less static in nature.</p>&#xA;&#xA;<p>For eg: In Post Service, i might copy author basic information like name in post microservices, because when somebody making a request to the post microservice to get list of post based on some filter , i do not want to get name of author for each post.</p>&#xA;&#xA;<p>Also the side effect of copying data is maintaining its consistency. So make sure you business really demands it.</p>&#xA;"
49003694,48971368,1987266,2018-02-27T07:52:44,"<p>We also started our journey some time back and i started writing a blog series for exactly the same thing: <a href=""https://dzone.com/articles/how-i-started-my-journey-in-micro-services-and-how"" rel=""nofollow noreferrer"">https://dzone.com/articles/how-i-started-my-journey-in-micro-services-and-how</a></p>&#xA;&#xA;<p>Basically what i understood is to break my problem in diff. microservices, i need a design framework which Domain Driven Design gives(Domain Driven Design Distilled Book by Vaugh Vernon).</p>&#xA;&#xA;<p>Then to implement the design (using CQRS and Event Sourcing and ...) i need a framework which provides all the above support.</p>&#xA;&#xA;<p>I found <a href=""https://www.lagomframework.com/"" rel=""nofollow noreferrer"">Lagom</a> good for this.(Eventuate , Spring Microservices are some other choices). </p>&#xA;&#xA;<p>Sample Microservices Domain analysis using Domain Driven Design by Microsoft: <a href=""https://docs.microsoft.com/en-us/azure/architecture/microservices/domain-analysis"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/architecture/microservices/domain-analysis</a></p>&#xA;&#xA;<p>One more analysis is: <a href=""http://cqrs.nu/tutorial/cs/01-design"" rel=""nofollow noreferrer"">http://cqrs.nu/tutorial/cs/01-design</a></p>&#xA;&#xA;<p>After reading on Domain Driven Design i think lagom and above links will help you to build a end to end application. If still any doubts , please raise :)</p>&#xA;"
49022920,49019203,1987266,2018-02-28T06:02:42,"<p>One way apart from Camunda, Conductor  is to send a event from Service A on some Messaging Queue (eg. lets say kafka ) which provides at least once delivery semantics.</p>&#xA;&#xA;<p>Then write a consumer which receive the event and do the orchestration part (talking to service B,C,D,E).</p>&#xA;&#xA;<p>As these all operations needs to be idempotent.First before starting orchestration create a RequestAgg. for the event from A and keep updating its state to represent where you reach in your orchestration journey. </p>&#xA;&#xA;<p>Now even if the other services are down or your node goes down. This should either reach the end or you should write functions to rollback as well.</p>&#xA;&#xA;<p>And to check the states and debug , you could see the read model of RequestAgg. </p>&#xA;"
48878677,48869041,1987266,2018-02-20T05:50:30,"<p>For writing these kind of flows, its best to use code a DFA into Orchestration. </p>&#xA;&#xA;<p>When the flow starts , system have a function to check the last state which was successfully completed, and give the function call to the next state in the DFA</p>&#xA;"
39618273,39561186,3432386,2016-09-21T13:40:53,"<p>After i getting a suggesting from Mr. Remus Rusanu.</p>&#xA;&#xA;<p>My solution is:</p>&#xA;&#xA;<p>Here is the interface for the topological sort:</p>&#xA;&#xA;<pre><code>public interface ITopologicalSort&#xA;    {&#xA;        string Id { get; }&#xA;        IList&lt;ITopologicalSort&gt; Dependencies { get; set; }&#xA;    }&#xA;</code></pre>&#xA;&#xA;<p>The topological sort:</p>&#xA;&#xA;<pre><code>public static Dictionary&lt;int, IList&lt;T&gt;&gt; TopologicalSort&lt;T&gt;(IEnumerable&lt;T&gt; source, Func&lt;T, IEnumerable&lt;T&gt;&gt; getDependenciesFunc) where T : class, ITopologicalSort&#xA;        {&#xA;            var groups = new Dictionary&lt;string, int&gt;();&#xA;            var sortedGroups = new Dictionary&lt;int, IList&lt;T&gt;&gt;();&#xA;            foreach (var item in source)&#xA;            {&#xA;                TopologicalSortInternal(item, getDependenciesFunc, sortedGroups, groups);&#xA;            }&#xA;            return sortedGroups;&#xA;        }&#xA;&#xA; private static int TopologicalSortInternal&lt;T&gt;(T item, Func&lt;T, IEnumerable&lt;T&gt;&gt; getDependenciesFunc, Dictionary&lt;int, IList&lt;T&gt;&gt; sortedGroups, Dictionary&lt;string, int&gt; groups) where T : class, ITopologicalSort&#xA;        {&#xA;            int level;&#xA;            if (!groups.TryGetValue(item.Id, out level))&#xA;            {&#xA;                groups[item.Id] = level = INPROCESS;&#xA;                var dependencies = getDependenciesFunc(item);&#xA;                if (dependencies != null &amp;&amp; dependencies.Count() &gt; 0)&#xA;                {&#xA;                    foreach (var dependency in dependencies)&#xA;                    {&#xA;                        var depLevel = TopologicalSortInternal(dependency, getDependenciesFunc, sortedGroups, groups);&#xA;                        level = Math.Max(level, depLevel);&#xA;                    }&#xA;                }&#xA;                groups[item.Id] = ++level;&#xA;                IList&lt;T&gt; values;&#xA;                if (!sortedGroups.TryGetValue(level, out values))&#xA;                {&#xA;                    values = new List&lt;T&gt;();&#xA;                    sortedGroups.Add(level, values);&#xA;                }&#xA;                values.Add(item);&#xA;            }&#xA;            return level;&#xA;        }&#xA;</code></pre>&#xA;&#xA;<p>What the code is doing?&#xA;He is sorted by dependencies.</p>&#xA;&#xA;<p>So for this example:</p>&#xA;&#xA;<pre><code>&lt;Services&gt;&#xA;    &lt;Service name=""ServiceA"" args="""" type=""CommonLib.IServiceA"" dependencies=""""/&gt;&#xA;    &lt;Service name=""ServiceB1"" args="""" type=""CommonLib.IServiceB1"" dependencies=""ServiceA""/&gt;&#xA;    &lt;Service name=""ServiceB2"" args="""" type=""CommonLib.IServiceB2"" dependencies=""ServiceA""/&gt;&#xA;    &lt;Service name=""ServiceC1"" args="""" type=""CommonLib.IServiceC1"" dependencies=""ServiceA,ServiceB1""/&gt;&#xA;    &lt;Service name=""ServiceC2"" args="""" type=""CommonLib.IServiceC2"" dependencies=""ServiceA,ServiceB2""/&gt;&#xA;    &lt;Service name=""ServiceD"" args="""" type=""CommonLib.IServiceD"" dependencies=""ServiceA,ServiceB2,ServiceC2""/&gt;&#xA;    &lt;Service name=""ServiceE"" args="""" type=""CommonLib.IServiceE"" dependencies=""ServiceA,ServiceB2,ServiceC2,ServiceD""/&gt;&#xA;    &lt;Service name=""ServiceF"" args="""" type=""CommonLib.IServiceF"" dependencies=""ServiceA,ServiceB1,ServiceC1,ServiceD,ServiceE""/&gt;&#xA;&lt;/Services&gt;&#xA;&#xA;Level 0: ServiceA&#xA;Level 1: ServiceB1, ServiceB2&#xA;Level 2: ServiceC2, ServiceC2&#xA;Level 3: ServiceD&#xA;Level 4: ServiceE&#xA;Level 5: ServiceF&#xA;</code></pre>&#xA;"
25975337,25974964,25282,2014-09-22T13:26:33,"<p>What does ""major impact"" in performance mean for your use case? Is 1 ms a major impact in performance? Is 10 ms? Is 0.1 ms? Is 0.01 ms? Without knowing your needs it's impossible to say whether or not it's a major impact.</p>&#xA;&#xA;<p>Whenever time efficiency is of important it's worth to measure. Measure the amount of time the unmarshaling takes you. Measure how much other time the other operations need. Decisions about speed tradeoffs and optimization should always be made with hard data. </p>&#xA;"
32608114,32604241,167874,2015-09-16T12:05:06,<p>Yes - use Guzzle/HTTP. If you need to scale later you'll be able to take advantage of the network easily. Latency won't be an issue - the traffic won't leave the box.</p>&#xA;
46572280,46571540,303810,2017-10-04T18:59:17,"<p>Preferences on checked/unchecked exceptions are like a religion.</p>&#xA;&#xA;<p>You normally don't call controller methods in your own code. Usually you won't have code like:</p>&#xA;&#xA;<pre><code>try {&#xA;   runClass.doSomething();&#xA;} catch(ExceptionRA exra) {&#xA;  // Handle handle handle&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>So you won't directly profit from checked exceptions throws from the controller method. They won't force any checked handling because there's nowhere to handle them. So you can just as well throw unchecked exceptions and make method signature shorter. There's no added value in throwing checked exceptions from controller methods.</p>&#xA;&#xA;<p>An exception can also be thrown by some busines service method that controller is calling. This rises two questions:</p>&#xA;&#xA;<ul>&#xA;<li>Should you use checked or unchecked exceptions in your business logic?</li>&#xA;<li>If business logic throws checked exception, should you wrap it as an unchecked exception? Or just pass through?</li>&#xA;</ul>&#xA;&#xA;<p>The answer to the first question is - you should not let this decision be influenced by what you do in controller. Do it according to your ""exception religion"". If you prefer checked exception, use checked exceptions, no matter if you'd use unchecked in controller or not.</p>&#xA;&#xA;<p>No to the second question. Assuming your business logic throws a checked exception. If this exception would adequately represent the situation to the caller, there's no reason to wrap it into any other exception, be it checked or unchecked. If your business exception does not explain the situation correctly to the caller, better wrap it. Then, unchecked exception will make your signature shorter.</p>&#xA;&#xA;<p>Now a personal opinion. I generally prefer checked exceptions for everything except programming errors. I also use them in controllers (never mind somewhat longe method signatures).</p>&#xA;"
46623319,46623194,303810,2017-10-07T17:52:49,<p>Actually you should not be doing load balancing/service discover etc. in the front-end. So the question about whether it is possible in JavaScript or with which libraries is irrelevant.</p>&#xA;&#xA;<p>Typically you'll have an API gateway or a (load balancing) proxy which works with your service registry and routes requests accordingly. In the current project we use Consul for service registry and Nginx + consul-template as proxy. We plan to migrate to some API gateway.</p>&#xA;&#xA;<p>With this setup your front-end will connect to just one central endpoint which would do load balancing/routing to individual service instances behind the scenes. Thus your front-end will not need to implement anything like Eureka/Ribbon etc.</p>&#xA;
49477029,49476498,303810,2018-03-25T14:38:10,"<p>I think the core question is if it is still the same operation (with different parameters) or not. This should be clear from the semantics of the operation.<br>&#xA;From what I read in your question, it seems like just subscribing is quite different from subscribe + monitor.</p>&#xA;&#xA;<p>Adding the <code>monitor</code> parameter not only substantialy modifies what happens, but it also changes return types etc.</p>&#xA;&#xA;<p>So from my vote goes to a new endpoint.</p>&#xA;"
42281785,42250009,1883842,2017-02-16T18:20:54,"<p>Really good question and I'd be really interested in the architecture you come up with.</p>&#xA;&#xA;<p>Timer events are processed by the Job Scheduler.&#xA;In more recent versions of Activiti (>5,16 I believe) the Timer and Async queue managers have been separated but they still use the same underlying data table (ACT_JOB).</p>&#xA;&#xA;<p>Each Job Executor will tag a block of timer events whose time is past due and put them into the timer queue for processing. You can set the number of tasks picked up each pass as well as the timeout and queue size in the engine configuration.</p>&#xA;&#xA;<p>So, to your specific question: "" Is the execution of the timer a function of a date/time expression kept in the (permanent) persistence layer (thus it's evaluated when the engine interacts with that expression)? Or is it a process that keeps a running clock and then fires when it expires? ""</p>&#xA;&#xA;<p>The Timer ACT_JOB table holds a DueDate (DateTime) which the Timer thread will use when it queries the next set of jobs to process.&#xA;Which, if I understand your language is a variation of option 1.</p>&#xA;&#xA;<p>Greg</p>&#xA;&#xA;<p>""</p>&#xA;"
38943549,38820356,1440706,2016-08-14T15:11:42,"<p>No, by default one http request into Kong will only match to one upstream url.</p>&#xA;&#xA;<p>The Kong <a href=""https://getkong.org/plugins/request-transformer/"" rel=""nofollow"">request</a> and <a href=""https://getkong.org/plugins/response-transformer/"" rel=""nofollow"">response</a> transformation plugins handle <strong>simple</strong> Header, Body and Query String manipulation but Kong doesn't handle API aggregation. </p>&#xA;&#xA;<p>Obviously, Kong is very easy to modify via it's own plugin mechanism but emulating what appears to be complex metadata driven dynamic orchestration tool that Netflix describe (via Lua) is going to be non-trivial.</p>&#xA;&#xA;<p>If you used Kong here it would likely only be as the gateway (which it does well) and build out simple orchestration APIs yourself that aggregated your backend calls. </p>&#xA;"
47059635,47058984,2756547,2017-11-01T16:41:12,"<p>There is <code>AsyncRabbitTemplate</code> for your purpose do not block the caller until the reply: <a href=""https://docs.spring.io/spring-amqp/docs/2.0.0.RELEASE/reference/html/_reference.html#async-template"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-amqp/docs/2.0.0.RELEASE/reference/html/_reference.html#async-template</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Version 1.6 introduced the <code>AsyncRabbitTemplate</code>. This has similar <code>sendAndReceive</code> (and <code>convertSendAndReceive</code>) methods to those on the <code>AmqpTemplate</code> but instead of blocking, they return a <code>ListenableFuture</code>.</p>&#xA;</blockquote>&#xA;&#xA;<pre><code> RabbitConverterFuture&lt;String&gt; future = this.template.convertSendAndReceive(""foo"");&#xA;future.addCallback(new ListenableFutureCallback&lt;String&gt;() {&#xA;&#xA;    @Override&#xA;    public void onSuccess(String result) {&#xA;        ...&#xA;    }&#xA;&#xA;    @Override&#xA;    public void onFailure(Throwable ex) {&#xA;        ...&#xA;    }&#xA;&#xA;});&#xA;</code></pre>&#xA;"
40940497,40930432,2756547,2016-12-02T20:40:30,"<p>I have never used CAS before, but looks like you don't share how you get <code>headers.serviceTicket</code>.</p>&#xA;&#xA;<p>I think you idea to propagate <code>ticket</code> via URL param is good, but first of all we have to extract it from the incoming URL:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Upon successful login, CAS will redirect the user’s browser back to the original service. It will also include a ticket parameter, which is an opaque string representing the ""service ticket"". Continuing our earlier example, the URL the browser is redirected to might be <a href=""https://server3.company.com/webapp/login/cas?ticket=ST-0-ER94xMJmn6pha35CQRoZ"" rel=""nofollow noreferrer"">https://server3.company.com/webapp/login/cas?ticket=ST-0-ER94xMJmn6pha35CQRoZ</a>. </p>&#xA;</blockquote>&#xA;&#xA;<p><a href=""http://docs.spring.io/spring-security/site/docs/4.2.0.RELEASE/reference/htmlsingle/#cas"" rel=""nofollow noreferrer"">http://docs.spring.io/spring-security/site/docs/4.2.0.RELEASE/reference/htmlsingle/#cas</a></p>&#xA;&#xA;<p>For this purpose we can do like this:</p>&#xA;&#xA;<pre><code>&lt;int-http:inbound-channel-adapter path=""/user*"" supported-methods=""GET, POST"" channel=""userRequestChannel""&gt;&#xA;       &lt;int-http:header name=""serviceTicket"" expression=""#requestParams.ticket""/&gt;&#xA;&lt;/int-http:inbound-channel-adapter&gt;&#xA;</code></pre>&#xA;&#xA;<p>Otherwise, please, share exception on the matter and try to trace network traffic to determine a gap.</p>&#xA;&#xA;<p><strong>UPDATE</strong></p>&#xA;&#xA;<p>According to the description on that Spring Security page for CAS we have:</p>&#xA;&#xA;<blockquote>&#xA;  <p>... The principal will be equal to <code>CasAuthenticationFilter.CAS_STATEFUL_IDENTIFIER</code>, whilst the credentials will be the service ticket opaque value...</p>&#xA;</blockquote>&#xA;&#xA;<p>So, looks like we don't need to worry about request param in the <code>&lt;int-http:inbound-channel-adapter&gt;</code> and just rely on the <code>SecurityContext</code> in the <code>&lt;int-http:outbound-gateway&gt;</code>:</p>&#xA;&#xA;<pre><code>&lt;int-http:uri-variable name=""ticket"" &#xA;         expression=""T(org.springframework.security.core.context.SecurityContextHolder).context.authentication.credentials""/&gt;&#xA;</code></pre>&#xA;"
42190733,42190315,2756547,2017-02-12T17:20:00,"<p>It definitely looks like <a href=""http://docs.spring.io/spring-integration/reference/html/messaging-routing-chapter.html#scatter-gather"" rel=""nofollow noreferrer"">Scatter-Gather</a>.</p>&#xA;&#xA;<p>You have a <code>PublishSubscribeChannel</code> and can add and remove subscribers for it at run time. </p>&#xA;&#xA;<p>Regarding <code>plug &amp; unplug</code> question you can take a look into recent <a href=""https://spring.io/blog/2016/09/27/java-dsl-for-spring-integration-1-2-release-candidate-1-is-available"" rel=""nofollow noreferrer""><code>IntegrationContext</code></a> feature in the Spring Integration Java DSL 1.2.</p>&#xA;"
49925428,49925171,2756547,2018-04-19T15:59:07,"<p>Right, you use Spring Integration within you Microservices and Data Flow to connect and orchestrate them. That is just a side effect that Spring Cloud Data Flow uses Spring Integration and Spring Batch for its internal logic.</p>&#xA;&#xA;<p>In this picture you might just miss the fact of the Spring Cloud Stream level in between: <a href=""https://cloud.spring.io/spring-cloud-stream/"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-stream/</a></p>&#xA;"
48651097,48650993,2756547,2018-02-06T20:05:25,"<p>You can consider to disable exchange creation there via configuration properties:</p>&#xA;&#xA;<pre><code>declareExchange&#xA;&#xA;    Whether to declare the exchange for the destination.&#xA;&#xA;    Default: true.&#xA;</code></pre>&#xA;&#xA;<p><a href=""https://docs.spring.io/spring-cloud-stream/docs/Elmhurst.M3/reference/htmlsingle/#_rabbitmq_consumer_properties"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-cloud-stream/docs/Elmhurst.M3/reference/htmlsingle/#_rabbitmq_consumer_properties</a></p>&#xA;"
36273723,36272482,2756547,2016-03-29T01:05:03,"<p>Looks like it's a bug in the Spring Boot: <a href=""https://github.com/spring-projects/spring-boot/issues/5517"" rel=""nofollow"">https://github.com/spring-projects/spring-boot/issues/5517</a>.</p>&#xA;&#xA;<p>We have to declare something like this ourselves as a workaround:</p>&#xA;&#xA;<pre><code>@Bean&#xA;public MessageChannel metricsChannel() {&#xA;    return new DirectChannel();&#xA;}&#xA;&#xA;@Bean&#xA;@ExportMetricWriter&#xA;public MessageChannelMetricWriter messageChannelMetricWriter() {&#xA;    return new MessageChannelMetricWriter(metricsChannel());&#xA;}&#xA;&#xA;@Bean&#xA;@ServiceActivator(inputChannel = ""metricsChannel"")&#xA;public MessageHandler metricsHandler() {&#xA;    return System.out::println;&#xA;}&#xA;</code></pre>&#xA;"
28115457,28114758,2756547,2015-01-23T17:30:24,"<p>There is no anything specific on the matter.</p>&#xA;&#xA;<p>Spring Boot utilizes <code>IntegrationAutoConfiguration</code> to switch on <code>@EnableIntegration</code> and with that you can go ahead with best from both worlds: Boot stuff to inject, e.g. <code>AmqpTemplate</code>, <code>MongoTemplate</code>, <code>DataSource</code> etc. to the Spring Integration configs or use from SI XML components definitions.</p>&#xA;&#xA;<p>See our samples on the matter which is based on the Spring Boot: </p>&#xA;&#xA;<p><a href=""https://github.com/spring-projects/spring-integration-samples/tree/master/dsl"" rel=""nofollow"">https://github.com/spring-projects/spring-integration-samples/tree/master/dsl</a>&#xA;<a href=""https://github.com/spring-projects/spring-integration-samples/tree/master/basic/web-sockets"" rel=""nofollow"">https://github.com/spring-projects/spring-integration-samples/tree/master/basic/web-sockets</a>&#xA;<a href=""https://github.com/spring-projects/spring-integration-samples/tree/master/applications/stomp-chat"" rel=""nofollow"">https://github.com/spring-projects/spring-integration-samples/tree/master/applications/stomp-chat</a></p>&#xA;"
51953088,51950785,6164419,2018-08-21T16:30:52,"<p>I think it makes more sense to put the Dockerfiles and its deployment files(k8s YAMLs) all in the same repo of its service, this makes easy to build and deploy the service since the CI/CD tool could just pull de new code, build the image and then deploy. You could also go the Helm way and quit all the deployment files from the service repositories, and they could be all together in a single helm chart. The Helm path makes easy to see the application as a whole. You should use <a href=""https://www.telepresence.io/"" rel=""nofollow noreferrer"">Telepresence</a> if you dont want run all the services on the developer machine.</p>&#xA;"
44130741,43850359,3154883,2017-05-23T09:21:34,"<p>The calls through REST call is a very good option if you need sync calls with return. If you can make a Async call, you could use a message architecture with RabbitMQ using Spring AMQP for example.</p>&#xA;&#xA;<p>A good approach is when you are communicating with 3rd parties and other services, if you have a failover plan, so your service continue working.</p>&#xA;&#xA;<p>The answer for this is to use a Circuit Breaker in you application.</p>&#xA;&#xA;<p>Here are two examples:</p>&#xA;&#xA;<ul>&#xA;<li>Netflix Hystrix</li>&#xA;<li>CircuitBreaker (from Spring Retry)</li>&#xA;</ul>&#xA;&#xA;<p>I am currently working with the Spring Retry and is working as expected. </p>&#xA;&#xA;<p>The idea is: </p>&#xA;&#xA;<ul>&#xA;<li>if a call to a external service is failing after some attempts, you return a default answer and lets the system recover before trying again.</li>&#xA;<li>After some time, you start calling the external service again (hopping now is working).</li>&#xA;</ul>&#xA;&#xA;<p>Here is an example in my Github where I implement the @CircuitBreaker annotation together with the @Recover.</p>&#xA;&#xA;<p><a href=""https://github.com/BHRother/spring-circuit-breaker-example"" rel=""nofollow noreferrer"">spring-circuit-breaker-example</a></p>&#xA;"
42694564,39540366,928451,2017-03-09T11:45:28,"<p>I'm the author of the shipping example. Sorry for not seeing your question earlier.</p>&#xA;&#xA;<p>This particular example needs a bit of explanation. It is an example based on tactical patterns from Domain Driven Design, which means that we need to understand what we are talking about when we're referring to a <em>service</em>.</p>&#xA;&#xA;<p>There are <em>application services</em> that deal with the use cases offered by the application, e.g. <code>booking.Service</code>. There are <em>domain services</em> that reside in the domain layer and provides your domain with concepts that aren't necessarily bound to a domain object. In the shipping example, <code>routing.Service</code> is a domain service whose implementation actually queries another application, in this case it talks to <a href=""https://github.com/marcusolsson/pathfinder"" rel=""nofollow noreferrer"">this routing service</a>.</p>&#xA;&#xA;<p>Application and domain services are merely ways of organizing our code. Putting it differently, these services communicate within a process, while <em>microservices</em> typically communicate over a network using some form of common transport, e.g. JSON, gRPC and so on.</p>&#xA;&#xA;<p>Coming back to your question, what I believe you are looking for is the implementation of the <code>routing.Service</code> which you can find <a href=""https://github.com/go-kit/kit/blob/master/examples/shipping/routing/proxying.go"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;&#xA;<p>The proxy service used here is explained under <strong>Client-side endpoints</strong> on <a href=""http://gokit.io/examples/stringsvc.html"" rel=""nofollow noreferrer"">this page</a>, and is used to make requests from your application to another.</p>&#xA;&#xA;<p>If you want more detail, I wrote a <a href=""https://www.citerus.se/part-3-domain-driven-design-in-go"" rel=""nofollow noreferrer"">blog post</a> on the subject a while ago.</p>&#xA;"
47120463,47119752,18627,2017-11-05T10:23:26,"<p>For Kafka-based implementation, you just need to make sure that all events belonging to same ""logical group"" (id?) are landing at the same partition - in this case you can add more consumers (up to number of partitions) to handle events in parallel, but every consumer will handle events only for their logical group(s).</p>&#xA;&#xA;<p>You can achieve this behavior by selecting correct key/partitioning approach - either use ID as a key, or come with more sophisticated partitioning implementation.</p>&#xA;"
49390884,49390064,6180803,2018-03-20T17:41:36,"<p>You can write your Deployment manifest (like <a href=""https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment"" rel=""nofollow noreferrer"">here</a>) then you can write your Service manifest (like <a href=""https://kubernetes.io/docs/concepts/services-networking/service#defining-a-service"" rel=""nofollow noreferrer"">here</a>) and then you can put them in the same file with a <code>---</code> between them (like <a href=""https://github.com/kubernetes/helm/blob/master/docs/chart_template_guide/yaml_techniques.md#embedding-multiple-documents-in-one-file"" rel=""nofollow noreferrer"">here</a>). That is part of the YAML spec rather than a Kubernetes specific thing.</p>&#xA;&#xA;<p>For an example of how you can write the Service and Deployment so that they target the same set of Pods, you can look at the definition of the default backend of the nginx ingress controller for inspiration <a href=""https://github.com/kubernetes/ingress-nginx/blob/master/deploy/default-backend.yaml"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;"
50449895,50438873,109251,2018-05-21T13:22:36,"<p>This question and of course the answers are going to be opinion based, but generally when thinking in terms of micros services, think not in terms as things like loads of API's  and VM's  etc.   Instead think in terms of.   When i upload an image,  its needs to be resized, and the table updated to give a url for the thumb. or when XXX record is updated in database, Run XXX in order to create a report, or update Azure search.  and that each service, just knows how to do a single thing only.  I.E   Resize an image.</p>&#xA;&#xA;<p>Now one could say.   I have a system, A repo library, and some functions library.   When an image is posted, I upload, then call this, and that  etc.</p>&#xA;&#xA;<p>With Micor services.   You would instead just add the image to a queue.   Create an azure function that has a queue trigger.   that would resize and save both the large and the thumb to storage.   this would then either update the database, or in true micro service, it would add a queue to store the new info,  another function would watch that queue and insert into the database.</p>&#xA;&#xA;<p>You can use the DB queue from anything.   You can use the Blob queue from anything.   Your main API,   does not care how images are handled.   You can change your functions one day to maybe save to dropbox, instead of azure blob.   All really easy, with no re-build of the API, because the API does not care.</p>&#xA;&#xA;<p>A good example I use it for is email and SMS.   My systems dont know how to send an email, or an SMS.   They only know how to add to a queue.    My microservices.   SendEmail and SendSMS  do know how to do it,  and I can change how and who i send that content with, really easy.   I can tomorrow  change from Twilio  to send grid, without ever telling the API that i've done it. </p>&#xA;&#xA;<p>On a more complex thing.   I have approval,   at the moment that approval sends an email or SMS to either user or admin, and that can change over time.    So I have an SMS server, Email Service and and approvalService.    when approval happens,  it just adds a config to the queue,  The rest is done by a logic app, that knows  to send an email to XXX and an SMS to XXX and then update database.   My api,  is just a post, that creates a queue.</p>&#xA;&#xA;<p>Basically what I am saying here is to get started, maybe porting an existing app.  Start with the workflow stuff,  like send an email, resize an image, create a report,  create a PDF,   email 50 subscribers etc.   and take all that code out and put into there own micro service that just knows how to do one thing.   Then when you grow with confidence,  create a workflow from all of these services with Logic Apps, let azure take care of the rest, thats what they want to do.</p>&#xA;"
37289904,36701111,6318276,2016-05-18T03:44:12,"<p>Let me try and add some more details to this scenario to stress what may or may not qualify as an event in the context of Product and Biiling. The Billing-MS would need to talk to Product-Ms only in case an Order is placed. Placing an Order would mostly be for a separate MS let's say Order-MS. When an order is created or placed, it will contain information of Products as line items.</p>&#xA;&#xA;<p>Creation of an Order can be considered as an event. When Order creation event occurs, it can be pushed to a Queue for the Billing service. Queue should be implemented as a Work-queue in RabbitMQ. This way, multiple instances of the Billing-MS can subscribe to the same Queue but it'll be processed by one and only one Worker. There is no role of RIBBON in registering a service as a Worker to RabbitMQ. Each instance registers to a Queue and RabbitMQ decides RoundRobin which instance of Billing Service gets to process this event.</p>&#xA;&#xA;<p>Getting details of Products in an Order for the Billing-Ms should be a Service-to-Service call load balanced via Ribbon (if that's what you are using). Getting Product details is not really an event, placing an Order is, hence the difference.  </p>&#xA;&#xA;<p>Also, Gateway should be used for exposing your Edge services. For Service-to-Service calls, it would not be ideal to hop via Gateway service.</p>&#xA;"
43102959,41881610,1017284,2017-03-29T19:55:07,"<p>I just ran into the same issue myself, and created the following issue:</p>&#xA;&#xA;<p><a href=""https://jira.spring.io/browse/SPR-15396"" rel=""nofollow noreferrer"">https://jira.spring.io/browse/SPR-15396</a></p>&#xA;&#xA;<p>Hopefully this is getting configurable in Spring 4.3.8.</p>&#xA;&#xA;<p>In the meantime, you have to create a bean of type <code>FormBodyWrapperFilter</code> (that overrides the one in <code>ZuulConfiguration</code>). In the constructor, you pass a copy of <code>FormHttpMessageConverter</code>, which extends from <code>FormHttpMessageConverter</code>, and you change the encoding used in <code>FormHttpMessageConverter.MultipartHttpOutputMessage#getAsciiBytes(String)</code> to UTF-8 (you might also want to delete any references to javax-mail, unless you have that on classpath). You need a pretty recent version of Spring Cloud Netflix to do this.</p>&#xA;&#xA;<p>Example:</p>&#xA;&#xA;<pre><code>@Bean&#xA;FormBodyWrapperFilter formBodyWrapperFilter() {&#xA;    return new FormBodyWrapperFilter(new MyFormHttpMessageConverter());&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Then you create a copy of <code>FormHttpMessageConverter</code>, and change the following method:</p>&#xA;&#xA;<pre><code>    private byte[] getAsciiBytes(String name) {&#xA;        try {&#xA;            // THIS IS THE ONLY MODIFICATION:&#xA;            return name.getBytes(""UTF-8"");&#xA;        } catch (UnsupportedEncodingException ex) {&#xA;            // Should not happen - US-ASCII is always supported.&#xA;            throw new IllegalStateException(ex);&#xA;        }&#xA;    }&#xA;</code></pre>&#xA;"
34749915,34730542,639172,2016-01-12T17:17:54,<p>Issue was resolved by adding following work around in the docker file:</p>&#xA;&#xA;<pre><code># Fix DNS resolution issues when nss is not installed&#xA;RUN echo 'hosts: files mdns4_minimal [NOTFOUND=return] dns mdns4' &gt;&gt; /etc/nsswitch.conf&#xA;&#xA;# Set DNS cache to 10 seconds (Cache is permanent by default). Network hosts are volatile in Docker clusters.&#xA;RUN grep '^networkaddress.cache.ttl=' /opt/${JAVA_PACKAGE}/jre/lib/security/java.security || echo 'networkaddress.cache.ttl=10' &gt;&gt; /opt/${JAVA_PACKAGE}/jre/lib/security/java.security&#xA;</code></pre>&#xA;
51941940,51921033,53444,2018-08-21T05:12:07,"<p>As indicated in my comment, some starters were renamed: <a href=""https://github.com/spring-projects/spring-cloud/wiki/Spring-Cloud-Edgware-Release-Notes"" rel=""nofollow noreferrer"">https://github.com/spring-projects/spring-cloud/wiki/Spring-Cloud-Edgware-Release-Notes</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>A number of starters did not follow normal Spring Cloud naming conventions. In Edgware, use of the deprecated starter will log a warning with the name of the new starter to use in its place.</p>&#xA;</blockquote>&#xA;&#xA;<p>So change:&#xA;<code>spring-cloud-starter-eureka-server</code>&#xA;to <code>spring-cloud-starter-netflix-eureka-server</code>.</p>&#xA;"
49983187,49980008,527813,2018-04-23T14:05:12,<p>We can run more one than one Docker at a time in a host but yes we will hit the limitation of binding the same port to the docker; so to resolve this we need to bind different port in the host to docker that is if you are running mongo-db then its default port is 27017 so we can run two mongo-db as -p 27017:27017 for Docker D1 and -p 27018:27017 for Docker D2 and 5000:27017 for docker D3; Like this you can bind different host port to map to 27017 for mongo-db port; Now your question is how to manage this ports from host then I would recommend you to use nginx for port managing in the host machine.</p>&#xA;&#xA;<p>Coming to your next question all dockers are connected to default docker0 bridge network so we can connect to any of the dockers connected to default bridge 'docker0' network; If I am right  it will come with ipaddress of 172.x.x.x network. Get inside to the docker and run 'ip addr' to see the ip-address assigned to the dockers and you can test connection by running ping command.</p>&#xA;
42467227,27007353,208581,2017-02-26T10:16:53,"<p>I realise this is possibly not a good answer but what the heck.  Your question was:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Given a database that consists entirely of related tables, how does&#xA;  one denormalize this into smaller fragments (groups of tables)</p>&#xA;</blockquote>&#xA;&#xA;<p>WRT the database design I'd say <strong><em>""you can't without removing foreign keys""</em></strong>. </p>&#xA;&#xA;<p>That is, people pushing Microservices with the strict no shared DB rule are asking database designers to give up foreign keys (and they are doing that implicitly or explicitly). When they don't explicitly state the loss of FK's it makes you wonder if they actually know and recognise the value of foreign keys (because it is frequently not mentioned at all).</p>&#xA;&#xA;<p>I have seen big systems broken into groups of tables. In these cases there can be either A) no FK's allowed between the groups or B) one special group that holds ""core"" tables that can be referenced by FK's to tables in other groups.</p>&#xA;&#xA;<p>... but in these systems ""groups of tables"" is often 50+ tables so not small enough for strict compliance with microservices.</p>&#xA;&#xA;<p>To me the other related issue to consider with the Microservice approach to splitting the DB is the impact this has reporting, the question of how all the data is brought together for reporting and/or loading into a data warehouse. </p>&#xA;&#xA;<p>Somewhat related is also the tendency to ignore built in DB replication features in favor of messaging (and how DB based replication of the core tables / DDD shared kernel) impacts the design.</p>&#xA;&#xA;<p><strong>EDIT: (the cost of JOIN via REST calls)</strong> </p>&#xA;&#xA;<p>When we split up the DB as suggested by microservices and remove FK's we not only lose the enforced declarative business rule (of the FK) but we also lose the ability for the DB to perform the join(s) across those boundaries.</p>&#xA;&#xA;<p>In OLTP FK values are generally not ""UX Friendly"" and we often want to join on them.</p>&#xA;&#xA;<p>In the example if we fetch the last 100 orders we probably don't want to show the customer id values in the UX. Instead we need to make a second call to customer to get their name.  However, if we also wanted the order lines we also need to make another call to the products service to show product name, sku etc rather than product id.</p>&#xA;&#xA;<p>In general we can find that when we break up the DB design in this way we need to do a lot of ""JOIN via REST"" calls. So what is the relative cost of doing this?</p>&#xA;&#xA;<p><strong>Actual Story: Example costs for 'JOIN via REST' vs DB Joins</strong></p>&#xA;&#xA;<p>There are 4 microservices and they involve a lot of ""JOIN via REST"". A benchmark load for these 4 services comes to <strong>~15 minutes</strong>. Those 4 microservices converted into 1 service with 4 modules against a shared DB (that allows joins) executes the same load in <strong>~20 seconds</strong>.</p>&#xA;&#xA;<p>This unfortunately is not a direct apples to apples comparison for DB joins vs ""JOIN via REST"" as in this case we also changed from a NoSQL DB to Postgres.</p>&#xA;&#xA;<p>Is it a surprise that ""JOIN via REST"" performs relatively poorly when compared to a DB that has a cost based optimiser etc.</p>&#xA;&#xA;<p>To some extent when we break up the DB like this we are also walking away from the 'cost based optimiser' and all that in does with query execution planning for us in favor of writing our own join logic (we are somewhat writing our own relatively unsophisticated query execution plan).</p>&#xA;"
47634625,47630891,672453,2017-12-04T13:36:51,<p>Set <code>DelayLoadMetadata</code> to <code>true</code>.</p>&#xA;
42337411,42332737,672453,2017-02-20T05:57:04,"<p>This is a good use case for an extension grant. See the docs:</p>&#xA;&#xA;<p><a href=""https://identityserver4.readthedocs.io/en/release/topics/extension_grants.html"" rel=""nofollow noreferrer"">https://identityserver4.readthedocs.io/en/release/topics/extension_grants.html</a></p>&#xA;"
34164081,30237292,5656005,2015-12-08T19:12:10,"<p>While there are some similarities to the ideas behind the original EJB spec and what is being done with Microservices there are many differences. </p>&#xA;&#xA;<p>EJB provided a standardized way of building component based architectures, with contracts to ensure the products of a bean can be consumed by other architectures, while abstracting away transaction, state, and thread management. The idea of building components is very similar, the biggest change is we are now calling these components services. The idea of contract based development is also similar.</p>&#xA;&#xA;<p>Some of the high level differences are provided below:</p>&#xA;&#xA;<p>Within the specification for EJB Sun states ""Enterprise beans are intended to be relatively coarse-grained business objects"" this is anathema to a good Microservice implementation. With Microservices the best design is a bounded contexts with a single concern. </p>&#xA;&#xA;<p>In an EJB 1.x architecture the container is the persistence provider, while in a Microservice architecture the each service manages its own data and persistence.</p>&#xA;&#xA;<p>With the Microservice pattern transaction management is simplified by minimizing your the scope and never having transactions cross Microservice boundaries. </p>&#xA;&#xA;<p>With Microservices thread pools are per service or instance of the service. If the thread pool is exhausted ideally you spawn another instance of the service. In an EJB 1.x environment the thread management is the responsibility of the container.</p>&#xA;&#xA;<p>There are many other differences between a Microservice architecture and an EJB 1.x architecture, but these are some highlights. I have worked with implementations of both architectures and the maintenance costs of a Microservice architecture appear to be lower so far. Especially when considering the mess EJBs have become within monolithic architectures. </p>&#xA;"
39197360,39126827,4675269,2016-08-29T01:26:18,"<p>The common approach in microservices architecture is to use a single authentication 'gateway', and usually it's a part of an API gateway.</p>&#xA;&#xA;<p>So besides your passport.ms.com, you have somewhat of a proxy that will check access token from the header and if it's invalid - give an error. If the token is valid - proxy the request to corresponding microservice.</p>&#xA;&#xA;<p>This way you don't have to repeat yourself - you don't have to implement authentication N times for each microservice. </p>&#xA;&#xA;<p>Then, if you need more granular control - what exactly a user can access (usually called authorisation), then you traditionally implement it at each specific microservice.</p>&#xA;&#xA;<p>In short, your microservices shouldn't care if the incoming request is authenticated - it's already been pre-filtered for them. Microservices only decide whether the user X can do action Y.</p>&#xA;&#xA;<p>PS. You can combine API gateway with Passport/Oauth facility or you may run them separately - that's up to you. AWS already offers API gateway as a service (proving how trendy microservices are becoming) but I couldn't find any good open source analogues. </p>&#xA;"
51420294,51419192,7824581,2018-07-19T10:28:06,"<p>create one common entity application and add that entity application as a dependency. For example assume you have stored user data in micorservice1(MC1) and need this class(User) in other microservices(like MC2, MC3,MC4, and so  on) then you can create one entity application like util and add this dependency in required microservices. </p>&#xA;"
40393872,39998701,1608389,2016-11-03T04:26:52,"<p>I'm learning microservices with seneca too.<br>&#xA;Using express version ~4.13.4 and seneca version ^2.0.0 works for me.</p>&#xA;&#xA;<p>More information can be found in seneca getting started examples at this URL:&#xA;<a href=""https://github.com/senecajs-attic/getting-started"" rel=""nofollow noreferrer"">https://github.com/senecajs-attic/getting-started</a></p>&#xA;&#xA;<p>Look at their package.json file. Hope it helps!</p>&#xA;"
51893670,51861930,1608389,2018-08-17T10:42:55,"<p>Seneca is explained by Richard Rodger in <a href=""http://www.richardrodger.com/seneca-microservices-nodejs#.W3aj8p9pFhE"" rel=""nofollow noreferrer"">this post</a>. The chapter ""Service Discovery"" talks about meshing the microservices in a network.</p>&#xA;&#xA;<p>For my applications I use the <a href=""https://github.com/senecajs/seneca-mesh"" rel=""nofollow noreferrer"">seneca-mesh</a> plugin. This plugin README says: </p>&#xA;&#xA;<blockquote>&#xA;  <p>To join the network, all a service has to do is contact one other&#xA;  service already in the network. The network then shares information&#xA;  about which services respond to which patterns. There is no need to&#xA;  configure the location of individual services anywhere.</p>&#xA;</blockquote>&#xA;&#xA;<p>Reading Richard's post and the plugin documentation could be a good starting point for your project. Hope it helps!</p>&#xA;"
47807382,47804267,3514821,2017-12-14T06:46:55,"<p>Through consul builtin UI there is no feature of getting docker metric like cpu, memory .</p>&#xA;&#xA;<p>Instead you can use tools like</p>&#xA;&#xA;<p><a href=""https://github.com/google/cadvisor"" rel=""nofollow noreferrer"">https://github.com/google/cadvisor</a></p>&#xA;"
34502851,34502682,5482437,2015-12-29T00:38:44,"<p>Are you trying to get the posted data from a form/etc?</p>&#xA;&#xA;<p>Try using express.</p>&#xA;&#xA;<p><code>npm install express -save</code></p>&#xA;&#xA;<p>You can get posted data from a url with the ff:</p>&#xA;&#xA;<pre><code>app.post('*', function(request, response){  &#xA;    var post = {};&#xA;    if(Object.keys(request.body).length){&#xA;        for(var key in request.body){&#xA;            post[key] = request.body[key];&#xA;            console.log(key+'=&gt;'+post[key];&#xA;        }&#xA;    }&#xA;});&#xA;</code></pre>&#xA;"
44601753,44597832,2220997,2017-06-17T07:00:24,"<p>Problem with DB per tenant is that you have to add new connection definition to all your app instances, every db instance has its own lifecycle, needs separate configuration for backup, permissions, monitoring, etc. In this regard the schema based approach is easier to implement and scale. At least in a scope of relational DBs. You can also go with discriminator per tenant, so tablespace is shared among tenants but each entry has a unique tenant id as a discriminator column. Depending on your business, you can also mix and match the strategies, e.g.schema per tenant with dedicated db instance for customers with premium plan...</p>&#xA;"
48410872,48384182,393157,2018-01-23T21:10:38,"<p>You basically have to open a node port on the instances where the Kubernetes Pods are running. Then you need to let the ALB point to those instances. There are two ways of configuring this. Either via Pods or via Services.</p>&#xA;&#xA;<p>To configure it via a <strong>Service</strong> you need to specify <code>.spec.ports[].nodePort</code>. In the default setup the port needs to be between <code>30000</code> and <code>32000</code>. This port gets opened on every node and will be redirected to the specified Pods (which might be on any other node). This has the downside that there is another hop, which also can cost money when using a multi-AZ setup. An example Service could look like this:</p>&#xA;&#xA;<pre><code>---&#xA;apiVersion: v1&#xA;kind: Service&#xA;&#xA;metadata:&#xA;  name: my-frontend&#xA;  labels:&#xA;    app: my-frontend&#xA;&#xA;spec:&#xA;  type: NodePort&#xA;  selector:&#xA;    app: my-frontend&#xA;  ports:&#xA;  - port: 8080&#xA;    nodePort: 30082&#xA;</code></pre>&#xA;&#xA;<p>To configure it via a <strong>Pod</strong> you need to specify <code>.spec.containers[].ports[].hostPort</code>. This can be any port number, but it has to be free on the node where the Pod gets scheduled. This means that there can only be one Pod per node and it might conflict with ports from other applications. This has the downside that not all instances will be healthy from an ALB point-of-view, since only nodes with that Pod accept traffic. You could add a sidecar container which registers the current node on the ALB, but this would mean additional complexity. An example could look like this:</p>&#xA;&#xA;<pre><code>---&#xA;apiVersion: extensions/v1beta1&#xA;kind: Deployment&#xA;&#xA;metadata:&#xA;  name: my-frontend&#xA;  labels:&#xA;    app: my-frontend&#xA;&#xA;spec:&#xA;  replicas: 3&#xA;&#xA;  selector:&#xA;    matchLabels:&#xA;      app: my-frontend&#xA;&#xA;  template:&#xA;    metadata:&#xA;      name: my-frontend&#xA;      labels:&#xA;        app: my-frontend&#xA;&#xA;    spec:&#xA;      containers:&#xA;      - name: nginx&#xA;        image: ""nginx""&#xA;&#xA;        ports:&#xA;        - containerPort: 80&#xA;          hostPort: 8080&#xA;</code></pre>&#xA;"
33818507,33814080,202694,2015-11-20T03:33:13,"<p>I wouldn't advise having every developer build every microservice.  I would propose some sort of continuous integration environment.  One centralized build server connected to all of the git repos.  </p>&#xA;&#xA;<p>Each time a repo is updated the build server should detect the change, build the code, run unit (and or functional) tests, and then push the service to some sort of integration environment.  The build server may then also run some integration testing against the deployed service.</p>&#xA;&#xA;<p>Most developers should be able to do all their development and test without needing access to the other microservices.  If a developer is building service X, which depends on Y &amp; Z and is depended on by A &amp; B then the developer should, for the most part, only have service X.  For unit testing services Y &amp; Z should be mocked/simulated.</p>&#xA;&#xA;<p>The challenge is going to be preventing the developer from breaking services A &amp; B by making a change to service X.  That sort of integration testing tends to be trickier as developers working on service X often don't know details (or even how to use) upstream services (e.g. A &amp; B).</p>&#xA;&#xA;<p>The way to tackle that I believe would be to have regular integration testing, either triggered by the build of service X or run on a regular basis.  With a project this complicated a strong and robust unit test philosophy and integration test framework is going to be essential.</p>&#xA;"
49510193,49482516,4274374,2018-03-27T10:14:37,"<p>Here is another one (website states it works on both mac and pc) with free trial <a href=""http://www.conceptdraw.com/solution-park/software-uml"" rel=""nofollow noreferrer"">ConeptDraw</a></p>&#xA;"
47552733,47552159,3624390,2017-11-29T12:19:18,"<p>You can't call getReader a second time. This link explains why: <a href=""https://stackoverflow.com/questions/6322462/resetting-a-httprequest-after-calling-request-getreader"">resetting a HttpRequest after calling request.getReader()</a>. You can pass this information through a thread local if its something globally used through the application. You would need to make sure to reset the thread local appropriately. You could also catch the previous exception, wrap it in a custom exception class, and put this information inside of your custom exception. Then you can throw your custom exception that has the information you need.</p>&#xA;"
52105672,52104915,299462,2018-08-30T21:36:01,"<p>AWS Lambda supports tagging. That is the cleanest way to understand billing on Lambda by tags.</p>&#xA;&#xA;<p>You can tag your microservices for cost allocation and billing.</p>&#xA;&#xA;<p><strong>More on that:</strong></p>&#xA;&#xA;<p><a href=""https://docs.aws.amazon.com/lambda/latest/dg/tagging.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/lambda/latest/dg/tagging.html</a></p>&#xA;&#xA;<p>Hope it helps.</p>&#xA;"
44832047,44786479,5285062,2017-06-29T17:59:39,"<p>I was just browsing through when I checked this:</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/04z2x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/04z2x.png"" alt=""API Gateway Responsibilities""></a></p>&#xA;&#xA;<p>Caching at API Gateway makes lot of sense as it would save call to a service and caching code would not be duplicated at each service but would remain centralized with API Gateway itself.</p>&#xA;"
41758227,41757509,1421254,2017-01-20T07:29:34,"<p>The usual approach is to put every independent component into a separate container. General Docker idea is 1 container = 1 logical task. 1 task is not exactly 1 process, it's just the smallest independent unit. </p>&#xA;&#xA;<p>So you would need to find 4 basic images (probably existing ones from Docker registry should fit):</p>&#xA;&#xA;<ul>&#xA;<li>PHP7-NGINX </li>&#xA;<li>PYTHON-FLASK-NGINX </li>&#xA;<li>MariaDB </li>&#xA;<li>Dgraph</li>&#xA;</ul>&#xA;&#xA;<p>You can use <a href=""https://hub.docker.com/search/"" rel=""nofollow noreferrer"">https://hub.docker.com/search/</a> to search for appropriate images.</p>&#xA;&#xA;<p>Then create custom Docker file for every component (taking either PHP7-NGINX or PYTHON-FLASK-NGINX as a parent image). </p>&#xA;&#xA;<p>You probably would not need custom Docker file for databases. Typically database images require just mounting config file into image using <code>--volume</code> option, or passing environment arguments (see description of base image for details).</p>&#xA;&#xA;<p>After that, you can just write docker-compose.yml and define here how your  images are linked and other parameters. That would look like <a href=""https://github.com/wodby/docker4drupal/blob/master/docker-compose.yml"" rel=""nofollow noreferrer"">https://github.com/wodby/docker4drupal/blob/master/docker-compose.yml</a> .&#xA;By the way, github is full of good examples of docker-compose.yml</p>&#xA;&#xA;<p>If you are going to run services on different servers, then you can create a Swarm cluster, and run your docker-compose.yml against it: <a href=""https://docs.docker.com/compose/swarm/"" rel=""nofollow noreferrer"">https://docs.docker.com/compose/swarm/</a> . After that, you can scale easily by deploying as many instances of each microservice as you need (that's why it's more useful to have separate images for every microservice).</p>&#xA;"
41019800,41019199,2679750,2016-12-07T14:20:01,"<p>In your <code>Startup.cs</code> class, do you have this line? :</p>&#xA;&#xA;<pre><code> public void ConfigureAuth(IAppBuilder app)&#xA; {&#xA;     app.UseCors(Microsoft.Owin.Cors.CorsOptions.AllowAll);&#xA; }&#xA;</code></pre>&#xA;&#xA;<p>There are also a couple NuGet packages associated with Cors:</p>&#xA;&#xA;<pre><code>&lt;package id=""Microsoft.AspNet.Cors"" version=""5.0.0"" targetFramework=""net45"" /&gt;&#xA;&lt;package id=""Microsoft.Owin.Cors"" version=""3.0.1"" targetFramework=""net45"" /&gt;&#xA;</code></pre>&#xA;"
37085791,36760694,2811078,2016-05-07T07:29:30,"<pre><code>import java.io.File;&#xA;import java.util.concurrent.locks.ReentrantLock;&#xA;&#xA;import org.slf4j.Logger;&#xA;import org.slf4j.LoggerFactory;&#xA;import org.springframework.batch.core.Job;&#xA;import org.springframework.batch.core.JobExecution;&#xA;import org.springframework.batch.core.JobParameters;&#xA;import org.springframework.batch.core.JobParametersBuilder;&#xA;import org.springframework.batch.core.JobParametersInvalidException;&#xA;import org.springframework.batch.core.repository.JobExecutionAlreadyRunningException;&#xA;import org.springframework.batch.core.repository.JobInstanceAlreadyCompleteException;&#xA;import org.springframework.batch.core.repository.JobRestartException;&#xA;import org.springframework.beans.factory.annotation.Autowired;&#xA;import org.springframework.beans.factory.annotation.Qualifier;&#xA;import org.springframework.integration.annotation.ServiceActivator;&#xA;import org.springframework.stereotype.Component;&#xA;@Component&#xA;public class BatchJobScheduler  {&#xA;&#xA;    private static final Logger logger = LoggerFactory.getLogger(BatchJobScheduler.class);&#xA;    @Autowired&#xA;protected JobLauncher jobLauncher;&#xA;&#xA;    @Autowired&#xA;    @Qualifier(value = ""job"")&#xA;    private Job job;&#xA;&#xA;    @ServiceActivator(inputChannel = ""cfpFileIn"")&#xA;    public void run(File file) {&#xA;        String fileName = file.getAbsolutePath();&#xA;        logger.info(""BatchJobScheduler Running #################""+fileName);&#xA;        JobParameters jobParameters = new JobParametersBuilder().addString(&#xA;                ""input.file"", fileName).toJobParameters();&#xA;&#xA;&#xA;        try {&#xA;&#xA;            JobExecution execution = jobLauncher.run(job,&#xA;                    jobParameters);&#xA;            logger.info("" BatchJobScheduler Exit Status : "" + execution.getStatus() +""::""+execution.getAllFailureExceptions());&#xA;&#xA;        } catch (JobExecutionAlreadyRunningException | JobRestartException&#xA;                | JobInstanceAlreadyCompleteException&#xA;                | JobParametersInvalidException e) {&#xA;            logger.error("" BatchJobScheduler Exit Status : Exception ::"",e);&#xA;        }&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
37085731,36760694,2811078,2016-05-07T07:22:31,"<pre><code>   # create one configuration file and bind an input file channel here&#xA;&#xA;    &lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;&#xA;    &lt;beans xmlns=""http://www.springframework.org/schema/beans""&#xA;        xmlns:int=""http://www.springframework.org/schema/integration""&#xA;        xmlns:int-file=""http://www.springframework.org/schema/integration/file""&#xA;        xmlns:int-mail=""http://www.springframework.org/schema/integration/mail""&#xA;        xmlns:int-stream=""http://www.springframework.org/schema/integration/stream""&#xA;        xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:context=""http://www.springframework.org/schema/context""&#xA;        xsi:schemaLocation=""http://www.springframework.org/schema/integration/mail http://www.springframework.org/schema/integration/mail/spring-integration-mail.xsd&#xA;            http://www.springframework.org/schema/integration http://www.springframework.org/schema/integration/spring-integration.xsd&#xA;            http://www.springframework.org/schema/integration/file http://www.springframework.org/schema/integration/file/spring-integration-file.xsd&#xA;            http://www.springframework.org/schema/integration/stream http://www.springframework.org/schema/integration/stream/spring-integration-stream.xsd&#xA;            http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&#xA;            http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd""&gt;&#xA;&#xA;        &lt;int:annotation-config /&gt;&#xA;            &lt;int:channel id=""cfpFileIn""&gt;&lt;/int:channel&gt;&#xA;    &lt;int-file:inbound-channel-adapter id=""cfpFileIn""&#xA;            directory=""${cfp.flight.data.dir}"" auto-startup=""true"" scanner=""csvDirScanner""&gt;&#xA;            &lt;int:poller fixed-delay=""${cfp.flight.data.dir.polling.delay}""&gt;&lt;/int:poller&gt;&#xA;        &lt;/int-file:inbound-channel-adapter&gt;&#xA;    &lt;bean id=""csvDirScanner""&#xA;            class=""org.springframework.integration.file.WatchServiceDirectoryScanner""&gt;&#xA;            &lt;constructor-arg index=""0"" value=""${cfp.flight.data.dir}"" /&gt;&#xA;            &lt;property name=""filter"" ref=""csvCompositeFilter"" /&gt;&#xA;            &lt;property name=""autoStartup"" value=""true"" /&gt;&#xA;        &lt;/bean&gt;&#xA;&#xA;    &lt;bean id=""csvCompositeFilter""&#xA;            class=""org.springframework.integration.file.filters.CompositeFileListFilter""&gt;&#xA;            &lt;constructor-arg&gt;&#xA;                &lt;list&gt;&#xA;                    &lt;bean&#xA;                        class=""org.springframework.integration.file.filters.SimplePatternFileListFilter""&gt;&#xA;                        &lt;constructor-arg value=""*.csv"" /&gt;&#xA;                    &lt;/bean&gt;&#xA;                    &lt;ref bean=""persistentFilter"" /&gt;&#xA;                &lt;/list&gt;&#xA;            &lt;/constructor-arg&gt;&#xA;        &lt;/bean&gt;&#xA;&#xA;    &lt;bean id=""persistentFilter""&#xA;            class=""org.springframework.integration.file.filters.FileSystemPersistentAcceptOnceFileListFilter""&gt;&#xA;            &lt;constructor-arg index=""0"" ref=""metadataStore"" /&gt;&#xA;            &lt;constructor-arg index=""1"" name=""prefix"" value="""" /&gt;&#xA;            &lt;property name=""flushOnUpdate"" value=""true"" /&gt;&#xA;        &lt;/bean&gt;&#xA;&#xA;    &lt;bean name=""metadataStore""&#xA;            class=""org.springframework.integration.metadata.PropertiesPersistingMetadataStore""&gt;&#xA;            &lt;property name=""baseDirectory"" value=""${metadata.dir}""&gt;&lt;/property&gt;&#xA;        &lt;/bean&gt;&#xA;&lt;/beans&gt;&#xA;</code></pre>&#xA;"
48915546,48914388,1240518,2018-02-21T21:11:17,"<p>The short answer might be to break up the parts logically into modules and then require them where needed.</p>&#xA;&#xA;<p>Without knowing how your code is organized, here is a simple example that might get you moving in the right direction.</p>&#xA;&#xA;<h2>Before</h2>&#xA;&#xA;<p><strong>index.js</strong></p>&#xA;&#xA;<pre><code>// index.js&#xA;'use strict';&#xA;&#xA;const answer = 42;&#xA;const run = (output) =&gt; console.log(`Inside: run() - ${output}`);&#xA;&#xA;run(answer); // Inside: run() - 42&#xA;</code></pre>&#xA;&#xA;<h2>After</h2>&#xA;&#xA;<p><strong>index.js</strong></p>&#xA;&#xA;<pre><code>// index.js&#xA;'use strict';&#xA;&#xA;const process = require('./modules/process');&#xA;const answer = 42;&#xA;&#xA;process.run(answer); // Inside: process.run() - 42&#xA;</code></pre>&#xA;&#xA;<p><strong>modules/process.js</strong></p>&#xA;&#xA;<pre><code>// modules/process.js&#xA;'use strict';&#xA;&#xA;const run = (output) =&gt; console.log(`Inside: process.run() - ${output}`);&#xA;&#xA;module.exports = { run };&#xA;</code></pre>&#xA;&#xA;<h2>Steps</h2>&#xA;&#xA;<ol>&#xA;<li>Create the module file. One option is to create a <code>modules</code> folder and put the module files in there.</li>&#xA;<li>Add the <code>require</code> statement at the top of the original <code>index.js</code> file. </li>&#xA;<li>In the new module file, move over a function.</li>&#xA;<li>Add the function to the <code>module.exports</code>.</li>&#xA;<li>Change the call to use the module version.</li>&#xA;</ol>&#xA;&#xA;<p><strong>Pointers</strong></p>&#xA;&#xA;<ul>&#xA;<li>Baby steps: Move one function at a time and make sure that you haven't broken anything before moving the next function.</li>&#xA;<li>Try it: There are many variations and ways to organize the code. Don't be too afraid that you will ""do it wrong"".</li>&#xA;</ul>&#xA;"
51251726,51242037,1113542,2018-07-09T18:15:39,"<p>Analyzing to your problem, don’t you think Product and Owner cannot have many to many relationship.&#xA;How can be one product could be owned by two different owners (users)?&#xA;Generally, product ID in the e-commerce system identifies 1 unique physical entity. Even if there are more than 1 number of similar products (which is an actual production scenario) IMO each product will have different product ID.</p>&#xA;&#xA;<p>But still, let’s consider some different case where two entities are dependent on each other. In here the concepts that come in a picture is <strong>bounded context</strong>.</p>&#xA;&#xA;<p>Every service should own its data and should be responsible for its integrity and mutability. Each service should exist independently i.e. with the ability to be changed and moved in and out of a runtime environment without side effects to other services.</p>&#xA;&#xA;<p>Let’s take a clearer example of company which needs to manage <strong>Project</strong> among their <strong>employees</strong>. We could see two different contexts here (right now ignoring all other company related handling) </p>&#xA;&#xA;<ol>&#xA;<li>Project</li>&#xA;<li>Employee</li>&#xA;</ol>&#xA;&#xA;<p>As depicted in the picture, we cannot have a direct reference of Employee entity in the Project entity.</p>&#xA;&#xA;<p>The graceful solution is: assuming the object models are mapped to a relational data source, instead of the Project Service having to map an Employee type, it only must map the <strong>employee id</strong> attribute.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/6NHkf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6NHkf.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>The underlying mapped table for the Project_Resource model will not materialize an Employee object from the database. In order to obtain or mutate an Employee, you will utilize the employee service API calls from employee context.</p>&#xA;&#xA;<p>With this, there are some more mechanisms that must be introduced to support this isolation. Specifically, when an employee is deleted through the employee service, how are other services made aware of this deletion? Employee service synchronously calling Product service will result in the high coupling. The asynchronous pattern should be used in this case to decouple other services (more towards publishing the event and another service could decide if they have to take action or not)</p>&#xA;&#xA;<p>The following blog explains very nicely other use case and other solution depicting the use of Bounded context taking care of low coupling and promoting cohesion.&#xA;<a href=""http://The%20following%20blog%20explains%20very%20nicely%20other%20use%20case%20and%20other%20solution%20depicting%20the%20use%20of%20Bounded%20context%20taking%20care%20of%20low%20coupling%20and%20promoting%20cohesion.%20%20https://hackernoon.com/microservices-bounded-context-cohesion-what-do-they-have-in-common-1107b70342b3"" rel=""nofollow noreferrer"">https://hackernoon.com/microservices-bounded-context-cohesion-what-do-they-have-in-common-1107b70342b3</a></p>&#xA;"
40110797,40097751,526535,2016-10-18T14:21:23,"<p>I followed the steps in the article you linked and it works as expected.</p>&#xA;&#xA;<p>Just do:</p>&#xA;&#xA;<pre><code>minikube service hello-minikube --url&#xA;</code></pre>&#xA;&#xA;<p>You will get a url like <code>http://192.168.99.100:32382/</code> - the port and IP could and will change for you. Also note that the exposed port will be a random port like the <code>32382</code> and not <code>8080</code> that the pod uses.</p>&#xA;&#xA;<p>Use the url in your browser, say and you should be able to see the output of the service.</p>&#xA;"
46147193,46065028,727495,2017-09-11T01:51:23,<p>I have used spring framework's <code>MappingJackson2HttpMessageConverter</code> to deserialize the object. Here is my implementation code.</p>&#xA;&#xA;<p><em>Write a static class with the below method.</em></p>&#xA;&#xA;<pre><code>public class MappingJacksonFactory {&#xA;&#xA;public static MappingJackson2HttpMessageConverter create() {&#xA;    MappingJackson2HttpMessageConverter converter = new MappingJackson2HttpMessageConverter();&#xA;    ObjectMapper objectMapper = converter.getObjectMapper();&#xA;    objectMapper.enable(DeserializationFeature.USE_BIG_DECIMAL_FOR_FLOATS);&#xA;    objectMapper.enable(SerializationFeature.WRITE_BIGDECIMAL_AS_PLAIN);&#xA;    SimpleModule module = new TypeModuleFactory();&#xA;    objectMapper.registerModule(module);&#xA;    return converter;&#xA;   }&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p><em>Use the above code inside <code>setMessageConverter()</code> method&#xA;converters.</em></p>&#xA;&#xA;<pre><code>RestAssuredMockMvc.standaloneSetup(MockMvcBuilders.standaloneSetup(new yourController()))&#xA;            .setMessageConverters(MappingJacksonFactory.create());&#xA;</code></pre>&#xA;
49609493,49606124,92359,2018-04-02T10:24:24,"<p>The pattern of database-per-service is <a href=""http://microservices.io/patterns/data/database-per-service.html"" rel=""nofollow noreferrer"">discussed here</a>, and breaks down the options to:</p>&#xA;&#xA;<ol>&#xA;<li>Shared database, but private tables per service.</li>&#xA;<li>Shared database, but private schema per service.</li>&#xA;<li>Separate database per service.</li>&#xA;</ol>&#xA;&#xA;<p>Obviously 3 is going to be the most expensive, as you would want each Neo4j instance to be on its own server so it has dedicated memory and hardware, and if you need a clustering solution then this becomes a separate cluster-per-service. Not recommended, especially if just ideology is the driver for this decision.</p>&#xA;&#xA;<p>1 and 2 are better options, especially if the data accessed across microservices is related intrinsically, as Neo4j works best when storing connected data, and loses its value the more the data is siloed between multiple databases.</p>&#xA;&#xA;<p>That said, there are some challenges with these options.</p>&#xA;&#xA;<p>Neo4j does not use tables, and does not currently have separate schema to divide visibility of data between different users.</p>&#xA;&#xA;<p>While you can have the microservice only use defined queries that only touch specific parts of the graph, this is often looser control than is required.</p>&#xA;&#xA;<p>You can instead use the <a href=""https://neo4j.com/docs/operations-manual/current/security/authentication-authorization/subgraph-access-control/"" rel=""nofollow noreferrer"">subgraph access control</a> approach, and this is the one I would most recommend for your needs.</p>&#xA;&#xA;<p>This boils down to creating procedures that encapsulate the queries you want to be available for each microservice (that either use the Java API directly, or make a Cypher query from the procedure code), and then creating custom roles for each microservice (without read permissions), but allowing them to invoke the appropriate procedures. This ensures that the custom role per microservice is only allowed to interact with the graph through the allowed procedures (and the procedures of course can do whatever they want without being constrained by the roles or permissions of the calling user).</p>&#xA;&#xA;<p>As far as a multi-tenancy approach, keeping data separate between different graphs on a single database, that is a feature of high interest right now, and implementation is in the works. Look for this in upcoming releases in 2018. That said, whether this would be useful to you or not depends upon the implementation of this feature, and the nature of your data.</p>&#xA;"
49361326,49324723,7173575,2018-03-19T10:58:55,"<p>Registering the actual microservices with <code>eureka.instance.hostname=localhost</code> or <code>eureka.instance.ip-address=127.0.0.1</code> to the Eureka server, combined with a binding of the microservice to localhost (<code>server.address=127.0.0.1</code>) did the job.</p>&#xA;&#xA;<p>These are the application.properties files:</p>&#xA;&#xA;<p><strong>application.properties of my Eureka service discovery:</strong></p>&#xA;&#xA;<pre><code>spring.application.name=service-discovery&#xA;server.port=8761&#xA;server.address=127.0.0.1&#xA;eureka.client.registerWithEureka=false&#xA;eureka.client.fetchRegistry=false&#xA;</code></pre>&#xA;&#xA;<p><strong>application.properties of my Zuul API gateway:</strong></p>&#xA;&#xA;<pre><code>spring.application.name=api-gateway&#xA;zuul.prefix=/api&#xA;server.port=8080&#xA;ribbon.eureka.enabled=true&#xA;eureka.client.registerWithEureka=false&#xA;zuul.routes.library.path=/library/**&#xA;zuul.routes.library.serviceId=library&#xA;zuul.routes.library.stripPrefix=false&#xA;</code></pre>&#xA;&#xA;<p><strong>application.properties of my actual REST service:</strong></p>&#xA;&#xA;<pre><code>spring.application.name=library&#xA;server.servlet.context-path=/library&#xA;server.port=8090&#xA;server.address=127.0.0.1&#xA;ribbon.eureka.enabled=true&#xA;eureka.client.registerWithEureka=true&#xA;eureka.instance.hostname=localhost&#xA;eureka.instance.ip-address=127.0.0.1&#xA;</code></pre>&#xA;&#xA;<p>The ""library"" microservice is now only available from localhost but still registered at Eureka and behind the Zuul API gateway.</p>&#xA;"
51124326,50830715,6465223,2018-07-01T14:47:11,"<p>So I ran into this as well.</p>&#xA;&#xA;<p>Basically there is some sort of network timeout that happens on AKS that cuts all connections out of a pod.  As you mentioned this results in seemingly random errors that are difficult to trouble shoot since you only get to see them once (as hitting the same service again results in the expected correct behavior).</p>&#xA;&#xA;<p>More details on my question here:&#xA;<a href=""https://stackoverflow.com/questions/50706483/what-azure-kubernetes-aks-time-out-happens-to-disconnect-connections-in-out"">What Azure Kubernetes (AKS) &#39;Time-out&#39; happens to disconnect connections in/out of a Pod in my Cluster?</a></p>&#xA;&#xA;<p>In my case AKS (or potentially Kubernetes) was disconnecting / severing my Ghost blog connection to my database after a time but not notifying the service which then resulted in strange errors related to the service not realizing that it was disconnected and not being able to continue to utilize the connection it expects to be available / open.</p>&#xA;&#xA;<p>Thats not a solution just more background!</p>&#xA;&#xA;<p>I am debating whether to open a ticket on Azure AKS GitHub (and with my support subscription) to request more information.  If I hear back I will update this answer!</p>&#xA;"
51432586,50830715,6465223,2018-07-19T22:32:49,"<p>Finally figured this out.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Azure Load Balancers / Public IP addresses have a default 4 minute idle connection timeout.</p>&#xA;</blockquote>&#xA;&#xA;<p>Essentially anything running through a Load Balancer (whether created by an Azure AKS Kubernetes Ingress or otherwise) has to abide by this.  While you CAN change the timeout there is no way to eliminate it entirely (longest idle timeout duration possible is 30 minutes).</p>&#xA;&#xA;<p>For that reason it makes sense to implement a connection pooling / monitoring solution that will track the idle time that has elapsed on each of your connections (through the load balancer / public IP) and then disconnect / re-create any connection that gets close to the 4 minute cutoff.</p>&#xA;&#xA;<p>We ended up implementing PGbouncer (<a href=""https://github.com/pgbouncer/pgbouncer"" rel=""nofollow noreferrer"">https://github.com/pgbouncer/pgbouncer</a>) as an additional container on our Azure AKS / Kubernetes Cluster via the awesome directions which can be found here: <a href=""https://github.com/edoburu/docker-pgbouncer/tree/master/examples/kubernetes/singleuser"" rel=""nofollow noreferrer"">https://github.com/edoburu/docker-pgbouncer/tree/master/examples/kubernetes/singleuser</a></p>&#xA;&#xA;<p>Overall I can see the need for the timeout but MAN was it hard to troubleshoot.  Hope this saves you guys some time!</p>&#xA;&#xA;<p>More details can be found on my full post over here: <a href=""https://stackoverflow.com/questions/50706483/what-azure-kubernetes-aks-time-out-happens-to-disconnect-connections-in-out"">What Azure Kubernetes (AKS) &#39;Time-out&#39; happens to disconnect connections in/out of a Pod in my Cluster?</a></p>&#xA;"
40206404,40201069,1105351,2016-10-23T18:29:00,<p>Did you use mongo(same as the container name mentioned in docker-compose.yml) as your host in mongoid.yml?</p>&#xA;
49993084,49992237,1304519,2018-04-24T03:44:48,"<p>You did not explain what kind of test you want to do. The way you are doing it it seems you are running instances with database so I treat it more like an integration test.</p>&#xA;&#xA;<p>I assume you have written unit tests before doing this.</p>&#xA;&#xA;<p>If you wish to test independently I suggest making use of <a href=""http://onsi.github.io/ginkgo/"" rel=""nofollow noreferrer"">http://onsi.github.io/ginkgo/</a></p>&#xA;&#xA;<p>This will help you test most of the apis and business scenario and test the flow of your orderService and userService individually.</p>&#xA;"
50826080,50800231,9829319,2018-06-12T21:28:17,"<p>If you dont use same types on both of your services, you can try:</p>&#xA;&#xA;<p><a href=""https://github.com/aeb-labs/graphql-weaver"" rel=""nofollow noreferrer"">https://github.com/aeb-labs/graphql-weaver</a></p>&#xA;"
50828795,50814965,3016392,2018-06-13T03:45:44,"<p>I do not know how to edit my own question above as I do not see the edit option hence I'm adding the additional details here to get the resolution of my query.</p>&#xA;&#xA;<p><strong>Controller class:-</strong></p>&#xA;&#xA;<pre><code>@RestController&#xA;</code></pre>&#xA;&#xA;<p>public class CategorySearchController {</p>&#xA;&#xA;<pre><code>private final CategorySearchService categorySearchService;&#xA;&#xA;@Autowired&#xA;public CategorySearchController(CategorySearchService categorySearchService) {&#xA;    this.categorySearchService = categorySearchService;&#xA;}&#xA;&#xA;&#xA;@GetMapping(path = ""/search-category"")&#xA;public Mono&lt;CategoryResponse&gt; searchCategories(SearchRequest categorySearchRequest){&#xA;    return categorySearchService&#xA;            .searchCategories()&#xA;            .switchIfEmpty(Mono.error(&#xA;                    new EntityNotFoundException(""No category matching "" + categorySearchRequest.getSearchTerm() + "" was found"")));&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>}</p>&#xA;&#xA;<p><strong>Service class:-</strong></p>&#xA;&#xA;<pre><code>@Service&#xA;public class CategorySearchServiceImpl implements CategorySearchService {&#xA;    private String baseUrl = ""https://demo0954903.mockable.io"";&#xA;&#xA;    @Override&#xA;    public Mono&lt;CategoryResponse&gt; searchCategories() {&#xA;        WebClient webClient = WebClient.create(baseUrl);&#xA;        return webClient.&#xA;                 get()&#xA;                .uri(""/category-search"")&#xA;                .retrieve()&#xA;                .bodyToMono(CategoryResponse.class);&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
50853389,50814965,3016392,2018-06-14T08:50:13,<p>I found the solution for this issue. I need to add the proxy in the webclient as follows:-</p>&#xA;&#xA;<pre><code>private final WebClient webClient;&#xA;ReactorClientHttpConnector connector = new ReactorClientHttpConnector(&#xA;                options -&gt; options.httpProxy(addressSpec -&gt; {&#xA;                return addressSpec.host(PROXY_HOST).port(PROXY_PORT);&#xA;                }));&#xA;&#xA;        this.webClient = webClientBuilder.baseUrl(BASE_URL).clientConnector(connector).build();&#xA;</code></pre>&#xA;
50853523,50828793,3016392,2018-06-14T08:56:38,<p>I found the solution for this issue. I need to add the proxy in the webclient as follows:-</p>&#xA;&#xA;<pre><code>private final WebClient webClient;&#xA;ReactorClientHttpConnector connector = new ReactorClientHttpConnector(&#xA;                options -&gt; options.httpProxy(addressSpec -&gt; {&#xA;                return addressSpec.host(PROXY_HOST).port(PROXY_PORT);&#xA;                }));&#xA;&#xA;        this.webClient = webClientBuilder.baseUrl(BASE_URL).clientConnector(connector).build();&#xA;</code></pre>&#xA;
46623257,46623194,2558152,2017-10-07T17:45:43,"<p>Yes. Definitely you can choose technology of your choice for developing front end application. From your front end application, you make calls to API endpoint that you expose via your spring boot application. </p>&#xA;&#xA;<p>You might want to expose your services via single API gateway that will help you route requests to designated micro services using your discovery server.</p>&#xA;"
50449581,50448620,8927944,2018-05-21T13:05:02,"<p>Just as <code>Labels</code>, <code>Annotations</code> are key-value pairs which represent metadata that is attached to a Kubernetes object. &#xA;But contrary to <code>Labels</code>, which are internally utilized to find a collection of objects which satisfy specific conditions, the purpose of <code>Annotations</code> is simply to attach relevant metadata, which should not be used as a filter to identify those objects.</p>&#xA;&#xA;<p>What if we wanted to describe whose person was responsible for generating a specific .yaml file? </p>&#xA;&#xA;<p>We could attach such information to the Kubernetes's object, so that when we need to know who created such object, we can simply run <code>kubectl describe ...</code></p>&#xA;&#xA;<p>Another useful example, could be to add an annotation to a <code>Deployment</code> before a rollout, explaining what modifications occurred on the new version of the Deployment object. That information could be retrieved later while checking the history of your deployment versions.</p>&#xA;&#xA;<p>But as you have realized with the <code>Ingress</code> example, with <code>Annotations</code> we can also perform advanced configuration on such objects. This is not limited only to Ingress, and for instance you can also provide configuration for running Prometheus on a Kubernetes cluster. You can check the details <a href=""https://prometheus.io/docs/prometheus/latest/configuration/configuration/"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;"
51525322,51524648,9977800,2018-07-25T18:16:06,"<p>Well this is an interesting question. In our team we discussed about this topic a lot. Basically you have some parameters affecting the individual answer to this question. But you should always decode and verify granted tokens on the microservice level, too. Because they contain relevant information for authentication and in some cases even for authorization. If your microservices run in a enclosed environment (e.g. on enclosed Kubernetes cluster, where only the API-Gateway is available to the outside) you could use this ""mixed"" solution.</p>&#xA;&#xA;<p>You can really consider just to verify the AccessToken at the API-Gateway and let the other microservices rely on the API Gateway. The API Gateway could than exchange the AccessToken into another AuthToken, only valid in the microservice-context. This new generated AuthToken can for example contain more sensitive application-bound information, because it is not exposed to the client. The Client gets only a so called opaque token. See <a href=""https://medium.com/tech-tajawal/microservice-authentication-and-authorization-solutions-e0e5e74b248a"" rel=""nofollow noreferrer"">https://medium.com/tech-tajawal/microservice-authentication-and-authorization-solutions-e0e5e74b248a</a>&#xA;<a href=""https://i.stack.imgur.com/m3hUW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m3hUW.png"" alt=""enter image description here""></a></p>&#xA;"
45627425,45626777,3839944,2017-08-11T05:15:12,"<p>You should use Queues to handle the tasks that needs not to be completed in real time.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Append the tasks in queue and when there is a room, processor will take tasks from queue and will handle &amp; will remove from queue.</p>&#xA;</blockquote>&#xA;&#xA;<p><strong>Example :</strong> </p>&#xA;&#xA;<blockquote>&#xA;  <ol>&#xA;  <li><p>Assuming your application deals with images, users are uploading so many images. Upload the tasks in a queue to compress the images. And when processor is free it will compress the queued images.</p></li>&#xA;  <li><p>When you want to write some kind of logs of your system, give it to the queue and one process will take logs from queue and write that to disk. So the main process will not waste its time for the I/O operations.</p></li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p><strong>Suggestion :</strong> </p>&#xA;&#xA;<blockquote>&#xA;  <p>If you want the real time responses, you should not use the queue. You need to ping the queue constantly to read the incomings, and that is bad practice. And there is no guarantee that queue will handle your tasks immediately.</p>&#xA;</blockquote>&#xA;&#xA;<p><strong>So the solutions are :</strong> </p>&#xA;&#xA;<blockquote>&#xA;  <ol>&#xA;  <li><p><strong>Redis cache</strong> - You can put your messages into cache and other process will read that message. Redis is ""In memory data-structure"". It is very fast and easy to use. Too much libraries and good resources available on the Internet, as it is open source. <a href=""https://redis.io/"" rel=""nofollow noreferrer"">Read more about Redis.</a> But here you also need to keep check whether there is some kind of message available and if available read from it, process and give response. But to read from Redis, is not very much costlier. With redis, you do not need to worry about memory management, it is well managed by open source community.</p></li>&#xA;  <li><p><strong>Using Sockets</strong>. Socket is very much faster, you can make this lightweight(if you want) as it is event based. One process will ping on port and other process will listen and give response. But you need to manage memory. If the buffered memory gets full, you can not put more messages here. If there are so many users producing messages, you need to manage to whom to you want to respond.</p></li>&#xA;  </ol>&#xA;</blockquote>&#xA;&#xA;<p>So it depends upon your requirement, like do you want to read messages constantly?, do you want to make one to one communication or many to one communication?</p>&#xA;"
37508534,36571963,569544,2016-05-29T10:01:04,"<p>We ended up using git dependencies and not devpi.&#xA;I think when git is used, there is no need to add another package repository as long as pip can use this.</p>&#xA;&#xA;<p>The core issue, where the branch code (because of a second level dependency) is different from the one merged to master is not solved yet, instead we work around that by working to remove that second level dependency.</p>&#xA;"
46661140,46660853,1360584,2017-10-10T07:40:45,"<p>I don't know if it is the best solution, but it is the one i used: &#xA;First say to the spring boot maven plugin to create an executable jar : </p>&#xA;&#xA;<pre><code>&lt;configuration&gt;&#xA;    &lt;executable&gt;true&lt;/executable&gt;&#xA;&lt;/configuration&gt;&#xA;</code></pre>&#xA;&#xA;<p>After that you can add your application as a service in <code>init.d</code> and make it start when the container starts. &#xA;You can find a better explaination here : <a href=""http://www.baeldung.com/spring-boot-app-as-a-service"" rel=""nofollow noreferrer"">http://www.baeldung.com/spring-boot-app-as-a-service</a></p>&#xA;"
46587928,46585082,1360584,2017-10-05T14:17:15,"<p>Two solutions : </p>&#xA;&#xA;<p><strong>Async call from your client :</strong> &#xA;Spring provides an Asynchronous version of the RestTemplate :&#xA;<a href=""https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/web/client/AsyncRestTemplate.html"" rel=""nofollow noreferrer"">AsyncRestTemplate</a>&#xA;with this solution, your client is asynchronous, you don't need to store the data in a table with the transaction id and stuff. </p>&#xA;&#xA;<p><strong>Make your endpoint Asynchronous (if you don't need the response) :</strong> &#xA;Spring lets you create <a href=""https://spring.io/guides/gs/async-method/"" rel=""nofollow noreferrer"">asynchronous methods(services)</a> that you can call from your RestController. With this solution you can do what you described in the question(creating and storing a transaction id that will be returned directly to the client and start the async job). </p>&#xA;"
29162755,29117570,317384,2015-03-20T08:57:51,"<p>Trying to aggregate the different approaches here.</p>&#xA;&#xA;<h1>Domain Events</h1>&#xA;&#xA;<p>The dominant approach for this seems to be using domain events, where each service publish events regarding what have happened and other services can subscribe to those events.&#xA;This seems to go hand in hand with the concept of <strong>smart endpoints, dumb pipes</strong> that is described by Martin Fowler here: <a href=""http://martinfowler.com/articles/microservices.html#SmartEndpointsAndDumbPipes"" rel=""noreferrer"">http://martinfowler.com/articles/microservices.html#SmartEndpointsAndDumbPipes</a></p>&#xA;&#xA;<p><img src=""https://i.stack.imgur.com/aC5hg.png"" alt=""Domain events""></p>&#xA;&#xA;<h1>Proxy</h1>&#xA;&#xA;<p>Another apporach that seems common is to wrap the business flow in its own service.&#xA;Where the proxy orchestrates the interaction between the microservices like shown in the below picture:</p>&#xA;&#xA;<p><img src=""https://i.stack.imgur.com/DAehC.png"" alt=""Proxies"">.</p>&#xA;"
44133188,44129347,7527624,2017-05-23T11:12:30,<p>Need to setup messaging for that. </p>&#xA;&#xA;<p>It’s common to use a persistent queue such as RabbitMQ. The microservice responsible for sending emails then consumes the messages from the queue and handles them appropriately. </p>&#xA;&#xA;<p>If you run into a problem of your single instance of email microservice not being enough you can simply fork another instance and deploy it instantly. This is because when a message from the message queue is consumed it’s gone unless you tell it to return (to be requeued). I.e. any successfully sent email will consume the the message hence the request to send an email is no longer within the system.</p>&#xA;
46612687,46605663,7203016,2017-10-06T19:27:49,"<p>I know I'm spitting into the wind here, but since.</p>&#xA;&#xA;<ol>&#xA;<li>these are <em>your</em> types for <em>your</em> microservices in <em>your</em> project;</li>&#xA;<li>you want to separate instances from the type definitions only as a mechanism to separate concerns and not to ""open"" the type definition to random instances defined all over the place;</li>&#xA;<li>you presumably aren't planning to release this as a general-purpose library for others where some miscreant would define an incompatible <code>Store</code> instance on one of your types;</li>&#xA;</ol>&#xA;&#xA;<p>then it seems reasonable to assume that your orphan instances -- despite being orphans -- can reasonably be taken as globally unique instances defined for those type/class combinations.  So, this would be a justified use, if ever there was one, of <code>-fno-warn-orphans</code>.</p>&#xA;&#xA;<p>The alternatives you mention for avoiding orphans (newtype wrappers and per-microservice type definitions) sound tiresome and error-prone, respectively, so much so that it seems preferable to keep the instances with the types despite the unnecessary dependencies.</p>&#xA;"
44451332,44435017,6555593,2017-06-09T07:01:01,"<p>Keep the ""port number same in ServiceManifest.xml file for both the Microservices.""</p>&#xA;&#xA;<p>Just make the URL different for different services</p>&#xA;&#xA;<p>For Ex localhost:8100/service1/....<br>&#xA; localhost:8100/service2/....</p>&#xA;&#xA;<p>""Different appRoot for different services""</p>&#xA;"
48930424,48909754,1466825,2018-02-22T14:52:59,"<p>The RabbitMQ team monitors <a href=""https://groups.google.com/forum/#!forum/rabbitmq-users"" rel=""nofollow noreferrer"">this mailing list</a> and only sometimes answers questions on StackOverflow.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>Your approach seems reasonable. Topic exchanges and queues bound to that topic is what I would have suggested. Step 1 of your process does require a single queue to synchronize handshake messages with all of your conusmers, which could become a bottleneck depending on your message rate.</p>&#xA;&#xA;<p>Remember that a message that can't be routed anywhere will be lost, so I would suggest reading about alternate exchanges here - <a href=""https://www.rabbitmq.com/ae.html"" rel=""nofollow noreferrer"">https://www.rabbitmq.com/ae.html</a></p>&#xA;"
46798450,46746885,484041,2017-10-17T20:13:44,"<p>No matter if you use DDD or not, the four tenets of SOA are:</p>&#xA;&#xA;<ul>&#xA;<li>Services have explicit boundaries </li>&#xA;<li>Services are autonomous </li>&#xA;<li>Services share schema and contract, not class </li>&#xA;<li>Services interoperate based on policy</li>&#xA;</ul>&#xA;&#xA;<p>One of the consequences of this is that services never share their persistence.</p>&#xA;&#xA;<p>There are discussions about how service boundaries align with bounded contexts, but you can start simple and have one-to-one alignment to start with and bounded context is also something that never exposes it's persistence in any other way than using contracts.</p>&#xA;"
47431034,47427629,484041,2017-11-22T09:31:00,"<p>In general, your understanding is correct. However, I will summarise here too:</p>&#xA;&#xA;<ul>&#xA;<li>Events are used in pub/sub. Messages are published, and all subscribers get them. Published does not know how many subscribers will get the event, if any.</li>&#xA;<li>Commands are sent to a known address. There is only one subscriber, which will get this message. This is used for fire and forget.</li>&#xA;<li>Responses are also sent to a specific endpoint, with additional metadata like response address. So the consumer can do what it needs to do and send a reply back. This is done asynchronously, but the sender waits for response.</li>&#xA;</ul>&#xA;&#xA;<p>MassTransit sagas with Automatonymous support any type of message processing. You need to map all messages that saga consumes as state machine events, but these can be both commands and events - technically it doesn't matter. Sagas can publish and send messages, also can send requests and wait for replies.</p>&#xA;&#xA;<p>As you question about publishing commands and using events for request-response. Techicanlly, MassTransit has no distinction in message types. Everything you publish is an event. Stuff you send can be a command or it can be something else, but this is not an event. When you use request-response, you have to send to a specific endpoint, so definitely this is not an event.</p>&#xA;"
43455148,43424176,484041,2017-04-17T16:13:20,"<p>If you add more consumers to your endpoint, like this</p>&#xA;&#xA;<pre><code>rabbit.ReceiveEndpoint(rabbitMqHost, ""mycompany.domains.queues"", conf =&gt;&#xA;{&#xA;    conf.Consumer&lt;RegisterCustomerConsumer&gt;();&#xA;    conf.Consumer&lt;ReserveTicketConsumer&gt;();&#xA;    conf.Consumer&lt;RefundTicketConsumer&gt;();&#xA;});&#xA;</code></pre>&#xA;&#xA;<p>And send messages like</p>&#xA;&#xA;<pre><code>await endpoint.Send&lt;IReserveTicket&gt;(new { TickedId = 123 });&#xA;</code></pre>&#xA;&#xA;<p>It will just work.</p>&#xA;&#xA;<p>The above solution assumes you do not heavy load, especially unequal load where you get millions of messages of one type and may be hundreds of other types. Having all of them in one endpoint will create consumption misbalance since there is only one queue for all those consumers. In such case, nothing stops you from defining as many endpoints as you need, each of them should have a separate queue. For example:</p>&#xA;&#xA;<pre><code>cfg.ReceiveEndpoint(rabbitMqHost, ""mycompany.domains.lowvolume"", &#xA;    c =&gt;&#xA;    {&#xA;        c.Consumer&lt;RegisterCustomerConsumer&gt;();&#xA;        c.Consumer&lt;RefundTicketConsumer&gt;();&#xA;    });&#xA;cfg.ReceiveEndpoint(rabbitMqHost, ""mycompany.domains.highvolume"", &#xA;    c =&gt; c.Consumer&lt;ReserveTicketConsumer&gt;();&#xA;</code></pre>&#xA;&#xA;<p>Just remember that since you have different queues, you need to use these addresses to get send endpoints.</p>&#xA;"
43500339,43497790,484041,2017-04-19T15:42:39,"<p>You can check the whole request/response sample in the <a href=""https://github.com/MassTransit/Sample-RequestResponse"" rel=""nofollow noreferrer"">Github repository</a>.</p>&#xA;&#xA;<p>To use request/response you need to use the request client as described in the <a href=""http://masstransit-project.com/MassTransit/usage/request-response.html"" rel=""nofollow noreferrer"">documentation</a>.</p>&#xA;&#xA;<p>On the request consumer, you need to use <code>context.Respond(...)</code> to send the response to your request client.</p>&#xA;&#xA;<p>Hence that request/response requires you to use the receiver address. Publishing requests is not supported since it makes no sense, you need to get only one response, not an unknown number of responses. The response is also sent to the requestor, not published, for the same reason.</p>&#xA;"
37741582,37699062,484041,2016-06-10T06:43:43,"<p>We do it like this:</p>&#xA;&#xA;<ul>&#xA;<li>All commands are issued as messages in durable queues with type-based routing</li>&#xA;<li>Processing takes places in isolated handlers</li>&#xA;<li>REST POST and PUT are only created for the API that should be accessible from legacy/external systems</li>&#xA;<li>These ""command""-style REST endpoints only form command as a message and send it via the message bus</li>&#xA;<li>REST GET is perfect for fetching the data and we do not use messaging there, although we could have some message handlers to retrieve data for long-running processes that can only use messages</li>&#xA;<li>Command (message) handlers always publish events about what they have done or not done</li>&#xA;<li>Downstream event processing can do whatever they want by subscribing to these events </li>&#xA;</ul>&#xA;"
46155060,46154092,484041,2017-09-11T11:49:35,"<p>We use MassTransit to implement CRQS extensively, but only for command handling. </p>&#xA;&#xA;<p>The reason for not to use messaging infrastructure to synchronise write and read models are discussed many times. The main reason there is that you have a chance of persisting a change and not publishing an event, because these are two different pieces of infrastructure. Unless you use something like DTC, you will not be able to guarantee the consistency among models.</p>&#xA;&#xA;<p>Also, at this point we also prefer to stay away from the ""God handler"" classes. MassTransit is particularly good at enforcing the SRP (single responsibility principle) by separating each consumer to a separate class.</p>&#xA;&#xA;<p>For general domain event-based integration (reactive event processing), we also use MassTransit.</p>&#xA;&#xA;<p>You can also have events that implement more than one message interface and by this you will have more comprehensive event processing:</p>&#xA;&#xA;<pre><code>public interface CustomerRegistered&#xA;{&#xA;    string FullName { get; }&#xA;}&#xA;&#xA;public interface OrderPlaced&#xA;{&#xA;    string Reference { get; }&#xA;    List&lt;OrderLine&gt; Lines { get; }&#xA;}&#xA;&#xA;public class NewCustomerOrderedStuff : CustomerRegistered, OrderPlaced&#xA;{&#xA;...&#xA;}&#xA;&#xA;public class CustomerRegisteredConsumer : IConsumer&lt;CustomerRegistered&gt;&#xA;&#xA;public class OrderPlacedConsumer : IConsumer&lt;OrderPlaced&gt;&#xA;</code></pre>&#xA;&#xA;<p>And each of those consumers will have different concern and can live in a separate bounded context (service).</p>&#xA;"
47834208,47833668,484041,2017-12-15T14:19:41,"<p>You cannot reply with null object. You have two options:</p>&#xA;&#xA;<ol>&#xA;<li>Add a boolean property to your response indicating it is not a success</li>&#xA;<li>Throwing an exception in the request consumer, then the fault message will be sent to the request client.</li>&#xA;</ol>&#xA;"
46523196,46522304,7570121,2017-10-02T09:49:17,"<p>I solved the issue by following this <a href=""https://github.com/OpenFeign/feign-form#spring-multipartfile-and-spring-cloud-netflix-feignclient-support"" rel=""nofollow noreferrer"">Link</a></p>&#xA;"
50675410,50672490,105929,2018-06-04T07:31:35,"<p>You need to isolate the services as so they do not share state/data. The design in your question is a single macroservice split into 3 correlated storage silos. Case in point, you cannot interpret a result form the 'Invoicing' service w/o correlating the data with the 'Products' response(s). </p>&#xA;&#xA;<p>Isolated microservices mean they own their data and they can operate independently. An invoice is complete as returned from the 'Invoices' service. It contains the product names, the customer name, every information on the invoice. All the data came from its own storage. A separate microservice could be 'Inventory', that operates all the product inventories, current stock etc. It would also have its own data, in its own storage. A 'product' can exist in both storage mediums, and there once was logical link between them (when the invoice was created), but the link is severed now. The 'Inventory' microservice can change its products (eg. remove one, add new SKUs etc) w/o affecting the existing Invoices (this is not only a microservice isolation requirement, is also a basic accounting requirement). I'm not going to enter here into details of what is a product 'identity' in real life. </p>&#xA;&#xA;<p>If you find yourself asking questions like you're asking it likely means you do not have microservices. You should think at your microservice boundaries while considering what happens if you replace all communication with async queued based requests (a response can come 6 days later): If the semantics break, the boundary is probably wrong. If the semantics hold, is the right track.</p>&#xA;"
42929138,42928059,3003337,2017-03-21T13:53:24,"<p>Use appropriate interceptor where you can do whatever you want. You can count number of threads (with a static variable) and reset every second or use a thread to do that.</p>&#xA;&#xA;<pre><code>@Component&#xA;public class RequestLoggingInterceptor extends OncePerRequestFilter&#xA;{&#xA;&#xA;@Override&#xA;protected void doFilterInternal(final HttpServletRequest request, final HttpServletResponse response, final FilterChain filterChain) throws ServletException, IOException {&#xA;    final long start = System.nanoTime();&#xA;    try {&#xA;        filterChain.doFilter(request, response);&#xA;    } finally {&#xA;        if( _logger.isInfoEnabled() ) {&#xA;            final long end = System.nanoTime();&#xA;            logger.info(buildMessage(request, end - start));&#xA;        }&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Microservice or monotlithic design is as per your requirement. If you have a big application and many modules and you want these modules to work independently and may have different database then you choose microservices. Microservices have a lot of advantages but only if you need them. For a basic operation normal monolithic is good (one rest project one static asset project and/or any other  common project if needed).</p>&#xA;"
43018701,43018514,3003337,2017-03-25T16:09:42,<p>It seems you can not load those properties. Follow either of the 2 options given below.</p>&#xA;&#xA;<p>1> You can add following bean in your configuration and that way you can autowire strings and use the way you are already using </p>&#xA;&#xA;<pre><code>@Bean&#xA;public static PropertySourcesPlaceholderConfigurer propertyConfigInDev() {&#xA;return new PropertySourcesPlaceholderConfigurer();&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>2></p>&#xA;&#xA;<pre><code>public AwsProxyResponse..{&#xA;@Autowired&#xA;private Environment env;&#xA;..&#xA;    public AwsProxyResponse handleRequest{&#xA;    ..&#xA;    String contextPath = env.getRequiredProperty(“server.contextPath”));&#xA;    ...&#xA;    }&#xA;}&#xA;</code></pre>&#xA;
45567957,45567201,4912329,2017-08-08T12:07:42,"<p>Use <code>docker login</code> command. (<a href=""https://docs.docker.com/engine/reference/commandline/login/"" rel=""noreferrer"">Official doc</a>)<br>&#xA;Enter your credentials, and then you can pull private image, only if you have an access.</p>&#xA;&#xA;<blockquote>&#xA;  <p>If you want to login to a self-hosted registry you can specify this by adding the server name.</p>&#xA;</blockquote>&#xA;&#xA;<pre><code>docker login localhost:8080&#xA;</code></pre>&#xA;&#xA;<p>Thanks to <a href=""https://stackoverflow.com/users/7683711/herm"">@herm's</a> comment, if you want to use swarm, use : &#xA;<code>--with-registry-auth</code> option.&#xA;Personnaly, I use this command :</p>&#xA;&#xA;<pre><code>docker stack deploy --with-registry-auth --compose-file dev.compose.yml myProjectName&#xA;</code></pre>&#xA;"
48084964,47282162,4074186,2018-01-03T20:52:40,<p>We decided that the microservices should fall under the API gateway and everything else would be the microservices client consuming those microservices through the API gateway.</p>&#xA;
43091929,39361356,2108874,2017-03-29T11:13:05,"<p>I am not sure if this is going to help you. But I believe you should follow this process</p>&#xA;&#xA;<ol>&#xA;<li>Create Service Fabric package for your application (using msbuild or VS).</li>&#xA;<li>Upload package into Service Fabric Image Store through api (<a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-deploy-remove-applications"" rel=""nofollow noreferrer"">example</a>). This will create a base <em>ApplicationType</em> for you which you will use later.</li>&#xA;<li>Now from that <em>ApplicationType</em> create a new application Instance using <a href=""https://docs.microsoft.com/en-us/powershell/servicefabric/vlatest/new-servicefabricapplication"" rel=""nofollow noreferrer"">New-ServiceFabricApplication</a> command. Just specify different <em>ApplicationName</em> and use same <em>ApplicationTypeName</em>. If you need to override some parameters, specify <em>ApplicationParameter</em>. </li>&#xA;<li>You can create as many Instances as you need from original <em>ApplicationType</em>.</li>&#xA;</ol>&#xA;"
27815639,27790905,18122,2015-01-07T08:55:51,"<p>The basic steps are the following ones:</p>&#xA;&#xA;<ol>&#xA;<li>The service needs to discover the resources of the other service.</li>&#xA;<li>The service then adds a link to the resources it renders where necessary.</li>&#xA;</ol>&#xA;&#xA;<p>I have a very rudimentary example of these steps in <a href=""https://github.com/olivergierke/rest-microservices"">this repository</a>. The example consists of two services: a service to provide geo-spatial searches for stores. The second service is some rudimentary customer management that optionally integrates with store service if it is currently available.</p>&#xA;&#xA;<p>Here's how the steps are implemented:</p>&#xA;&#xA;<h2>Resource discovery</h2>&#xA;&#xA;<p>In my example the consuming service (i.e. the customer one) uses Spring HATEOAS' <code>Traverson</code> API to traverse a set of link relations until it finds a link named <code>by-location</code>. This is done in <a href=""https://github.com/olivergierke/rest-microservices/blob/c30293eb0e389e9729378021257c5d236059a6b4/rest-microservices-customers/src/main/java/example/customers/integration/StoreIntegration.java#L69"">StoreIntegration</a>. So all the client service needs to know is the root URI (taken from the environment in my case) and a set of link relations. It <a href=""https://github.com/olivergierke/rest-microservices/blob/c30293eb0e389e9729378021257c5d236059a6b4/rest-microservices-customers/src/main/java/example/customers/integration/StoreIntegration.java#L56"">periodically checks the link</a> for existence using a <code>HEAD</code>-request.</p>&#xA;&#xA;<p>This of course can be done in a more sophisticated manner: hard-wiring the base URI into the client service might be considered suboptimal but actually works quite well if you're using DNS anyway (so that you can exchange the actual host behind the URI hard-coded). Nonetheless it's a decent pragmatic approach, still rediscovers the other service if it changes URIs, no additional libraries required.</p>&#xA;&#xA;<p>For an even more sophisticated approach have a look at <a href=""https://github.com/Netflix/eureka"">Netflix' Eureka library</a> which is basically a service registry. Also, you might wanna checkout the <a href=""http://projects.spring.io/spring-cloud/"">Spring Cloud integration</a> we have for that.</p>&#xA;&#xA;<h2>Augmenting resources with links</h2>&#xA;&#xA;<p>Spring HATEOAS provides a <code>ResourceProcessor</code> API that Spring Data REST leverages. It allows you to manipulate the <code>Resource</code> instance about to be rendered and e.g. add links to it. The implementation for the customers service can be found <a href=""https://github.com/olivergierke/rest-microservices/blob/c30293eb0e389e9729378021257c5d236059a6b4/rest-microservices-customers/src/main/java/example/customers/integration/CustomerResourceProcessor.java#L46"">here</a>.</p>&#xA;&#xA;<p>It basically takes the link just discovered in the steps above and expands it using well-known parameters and thus allows clients to trigger a store geo-search by just following the link.</p>&#xA;&#xA;<h2>Beyond that</h2>&#xA;&#xA;<p>You can find a more sophisticated variant of this example in the examples projects for <a href=""https://github.com/spring-cloud-samples/customers-stores"">Spring Cloud</a>. It takes the very same example but switches to Spring Cloud components such as Eureka integration, gathering metrics, adding UI etc.</p>&#xA;"
50450933,50448620,9521610,2018-05-21T14:22:37,"<p>As mentioned in Kubernetes <a href=""https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/"" rel=""nofollow noreferrer"">Documentation</a>, labels have a limited purpose of selecting objects and finding collections of objects that satisfy certain conditions. That put some limitation on the information you can store in labels. (Valid label values must be 63 characters or less and must be empty or begin and end with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (_), dots (.), and alphanumerics between.) </p>&#xA;&#xA;<p>However, annotations are not used to filter objects, so, you can put in annotation big/small structured/unstructured data that can contain characters, you’re not permitted to use in labels. Tools and libraries can retrieve annotations and use it to add some features to your cluster.</p>&#xA;&#xA;<p>Here are some examples of information that could be recorded in annotations:</p>&#xA;&#xA;<ul>&#xA;<li><p>Fields managed by a declarative configuration layer. Attaching these fields as annotations distinguishes them from default values set by clients or servers, and from auto-generated fields and fields set by auto-sizing or auto-scaling systems.</p></li>&#xA;<li><p>Build, release, or image information like timestamps, release IDs, git branch, PR numbers, image hashes, and registry address.</p></li>&#xA;<li><p>Pointers to logging, monitoring, analytics, or audit repositories.</p></li>&#xA;<li><p>Client library or tool information that can be used for debugging purposes: for example, name, version, and build information.</p></li>&#xA;<li><p>User or tool/system provenance information, such as URLs of related objects from other ecosystem components.</p></li>&#xA;<li><p>Lightweight rollout tool metadata: for example, config or checkpoints.</p></li>&#xA;<li><p>Phone or pager numbers of persons responsible, or directory entries that specify where that information can be found, such as a team website.</p></li>&#xA;<li><p>Options for Ingress object, (<a href=""https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/rewrite/README.md"" rel=""nofollow noreferrer"">nginx</a>,<a href=""https://github.com/kubernetes/ingress-gce/blob/master/docs/annotations.md"" rel=""nofollow noreferrer"">gce</a>)</p></li>&#xA;</ul>&#xA;"
50448499,50439512,9521610,2018-05-21T12:03:29,"<p>Starting from Kubernetes v1.6, <strong>kube-dns</strong> supports configuration for the <a href=""https://kubernetes.io/blog/2017/04/configuring-private-dns-zones-upstream-nameservers-kubernetes/"" rel=""nofollow noreferrer"">custom dns zones</a> (for example, <code>.consul.local</code>) with an external resolver, and for the external DNS servers for serving requests to the ""other zones”.</p>&#xA;&#xA;<p>To use this feature, two things should be configured properly:</p>&#xA;&#xA;<ol>&#xA;<li>Add the custom zones to kube-dns ConfigMap</li>&#xA;<li>Set the pod <a href=""https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/"" rel=""nofollow noreferrer"">dnsPolicy</a> to the ClusterFirst value <br>(search for details in section ""Pod’s DNS Policy"" of the linked document)</li>&#xA;</ol>&#xA;&#xA;<blockquote>&#xA;  <p>With the dnsPolicy set to “ClusterFirst” a DNS query is first sent to&#xA;  the DNS caching layer in kube-dns. From here, the suffix of the&#xA;  request is examined and then forwarded to the appropriate DNS. In this&#xA;  case, names with the cluster suffix (e.g.; “.cluster.local”) are sent&#xA;  to kube-dns. Names with the stub domain suffix (e.g.; “.acme.local”)&#xA;  will be sent to the configured custom resolver. Finally, requests that&#xA;  do not match any of those suffixes will be forwarded to the upstream&#xA;  DNS.</p>&#xA;</blockquote>&#xA;&#xA;<p>Here is an example of adding custom map for zone <code>.consul.local</code> and custom upstream services.</p>&#xA;&#xA;<pre><code>apiVersion: v1&#xA;kind: ConfigMap&#xA;metadata:&#xA;  name: kube-dns&#xA;  namespace: kube-system&#xA;data:&#xA;  stubDomains: |&#xA;    {“consul.local”: [“10.150.0.1”]}&#xA;  upstreamNameservers: |&#xA;    [""8.8.8.8"", ""8.8.4.4""]&#xA;</code></pre>&#xA;&#xA;<p>To apply this configuration, save it to file <code>kube-dns-consul-stubdomain.yml</code> and run the command (adjust the zone name and the server IP according to your needs):</p>&#xA;&#xA;<pre><code>kubectl create -f kube-dns-consul-stubdomain.yml&#xA;</code></pre>&#xA;&#xA;<p>This is an example of pod configuration with dnsPolicy</p>&#xA;&#xA;<pre><code>apiVersion: v1&#xA;kind: Pod&#xA;metadata:&#xA;  name: busybox&#xA;  namespace: default&#xA;spec:&#xA;  containers:&#xA;  - image: busybox&#xA;    command:&#xA;      - sleep&#xA;      - ""3600""&#xA;    imagePullPolicy: IfNotPresent&#xA;    name: busybox&#xA;  restartPolicy: Always&#xA;  hostNetwork: true&#xA;  dnsPolicy: ClusterFirst&#xA;</code></pre>&#xA;&#xA;<p>You can find these resources helpful to understand the details of Private DNS Zones feature:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://icicimov.github.io/blog/virtualization/Kubernetes-exposing-external-services-to-Pods-via-Consul/"" rel=""nofollow noreferrer"">Kubernetes - Exposing External Services to Pods via Consul</a>  </li>&#xA;<li><a href=""https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/"" rel=""nofollow noreferrer"">Customizing DNS Service</a>  </li>&#xA;<li><a href=""https://github.com/anubhavmishra/consul-dns-for-kubernetes#consul-dns-for-kubernetes"" rel=""nofollow noreferrer"">consul-dns-for-kubernetes</a>  </li>&#xA;</ul>&#xA;"
35682767,30027545,5984714,2016-02-28T12:51:14,"<p>Let's start with terminology. </p>&#xA;&#xA;<ul>&#xA;<li>A <code>verticle</code> is a Java class that usually extends <code>AbstractVerticle</code> and implements a start(..) method. A verticle can expose one or more HTTP endpoints and expore one or more <code>eventbus</code> endpoints.</li>&#xA;<li>A verticle runs inside a Vert.x <code>application</code> (previously called a 'module'). An application can contain one or more verticles. I usually keep it 1:1 to keep things small and simple.</li>&#xA;<li>A Vert.x application runs inside a Vert.x <code>instance</code>. You can run multiple instances of an application to increase parallelization.</li>&#xA;<li>A Vert.x instance runs inside a Vert.x <code>container</code>. A container is a running process with one or more instances of an application.</li>&#xA;<li>A Vert.x container runs inside a JVM.  </li>&#xA;</ul>&#xA;&#xA;<p>When building a microservices-style application with Vert.x, you typically want small independent logical units of work, call them services. Such a service should ideally run in its own process, be self-contained and indepedently upgradeable. Mapping it to the terminology above: build the service as a Vert.x application containing a single Verticle with the service logic.  </p>&#xA;&#xA;<p>Vert.x applications communicate with each other using the distributed eventbus, built with Hazelcast. This means that multiple JVM's running on the same server, or even on multiple servers, can communicate with each other over the Vert.x eventbus.  </p>&#xA;&#xA;<p>A web application built with Vert.x usually consists of one or more Vert.x applications exposing REST endpoints communicating over the eventbus with one or more Vert.x applications exposing (internal) eventbus endpoints. </p>&#xA;&#xA;<p>To answer your question: option 3 is the most common in Vert.x setups, and stays the closest to a microservices architecture. You can choose between 2 options there: you either run 1 application with a REST endpoint that handles all HTTP calls and delegates request processing over the eventbus to other applications, or you give each service (or at least, each service providing functionality for end users) its own REST endpoint. The latter is a bit more complex to setup since there are multiple HTTP endpoints to connect to from the frontend, but it's more scalable and has less single points of failure.</p>&#xA;"
35733293,35730116,5984714,2016-03-01T20:55:04,"<p>The <a href=""https://aws.amazon.com/sdk-for-java/"" rel=""nofollow"">AWS SDK for Java</a> is pretty good.<br>&#xA;You can either:</p>&#xA;&#xA;<ul>&#xA;<li>write an HTTP endpoint that SNS can post to (see <a href=""http://docs.aws.amazon.com/sns/latest/dg/SendMessageToHttp.example.java.html"" rel=""nofollow"">http://docs.aws.amazon.com/sns/latest/dg/SendMessageToHttp.example.java.html</a>)<br>&#xA;or </li>&#xA;<li>subscribe to an SQS topic (see <a href=""https://github.com/aws/aws-sdk-java/blob/master/src/samples/AmazonSimpleQueueService/SimpleQueueServiceSample.java"" rel=""nofollow"">https://github.com/aws/aws-sdk-java/blob/master/src/samples/AmazonSimpleQueueService/SimpleQueueServiceSample.java</a>).</li>&#xA;</ul>&#xA;"
42618008,42611968,5867722,2017-03-06T05:14:35,"<p>To honor docker's philosophy and take advantage of all the goodies microservices architecture has to offer, you should put each microservice in its own container.</p>&#xA;&#xA;<p>Some of the advantages of microservices architecture:</p>&#xA;&#xA;<ul>&#xA;<li>Improved fault isolation</li>&#xA;<li>Eliminates long-term commitment to a single technology stack</li>&#xA;<li>Makes it easier for a new developer to understand the functionality&#xA;of a service</li>&#xA;<li>Easier upgrade management</li>&#xA;<li>Improved Security</li>&#xA;<li>Resiliance</li>&#xA;<li>Easier Scaling</li>&#xA;<li>Easier Deployment</li>&#xA;<li>...</li>&#xA;</ul>&#xA;&#xA;<p>In order to do that, You should first create a user-defined network for your related containers. User-defined networks has internal DNS so you can resolve other containers ip on the same network by name. </p>&#xA;&#xA;<p>Then you need to split your <code>api_starter</code> to be microservice specific.</p>&#xA;&#xA;<p>Other tasks to do:</p>&#xA;&#xA;<ul>&#xA;<li>Specify persistent data path for each microservice (if applies)</li>&#xA;<li>Specify Network ports to expose.</li>&#xA;</ul>&#xA;"
45302251,45301606,5823946,2017-07-25T11:43:29,"<p>I would suggest you to take a look at the <a href=""https://github.com/rogeralsing/Microphone"" rel=""nofollow noreferrer"">Microphone</a> library in C#, which is easy to configure and use.</p>&#xA;&#xA;<p>It also seems that you can register differents services located on the same machines without problem with this library.</p>&#xA;"
37904222,37662379,6484682,2016-06-19T05:28:59,<p>I was also struggling with the same error (for MySql i.s.o MongoDB). I used jhipster-uml i.s.o jhipster:import-jdl to create Entities and it worked for me. Hope it works with MongoDB as well. Don't exactly know why it does not work with import-jdl. </p>&#xA;
51171017,51170392,10025674,2018-07-04T10:04:13,"<p>As you may know, in canonical rest REST API design path parameters and query parameters are used for different purposes:</p>&#xA;&#xA;<blockquote>&#xA;  <p>path: identify specific resource/resources,<br>&#xA;  query: sort/filter&#xA;  resources.</p>&#xA;</blockquote>&#xA;&#xA;<p>In your use case we have to deal with summarized data, which can be seen as a different resource, since in general it contains data of a different nature than the one in your /teams and /matches paths.</p>&#xA;&#xA;<p>In my opinion a good design for this use case may be doing a <strong>/stats path</strong> and then <strong>filter it by a query parameter</strong> (matches,wins,faults,...), this helps clarity and doesn't convolute the api too much, in the future you can also easily add more stats resources in an easy and maintanable way.</p>&#xA;"
51654752,51641688,6692043,2018-08-02T13:20:56,"<p>Not an answer to your question, but that could help you anyway: from what I understood in your comments, you don't really need to fully rebuild the index, you just need to source update events from a different place than the application executing the search queries.</p>&#xA;&#xA;<p>If your ""updating"" application uses Hibernate ORM, you can solve the issue without fully rebuilding the index: build the index automatically and incrementally in the ""updating"" application (using Hibernate Search in <a href=""https://docs.jboss.org/hibernate/search/5.10/reference/en-US/html_single/#_automatic_indexing"" rel=""nofollow noreferrer"">""automatic indexing"" mode</a> without ever doing any query), and make sure the index is made available to the ""searching"" applicaton.</p>&#xA;&#xA;<p>The latter can be achieved either:</p>&#xA;&#xA;<ul>&#xA;<li>by using the experimental <a href=""https://docs.jboss.org/hibernate/search/5.10/reference/en-US/html_single/#elasticsearch-integration"" rel=""nofollow noreferrer"">Elasticsearch integration</a> and connecting both applications to the same cluster: one will update it, the other will use it to search.</li>&#xA;<li>or by using the Lucene integration with the <code>filesystem-master</code>/<code>filesystem-slave</code> <a href=""https://docs.jboss.org/hibernate/search/5.10/reference/en-US/html_single/#directory-provider-table"" rel=""nofollow noreferrer"">directory providers</a>, which will allow you to periodically (and without downtime) copy the index from the ""updating"" application (master) to the ""searching"" application (slave). Note that if multiple applications update the index, you will need to use the <a href=""https://docs.jboss.org/hibernate/search/5.10/reference/en-US/html_single/#_backend"" rel=""nofollow noreferrer"">JMS or JGroups backend</a> to redirect all index updates to a master node and avoid conflicts. Beware, configuring JMS or JGroups won't be obvious.</li>&#xA;</ul>&#xA;"
45116062,45115860,1061579,2017-07-15T08:15:19,"<p><a href=""https://github.com/go-kit/kit"" rel=""nofollow noreferrer"">Go-kit</a> and <a href=""https://github.com/micro/go-micro"" rel=""nofollow noreferrer"">go-micro</a> are for writing microservices, it won't solve your problem.</p>&#xA;&#xA;<p>You should use a reverse proxy in front of your applications, for instance Nginx: <a href=""https://www.nginx.com/resources/admin-guide/reverse-proxy/"" rel=""nofollow noreferrer"">https://www.nginx.com/resources/admin-guide/reverse-proxy/</a></p>&#xA;&#xA;<p>Here is an example of a nginx configuration file that does what you want:</p>&#xA;&#xA;<pre><code>server {&#xA;   listen 80 default_server;&#xA;   server_name your-domain;&#xA;&#xA;   location /app1 {&#xA;      proxy_set_header X-Real-IP $remote_addr;&#xA;      proxy_pass http://localhost:3000;&#xA;   }&#xA;&#xA;   location /app2 {&#xA;      proxy_set_header X-Real-IP $remote_addr;&#xA;      proxy_pass http://localhost:4000;&#xA;   }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>This way, you still deploy your applications on different ports, but they will only be exposed through the port 80.</p>&#xA;"
42238598,42237967,3363019,2017-02-15T00:12:38,"<p>One way I have solved this particular problem is by using <a href=""https://auth0.com/"" rel=""nofollow noreferrer"">Auth0</a> and JWT. </p>&#xA;&#xA;<p>Users log in using <a href=""https://auth0.com/docs/libraries/lock"" rel=""nofollow noreferrer"">Auth0 Lock</a> and after that my web app stores a <a href=""https://jwt.io/"" rel=""nofollow noreferrer"">JWT</a> in local storage. This gets passed in the header of each ajax request to the server. The server is responsible for validating the JWT on each request. In order to roll this out for a React Native app, the same approach can be used with the <a href=""https://github.com/auth0/react-native-lock"" rel=""nofollow noreferrer"">react-native-lock</a>.</p>&#xA;&#xA;<p>In regards specifically to Passport it looks that there are <a href=""https://www.npmjs.com/package/passport-jwt"" rel=""nofollow noreferrer"">JWT libs</a> which might mean that you don't need to rely on something like Auth0. </p>&#xA;"
48880875,48824086,6446027,2018-02-20T08:36:20,"<p>I had the same problem in my microservice architecture.&#xA;According to this <a href=""http://lists.jboss.org/pipermail/keycloak-user/2016-February/004810.html"" rel=""nofollow noreferrer"">this</a> answer from Stian Thorgersen all ReST-Services should be bearer-only. Thus they should accept a token, but never perform a redirect to the login page.</p>&#xA;&#xA;<p>Therefore I extracted the UI component from the service and serve it on a separate NGINX instance. The UI performs the redirect to my Keycloak login page and adds the token to every additional GET/POST/whatever request to secured services. I changed all ReST-Services to <code>keycloak.bearer-only=true</code>, both in the service and Keycloak configuration. It now works like a charm.</p>&#xA;&#xA;<p>Hope this helps you. <br>&#xA;Freddy</p>&#xA;"
49122202,29117570,4087243,2018-03-06T01:44:28,<p>the answer to the original question is SAGA pattern.</p>&#xA;
39810075,39686804,6906571,2016-10-01T18:24:35,"<p>This does not matter, as this kind of decision is usually should be made based on your own architecture preferences and limitations. It depends on what do you think of is Better for you, cuz it is strictly architectural. It is more about defining how do you like you data streams to flow.</p>&#xA;&#xA;<p>What I can offer to think of here, is why do you want to use Consul? Because Consul would not give you a redundancy. It's a dynamic service discovery solution. You use it when you have a lot of dynamically appearing/disappearing services and you want to discover them and use and track their health.</p>&#xA;&#xA;<p>If you just want redundancy in a simple static cluster, maybe simple reverse proxy servers will be a good hit for you, like haproxy, nginx, and etc.</p>&#xA;"
43022828,43018613,1863627,2017-03-25T22:45:10,"<p>I thought the service registry was a pre-setup eureka server like the one I was running locally, but that is not the case. </p>&#xA;&#xA;<p>It uses eureka behind the scenes, but is a separate process altogether and requires the dependency below to be added.</p>&#xA;&#xA;<p>pom.xml:</p>&#xA;&#xA;<pre><code>&lt;dependencyManagement&gt;&#xA;    &lt;dependencies&gt;&#xA;        &lt;dependency&gt;&#xA;            &lt;groupId&gt;io.pivotal.spring.cloud&lt;/groupId&gt;&#xA;            &lt;artifactId&gt;spring-cloud-services-dependencies&lt;/artifactId&gt;&#xA;            &lt;version&gt;1.2.0.RELEASE&lt;/version&gt;&#xA;            &lt;type&gt;pom&lt;/type&gt;&#xA;            &lt;scope&gt;import&lt;/scope&gt;&#xA;        &lt;/dependency&gt;&#xA;      ...&#xA;    &lt;/dependencies&gt;&#xA;&lt;/dependencyManagement&gt;&#xA;&lt;dependencies&gt;&#xA;    ...&#xA;    &lt;dependency&gt;&#xA;        &lt;groupId&gt;io.pivotal.spring.cloud&lt;/groupId&gt;&#xA;        &lt;artifactId&gt;spring-cloud-services-starter-service-registry&lt;/artifactId&gt;&#xA;    &lt;/dependency&gt;&#xA;    ...&#xA;&lt;/dependencies&gt;&#xA;</code></pre>&#xA;&#xA;<p>Result:&#xA;<a href=""https://i.stack.imgur.com/ZHUvg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZHUvg.png"" alt=""enter image description here""></a></p>&#xA;"
47647683,47647560,2739147,2017-12-05T06:50:26,<p>This is common situation where we as developer gets confused. I would suggest to have a common jar(shared) which can be used in both micro services (A and B). It is nothing but sharing a third resource as we use third-party libraries. &#xA;In my current project we were in the same situation and we found the best approach to have separate shared libraries(<em>api-shared</em> as name) and consuming it as jar in different micro-services.</p>&#xA;&#xA;<p>In your second approach you ended up with redundant code and also difficult to maintain. Lets say if you have any changes in entity then you have to change in both the entities which is not quite a good way to synchronize the thing.</p>&#xA;&#xA;<p>All in all I would suggest you to use shared jar for both micro services.</p>&#xA;&#xA;<p>Regards</p>&#xA;&#xA;<p>Techno</p>&#xA;
44665294,39920488,8191435,2017-06-21T01:06:30,"<p>Falcor is really a great api if it is used in the correct way for very relevant use cases, like :</p>&#xA;&#xA;<ul>&#xA;<li>If your page has to make multiple REST end point calls</li>&#xA;<li>These calls don't depend on each other</li>&#xA;<li>All the REST calls happens on initial page load</li>&#xA;<li>Performance : If you want to cache the REST responses (for example, the microservice uses gemfire caching, you may not need falcor cache. You could still use falcor caching if you want to reduce the network latency) </li>&#xA;<li>Server requests batching : When running Falcor in node environment, you may want to cut down the amount of calls to node server from the client side.</li>&#xA;<li>Easier response parsing : If you don't want the client code to worry about extracting the data-points from REST response (Including error handling)&#xA;and so on ..</li>&#xA;</ul>&#xA;&#xA;<p>However, there are plenty of situations where falcor does not serve the purpose as much and feel that it is better off calling the end point directly :</p>&#xA;&#xA;<ul>&#xA;<li>If REST calls are dependent on one another</li>&#xA;<li>If you want to pass lot of parameters for calling the end point</li>&#xA;<li>If you don't intend to cache the response(s)</li>&#xA;<li>If you want to share some secure cookies (ex:XSRF tokens) with the REST web service</li>&#xA;</ul>&#xA;"
39370040,39369856,2099148,2016-09-07T12:31:19,"<p>You could start using the model dependency diagram from RubyMine: <a href=""https://www.jetbrains.com/help/ruby/2016.2/viewing-model-dependency-diagram.html"" rel=""nofollow"">https://www.jetbrains.com/help/ruby/2016.2/viewing-model-dependency-diagram.html</a></p>&#xA;&#xA;<p>Another tool which includes controller dependencies is <a href=""https://github.com/preston/railroady"" rel=""nofollow"">https://github.com/preston/railroady</a></p>&#xA;"
33524446,30027545,463052,2015-11-04T14:30:23,"<p>I think you've got to have solid <em>scalability</em> reasons for partitioning your services, and there isn't any one-size-fits-all approach to dealing with the lifecycle and addressing issues you'll run into getting those services interacting with each other.  Whether a 'verticle' or some other thing is listening on a socket, I'd have thought that limiting the number of endpoints/addresses would cause the fewest headaches in that direction.  In any case, the code-entity responsible for associating a given verticle to its socket would need to expose lifecycle-controls to some orchestration framework somehow ... just as it would if it were not a verticle listening there.</p>&#xA;"
47448633,47329630,5077196,2017-11-23T06:05:11,"<p>We recently worked on something similar, the way we did it is:</p>&#xA;&#xA;<p>RabbitMQ  was hosted separately, and buses/queues creation and management were done from the services that use messaging.</p>&#xA;&#xA;<p>For each service that receives messages you use Maastransit to create a queue because service will be receiving messages using this queue.</p>&#xA;&#xA;<p>You will be using publish/subscribe way of messaging so as mentioned above, inside each service, create a queue with logical name and connect to RabbitMQ server address.</p>&#xA;&#xA;<p>Services that represent senders will publish messages of a custom type you create, and services that represent receivers will subscribe to this type of messages by having a consumer for this type registered inside the bus created.</p>&#xA;&#xA;<p>Hope it helps.</p>&#xA;"
40939976,30908112,7242956,2016-12-02T19:59:00,<p>Gateway should do only authentication and not authorizations.  The authorizations are handled inside the service as the services only maintains who can access it.  I would get the Inventory service to get the list of locations that the user is authorized to access.  </p>&#xA;&#xA;<p>The whole orchestration will be happening at the UI level so that the inventory services is not building a hard dependency on the Location service.</p>&#xA;&#xA;<p>This is one approach - not sure if this will work for you.</p>&#xA;
51980792,51980596,5224652,2018-08-23T07:50:18,"<p>Copying of one bean to another is possible using <a href=""http://commons.apache.org/proper/commons-beanutils/javadocs/v1.8.3/apidocs/org/apache/commons/beanutils/BeanUtils.html"" rel=""nofollow noreferrer"">Apache commons library</a> </p>&#xA;&#xA;<p>for direct object to object copy you can use</p>&#xA;&#xA;<blockquote>&#xA;  <p>copyProperties(Object dest, Object orig)</p>&#xA;</blockquote>&#xA;&#xA;<p>for individual copy you can use</p>&#xA;&#xA;<blockquote>&#xA;  <p>setProperty(Object bean, String name, Object value)</p>&#xA;</blockquote>&#xA;&#xA;<p>This is alternative solution i have used in my projects</p>&#xA;"
49661550,49659871,6142412,2018-04-04T22:44:27,"<p>A Hystrix Command can run on the calling thread if you use SEMAPHORE as the execution isolation strategy </p>&#xA;&#xA;<p><a href=""https://github.com/Netflix/Hystrix/wiki/How-it-Works#semaphores"" rel=""nofollow noreferrer"">https://github.com/Netflix/Hystrix/wiki/How-it-Works#semaphores</a>&#xA;<a href=""https://github.com/Netflix/Hystrix/wiki/Configuration#thread-or-semaphore"" rel=""nofollow noreferrer"">https://github.com/Netflix/Hystrix/wiki/Configuration#thread-or-semaphore</a></p>&#xA;"
51810737,51786390,1162956,2018-08-12T16:31:48,"<p>Usually there are main 3 benefits for adapting microservices:</p>&#xA;&#xA;<ol>&#xA;<li>Scalability, which isn't your interest. </li>&#xA;<li>Maintainability, in which each microservice has a clear usecase which result in a small amount of code, which result maintainability easier. </li>&#xA;<li>Fault-tolerance, if some microservice fail, others still functioning.</li>&#xA;</ol>&#xA;&#xA;<p>If you care only about complexity, Domain Driven Design can really help here, by dividing your monolith into different domains that can be distributed to different teams. Regarding the architecture, you can adopt normal SOA architecture. If you have a well defined domains, SOA or Microservices is just a deployment architecture then.</p>&#xA;&#xA;<p><strong>how do they share the same data in one db?</strong>&#xA;That is a very abstract question, thus the answer will be a bit abstract. Usually they don't share data in same db, in most cases each microservice will have its own db, some cases a cluster of microservices can share one db.</p>&#xA;"
51810635,51799330,1162956,2018-08-12T16:17:44,"<p>API Gateway is not for communicating between services, as mentioned in the previous answer it would be a gate for requests before they access your microsrvices. However, it seems that you would like to have synchronous microservices approach, which has the following drawbacks:</p>&#xA;&#xA;<ol>&#xA;<li>Coupling between microservices, if microservice X depends on microservice Y to work, if Y goes down, then X goes down as well.</li>&#xA;<li>High latency, if service 1 calls service 2 and service 3, then each request response time will be the sum of the 3 services.</li>&#xA;<li>In case you a service which orchestrate calls with multiple services to process a request, it is a single point of failure.</li>&#xA;</ol>&#xA;&#xA;<p>The only benefit I can think of is that your business use-cases will be better controlled and more clear than the reactive event driven ones.</p>&#xA;"
41854051,41853686,1035468,2017-01-25T14:25:23,"<p>Yes, it is the best way to utilize containers, and it will allow you to scale, also you can use the official RabbitMQ container and concentrate on your application.</p>&#xA;"
40502380,40458770,2889165,2016-11-09T08:09:10,"<p>One common setup is to have an <a href=""http://microservices.io/patterns/apigateway.html"" rel=""nofollow noreferrer"">API gateway</a> that verify all incoming requests by their JWT. The API Gateway validates the signature of the JWT (or decrypt it for encrypted JWT's), checks the the expiry time etc, and extract the scopes and the User ID (sub) from it.</p>&#xA;&#xA;<p>It then compare the scopes with a set of defined scopes for each micrto service, and if the scope provides the user (subject) access, the request is forwarded to the micro service. The User ID (sub in the JWT), along with other needed information stored in the JWT is placed in custom requests headers like X-IGNACIO-SUBJECT</p>&#xA;"
44305609,44305351,4266957,2017-06-01T10:53:35,"<p>You should call <code>httpCon.connect();</code> only after you write your parameters in the body and not before. Your code should look like this:</p>&#xA;&#xA;<pre><code>URL url = new URL(""http://localhost:4567/"");&#xA;HttpURLConnection httpCon = (HttpURLConnection) url.openConnection();&#xA;httpCon.setDoOutput(true);&#xA;httpCon.setRequestMethod(""POST"");&#xA;OutputStream os = httpCon.getOutputStream();&#xA;OutputStreamWriter osw = new OutputStreamWriter(os, ""UTF-8"");    &#xA;osw.write(""Just Some Text"");&#xA;osw.flush();&#xA;osw.close();&#xA;os.close();  //don't forget to close the OutputStream&#xA;httpCon.connect();&#xA;&#xA;//read the inputstream and print it&#xA;String result;&#xA;BufferedInputStream bis = new BufferedInputStream(conn.getInputStream());&#xA;ByteArrayOutputStream buf = new ByteArrayOutputStream();&#xA;int result2 = bis.read();&#xA;while(result2 != -1) {&#xA;    buf.write((byte) result2);&#xA;    result2 = bis.read();&#xA;}&#xA;result = buf.toString();&#xA;System.out.println(result);&#xA;</code></pre>&#xA;"
51829660,51829581,1322957,2018-08-13T19:53:21,"<p>Spring Cloud is just a set of tools (software) commonly used in the cloud, AWS is one of many cloud options, a place where you can deploy your apps.</p>&#xA;"
44302578,44301997,2509773,2017-06-01T08:40:27,"<p>I think the best place to start with is to read through the overview of all these projects <a href=""http://projects.spring.io/spring-cloud/"" rel=""nofollow noreferrer"">here</a> to get a better understanding of the objectives each of these projects achieve.</p>&#xA;"
36638557,36615117,2509773,2016-04-15T04:57:35,"<p>If you can programmatically create the identifier earlier while receiving from the message source, you can embed the identifier as part of the message header and subsequently use the message header information during database inserts and in any other consumers.</p>&#xA;&#xA;<p>But this approach requires a separate verification by the other consumers against the database to process only the committed transactions (if you are concerned about processing only the inserts).</p>&#xA;"
45237124,45236389,2509773,2017-07-21T11:59:57,"<p>Can Spring Cloud Data Flow solve your requirement to control the number of instances deployed?</p>&#xA;&#xA;<p>and, there is a community released Spring Cloud Data Flow server for OpenShift:&#xA;<a href=""https://github.com/donovanmuller/spring-cloud-dataflow-server-openshift"" rel=""nofollow noreferrer"">https://github.com/donovanmuller/spring-cloud-dataflow-server-openshift</a></p>&#xA;"
51489196,51481780,6515149,2018-07-24T01:28:28,<p>The UAA server will have a port as well as a host name. Both will need to be specified. To specify the port you will need to change your application.properties. </p>&#xA;
51498410,51498267,4091597,2018-07-24T12:16:10,"<p>I didn't get why you have a GUI for each micro service, however you can achieve this by adding authentication for each one with a shared data protection key.</p>&#xA;&#xA;<p>You can make this by configuring the data protection to use shared folder as key store, or implement your own one to have one shared store.</p>&#xA;"
44484579,44274982,2328781,2017-06-11T13:48:20,"<p>You can try <code>server.connection-timeout=5000</code> in your application.properties.From the <a href=""https://docs.spring.io/spring-boot/docs/current/reference/html/common-application-properties.html"" rel=""nofollow noreferrer"">official documentation</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p>server.connection-timeout= # Time in milliseconds that connectors will wait for another HTTP request before closing the connection. When not set, the connector's container-specific default will be used. Use a value of -1 to indicate no (i.e. infinite) timeout.</p>&#xA;</blockquote>&#xA;&#xA;<p>UPDATE:&#xA;Just noticed that you use microservice architecture, so in case you need to handle timeouts when communicating between microservices, I would suggest handling it on the client side instead of the server side. <strong>If the microservice you are trying to call is overloaded and its performance degrades to the point where it drastically affects the user experience sometimes it's better to return some fallback data than just drop the request.</strong> </p>&#xA;&#xA;<p>Imagine we have an e-commerce web-site that has microservice architecture and one of its microservices that gives recommendations to the user becomes extremely slow. In this case, the preferred solution would be to return some fallback data which could be top 10 popular products this month rather than showing 5xx error page to the customer. Moreover, in case subsequent requests fail with a timeout, we can make a decision to avoid sending requests to the 'recommendation-service'  and return fallback data immediately. After some time we can try sending a request to the 'recommendation-service' again, and if it became healthy - just use it instead of the fallback data. </p>&#xA;&#xA;<p>This is called <strong>Circuit Breaker</strong> pattern and there is already an implementation of it in a framework called Hystrix. Here is a nice article explaining it in depth: <a href=""http://www.baeldung.com/spring-cloud-netflix-hystrix"" rel=""nofollow noreferrer"">http://www.baeldung.com/spring-cloud-netflix-hystrix</a>. Spring Cloud Feign + Spring Cloud Hystrix looks really nice especially taking into account that they work with Discovery services out-of-the-box (e.g. Spring Cloud Eureka).</p>&#xA;"
44486519,44038536,2328781,2017-06-11T17:10:44,"<p>It seems to me that you are trying to work with your microservices as it was a monolith. One of the most important features in the microservice architecture is the ability to work on each microservice independently, which means that you do not need to run the whole infrastructure to develop a feature. Imagine a developer at Netflix who needs to run several hundreds of microservices on their PC to develop a feature - that would be crazy.</p>&#xA;&#xA;<p><strong>Microservices is all about ownership</strong>. Usually, different teams work on different microservices or some set of microservices which makes it really important to build good test design to make sure that whenever all the microservices are up and running as one cohesive system everything works as expected.</p>&#xA;&#xA;<p>All that said, when you are developing your microservice you don't have to rely on other microservices. Instead, you better mock all the interactions with other microservices and write tests to check whether your microservice is doing what it has to do. I would suggest you <strong><a href=""http://wiremock.org/"" rel=""nofollow noreferrer"">wiremock</a></strong> as it has out-of-the-box support for Spring Boot. Another reason is that it is also supported by <strong>Spring Cloud Contract</strong> that enables you to use another powerful technique called Consumer Driven Contracts which makes it possible to make sure that contract between two microservices is not broken at any given time.</p>&#xA;&#xA;<p>These are the integration tests (they run on a microservice level) and they have a very fast feedback because of the mocks, on the other hand, you can't guarantee that your application works fine after running all of them. That's why there should be another category of more coarse grained tests, aka end-to-end tests. These tests should be running against the whole application, meaning that all the infrastructure must be up and running and ready to serve your requests. This kind of tests is usually performed automatically by your CI, so you do not need all the microservices running on your PC. This is where you can check whether you API gateway works fine with other services.</p>&#xA;&#xA;<p>So, ideally, your test design should follow the following <a href=""https://i.stack.imgur.com/UYbzQ.png"" rel=""nofollow noreferrer"">test pyramid</a>. The more coarse grained tests you have the less amount of them should be kept within the system. There is no silver bullet if to speak about proportions, rough numbers are: </p>&#xA;&#xA;<ol>&#xA;<li>Unit tests - 65%</li>&#xA;<li>Integration tests - 25%</li>&#xA;<li>End-to-end tests - 10%</li>&#xA;</ol>&#xA;&#xA;<p><strong>P.S:</strong> Sorry for not answering your question directly, I had to use the analogy with tests to make it more clear. All I wanted to say is that <strong>in general, you don't need to take care of the whole system while developing. You need to take care of your particular microservice and mock out all the interactions with other services.</strong></p>&#xA;"
44486636,44115310,2328781,2017-06-11T17:23:26,"<blockquote>&#xA;  <p>headers.set(""Session"",sessionID);</p>&#xA;</blockquote>&#xA;&#xA;<p>I assume that the problem is that you are using the wrong identifier. As far as I know, it is JSESSIONID by default.</p>&#xA;&#xA;<p>Another problem that I can see here is that JSESSIONID expected to be in cookies. Try to put it in cookies when sending a request to your 'microservice2'.</p>&#xA;"
44486976,44000273,2328781,2017-06-11T17:55:16,"<p>If you do not need an immediate response to the calling system I would suggest to you to use <a href=""http://java-design-patterns.com/patterns/event-driven-architecture/"" rel=""nofollow noreferrer"">event-driven</a> approach if that's feasible. So instead of REST services, you will have a message broker and your services will be subscribed for certain events. This will hide your consumers behind the message broker which will make your system less coupled.</p>&#xA;&#xA;<p>This can be implemented via <strong>Spring Cloud Stream</strong>, where you will have a Sink (microservice producing events, transformer - microservice that makes intermediate transformations possible and a source - microservice that receives a final result for further processing).</p>&#xA;&#xA;<p>Another possible case could be <strong>Camel</strong>. It has basically all the integration patterns built in, so it should not be a problem to implement the solution either based on REST APIs or events.</p>&#xA;"
44487153,43927492,2328781,2017-06-11T18:14:52,"<p>It boils down to a test technique that you use. Here my recent answer in another topic that you could find useful <a href=""https://stackoverflow.com/a/44486519/2328781"">https://stackoverflow.com/a/44486519/2328781</a>.</p>&#xA;&#xA;<p>In general, I think that Wiremock is a good choice because of the following reasons:</p>&#xA;&#xA;<ol>&#xA;<li>It has out-of-the-box support by <strong>Spring Boot</strong></li>&#xA;<li>It has out-of-the-box support by <strong>Spring Cloud Contract</strong>, which gives a possibility to use a very powerful technique called <a href=""https://martinfowler.com/articles/consumerDrivenContracts.html"" rel=""nofollow noreferrer"">Consumer Driven Contracts</a>. </li>&#xA;<li>It has a <a href=""http://wiremock.org/docs/record-playback/"" rel=""nofollow noreferrer"">recording feature</a>. Setup your Wiremock as a proxy and make requests through it. This will generate stubs for you automatically based on your requests and responses.</li>&#xA;</ol>&#xA;"
44487635,43661844,2328781,2017-06-11T19:02:27,"<p>I think the phrase 'changing database without changing code' <strong>doesn't</strong> mean that if you add/remove fields in DB you do not have to modify your codebase - it just doesn't make any sense.</p>&#xA;&#xA;<p>What it really means is that <strong>you should use good database abstractions</strong>, so in case you need to change your database vendor from, let's say, MYSQL to OracleDB your Java code should stay the same. The only thing that may differ is some configurations.</p>&#xA;&#xA;<p>A good example of it is ORM like <strong>Hibernate</strong>. You write your java code once, no matter what is the SQL Database that you are using underneath. To switch databases the only thing that you need to change is a dialect configuration property (In reality it's not that easy to do, but probably easier than if we were coupled to a one specific DB).</p>&#xA;&#xA;<p><strong>Hibernate gives you a good abstraction over SQL databases</strong>. Nowadays we have a new trend - having the abstraction over different DB families like SQL and NoSQL. <strong>So in the ideal world, your codebase should stay unchanged even if you want to change MySQL to MongoDB or even Neo4j</strong>. <a href=""http://projects.spring.io/spring-data/"" rel=""nofollow noreferrer"">Spring Data</a> probably is the most popular framework that tries to solve this problem. Another framework that I found recently is <a href=""https://github.com/impetus-opensource/Kundera"" rel=""nofollow noreferrer"">Kundera</a> but I haven't used it so far.</p>&#xA;&#xA;<p>So answering your question - you do not need to keep your SQL queries as system variables. All you need to do is to use proper abstractions in your language of choice.</p>&#xA;"
44488178,43440170,2328781,2017-06-11T19:57:41,"<p>I assume that you are not following the latest Spring conventions and best practices which tell us to <a href=""http://olivergierke.de/2013/11/why-field-injection-is-evil/"" rel=""nofollow noreferrer"">prefer constructor based injection over field based injection</a>.</p>&#xA;&#xA;<p>If you had your bean declared like this:</p>&#xA;&#xA;<pre><code>public class FxRatesEventPublisher {&#xA;    private final Integer maxRecoveryAge;&#xA;    private final SomeDependency someDependency;&#xA;&#xA;    public FxRatesEventPublisher(@Value(""${publisher.rate.maxRecoveryAge}"") Integer maxRecoveryAge, @Autowired SomeDependency someDependency) {&#xA;        this.maxRecoveryAge = maxRecoveryAge;&#xA;        this.someDependency = someDependency;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>then you could instantiate it like this:</p>&#xA;&#xA;<pre><code>// Create an instance with your test values injected. Note that you could inject mocked dependency here as well as the real one.&#xA;FxRatesEventPublisher fxRatesEventPublisher = new FxRatesEventPublisher(24, mockDependency); &#xA;</code></pre>&#xA;&#xA;<p>In this case, it's much easier to test your components, as you can pass any values to the constructor. I agree that this seems to be less pretty than the property based injection, but worth taking a look at least.</p>&#xA;"
45526841,45526675,2328781,2017-08-05T22:19:02,"<p>Even if you use fixed thread pool <strong>all your threads in it will be blocked on step 2 meaning that they won't do any meaningful job</strong> - just wait for your API to return a response which is not a pragmatic resource management. In this case, you will be able to handle a limited amount of incoming requests since threads in the thread pool will be always busy instead of handling new requests.</p>&#xA;&#xA;<p><strong>In the case of a non-blocking client, you are blocking just one single thread</strong> (let's call it dispatcher thread) which is responsible for sending and waiting for all the requests/responses. It will be running in a ""while loop"" (you could call it an event loop) and check whether all the packages were received as a response so they are ready for worker threads to be picked up. </p>&#xA;&#xA;<p><strong>In the latter scenario, you get a larger amount of available threads ready to do some meaningful job</strong>, so your throughput will be increased.</p>&#xA;"
44705588,44692442,2328781,2017-06-22T17:11:20,"<p>This happens due to your certificates being self-signed with ""<strong>Subject name (DN)</strong>"" not matching 127.0.0.1. You have two options here:</p>&#xA;&#xA;<ul>&#xA;<li>Create a certificate with DN=127.0.0.1</li>&#xA;<li><a href=""https://github.com/spring-cloud/spring-cloud-netflix/issues/1057"" rel=""nofollow noreferrer"">Disable host verification</a></li>&#xA;</ul>&#xA;&#xA;<p><strong>UPDATE:</strong> </p>&#xA;&#xA;<p>Just noticed your question:</p>&#xA;&#xA;<blockquote>&#xA;  <p>I want to know that the concept of https over zuul</p>&#xA;</blockquote>&#xA;&#xA;<p>Usually, nobody supports HTTPS between zuul and the underlying microservices because:</p>&#xA;&#xA;<ul>&#xA;<li><strong>It affects performance</strong>. Imagine all your microservices use HTTPS for internal communication. HTTPS encryption, decryption, handshakes, etc. much more consumes resources comparing to plain HTTP communication.</li>&#xA;<li><strong>Supporting HTTPS for all the microservices will make you cry</strong>. In large systems where you have hundreds (or thousands of microservices) changing certificates because some of them are expired will be a headache.</li>&#xA;</ul>&#xA;&#xA;<p>A common use case is to have your API Gateway running over HTTPS. But communication from the gateway to the underlying services as well as intercommunication between microservices should be over HTTP. The thing is that anyway <strong>you have to focus on secure networking for your microservices instead of secure communication between them</strong>. You system under the gateway should use a private network where nobody should have an access to.</p>&#xA;&#xA;<p>There might be a case where you must have HTTPS between microservices for one more layer of security, but it's uncommon and related mostly to Banking. On the other hand, you could add HTTPS to services with a very sensitive data while the rest of them can stay on HTTP. It's more a question of the requirements that you have.</p>&#xA;"
34470230,34450732,2345933,2015-12-26T09:56:16,"<blockquote>&#xA;  <p>Is there a way to architect this three services (consumer, reporting, curation) to be loosely coupled and don't depend directly on the database integration between them?</p>&#xA;</blockquote>&#xA;&#xA;<p>Do not expose the database to the consumer, reporting and curation services but instead expose an API (e.g. REST API) of a new service, which will exclusively access the database. Make these services depend not on the database, but on this API and hide the database from the consumer, reporting and curation services.</p>&#xA;&#xA;<p>If you have numerous bounded contexts then you can create a separate service for each bounded context:</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/8PG3J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8PG3J.png"" alt=""suggested microservices architecture""></a></p>&#xA;"
27727702,27723156,3724237,2015-01-01T01:27:53,"<p>Yes. For better reusability and easy maintenance you may need to publish common components as separate jar artifact(s) and refer it as a dependency in each micro services.</p>&#xA;&#xA;<p>A sample project structure can be something like this, </p>&#xA;&#xA;<p><img src=""https://i.stack.imgur.com/XIYUP.png"" alt=""enter image description here""></p>&#xA;"
27286534,25595492,3724237,2014-12-04T04:53:29,"<p>Chris Sterling explained standard authentication practice above and it makes absolute sense. I just want to put another thought here for some practical reasons.</p>&#xA;&#xA;<p>We implemented authentication services and multiple other micro services relying on auth server in order to authorize resources. At some point we ran in to performance issues due to too many round trips to authentication server, we also had scalability issues for auth server as number of micro services increased. We changed the architecture little bit to avoid too many round trips.</p>&#xA;&#xA;<p>Auth server will be contacted only once with credentials and it will generate the token based on a private key. Corresponding public key will be installed in each client (micro service server) which will be able to validate the authentication key with out contacting auth server. Key contain time generated and a client utility installed in micro service will validity as well. Even though it was not standard implementation we have pretty good success with this model especially when all the micro services are internally hosted. </p>&#xA;"
36945628,36877722,1376205,2016-04-29T18:57:17,"<p>From your description of the problem, it sounds like ""people"" are at the center of the functionality and the use case of the service if I understand this correctly.</p>&#xA;&#xA;<ul>&#xA;<li>Search for people by native country</li>&#xA;<li>Search for people by language</li>&#xA;<li>Add a person with both their native country and the languages spoken</li>&#xA;<li>List all the languages</li>&#xA;</ul>&#xA;&#xA;<p>Since the three required features are around people and one feature requiring just listing the languages, I would argue that this should be <strong>one</strong> microservice (again without knowing if there are external services that depends on the other possible entity services). My argument here would be that in order to serve requests, people is the entity of interest with the native country and language being just a dimension to retrieve users. </p>&#xA;&#xA;<p>If you break each of the entities, people, language, and country into different microservices, the services would be too small and the complexity would increase eg. you might need to make multiple requests to multiple services to generate a single response while there may not be a need to. As for the one last feature that doesn't quite revolve around people, I would say that its too small of a feature to be in a microservice. Until there becomes a need for the last feature to be a standalone service, I would advise for putting this into the ""people"" microservice.</p>&#xA;"
33071719,33041733,1376205,2015-10-12T00:48:54,"<p>@Luxo is spot on. I'd just like to offer a slight variation and bring about the organizational perspective of it. Not only does microservices allow the applications to be decoupled but it may also help on an organizational level. The organization for example would be able to divide into multiple teams where each may develop on a set of microservices that the team may provide. </p>&#xA;&#xA;<p>For example, in larger shops like Amazon, you might have a personalization team, ecommerce team, infrastructure services team, etc. If you'd like to get into microservices, Amazon is a very good example of it. Jeff Bezos made it a mandate for teams to communicate to another team's services if they needed access to a shared functionality. See <a href=""http://apievangelist.com/2012/01/12/the-secret-to-amazons-success-internal-apis/"" rel=""noreferrer"">here</a> for a brief description. </p>&#xA;&#xA;<p>In addition, engineers from <a href=""https://twitter.com/adrianco/status/441169921863860225"" rel=""noreferrer"">Etsy and Netflix</a> also had a small debate back in the day of microservices vs monolith on Twitter. The debate is a little less technical but can offer a few insights as well.</p>&#xA;"
33082343,33078422,1376205,2015-10-12T13:24:43,"<p>The gateway doesn't necessarily have to know about each endpoint specifically. I imagine you're probably trying to get at the problem that with knowing the endpoints, encapsulation is broken and there exists an implicit coupling. </p>&#xA;&#xA;<p>Personally, I like the combination of an abstraction usually provided by the downstream service. Since all services provide a contract to how someone may make RPC calls to the services, it should also in that scenario  provide a client for a layer of abstraction. The client internally will know the endpoints but the methods could be abstract eg. a user-service to list users may call <code>userServiceClient.getUsers</code> or something like that. The other notion of service registry can also exist in this client as it should know what the service it is representing is registering to. </p>&#xA;&#xA;<p>Now in the API gateway, this could be imported as a package and used without knowing the endpoints but knowing that there exists some user-service that can list users. The other benefit of having the abstraction is that it may use a protocol without requiring the upstream service to have any idea as to its implementation. Hope this helps!</p>&#xA;"
32908208,32838312,1376205,2015-10-02T13:25:15,"<p>The logic you have is not incorrect but what would probably be better is to build a layer of abstraction on top of making requests to an another service eg. the API gateway to another microservice. Lets just call that microservice B for this instance (API gateway to make a request to B). </p>&#xA;&#xA;<p>B in this case should provide its own client on how another service should interact with it, whether its through HTTP or WebSockets, the protocol is up to B because B understands how one should communicate with it. The argument for the client and the service being implemented together is that these two components should have a higher level of cohesion since technically they are bound by a contract eg. if a requests needs to be made to a service, it needs to adhere to the contract that the service requires.</p>&#xA;&#xA;<p>In simple pseudocode with Express:</p>&#xA;&#xA;<pre><code>// implemented elsewhere, ideally next to the service that it communicates with&#xA;function BServiceClient() {&#xA;  // ...&#xA;}&#xA;&#xA;// the API gateway's calling code&#xA;app.get('...', function(request, response, next) {&#xA;  // create an instance of the service client&#xA;  var bServiceClient = new BServiceClient();&#xA;&#xA;  // retrieving the users from an abstracted endpoint&#xA;  bServiceClient.GetUsers();&#xA;&#xA;  // do some processing and then render a response or call next&#xA;});&#xA;</code></pre>&#xA;&#xA;<p>In order for it to be more testable, you might have to write your own wrapper around the <code>app</code> to do the proper dependency injection for injecting the client to make the routes more testable. Otherwise, you might be able to create another function that can inject the client and create the client at the handler level that calls the newly created function. The newly created function could then be tested. However, I prefer the former approach of using the wrapper. Hope this helps!</p>&#xA;"
33070672,33059557,1376205,2015-10-11T22:13:07,"<p>I don't think there are a lot of resources that directly prepares you for a microservices architecture. At least not that I know of. The closest I can think of is the <a href=""http://rads.stackoverflow.com/amzn/click/0321125215"" rel=""nofollow"">Domain Driven Design</a> book from Eric Evans. </p>&#xA;&#xA;<p>Its more of a software design book but in my opinion, microservices is really just an architecture that mimics software design. It's the attempt of separating concerns of an application to different categorized components.</p>&#xA;&#xA;<p>The most useful concept is probably the bounded contexts and service objects for the microservices architecture. The bounded contexts are the sub-level domains for which the services should be scoped and the service objects will be the actual services down the line. These service objects should be loosely coupled to make the migration to microservices seamless.</p>&#xA;&#xA;<p>Finally, during the migration to a microservices architecture, the service objects can be converted to a client-like object that abstracts away the interservice communication protocol to a given service. Hope this helps!</p>&#xA;"
32910715,32831192,1376205,2015-10-02T15:31:49,"<p>I think at this point I would ask myself what exactly is the domain you are trying to model against. If the domain is strictly rendering movies and the reviews for the movies, my question would be why are there two separate services, the movie and movie review service.</p>&#xA;&#xA;<p>In essence, I would merge the two services together into a single service and call it a movie-reviews-service since reviews for the movies is all thats cared about. In this case, there would no longer be a problem with joins.</p>&#xA;&#xA;<p>Personally, I think the question to really ask is whether the movie service should exist and what kind of role it plays. In your example, it seems a extraneous to be broken into a separate service. While this may not be a satisfactory answer, the example provided is technically a little too simple to make a microservices architecture worthwhile since there are less components requiring the separation of concern to really break them down further into multiple services. </p>&#xA;&#xA;<p>If the example was complex enough to warrant a microservices architecture to have these two separate services, it would just be a matter of redundancy of data in the movie-reviews service and the movies-service in order to fully denormalize. The idea being that a service should try to entirely rely on itself as much as possible rather than making multiple requests to very granular services leading to an antipattern -- the nanoservices architecture. Hope this helps!</p>&#xA;"
32409527,32369075,1376205,2015-09-05T04:33:05,"<p>Yes to @Fritz's point -- universal data modeling and microservices are really two different concepts and are very difficult if not impossible to be used together. I would like to add that the reasoning for polyglot persistence is also because of how the data should be modeled. Microservices allow the use of different data stores that can best model the data according to their domain.</p>&#xA;&#xA;<p>To elaborate more, I don't think it would do justice to mention microservices and data modeling but not domain driven design. From my experience, domain driven design really helps in thinking about services, their responsibilities, and their right to exist. For instance, I found it often to be the case that there are usually a collection of services that carries out a particular domain functionality. An example could be an e-commerce application that have payments, shopping carts, etc. These could be separated into different ""bounded contexts"" based on domain driven design terminology. </p>&#xA;&#xA;<p>With the different bounded contexts, each microservice no longer sees the same concept in the system the same, so in effect, there is no real universal data model. The easiest example that I can think of to show this, is when you also want reporting on the metrics in the system. If the example was an ecommerce application, the notion of a transaction in the orders microservice are going to be different than transactions in a reporting service. The reporting service for instance may want to know about transactions at a sub-level such as the profit or revenue generated for a particular order instead of the particular line items in an order. However, in the perspective of the orders service, the order details such as the line items and the address of the individual that made the purchase are probably important and should be known. This should then require two different data models.</p>&#xA;&#xA;<p>With respect to domain modeling, I may be a bit extreme but I would go as far as saying that if there are multiple services sharing the same data source, they should really be the same service; there should be only one service for a single data source. My arguments for that would be that the domain hasn't been properly modeled and that the coupling makes it different to evolve any one service if there are multiple services that relies on a single data source. The case could be that one service requires the schema of the data source to change while the other one does not but still is required to accommodate the schema change. Hope this helps!</p>&#xA;"
32415440,31842622,1376205,2015-09-05T16:43:19,"<p>From my experience, in a microservices architecture, it is often useful to have a service that acts like an API gateway that front loads to the more domain specific microservices that does the work. The responsibility of the API gateway could be to aggregate results and return them to the front end but consolidating responses that are returned from the microservices would be coupling the knowledge of the two services and leaking some domain knowledge into the API gateway layer. The API gateway should probably be as thin as possible and should reach out to services to accomplish something. </p>&#xA;&#xA;<p>The use case here that you're describing would be trying to authenticate the user before reaching out to the login service and then the article or comments service. Altogether the front end would still stay monolithic if they are a part of the same application. </p>&#xA;&#xA;<p>If the application becomes big enough, the application would be separated by products but probably still rely on a core set of services. In that case, they would probably live in different UIs so that would make it less complex (kind of like microservices on the back end). Just as a side note, that a microservices architecture usually introduces a set of core services that can be utilized by different teams and therefore different applications that have different UIs. An example being an ecommerce application, that has customer service department editing orders for servicing customers and customers using an orders service to make purchases. In effect, these are two applications and they will have two different UIs. Hope this helps!</p>&#xA;&#xA;<p>The other thing that I'd like to point out is that a microservices architecture is only great when the application becomes too large and complex. A microservices architecture requires more resources as it has some additional overhead. Start with a monolithic first :).</p>&#xA;"
33712146,33706599,1376205,2015-11-14T18:59:32,"<p>I would advocate putting it in memory but not quite accessing it as a global. You can add a layer of abstraction to the configuration that retrieves the parameters and injects it into your application setup. The configuration layer would then be responsible just for reading the configuration whether its from the environment variables or your service registry.</p>&#xA;&#xA;<p>There's really no advantage of writing it to a config file after its been read from the service registry just because the configuration is probably dynamic. In this scenario, when a required downstream service is down, you'll probably use the configuration abstraction to again, read from the service registry to find another instance of the service to use to prevent downtime. Writing the configuration to a file pretty much assumes that your configuration is going to be static and singular over being dynamic.</p>&#xA;"
33176842,33165216,1376205,2015-10-16T18:03:32,"<p>You pretty much asked three questions and they're all somewhat related so I'll try my best to address all three together.</p>&#xA;&#xA;<p>For one, request routing in an API gateway is more than just a proxy and the implementation would not involve conditions to examine the request before shipping it off to a downstream service. API gateway would likely be the only entry point to your services in which authentication would also be taken care of on the layer to make ensure that a request has the permission to go to a downstream service. Authentication is likely to be another service itself. The high level implementation of the API gateway is likely to consolidate most if not all of the endpoints on all the downstream services. </p>&#xA;&#xA;<p>Lets take a small example such as an e-commerce application that includes a service for listing products, searching products, and shopping carts. The API gateway would then also have these same endpoints and will delegate the request further to a service responsible for the request. The API in this example may have <code>/products</code> to list all products, <code>/products?query=...</code> to search products, and finally <code>/carts/:id/products</code> to list the products in a shopping cart. Hope this answers your question. </p>&#xA;&#xA;<p>Aside from that, I know that you've mentioned that its for a new project and just wanted to give you 2 cents that this may not be the best architecture to use for your new project if your team is really small because there is a large operational overhead. Overhead that requires standardization, automating deployments, integration, etc. Its probably best to start off with a traditional MVC architecture and slowly evolve it to microservices when the project has taken off.</p>&#xA;"
33223644,33194778,1376205,2015-10-19T20:48:30,"<p>This seems to be an architectural question and while it makes sense, the question is also a little open ended depending on what your system requires. A couple of things that come off the top of my head is:</p>&#xA;&#xA;<ol>&#xA;<li>Use the <a href=""https://wiki.apache.org/solr/DataImportHandler"" rel=""nofollow"">DataImportHandler</a> from Apache SOLR. </li>&#xA;<li>Use a message queue like Kafka or Kinesis and have the independent services consume from it to propagate to their data stores in this case, a search service backed by Apache SOLR and another service backed by MySQL.</li>&#xA;</ol>&#xA;&#xA;<p>Personally, I've never used the DataImportHandler myself but my initial thoughts are that it couples Apache SOLR to MySQL. Setting up the DataImportHandler requires Apache SOLR to know the MySQL schema, the access credentials, etc. Because of this, I would advise the second option which moves towards a <strong>shared-nothing</strong> architecture. </p>&#xA;&#xA;<p>I'm going to call the service that is backed by MySQL the ""entity"" service as it sounds like its going to be the canonical service for saving some particular type of object. The entity service and the search service will have its own particular consumer that ingests events from Kinesis or Kafka into their data stores, the search service to Apache SOLR and the entity service to MySQL. </p>&#xA;&#xA;<p>This helps decouple the services from knowing that each other exists and also allow each of them to scale independently from each other. It'll be a redundancy in data but it should be alright because the data access patterns are different. </p>&#xA;&#xA;<p>The other caveat that I'd like to mention is that it assumes that the entity for which you're saving is allowed to be asynchronous. Notice that messages in this system doesn't require it to be persisted in MySQL at the moment which in this case is the entity service. However, you may change it to your liking such that a message persists in the entity service and then is propagated through a queue to a search service to index. After its been index, you can just add additional endpoints to search Apache SOLR. Hope this gives you some insight on how some other architectures may come into play. If you give a little more insight as to your system and the entities that involved, you might be able to get a better answer.</p>&#xA;"
36818740,36812791,1376205,2016-04-24T02:45:20,"<p>In a microservices architecture, the procedure is to distill the use cases and the service boundaries of the application. In the question above, there are at least two service boundaries, namely one for transactions and another for reporting.</p>&#xA;&#xA;<p>When you have two different service boundaries, the typical approach is to duplicate some data elements between them eg. whenever you make a sale, the data, should be sent to both the reporting and transactional services. One possible approach of broadcasting the data to the different boundaries is to use a message queue. Duplicating the data allows them to be evolve and operate independently and become self sufficient which is one of the goals of microservices.</p>&#xA;&#xA;<p>A personal word of advice though, you might want to start with a monolith before going the microservices route. Microservices are generally more operationally heavy; it will be difficult to reason about its advantages during the initial application stages. It tends to work better after having developed the monolithic application since it would be easier to see what didn't work and what could be improved by a microservices-like system.</p>&#xA;"
35321980,35315679,1376205,2016-02-10T17:29:29,"<p>It's likely to depend on the domain and the importance of a request made; e.g. a request to create a bank account transfer vs a request to post a status update won't be created equal.</p>&#xA;&#xA;<p>With that said, they <em>could</em> still technically handle the error in the same way. My 2 cents would be to ask for a retry even in the case of a server failure or database failure. This might be a little more about software development process of mobile client server applications but I believe <strong>most</strong> fixes should inherently be on the server side. This is because people may not have updated the iPad and annoying them to do an upgrade is poor user experience. Putting responsibility for fixes on the server side also helps avoiding to go through the Google or Apple approval process before it hits app stores.</p>&#xA;&#xA;<p>Moving forward I would suggest the following:</p>&#xA;&#xA;<ul>&#xA;<li>Be resilient in the mobile application and handle all errors instead of letting the application crash: use try/catch blocks to handle errors, <strong>ask the users to retry</strong>, or let them know you have reported the problem. Send a log of the bug to the server if possible.</li>&#xA;<li><strong>QA and test thoroughly</strong> before putting it in the app store to avoid having to deploy multiple times because of bug fixes.</li>&#xA;<li>Architect your software such that fixes usually happen on the server side to minimize client-side headaches (and buggy live code). However, errors like not providing enough information in a request to send to the server should be handled on the mobile application -- but this kind of problem should be mitigated through a strict QA process. </li>&#xA;</ul>&#xA;&#xA;<p>Hope this helps.</p>&#xA;"
42401770,42400613,605134,2017-02-22T20:37:22,"<p>You can try installing this COM with regsvr32 via StartUp and see if that would work.</p>&#xA;&#xA;<p><a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-cloud-services-migration-worker-role-stateless-service#startup-tasks"" rel=""nofollow noreferrer"">Here are some notes on Startup Tasks</a>.</p>&#xA;"
51657223,51541318,3949518,2018-08-02T15:24:07,"<p>For me, There are some options that we can choose:</p>&#xA;&#xA;<ul>&#xA;<li><p>Do the mapping on server. book service get the list of books from DB and then get the authorIds, using them to internally call the author service for getting author name.</p></li>&#xA;<li><p>Let the client doing the lazy loading. In this case, the book service return list of books (id, name, authorId). The client then need to aggregate list of authorId and call author service to get the name of authors. Finally, the client do the map itself.</p></li>&#xA;<li><p>Denormalizing DB. We have 2 options </p></li>&#xA;</ul>&#xA;&#xA;<p>options 1: Add authorName column to book DB.</p>&#xA;&#xA;<p>options 2: Have a new NoSQL database to cache data from both book and author. With this option, It's also good for large scale application where the amount of request to get data is large (or even need search and the other query requirements).</p>&#xA;&#xA;<p>In the 2 above options. We need to make sure every change to author table need to be synced to the related denormalized piece of information.</p>&#xA;"
50466235,50454109,2580686,2018-05-22T11:11:10,"<p>In your case using direct REST calls should be fine.</p>&#xA;&#xA;<p><strong>Option 1 Use Rest API :</strong> </p>&#xA;&#xA;<p>When you need synchronous communication. For example, your case. This option is suitable.</p>&#xA;&#xA;<p><strong>Option 2 Use AMQP :</strong></p>&#xA;&#xA;<p>When you need asynchronous communication. For example when your order service creates order you may want to notify product service to reduce the product quantity. Or you may want to nofity user service that order for user is successfully placed.</p>&#xA;&#xA;<p>I highly recommend having a look at <a href=""http://microservices.io/patterns/index.html"" rel=""noreferrer"">http://microservices.io/patterns/index.html</a>  </p>&#xA;"
45562723,45538292,5810894,2017-08-08T08:07:27,"<p>You can also test this use case in kafka with the command line tools. </p>&#xA;&#xA;<p>You create a producer with</p>&#xA;&#xA;<pre><code>bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test&#xA;</code></pre>&#xA;&#xA;<p>Then, you can create two different Consumer Groups (cgB, cgC) with </p>&#xA;&#xA;<pre><code>bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning --consumer-property group.id=cgB&#xA;&#xA;bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning --consumer-property group.id=cgC&#xA;</code></pre>&#xA;&#xA;<p>As soon as you send a message to the topic, both groups (B,C) will receive the message but will save what message they processed independently.  </p>&#xA;&#xA;<p>Better explained here: <a href=""https://kafka.apache.org/quickstart"" rel=""nofollow noreferrer"">Kafka quickstart</a></p>&#xA;"
46178634,46143509,5810894,2017-09-12T14:08:16,<p>You can try to set the loglevel just of the LoadBalancerContext to debug in application.properties</p>&#xA;&#xA;<pre><code>#logging&#xA;logging.level.com.netflix.loadbalancer.LoadBalancerContext=DEBUG&#xA;</code></pre>&#xA;
44656768,44642751,54734,2017-06-20T15:07:01,"<blockquote>&#xA;  <p>I looked to AWS Kinesis as event stream. I think it would be good for choreographed microservices.</p>&#xA;</blockquote>&#xA;&#xA;<p>I don't think that's the use case that Kinesis is designed for; see this <a href=""https://www.quora.com/What-is-the-difference-between-Kinesis-and-SQS-It-seems-capable-of-serving-similar-use-cases-apart-from-the-shards-and-partition-keys"" rel=""nofollow noreferrer"">overview by Aditya Krishnan</a>.  Or this previous <a href=""https://stackoverflow.com/questions/26623673/why-should-i-use-amazon-kinesis-and-not-sns-sqs"">question on stack overflow</a>.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Should I persist the events (orderCreated and orderPaid event) from AWS Kinesis to database in customer command-side service?</p>&#xA;</blockquote>&#xA;&#xA;<p>From my point of view, the issue is this: you really don't want events to leak out to subscribers <em>and then not appear in the book of record</em>.  So the usual ordering is to put the events into the durable store, and only after you get the acknowledgement of the write (which is a proxy for ""acknowledgment that we've reached our minimum durability guarantee""), then you start sharing the events out.</p>&#xA;&#xA;<p>So most designs reverse the order you propose - durable store (database) first, publish second.  But you are losing latency; subscribers can't see the event until the store finishes.  Depending on your design, you might be able to make up some of that with batch reads.</p>&#xA;&#xA;<blockquote>&#xA;  <p>We have tried the SQS and SNS. But, the performance is not good enough. It takes about 5 seconds to publish and consume the events.</p>&#xA;</blockquote>&#xA;&#xA;<p>Hmm, given what I see in the recommendations for Kinesis, it doesn't look like you are going to beat that by more than an <a href=""http://docs.aws.amazon.com/streams/latest/dev/kinesis-low-latency.html"" rel=""nofollow noreferrer"">order of magnitude</a>; they seem to be recommending full pipes rather than fast ones.</p>&#xA;"
47021549,47020319,54734,2017-10-30T18:02:21,"<blockquote>&#xA;  <p>First, if in a microservices architecture, each service can be developed independently how do we account for inter-service communication dependencies ?</p>&#xA;</blockquote>&#xA;&#xA;<p>Messages - you break direct coupling between the services by concentrating on the messages that they exchange, and concentrate on a change strategy for your schema that is forwards and backwards compatible (so that old services can read messages from new ones).</p>&#xA;&#xA;<p>Greg Young writes about these ideas in his book of <a href=""https://leanpub.com/esversioning"" rel=""nofollow noreferrer"">event versioning</a>.</p>&#xA;&#xA;<blockquote>&#xA;  <p>If the top level request fires off a bunch of cascading or interdependent events which need to be handled before returning a response to the caller, is this a scenario suited well for microservices ?</p>&#xA;</blockquote>&#xA;&#xA;<p>It's fine, actually, so long as you incorporate stale data into your design.</p>&#xA;&#xA;<p>Fundamentally, the response to the query takes time to travel to the client; unless you are locking out all writers while the data is in transit, there is every possibility the ""truth"" of the system will have changed while the packets were in flight.</p>&#xA;&#xA;<p>So you don't specify that queries describe the state ""now"", but rather that queries describe the state as of some time in the past.  So if you send a query request to service A, and the result includes data from service B, then the query result is going to include A's cached copy of B's data as of some particular time.</p>&#xA;&#xA;<p>So A's query of B to get the data is asynchronous with regards to the request sent to A.  If refreshed data arrives from B in time to answer the query, great -- you answer with somewhat fresher stale data.</p>&#xA;&#xA;<p>And yes, it can happen that C writes a change to B, gets an acknowledgement, then queries A... and gets back a response that does not include the changes that were already written and acknowledged.</p>&#xA;&#xA;<p>So you build into the solution that there is no universal clock.</p>&#xA;&#xA;<blockquote>&#xA;  <p>On the first question though, it seems as a developer of Service B, i would need to know all of the events that can be fired from Service A, and I would have a continual dependency if there are new events added.</p>&#xA;</blockquote>&#xA;&#xA;<p>Not all events.  You need a common format (like avro, or json, or protocol buffers) so that the event representation can be deserialized, and you need the consumer to be able to recognize the events that it does care about, but events that the consumer doesn't recognize can fall through to a single default handler.</p>&#xA;"
47555433,47554214,54734,2017-11-29T14:44:02,"<blockquote>&#xA;  <p>Microservice A only allows linking A to B if it has previously received a ""B created"" event and no ""B deleted"" event.</p>&#xA;</blockquote>&#xA;&#xA;<p>There's a potential problem here; consider a race between two messages, <code>link A to B</code> and <code>B Created</code>.  If the <code>B Created</code> message happens to arrive first, then everything links up as expected.  If <code>B Created</code> happens to arrive second, then the link doesn't happen.  In short, you have a business behavior that depends on you message plumbing.</p>&#xA;&#xA;<p><a href=""http://udidahan.com/2010/08/31/race-conditions-dont-exist/"" rel=""nofollow noreferrer"">Udi Dahan, 2010</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>A microsecond difference in timing shouldn’t make a difference to core business behaviors.</p>&#xA;  &#xA;  <p>A potential disadvantage for solution 2 could maybe be the added complexity of projecting these events in the read model, especially if more microservices and aggregates following the same pattern are added to the system.</p>&#xA;</blockquote>&#xA;&#xA;<p>I don't like that complexity at all; it sounds like a lot of work for not very much business value.</p>&#xA;&#xA;<p>Exception Reports might be a viable alternative.  <a href=""https://www.youtube.com/watch?v=LDW0QWie21s&amp;t=12m30s"" rel=""nofollow noreferrer"">Greg Young talked about this in 2016</a>.  In short; having a monitor that detects inconsistent states, and the remediation of those states, may be enough.</p>&#xA;&#xA;<p>Adding <em>automated</em> remediation comes later.  <a href=""https://abdullin.com/post/ddd-evolving-business-processes-a-la-lokad/"" rel=""nofollow noreferrer"">Rinat Abdullin described</a> this progression really well.</p>&#xA;&#xA;<p>The automated version ends up looking something like solution 2; but with separation of the responsibilities -- the remediation logic lives outside of microservice A and B.</p>&#xA;"
39595506,39593580,54734,2016-09-20T13:29:42,"<blockquote>&#xA;  <p>Could it also be considered as a wrong modeling? Such as - if the data were needed in A context, then it should have been part of that context from start.</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes, it could be.</p>&#xA;&#xA;<ul>&#xA;<li><a href=""http://udidahan.com/2010/11/15/the-known-unknowns-of-soa/"" rel=""nofollow"">The Known Unknowns of SOA</a></li>&#xA;<li><a href=""http://particular.net/blog/secret-of-better-ui-composition"" rel=""nofollow"">Secret of Better UI Composition</a></li>&#xA;</ul>&#xA;&#xA;<p>In summary, if micro-service B is the technical authority for the data that you need in this use case, then micro-service B should be providing that capability.</p>&#xA;&#xA;<blockquote>&#xA;  <p>is that considered a ""violation"" of current design and coupling?</p>&#xA;</blockquote>&#xA;&#xA;<p>In that design, if micro-service B is unavailable, then micro-service A can't provide value.  That sounds like coupling to me.</p>&#xA;&#xA;<p>My guess, if you are trapped in this pattern, would be to communicate synchronously with B, but with a local cache of the data for those occasions when B isn't available.  </p>&#xA;&#xA;<p>Some of the problems go away if the data being shared is immutable.</p>&#xA;&#xA;<blockquote>&#xA;  <p>This of course works only when working with cached data is allowed by business, otherwise A would need to own the data or throw an exception</p>&#xA;</blockquote>&#xA;&#xA;<p>If the writer of the data is B, then any data seen by A <em>is</em> cached data; service B could be changing the data while A is looking at it.  If A is making a decision that requires a live copy of data written by B, then you've got a bigger problem -- your service boundaries are in the wrong place.</p>&#xA;"
51123663,51120978,54734,2018-07-01T13:21:44,"<blockquote>&#xA;  <p>The REST interface is designed to be efficient for large-grain hypermedia data transfer, optimizing for the common case of the Web, but resulting in an interface that is not optimal for other forms of architectural interaction.&#xA;  -- <a href=""https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm#sec_5_1_5"" rel=""nofollow noreferrer"">Roy Fielding, 2000</a></p>&#xA;</blockquote>&#xA;&#xA;<p>""REST communication protocol is synchronous.""</p>&#xA;&#xA;<p>That's not quite right, on a couple of levels.</p>&#xA;&#xA;<p>First, there is no ""REST communication protocol""; REST is an architectural style.  </p>&#xA;&#xA;<p><a href=""https://tools.ietf.org/html/rfc7230"" rel=""nofollow noreferrer"">Hypertext Transport Protocol</a>, aka HTTP, is a an application protocol for for hypertext information systems.  REST is an architectural style, the web is the reference implementation.</p>&#xA;&#xA;<p>Second, HTTP isn't actually synchronous.  Because there are no generic <a href=""http://www.enterpriseintegrationpatterns.com/patterns/messaging/CorrelationIdentifier.html"" rel=""nofollow noreferrer"">correlation identifiers</a> in the metadata of the request, the client needs to keep track of the order in which requests were sent along a given connection.  See <a href=""https://tools.ietf.org/html/rfc7230#section-5.6"" rel=""nofollow noreferrer"">RFC 7230, Section 5.6</a>.  It's ""just"" messaging.</p>&#xA;&#xA;<p>Apache's <a href=""https://hc.apache.org/httpcomponents-core-ga/tutorial/html/nio.html#d5e612"" rel=""nofollow noreferrer"">HttpCore Tutorial</a> includes a discussion of non-blocking HTTP connections. </p>&#xA;"
49102704,49100994,54734,2018-03-05T02:37:22,"<blockquote>&#xA;  <p>I want to expand this pattern that the aggregate can write its data over 2 or more physical data sources depending on the underlying object type.</p>&#xA;</blockquote>&#xA;&#xA;<p>Why do you want to do that on purpose?</p>&#xA;&#xA;<p>In most cases, the persistence implementation is chosen to serve the domain, rather than the other way around.  So the happy path typically involves choosing a persistence solution that can record the state of the entire aggregate, and storing the entire thing within a single transaction.</p>&#xA;&#xA;<p>So if you find yourself trying to store an aggregate in two different places, you should take a hard careful look at why.</p>&#xA;&#xA;<p>One common answer is that you want to be able to query the aggregate state efficiently.  <a href=""/questions/tagged/cqrs"" class=""post-tag"" title=""show questions tagged &#39;cqrs&#39;"" rel=""tag"">cqrs</a> is a common solution here - rather than persisting the aggregate in two different data stores, you persist it to one and replicate it to another.  The queries can run very efficiently against the replica (although there is of course some additional latency between a change to the aggregate and the reflection of that change in the query results).</p>&#xA;&#xA;<p>Another common answer is that you really have <em>two</em> aggregates that reference each other.  Nothing wrong with storing two aggregates in different places.  You may be better served by making the distinction between the two explicit in your code.</p>&#xA;&#xA;<ul>&#xA;<li><a href=""http://www.addsimplicity.com/adding_simplicity_an_engi/2006/12/avoiding_two_ph.html"" rel=""nofollow noreferrer"">Dan Pritchett</a></li>&#xA;<li><a href=""https://lostechies.com/jimmybogard/2013/05/09/ditching-two-phased-commits/"" rel=""nofollow noreferrer"">Jimmy Bogard</a></li>&#xA;</ul>&#xA;&#xA;<blockquote>&#xA;  <p>How deals Domain Driven Design if within the aggregate - depending on the type of the underlying object - it is hydrated over different data sources?</p>&#xA;</blockquote>&#xA;&#xA;<p>Badly, just like everybody else.  </p>&#xA;"
49111969,49107497,54734,2018-03-05T13:50:23,"<blockquote>&#xA;  <p>I find it is highly coupled with other services and it's not independent.</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes.</p>&#xA;&#xA;<p>You can improve the independence by thinking carefully about the failure modes; how should the UI elements behave when the relevant data authority is not available.  You can, for example, have the homepage simply display a ""data not available"" message if the remote authority doesn't respond in a timely fashion.  Or you can have the homepage display a cached copy of some stale data.  It may even make sense to <em>always</em> render the response using cached data, with updates to the cache happening in the background.</p>&#xA;&#xA;<p>There are techniques that you can use to reduce the coupling between the services.<br>&#xA;One approach is described by Udi Dahan, under the umbrella of <a href=""http://udidahan.com/2012/06/23/ui-composition-techniques-for-correct-service-boundaries/"" rel=""nofollow noreferrer"">UI Composition Techniques</a>.  His idea is that you use an element, like a <em>widget</em>, to encapsulate the logic of display and the failure mode.</p>&#xA;&#xA;<p>The underlying mechanics aren't really all that different; the widget has all of the same problems we had before when the data isn't available.  But what it does do is separate out two problems (what do you do when the widget isn't available, what do you do when the data isn't available) that belong to different teams: the homepage team gets to decide what to do when the widget isn't available (perhaps a cached copy is used), the service team gets to decide what to do when the widget can't reach the backing data (perhaps the widget has its own cache of stale data).</p>&#xA;&#xA;<p>There's no magic; you've got a distributed system, and a consequence of this is that you need to consider in your design the fact that the remote processes won't always be available.</p>&#xA;"
46586339,46581254,54734,2017-10-05T13:04:05,"<blockquote>&#xA;  <p>In a microservice architecture a service is a single endpoint or a single module with several endpoints?</p>&#xA;</blockquote>&#xA;&#xA;<p>Conceptually, microservices are just services, which is to say they are just objects processing messages.  Messages come in, the service decides whether or not to update its own in memory black box, messages go out.</p>&#xA;&#xA;<p>The integration layer that you put in front of a microservice might have several different ""endpoints"" to help you pass in new messages to be processed.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Are the endpoints that are fine grained or the granularity is at a higher level? </p>&#xA;</blockquote>&#xA;&#xA;<p>Either way - there is fundamentally nothing wrong with having a single endpoint that sends a burst of messages to the service.</p>&#xA;&#xA;<blockquote>&#xA;  <p>I am now finding articles that say that in microservices architecture a service is associated to ""bounded context"".</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes, me too.  I haven't seen any that demonstrate that they mean ""Bounded Context"" in the sense that Eric Evans originally described in the blue book.</p>&#xA;"
44996936,44996290,54734,2017-07-09T13:29:34,"<blockquote>&#xA;  <p>Becouse when we build microservice and in models module some field of DTO model change we get error on compile time.</p>&#xA;</blockquote>&#xA;&#xA;<p>The problem you are facing is generally recognized as message versioning.</p>&#xA;&#xA;<p>A key idea in microservices is that they can be redeployed independently of each other; you should be able to change them at any time without breaking everything else.  To achieve this, we limit the coupling between microservices to the messages that they have in common.</p>&#xA;&#xA;<p>As a consequence of this; it follows that it is <em>really important</em> to get the message schemas ""right"".</p>&#xA;&#xA;<p><em>Right</em> in this circumstance doesn't mean that you figure out the correct messages to send the first time, but rather than you invest upfront design capital in understanding how messages change, and what message structures support those changes.</p>&#xA;&#xA;<p>The best single reference on the topic I know of is Greg Young's book <a href=""https://leanpub.com/esversioning/read"" rel=""nofollow noreferrer"">Versioning in an Event Sourced System</a>.</p>&#xA;&#xA;<p>Greg makes a case for using ""weak schemas"" to support forward and backwards compatibility; the basic ideas are</p>&#xA;&#xA;<ul>&#xA;<li>The meaning of a field never changes</li>&#xA;<li>The consumer must ignore <em>and</em> must forward fields it doesn't understand</li>&#xA;<li>Most (all?) fields should be optional; the consumer must be prepared for the event that some data element is missing from the message.  That might mean having an acceptable default value prepared, or alternative processing.</li>&#xA;</ul>&#xA;&#xA;<p>With the basics in mind; you can look at the details of <a href=""https://avro.apache.org/"" rel=""nofollow noreferrer"">Avro</a>, <a href=""https://thrift.apache.org/"" rel=""nofollow noreferrer"">Thrift</a>, <a href=""https://developers.google.com/protocol-buffers/"" rel=""nofollow noreferrer"">Protocol Buffers</a> to get a sense for how the standardized formats tend to evolve.  Martin Kleppmann's discussion of <a href=""https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html"" rel=""nofollow noreferrer"">Schema Evolution in Avro, Protocol Buffers, and Thrift</a> might be a good intermediate step.</p>&#xA;&#xA;<p>Message discipline is really important -- the schema is part of the API of the microservice.  Introducing a backwards incompatible change to the API introduces a lot of risk, and should not be undertaken lightly.</p>&#xA;&#xA;<blockquote>&#xA;  <p>I have models module with all DTO classes.</p>&#xA;</blockquote>&#xA;&#xA;<p>That may be an error; it's hard to say.  It is certainly an error to have a common core that routinely evolves in a way that isn't backwards compatible.  That habit should change.</p>&#xA;"
44939368,44927532,54734,2017-07-06T03:34:22,"<blockquote>&#xA;  <p>How do I pass information back to the client of the service (in this case the API gateway) about the ID of the new graph so that it can perform a query to get the representation of the new resource and send it to the user? Or at least get an ID so that the web client can ask the API gateway, etc?</p>&#xA;</blockquote>&#xA;&#xA;<p>By listening for the echo.</p>&#xA;&#xA;<p>The basic idea behind at least once delivery is that I'm going to send you a message, and keep sending it over and over until I receive a message that proves you've received at least one copy of my message.</p>&#xA;&#xA;<p>Therefore, my protocol looks something like</p>&#xA;&#xA;<ol>&#xA;<li>Establish a mail box where I can collect messages</li>&#xA;<li>Encode into the message instructions for delivering to my mailbox</li>&#xA;<li>Send the message to you</li>&#xA;<li>Check my mailbox&#xA;&#xA;<ul>&#xA;<li>if the answer is there, I'm done</li>&#xA;<li>otherwise, I send you another copy of the message</li>&#xA;</ul></li>&#xA;</ol>&#xA;&#xA;<p>The mail box could be implemented any number of ways -- it could be a callback; it could be a promise, it could be a <a href=""http://www.enterpriseintegrationpatterns.com/ramblings/09_correlation.html"" rel=""nofollow noreferrer"">correlation identifier</a>.  You could have the signal dispatched by the command handler, when it gets acknowledgement of the write by the book of record, or by the ""read model"" when the new resource is available.</p>&#xA;"
51996502,51996128,54734,2018-08-24T02:02:03,"<p>Martin Fowler's <em>Patterns of Enterprise Application Architecture</em> is probably a reasonable starting point.</p>&#xA;&#xA;<blockquote>&#xA;  <p>If you have complicated and ever changing business rules involving validation, calculations, and derivations, chances are that you'll want an object model to handle them.  On the other hand, if you have simple not-null checks and a couple of sums to calculate, a Transaction Script is a better bet.</p>&#xA;</blockquote>&#xA;&#xA;<p>Another heuristic to consider is whether or not this specific micro service contributes to your companies competitive advantage.  If you are building something you would rather buy, then a heave investment in domain modeling doesn't make a lot of sense.</p>&#xA;&#xA;<p>On the other hand, if you are expecting to live in this code until it becomes catastrophically successful and you all retire to a beach somewhere, then domain modeling becomes more compelling.</p>&#xA;&#xA;<p>Horses for Courses.</p>&#xA;"
48282058,48279479,54734,2018-01-16T13:09:59,"<blockquote>&#xA;  <p>At this point, the frontend is no more waiting for a response due to the fact that this kind of flow is asynchronous. How could we complete this flow in order to forward the user to the Movie Information Web page? We should wait that the creation process is done before querying the QueryManager. </p>&#xA;</blockquote>&#xA;&#xA;<p>Short answer: make the protocol explicit.</p>&#xA;&#xA;<p>Longer answer: a good place to look for inspiration here is HTTP.</p>&#xA;&#xA;<p>The front end makes a POST to the origin server; as a result the origin server places a message on the queue and <a href=""https://tools.ietf.org/html/rfc7231#section-6.3.3"" rel=""nofollow noreferrer"">sends a response back</a>.  </p>&#xA;&#xA;<blockquote>&#xA;  <p>The representation sent with this response ought to describe the request's current status and point to (or embed) a status monitor that can provide the user with an estimate of when the request will be fulfilled.</p>&#xA;</blockquote>&#xA;&#xA;<p>The client can then poll the endpoint to find out what progress has been made.</p>&#xA;&#xA;<p>For instance, the endpoint might be a query into the data store, that looks for evidence that the command manager has processed the original command; or it might be an endpoint that is watching the bus for the MovieCreated message, and changes its answer based on whether or not it has seen that.</p>&#xA;&#xA;<p>It may help clarify things to look into <a href=""https://tools.ietf.org/html/rfc7231#section-4.2.2"" rel=""nofollow noreferrer"">idempotent</a> request handling; when the Command Manager pulls a message off of its queue, <a href=""https://www.infoq.com/articles/no-reliable-messaging"" rel=""nofollow noreferrer"">how does it know</a> if it has previously processed a copy of that message?  Your polling endpoint should be able to use the same information to let the consumer know that the message has been successfully processed.</p>&#xA;"
46196089,46190467,54734,2017-09-13T11:16:04,"<blockquote>&#xA;  <p>The problem with the flow above is that since the transactional data save i.e. persistence to the event store and storage to the microservice's read data happen in a different transaction context if there is any failure at step 9 how should i handle the event which has already been propagated to the event store and the aggregate which has already been updated?</p>&#xA;</blockquote>&#xA;&#xA;<p>You retry it later.</p>&#xA;&#xA;<p>The ""book of record"" is the event store.  The downstream views (the ""published events"", the read models) are derived from the book of record.  They are typically behind the book of record in time (<a href=""https://academy.realm.io/posts/conflict-resolution-for-eventual-consistency-goto/"" rel=""nofollow noreferrer"">eventual consistency</a>) and are not typically synchronized with each other.</p>&#xA;&#xA;<p>So you might have, at some point in time, 105 events written to the book of record, but only 100 published to the queue, and a representation in your service database constructed from only 98.</p>&#xA;&#xA;<p>Updating a view is typically done in one of two ways.  You can, of course, start with a brand new representation and replay all of the events into it as part of each update.  Alternatively, you track in the metadata of the view how far along in the event history you have already gotten, and use that information to determine where the next read of the event history begins.</p>&#xA;"
46201276,46199874,54734,2017-09-13T15:21:49,"<blockquote>&#xA;  <p>Both write operations have to happen as one atomic operation.</p>&#xA;</blockquote>&#xA;&#xA;<p>There's a really important question to raise at this point: why?  What is the cost to the business if the remote event log is not synchronized with the book of record?</p>&#xA;&#xA;<p>If you don't need that synchronization, then a straight forward approach is to put a copy of your event log into the same database as your write model.  Udi Dahan discusses this approach in <a href=""https://vimeo.com/111998645"" rel=""nofollow noreferrer"">Reliable Messaging Without Distributed Transactions</a>.  After the write transaction succeeds, you can then replicate the events from the SQL store to the remote event log.</p>&#xA;&#xA;<p>This gives you a remote event log that is always consistent with <em>some</em> state in the past, but doesn't promise to be caught up to the present.</p>&#xA;&#xA;<p><em>This is usually good enough</em>; after all, the event log itself is a snapshot of the past, and the book of record may be changing while the representation of the event log is copied to the consumer.</p>&#xA;&#xA;<p>But if that won't do, your choices are to find a distributed transaction engine that provides an acceptable compromise, or to use sagas to undo your changes to the local store if the remote write fails.</p>&#xA;&#xA;<p>Yan Cui's discussion of the <a href=""http://theburningmonk.com/2017/07/applying-the-saga-pattern-with-aws-lambda-and-step-functions/"" rel=""nofollow noreferrer"">saga pattern in aws</a>, which in turn references Caitie McCaffrey's <a href=""https://www.youtube.com/watch?v=xDuwrtwYHu8"" rel=""nofollow noreferrer"">2015 talk on sagas in distributed systems</a>, raises this point:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Because the compensating actions can also fail so we need to be able to retry them until success, which means they have to be idempotent.</p>&#xA;  &#xA;  <p>In practice, there should be a reasonable upper limit on the no. of retries before you alert for human intervention.</p>&#xA;</blockquote>&#xA;&#xA;<p>So yes - you retry.</p>&#xA;"
49945873,49944789,54734,2018-04-20T16:17:25,"<blockquote>&#xA;  <p>I get how balances can be reversed but I do not see how we can accomplish the same effect for status</p>&#xA;</blockquote>&#xA;&#xA;<p>So the first thing to do would be to check with your domain experts to understand if the ubiquitous language has the concept of the status of an invoice between the reversal and the repayment.</p>&#xA;&#xA;<p>Based on what I see here, I would expect a reversal by itself to put the status on billed again.  We thought it was paid, but that entry is a mistake, so we would restore the object to its previous state.</p>&#xA;&#xA;<p>If that were correct, then we would have a status of <code>Billed</code> there.</p>&#xA;&#xA;<p>But it might not be -- this isn't an undo, it's a behavior within your domain.  It might be that this moves the domain to a previously undiscovered part of the state machine.</p>&#xA;&#xA;<blockquote>&#xA;  <p>how will I handle if api calls(commands) come out of order for the above events.</p>&#xA;</blockquote>&#xA;&#xA;<p>There are two different questions that may be hidden there - I will summarize each.</p>&#xA;&#xA;<p>If you are worried about downstream consumers reacting to <em>events</em>, it is important to design your consumers such that -- if they need to know the entire history, then they read from the history.  A consumer that reacts to the changes in history will read an ordered history from the event store, rather than reacting to messages that appear on message transport.  In other words, the published event acts like a notification, telling the consumer to refresh its copy of the history.</p>&#xA;&#xA;<p>If you are worried about the fact that you get the ""wrong"" history if commands appear out of order, then you need to review and integrate Udi Dahan's essay <a href=""http://udidahan.com/2010/08/31/race-conditions-dont-exist/"" rel=""nofollow noreferrer"">Race Conditions Don't Exist</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>A microsecond difference in timing shouldn’t make a difference to core business behaviors.</p>&#xA;</blockquote>&#xA;"
49992546,49990620,54734,2018-04-24T02:27:41,"<blockquote>&#xA;  <p>Immutability when using event sourcing good idea?</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes.</p>&#xA;&#xA;<blockquote>&#xA;  <p>My concern is if I implement my domain model as immutable will I not be creating a lot of domain objects just for my domain model to be up to date and that is wasted resources.</p>&#xA;</blockquote>&#xA;&#xA;<p>That's so.  Are those wasted resources expensive?  Have you measured?</p>&#xA;&#xA;<blockquote>&#xA;  <p>I want to know what if there is any upside to make my Domain model immutable if I am planning to use in with event sourcing. </p>&#xA;</blockquote>&#xA;&#xA;<p>The biggest is concurrency -- sharing state between concurrent operations is much easier to get correct when the state is immutable.  Consequently, immutable state is a lot easier to share - you don't need to make fresh copies of data just in case you hit a branch that wants to change things.</p>&#xA;&#xA;<p>Operations on immutable state are a lot easier to test, and a lot easier to test in isolation (if you squint, this is really the same point as above, in a slightly different context).</p>&#xA;"
50042116,50041559,54734,2018-04-26T11:42:49,"<p>You should be familiar with Pat Helland's paper <a href=""http://cidrdb.org/cidr2015/Papers/CIDR15_Paper16.pdf"" rel=""nofollow noreferrer"">Immutability Changes Everything</a>.</p>&#xA;&#xA;<p>You should also review Udi Dahan's <a href=""http://udidahan.com/2012/08/28/data-duplication-and-replication/"" rel=""nofollow noreferrer"">Data Duplication and Replication</a>, but read with care: Udi's service language is careful to distinguish physical boundaries from logical boundaries.  Make sure you are clear on which he is describing at any given time.</p>&#xA;"
49986434,49985156,54734,2018-04-23T17:07:06,"<blockquote>&#xA;  <p>I am dipping my feet into event sourcing pattern and trying to make sense of aggregates.I have read a few blogs and now I am more confused than ever before.</p>&#xA;</blockquote>&#xA;&#xA;<p>That is not your fault.</p>&#xA;&#xA;<p>The concept of aggregates comes from the description of domain modeling by Eric Evans.</p>&#xA;&#xA;<p>In a typical deployment, we have a database full of facts that we want to track.  We have a model in which those facts change over time.  We want to ensure that we track those changes correctly, meaning without introducing inconsistencies.</p>&#xA;&#xA;<p>And the coarse answer to that is that we place our database ""behind"" a <em>domain model</em> which includes all of the rules for how the data in the database should be allowed to change.  In the time of Evans, the domain model was a tier that lay between the application tier and the persistence tier.  These days, you are more likely to hear ""component"" or ""module"" rather than tier, but the role isn't much changed: protect the database from incorrect changes.</p>&#xA;&#xA;<p>If we examine the domain carefully, we will often find within the model clusters of data that exhibit an interesting property: the rules for changing the state of the cluster don't depend on any information outside of the cluster.</p>&#xA;&#xA;<p>Example: in a trading application, bids and offers for some commodity are matched and processed.  But the rules for matching one commodity (gold) are completely independent of the data associated with a different commodity (frozen concentrated orange juice).  You don't need to know anything about what's going on in FCOJ to correctly process the activity in the gold trade, and vice versa.</p>&#xA;&#xA;<p>These clusters, which can be considered in isolation, are <em>aggregates</em>.</p>&#xA;&#xA;<p>The two key properties of that isolation are</p>&#xA;&#xA;<ul>&#xA;<li>changes to state inside the aggregate don't depend on changes made outside the aggregate</li>&#xA;<li>changes made outside the aggregate don't depend on changes made inside the aggregate</li>&#xA;</ul>&#xA;&#xA;<p>So in this example, we might have a TradeBook ""aggregate"" for Gold, and a TradeBook ""aggregate"" for FCOJ.  To process an order, you would load the aggregate you need, apply the change to it, and save it, without ever needing to touch the other.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Will I have two aggregate classes</p>&#xA;  &#xA;  <ul>&#xA;  <li>AggregateEventsByInvoice</li>&#xA;  <li>AggregateEventsByInvoiceEmployee</li>&#xA;  </ul>&#xA;</blockquote>&#xA;&#xA;<p>No, you will probably have two <em>views</em> or <em>projections</em> based on the same event history.</p>&#xA;&#xA;<p>More precisely, in the architecture described by Evans, there would be one &#xA;""aggregate root"", and each of your use cases would be a different query in the API for that aggregate.</p>&#xA;&#xA;<p>But more recently, the practice is to recognize that the use cases for <em>reads</em> don't need the same constraints as the use cases for writes.  So today you are more likely to see a <em>view</em> (or <em>projection</em>) for each of your use cases, where the in memory representation of each is built from the events recorded in your data store.</p>&#xA;&#xA;<blockquote>&#xA;  <p>so what I understand is an aggregate is essentially anything that can uniquely identify all events related to single instance (in my case invoice). So in my case can the invoiceId be considered as an aggregate? </p>&#xA;</blockquote>&#xA;&#xA;<p>No.  In your case, the invoice is likely to be the aggregate.</p>&#xA;&#xA;<p>To be more precise, your domain model is presumably coordinating changes between the balance, adjustment, status, and payment of each invoice; these values are an example of the sort of cluster I was talking about before.  You can make changes to these values without having to consider, for example, the adjustment of Invoice[67890].</p>&#xA;&#xA;<blockquote>&#xA;  <p>so what I understand is an aggregate is essentially anything that can uniquely identify all events related to single instance (in my case invoice). </p>&#xA;</blockquote>&#xA;&#xA;<p>A problem is that this understanding doesn't align well with the existing literature, and is likely to lead to confused communications.</p>&#xA;&#xA;<p>In a document store, or key value store, the aggregate is analogous to the <em>document</em>, not the key that you use to look up the document.  In an RDBMS, the aggregate would be the related entities, and the id would be the primary key that you use to load the entities.  In an event store, the contents of the stream are describing the changes to the events in the aggregate, the id is just the key you use to find the correct events.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Is it okay for event store to have additional columns that are not aggregate id</p>&#xA;</blockquote>&#xA;&#xA;<p>Sure - you can store whatever metadata you like with the event.  Creating additional columns can improve your query performance, make it easier to shard your data, and so on.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Is it okay for us to try to load events from event store that query on columns in addition to aggregate it ? (invoice id , employee id) in this case.</p>&#xA;</blockquote>&#xA;&#xA;<p>Sure, you can query on the events any way you like.</p>&#xA;&#xA;<p>What's maybe not a good idea is trying to recover the current state of your domain model by replaying an arbitrary set of your events.</p>&#xA;&#xA;<p>In your example, events <code>[1,2,3,4,5]</code> taken together tell a coherent story about the invoice.  But trying to create an understanding of the invoice from event <code>[4]</code> by itself may not get you anywhere.</p>&#xA;&#xA;<p>Remember, an event isn't usually a complete representation of the state of the model after the change, but rather a description of the things that changed.  Think ""patch"", rather than ""snapshot"".</p>&#xA;"
50028843,50025857,54734,2018-04-25T18:13:12,"<p>I'd suggest getting a few points clear in your mind.</p>&#xA;&#xA;<p>You probably want to use the same plumbing no matter what changes you are making to the domain model; whether processing a new command, or undoing an earlier one, the actual mechanics should be treated the same way.</p>&#xA;&#xA;<p>If ""Undo"" is something you need, then that should be something your domain model understands how to do, and the shape of the data you keep in memory should support the capabilities.  So if you are designing a model for invoice that allows you to query specific events in its history, your in memory representation should be giving you access to those events.</p>&#xA;&#xA;<p>The first rule of performance optimization is ""don't"", but if you are worried about the costs of reading data from the database, you can cache hot representations of your models.</p>&#xA;&#xA;<p>If you are trying to figure out optimistic writes in a relational database, <a href=""http://blog.jonathanoliver.com/event-store-transaction-integrity-without-transactions/"" rel=""nofollow noreferrer"">Jonathan Oliver</a> is a good starting point.</p>&#xA;"
42055559,42053559,54734,2017-02-05T18:18:47,"<blockquote>&#xA;  <p>I have an application where I need to store some data in a database (mysql for instance) and then publish some data in a message queue. My problem is: If the application crashes after the storage in the database, my data will never be written in the message queue and then be lost (thus eventual consistency of my system will not be guaranted). How can I solve this problem ?</p>&#xA;</blockquote>&#xA;&#xA;<p>In this particular case, the answer is to load the queue data from the database.</p>&#xA;&#xA;<p>That is, you write the messages that need to be queued to the <em>database</em>, in the same transaction that you use to write the data.  Then, asynchronously, you read that data from the database, and write it to the queue.</p>&#xA;&#xA;<p>See <a href=""https://vimeo.com/111998645"" rel=""nofollow noreferrer"">Reliable Messaging without Distributed Transactions</a>, by Udi Dahan.</p>&#xA;&#xA;<p>If the application crashes, recovery is simple -- during restart, you query the database for all unacknowledged messages, and send them again.</p>&#xA;&#xA;<p>Note that this design really expects the consumers of the messages to be designed for <a href=""http://blog.jonathanoliver.com/messaging-at-least-once-delivery/"" rel=""nofollow noreferrer"">at least once delivery</a>.</p>&#xA;"
38728607,38711908,54734,2016-08-02T19:06:38,"<p>Decided I needed a bigger space to clarify my comments</p>&#xA;&#xA;<blockquote>&#xA;  <p>It's clear there are four microservices,</p>&#xA;</blockquote>&#xA;&#xA;<p>I don't think that's clear at all.</p>&#xA;&#xA;<p>It happens that this specific image comes from&#xA;<a href=""https://github.com/cer/event-sourcing-examples"" rel=""nofollow noreferrer"">https://github.com/cer/event-sourcing-examples</a></p>&#xA;&#xA;<p>In the overview, Richardson writes:</p>&#xA;&#xA;<pre><code>One of the neat things about the modular architecture is &#xA;that there are two ways to deploy these four services:&#xA;&#xA;monolithic-service - all services are packaged as a single Spring Boot executable JAR&#xA;&#xA;Microservices - three separate Spring Boot executable JARs&#xA;    accounts-command-side-service - command-side accounts&#xA;    transactions-command-side-service - command-side money transfers&#xA;    accounts-query-side-service - Account View Updater and Account View Reader&#xA;</code></pre>&#xA;&#xA;<p><a href=""https://plainoldobjects.com/presentations/building-and-deploying-microservices-with-event-sourcing-cqrs-and-docker/scala-by-the-bay-2014/"" rel=""nofollow noreferrer"">Presenting</a> at Scala By the Bay, he calls out the pattern of one microservice publishing into an event store, and a second microservice subscribing to those events and updating a view store. </p>&#xA;&#xA;<p>So I think you can make fair arguments for counting this as one, two, three, or four microservices.</p>&#xA;&#xA;<p>One thing to consider in your counting is that the Account View Adapter needs to have an understanding in common with Accounts, and an understanding in common with Transfers, in order to understand the state information contained within the events to produce the view (the Account View Reader does <em>not</em> need this shared understanding, because it is just copying the data from the view store).</p>&#xA;&#xA;<p><a href=""https://www.infoq.com/news/2015/02/service-boundaries-healthcare"" rel=""nofollow noreferrer"">Udi Dahan</a> has some interesting things to say about service boundaries; in particular, that you preserve loose coupling among your services by limiting the amount of data publicly shared by your services.  </p>&#xA;&#xA;<p>My application of his guideline to this diagram tells me that the Account View and the two aggregates must be part of the same boundary, because they all have access to the data which is private to this service.  </p>&#xA;&#xA;<p>Of course, you could still design these four service components separately, if you are careful to preserve backwards compatible changes to the event schema.</p>&#xA;&#xA;<blockquote>&#xA;  <p>It's clear there are four microservices, but I don't understand becouse the ""command side"" microservices don't have their own database.</p>&#xA;</blockquote>&#xA;&#xA;<p>You could view those two microservices as having distinct logical event stores that happen to be the same physical store.  There's some potentially headache involved if you decide that one of those logical stores needs to be migrated, but aggregate event streams are isolated from each other, so you don't need to worry to much about coupling between accounts and transfers.</p>&#xA;&#xA;<blockquote>&#xA;  <p>And how should be the EventStore db? Single table? One table for each service ?</p>&#xA;</blockquote>&#xA;&#xA;<p>A typical relational schema for an event store will treat the event itself as a blob, and expose some of the meta data (the eventId, for instance) so that it can be joined with other tables.  So a generic store probably looks like one table to hold the events, and another table to hold event streams.  There are some links to discussions of relational schemas in <a href=""https://stackoverflow.com/questions/38644548/storing-events-when-using-event-sourcing/38645731#38645731"">storing events when using event sourcing</a>.</p>&#xA;&#xA;<p>If you wanted to partition/shard the schema that into multiple physical tables, you could do that too.</p>&#xA;&#xA;<p>There are also non-relational stores that you might use.  If you look into the details of <a href=""https://geteventstore.com/"" rel=""nofollow noreferrer"">Event Store</a> (which is open source), you can learn a lot about how to implement a store.</p>&#xA;&#xA;<p>Or you can just treat it as a black box.</p>&#xA;"
38057892,38049329,54734,2016-06-27T15:30:31,"<blockquote>&#xA;  <p>Should my Identity BS listen for that changes too, or having bi-directional communication is not considered a good practice</p>&#xA;</blockquote>&#xA;&#xA;<p>I don't think bi-directional communication is necessarily the problem here.  What concerns me is that you seem to have two different BCs acting as the ""book of record"" for identity.</p>&#xA;&#xA;<p>In the <a href=""http://rads.stackoverflow.com/amzn/click/0321125215"" rel=""nofollow"">blue book</a>, Evans writes of bounded contexts being defined by meaning; when you cross from one context to another, you may need to change your understanding of what is understood by the common terminology: it doesn't necessarily follow that the rules for an aggregate in one context (User?) will be the same as in another.</p>&#xA;&#xA;<p>The given example, User, is potentially more twitchy - because it may be that the real world, rather than the model, is the book of record, and the ""aggregate"" is really just a dumb bag of data.</p>&#xA;&#xA;<blockquote>&#xA;  <p>If I use only reference id, wouldn't that mean that Discussion BC will not have necessary data in its domain</p>&#xA;</blockquote>&#xA;&#xA;<p>Necessary for what?  Do changes in the discussion book of record depend on data stored in the Identity book of record?</p>&#xA;&#xA;<blockquote>&#xA;  <p>Example for, in identity context i have user with username, first name and last name etc, but in discussion context I might have that same user but represented as moderator or poster with only necessary properties for that BC. If name changes in the identity context, it should propagate that changes to discussion.</p>&#xA;</blockquote>&#xA;&#xA;<p>That sounds as though you are describing it as necessary for your <em>reads</em>; as though you have a view of a discussion that includes a representation of the participants that includes their roles (which exist in the discussion BC) and their identities.</p>&#xA;&#xA;<p>Reads tend not to be a very interesting use case, because reads don't change the book of record.  As Udi hinted at, to build the view you basically need a reference id that you can use to pull the data you want out of some key value store.  Is there any reason to prefer that the KV store is part of <em>this</em> BC as opposed to <em>that</em> one?</p>&#xA;&#xA;<blockquote>&#xA;  <p>Consumers could be 3rd party companies and our company.</p>&#xA;</blockquote>&#xA;&#xA;<p>Connecting to the microservice(s) directly, or instead consuming an api that acts as a facade for the backing micro services?</p>&#xA;"
50441531,50435154,54734,2018-05-21T03:16:18,"<blockquote>&#xA;  <p>So the problem is this Should the air-miles micro service take decisions based on its own view model which is being updated from events coming from the current-account, and similarly, on picking which reward it should give out to the Customer?</p>&#xA;</blockquote>&#xA;&#xA;<p><em>Each consumer uses a local replica of a representation computed by the producer.</em></p>&#xA;&#xA;<p>So if <code>air-miles</code> needs information from <code>current-account</code> it should be looking at a local replica of a view calculated by the <code>current-account</code> service.</p>&#xA;&#xA;<p>The key idea is this: micro services are supposed to be isolated from one another; you should be able to redesign and deploy one without impacting the others.</p>&#xA;&#xA;<p>So try this thought experiment - suppose we had these three micro services, but all saving snapshots of current state, rather than events.  Everything works, then imagine that the <code>current-account</code> maintainer discovers that an event sourced implementation would better serve the business.</p>&#xA;&#xA;<p>Should the change to the <code>current-account</code> require a matching change in the <code>air-miles</code> service?  If so, can we really claim that these services are isolated from one another?</p>&#xA;&#xA;<blockquote>&#xA;  <p>Advantages of taking a decision on local view models</p>&#xA;</blockquote>&#xA;&#xA;<p>I don't particularly like these ""advantages""; first, they are dominated by the performance axis (please recall that the <em>second</em> rule of performance optimization is ""not yet"").  And second, that they assume that the service boundaries are correctly drawn; maybe the performance issue is evidence that the separation of responsibilities needs review.</p>&#xA;"
49537155,49536369,54734,2018-03-28T14:25:58,"<p>Requirements for event sourcing tend to look like requirements for a source control system</p>&#xA;&#xA;<ul>&#xA;<li>we need the capability to recover data that was over written</li>&#xA;<li>we need to be able to support temporal queries, meaning that we need to be able to answer questions about how the domain model looked in the past.</li>&#xA;</ul>&#xA;&#xA;<p>A dual way of thinking about it:</p>&#xA;&#xA;<ul>&#xA;<li>we need the ability to audit every change in the model, but the model is too complicated to backup the entire data set after every change</li>&#xA;</ul>&#xA;&#xA;<p>In other words, what business value could you add if your data model was stored in git's object database?</p>&#xA;"
42536427,42528718,54734,2017-03-01T16:01:26,"<blockquote>&#xA;  <p>The recipient of an invoice must be a valid contact. </p>&#xA;</blockquote>&#xA;&#xA;<p>So the first thing you need to be aware of - if two entities are part of different aggregates, you can't really implement ""apply a change to this entity only if <em>that</em> entity satisfies a specification"", because <em>that</em> entity could change between the moment you evaluate the specification and the moment you perform the write.</p>&#xA;&#xA;<p>In other words - you can only get <em>eventual</em> consistency across an aggregate boundary.</p>&#xA;&#xA;<p>The aggregate is the authority for its own state, but everything else (for example, the contents of the command message), it pretty much has to accept that some external authority has checked the data.</p>&#xA;&#xA;<p>There are a couple approaches you can take here</p>&#xA;&#xA;<p>1) You can blindly accept that the recipient specified in the command is valid.</p>&#xA;&#xA;<p>2) You can try to verify the validity of the recipient from some external authority (aka: a read model of some other aggregate) between <em>receiving</em> it from the untrusted source and submitting it to the domain model.</p>&#xA;&#xA;<p>3) You can blindly accept the command as described, but treat the invoice as provisional until the validity of the recipient is confirmed.  That means there is a second command to run on the invoice that certifies the recipient.</p>&#xA;&#xA;<p>Note - from the point of view of the model, these different commands are equivalent, but at the application layer they don't need to be -- you can restrict access to the command to trusted sources (don't make it part of the public api, require authorization that is only available to trusted sources, etc).</p>&#xA;&#xA;<p>Approach #3 is the most microservicy, as the two commands can be separated in time -- you can accept the CreateInvoice command as soon as it arrives, and certify the recipient asynchronously.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Where would you put approach 4), where the Invoices Microservice has it's own Contacts Store which gets updated whenever there's a ContactCreated or ContactDeleted event? Then both entities are part of the same service and boundary. Now it should be possible to make things consistent, right?</p>&#xA;</blockquote>&#xA;&#xA;<p>No.  You've made the two entities part of the same service, but the problem was never that they were in different <em>services</em>, but that they are in separate <em>aggregates</em> -- meaning we can be changing the entity states concurrently, which means that we can't ensure that they are immediately synchronized.</p>&#xA;&#xA;<p>If you wanted immediate consistency, you need a model that draws your boundaries differently.</p>&#xA;&#xA;<p>For instance, if the invoice entities were modeled as part of the Contacts aggregate, then the aggregate can ensure the invariant that new invoices require a valid recipient -- the domain model uses the copy of the state in memory to confirm that the recipient <em>was</em> valid when we loaded, and the write into the book of record verifies that the book of record hadn't changed since the load happened.</p>&#xA;&#xA;<p>The write of the aggregate state is a compare-and-swap in the book of record; if some concurrent process had invalidated the recipient, the CAS operation would fail.</p>&#xA;&#xA;<p>The trade off, of course, is that <em>any</em> change to the Contact aggregate would also cause the invoice to fail; concurrent editing of different invoices with the same recipient goes out the window.</p>&#xA;&#xA;<p>Aggregates are all or nothing; they aren't separable.</p>&#xA;&#xA;<p>Now, one out might be that your Invoice aggregate has a part that must be immediately consistent with the recipient, and another part where eventually consistent, or even inconsistent, is acceptable.  In which case your goal is to refactor the model.</p>&#xA;"
41087125,41082938,54734,2016-12-11T14:16:44,"<blockquote>&#xA;  <p>As you may know, the order of the events is important. </p>&#xA;</blockquote>&#xA;&#xA;<p>In some cases - but you'll want to be careful not to confuse time, order, and correlation.</p>&#xA;&#xA;<blockquote>&#xA;  <p>When we generate these events from different services running in different machines, how to manage the time (timestamp) going out of sync across these thereby resulting in an event order mismatch.</p>&#xA;</blockquote>&#xA;&#xA;<p>Give up the idea that there is an ""order"" to events that are happening in different places.  <a href=""http://queue.acm.org/detail.cfm?id=2745385"" rel=""nofollow noreferrer"">There is no now</a>.</p>&#xA;&#xA;<p>Udi Dahan on <a href=""http://udidahan.com/2010/08/31/race-conditions-dont-exist/"" rel=""nofollow noreferrer"">race conditions</a> in the business world:</p>&#xA;&#xA;<blockquote>&#xA;  <p>A microsecond difference in timing shouldn’t make a difference to core business behaviors. </p>&#xA;</blockquote>&#xA;&#xA;<p>If your micro service boundaries are correct, then events happening in two difference services at about the same time are coincident -- there isn't one correct ordering of them, because (to stretch an analogy) they are in different light cones.  The only ordering that is inherently real is that within a single aggregate event history.</p>&#xA;&#xA;<p>What can make real sense is tracking <em>causation</em>; these changes in this book of record are a reaction to those changes in that book of record.  </p>&#xA;&#xA;<p>One simple form of this is to track <a href=""http://krasserm.github.io/2015/01/13/event-sourcing-at-global-scale/"" rel=""nofollow noreferrer"">happens-before</a>, which is where ideas like <a href=""https://en.wikipedia.org/wiki/Vector_clock"" rel=""nofollow noreferrer"">vector clocks</a> begin to appear.</p>&#xA;&#xA;<p>In most discussions that I have seen, this information would be passed along as meta data of the recorded events.</p>&#xA;"
48842346,48838101,54734,2018-02-17T14:27:24,"<blockquote>&#xA;  <p>What are the recommendations to handle these scenarios?</p>&#xA;</blockquote>&#xA;&#xA;<p>Give your order service access to the data that it needs to filter out undesirable orders.</p>&#xA;&#xA;<p>The basic plot would be that, while the Inventory service is <em>the</em> authority for the state of inventory, your Orders service can work with a <em>cached copy</em> of the inventory to determine which orders to accept.</p>&#xA;&#xA;<p>Changes to the Inventory are eventually replicated into the cache of the Orders service -- that's your ""eventual consistency"".  If Inventory drops off line for a time, Order's can continue providing business value based on the information in its cache.</p>&#xA;&#xA;<p>You may want to be paying attention to the age in the data in the cache as well -- if too much time has passed since the cache was last updated, then you may want to change strategies.</p>&#xA;&#xA;<p>Your ""aggregates"" won't usually know that they are dealing with a cache; you'll pass along with the order data a <em>domain service</em> that supports the queries that the aggregate needs to do its work; the implementation of the domain service accesses the cache to provide answers.</p>&#xA;&#xA;<p>So long as you don't allow the abuser to provide his own instance of the domain service, or to directly manipulate the cache, then the integrity of the cached data is ensured.</p>&#xA;&#xA;<p>(For example: when you are <em>testing</em> the aggregate, you will likely be providing cached data tuned to your specific test scenario; that sort of hijacking is not something you want the abuser to be able to achieve in your production environment).</p>&#xA;"
44120407,44117865,54734,2017-05-22T19:07:20,"<blockquote>&#xA;  <p>I was wondering whether we have any standards for writing RESTful APIs for entities that are not even in 1st Normal Form ??</p>&#xA;</blockquote>&#xA;&#xA;<p>Yup; the good news is that they are the same standards that apply when entities are in 1st Normal Form:</p>&#xA;&#xA;<p><em>REST doesn't care how URI are spelled</em></p>&#xA;&#xA;<p>A large part of the point of REST is that clients are insulated from the details of your data storage.</p>&#xA;&#xA;<p>In other words, if you think these spellings make sense to identify resources in your integration domain when you store your data in a RDBMS</p>&#xA;&#xA;<pre><code>/fruits(/:id)&#xA;/colors(/:id)&#xA;/fruits/:id/colors&#xA;/colors/:id/fruits&#xA;</code></pre>&#xA;&#xA;<p>Then those spelling should also be satisfactory for an API backed by a NoSQL store.</p>&#xA;&#xA;<p>It's the job of <em>your implementation</em> to bridge the gap between the semantic meaning of the resources in your integration domain and your storage.</p>&#xA;&#xA;<p>Here's another spelling of the same idea; from the point of view of the client, all HTTP servers are document stores.  The URI is a key, the representation is the value.  There's an infinite number of keys available, and a small number of verbs (methods) that are expected to be supported for any given key.</p>&#xA;&#xA;<p>It's the job of your API to make your service (whatever it is) look like a dumb HTTP document store.</p>&#xA;"
45261124,45255905,54734,2017-07-23T02:59:46,"<blockquote>&#xA;  <p>how to get data from another service</p>&#xA;</blockquote>&#xA;&#xA;<p>There are two use cases for this.  In your specific case, what you are describing is somewhat akin to <a href=""https://particular.net/blog/secret-of-better-ui-composition"" rel=""nofollow noreferrer"">UI Composition</a>; you are creating a view that pulls data from two different sources.</p>&#xA;&#xA;<p>Key point #1: the data you are composing is stale -- by the time the email reaches its destination, the truth understood by the services may have changed anyway.  Therefore, there is inherent in the requirements some flexibility about time.</p>&#xA;&#xA;<p>Key point #2: In sending the email, you aren't changing the state of either service at all.  You are just making a copy of some part of it.  Reads are a <a href=""https://tools.ietf.org/html/rfc7231#section-4.2.1"" rel=""nofollow noreferrer"">safe</a> operation.</p>&#xA;&#xA;<p>Key point #3: Actually sending the email changes the ""real world"", not the services; it's an activity that can be performed concurrently with the service work.</p>&#xA;&#xA;<p>So what this would normally look like is that one of your read models (probably that of the order service) will support a query that lists orders for which emails will be sent.  Some process, running outside of the service, will periodically query that service for pending emails, query the required read models to compose the message, send it, and finally post a message to the input queue of the order service to share the information that the message was successfully sent.  The order service would see that, and the read model gets updated to indicate that the message has already been sent.</p>&#xA;"
45180136,45167593,54734,2017-07-19T02:51:32,"<blockquote>&#xA;  <p>A post to an event can be deleted by two kinds of users: the author of the post, and the host(s) of the event.</p>&#xA;</blockquote>&#xA;&#xA;<p>So the first thing I would start with is identifying where the <em>authority</em> for deleting a post resides -- using as a guiding principal the idea that there should be a single writer responsible for maintaining any given invariant.</p>&#xA;&#xA;<p>In this case, it would seem to be the Post service, reasonably enough.</p>&#xA;&#xA;<p>I'll presume that the plumbing includes some mechanism to detect that the user is who they say they are -- the authenticated identity being an <em>input</em> to the service.</p>&#xA;&#xA;<p>For the case where the author is deleting the post, verifying that the rule is satisfied should be trivial, in that we have the authority for creating and deleting the post in the same place.  There's no collaboration required here.</p>&#xA;&#xA;<p>So the tricky part is determining if the authenticated identity belongs to the event host.  Presumably, the authority to determine the event host lives in the Event service.</p>&#xA;&#xA;<p>Now, a reality check: if you query the event service to find out who the host of the event is, without holding a lock on that information, then the owner can be changing concurrently with the processing of the delete command.  In other words, there's potentially a data race between a ChangeOwner command and a DeletePost command.</p>&#xA;&#xA;<p><a href=""http://udidahan.com/2010/08/31/race-conditions-dont-exist/"" rel=""nofollow noreferrer"">Udi Dahan</a> observed:</p>&#xA;&#xA;<blockquote>&#xA;  <p>A microsecond difference in timing shouldn’t make a difference to core business behaviors.</p>&#xA;</blockquote>&#xA;&#xA;<p>In particular, the correctness of the behavior shouldn't depend on the order that your message transport system happens to deliver the commands.</p>&#xA;&#xA;<p>Your event sourced approach is the one that is closest to this idea.</p>&#xA;&#xA;<blockquote>&#xA;  <p>However, I'm not quite happy with this one either. It'd create a very intricate and error-prone system, where an unhandled (but potentially rare) event could make services go out of sync. Also, there's a risk that logic would end up in the wrong place. For example, the constraint in this question would be handled by PostService even though conceptually it's a property of the event entity.</p>&#xA;</blockquote>&#xA;&#xA;<p>Almost so -- the key idea is this: if the Post service is the authority for post lifetimes, then it must be permitted to announce that it <em>has changed its mind</em>.  You build into your design the notion that the authority makes its best decision with the information it has available, and applies corrections (sometimes referred to as compensating events) when new information would invalidate an earlier choice.</p>&#xA;&#xA;<p>So when the delete command arrives, you check to see if it is from the host.  If so, you can <a href=""http://udidahan.com/2009/09/01/dont-delete-just-dont/"" rel=""nofollow noreferrer"">mark the post</a> right away.  If it isn't from the host, you remember that somebody wanted to delete the post, and if it later turns out an update to the event informs you that same somebody is the host, then you can apply the mark.</p>&#xA;&#xA;<p>And the same approach works in the opposite case - the delete came from the host, so the post was marked.  Whoops! we just found out that imposter wasn't the host.  OK, so show the post again.</p>&#xA;"
47313279,47311911,54734,2017-11-15T17:04:03,"<blockquote>&#xA;  <p>I think I'm doing projections wrong. </p>&#xA;</blockquote>&#xA;&#xA;<p>I think so too; it sounds like you have your queries coupled to your projections</p>&#xA;&#xA;<blockquote>&#xA;  <p>When a read query comes in, it loads the entire projected state from S3 (for all aggregates), queries the event sources for all newer events, computes the latest state</p>&#xA;</blockquote>&#xA;&#xA;<p>Yeah, that sounds like a mess.  Or more specifically, that sounds like the query is triggering the work to be done by the projection.</p>&#xA;&#xA;<p>If you can decouple the queries from the projections, then things get easier.  The basic idea being that your queries don't describe the <em>current</em> state, they describe the state <em>as of the last time the projection ran</em>.</p>&#xA;&#xA;<p>Same idea, different spelling: you answer queries from the documents that you cache in S3.  When new events are detected, your projections run, load the new data as needed, compute the new document, and replace the entries in the cache.</p>&#xA;&#xA;<p>I think of a triangle</p>&#xA;&#xA;<ul>&#xA;<li>Commands bring information from the outside to the book of record</li>&#xA;<li>Projections bring information from the book of record to the cache</li>&#xA;<li>Queries bring information from the cache to the outside world</li>&#xA;</ul>&#xA;&#xA;<p>where each leg of the triangle runs asynchronously with the others.</p>&#xA;&#xA;<p>I suggest you work backwards from the queries - what documents do you need to support each query? what are the latency targets that you have to beat?  Then you start balancing tradeoffs - for this new query, do I create the result from the existing documents, or do I need a new document built with a finer grain?</p>&#xA;&#xA;<blockquote>&#xA;  <p>if I understand correctly, I should be triggering the projection updates as events come in, instead of in aggregate when the query is made. That saves me from querying the event store for new events on every query</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes, and... events are only one way of triggering; you could also have the projection processes triggered by a clock (check every 15 minutes to see if we need to update) or at the whim of a human operator (hmm, it looks like your account balance is stale, let me try to update that for you).  More than one way to do it, and you can mix and match strategies.</p>&#xA;&#xA;<blockquote>&#xA;  <p>I would still need to load the entire state, both when updating the projection, and when loading a single aggregate.</p>&#xA;</blockquote>&#xA;&#xA;<p>Not necessarily.  There's no rule that says you can't use the previously cached representation as a starting point, and then pull from the book of record only the changes that you need.</p>&#xA;&#xA;<p>For instance, suppose you are building a view that combines aggregates <code>A{id:7}</code> and <code>B{id:9}</code>.  You grab the cached copy, and look in its meta data (where you put it on your previous write) and find something inside it like <code>metadata:{A:{id:7, version:21}, B:{id:9, version:19}}</code>.  Now you only need to load the events after the ones you used last time, update your local copy in memory, update the local copy of the metadata, and push the lot to the cache.</p>&#xA;"
39284771,37150273,6681806,2016-09-02T05:19:37,"<p>You can use the rancher-metadata api like this:</p>&#xA;&#xA;<p><code>http://rancher-metadata/latest/self/container/service_name</code></p>&#xA;&#xA;<p>you can get the <code>service_name</code>, <code>hostname</code>, <code>primary_ip</code> and others.</p>&#xA;&#xA;<p>So you can use in Dockerfile like:</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/98UeS.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/98UeS.jpg"" alt=""enter image description here""></a></p>&#xA;"
35651076,35613841,3084239,2016-02-26T11:56:35,"<blockquote>&#xA;  <p>Is there anything in Azure suited to this?</p>&#xA;</blockquote>&#xA;&#xA;<p>There is another solution in Azure for the microservice approach, one that I think will get a lot of traction, called <a href=""https://azure.microsoft.com/en-us/documentation/articles/service-fabric-overview/"" rel=""nofollow"">Service Fabric</a>.</p>&#xA;&#xA;<p>The concern here is that you have an existing application, and it might be a lot harder to adapt it to work with Service Fabric, but it is certainly worth a look.</p>&#xA;&#xA;<p><a href=""https://github.com/Azure-Samples/service-fabric-dotnet-web-reference-app"" rel=""nofollow"">Here you can find a Service Fabric example of a web service.</a></p>&#xA;&#xA;<p>Hope this helps!</p>&#xA;&#xA;<p>Best of luck!</p>&#xA;"
39476974,33165216,2631119,2016-09-13T18:43:26,"<p>Depending on your architecture, you can use some really cool software like Weave with CoreOS (<a href=""https://github.com/weaveworks/weave"" rel=""nofollow"">https://github.com/weaveworks/weave</a>). We are using Docker to distribute our applications onto the CoreOS nodes, and then the internal DNS is handled by Weave.</p>&#xA;&#xA;<p>This is really great because we can just forward the request through to the application name with a port, and then we are off and away.</p>&#xA;&#xA;<p>For example, a user requests application.com/api/apiName/request/path</p>&#xA;&#xA;<p>Our gateway was implemented with Node.js, and it takes the apiName after /api to route it to that api, and then the following path of the URL to append to the call itself.</p>&#xA;&#xA;<p>So the request from the gateway would be proxied internally as apiName:8080/request/path. In that regard, the API requires no changes when new services come up, as the path is created dynamically from your request.</p>&#xA;&#xA;<p>This is great because we don't have to worry about tracking paths from the different API's and storing them somewhere.</p>&#xA;&#xA;<p>If not that, you would have to maintain some (probably external) list of endpoints to make it easier for you. This could be done programmatically from the API's themselves.</p>&#xA;&#xA;<p>I'm not sure what your requirements are, however, and as Will has answered, it does incur a pretty big infrastructure cost. However, our releases are fast and painless because we don't have to worry about making code changes in multiple layers, and the new API's get our gateway proxying, logging, and authentication for free.</p>&#xA;"
44460536,44460206,209103,2017-06-09T14:37:06,"<p>You can deploy/undeploy individual functions with the <a href=""https://github.com/firebase/firebase-tools/releases/tag/v3.8.0"" rel=""nofollow noreferrer"">Firebase tools/CLI version 3.8 or higher</a> by specifying what function(s) to update: <code>firebase deploy --only functions:function1,function2</code>. It will still deploy all the code in your project, since the CLI doesn't ""know"" what file(s) are needed for each specific function. But it will then only update the function(s) that you specify.</p>&#xA;"
43545184,43545080,7682767,2017-04-21T14:25:21,<p>That info is in <strong><em>HEADERS - CONTENT TYPE</em></strong></p>&#xA;
48717968,48715395,999375,2018-02-10T06:32:33,"<p>Within a container, localhost points to the container itself and not the bridged network.</p>&#xA;&#xA;<p>As you are using docker-compose, you should point to the container name defined in docker-compose.yml file (e.g. <em>underrule.worldservice</em> -> <a href=""http://underrule.worldservice:port"" rel=""nofollow noreferrer"">http://underrule.worldservice:port</a>).</p>&#xA;&#xA;<p>Another way is to point to the docker bridge network ip. On linux this IP is by default 172.17.0.1. On windows it depends on the version of docker you are running you should inspect your configuration (<code>ipconfig</code>).</p>&#xA;"
52082037,51982133,6899919,2018-08-29T16:19:01,"<p>After continued research, it became apparent that message queues are not part of the solution.  Point #5 in <a href=""https://stackify.com/message-queues-12-reasons/"" rel=""nofollow noreferrer"">this article</a>, ""...or doesn’t even care about the result..."" suggests (by implication) that we are simply using the wrong approach.</p>&#xA;&#xA;<p>We changed our design so that request ordering is not important, and will make direct calls to AWS Lambda functions using the invoke api.</p>&#xA;"
51673582,42648060,3165903,2018-08-03T13:18:55,"<p>It's better to setup security by credentials <code>username</code> and <code>password</code> for most of endpoints listed here:&#xA;<a href=""https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html</a></p>&#xA;&#xA;<p>Exceptions from the rule are <code>health</code> and <code>info</code> endpoints which won't be protected by credentials.</p>&#xA;&#xA;<p>You can set <code>username</code> and <code>password</code> in <code>application.properties</code> like that:</p>&#xA;&#xA;<pre><code>security.user.name=admin&#xA;security.user.password=secret&#xA;</code></pre>&#xA;"
27865893,27865238,41423,2015-01-09T17:22:56,"<p>The 'problem' with a multi-module parent pom is that, without complicated profiles, it locks the modules in the same release cycle (assuming you're using the <a href=""http://maven.apache.org/maven-release/maven-release-plugin/"">Release Plugin</a>, which you should be).</p>&#xA;&#xA;<p>The way I work with Maven is to have a parent pom that declares:</p>&#xA;&#xA;<ul>&#xA;<li>common dependencies (logging APIs, JUnit, etc).</li>&#xA;<li>common plugins.</li>&#xA;<li>all dependencies in the <a href=""http://maven.apache.org/pom.html#Dependency_Management""><code>dependencyManagement</code></a> section.</li>&#xA;<li>all plugins in the <a href=""http://maven.apache.org/pom.html#Plugin_Management""><code>pluginManagement</code></a> section.</li>&#xA;</ul>&#xA;&#xA;<p>Each module delcares the parent pom as its parent but the parent knows nothing about the modules.</p>&#xA;&#xA;<p>The benefit of this comes from the last to two bullets above, the 'management' sections.  Anything contained in a 'management' section needs to be redeclared in a module that wants to use a particular dependency or plugin.  </p>&#xA;&#xA;<p>For example the parent might look like this:</p>&#xA;&#xA;<pre><code>&lt;project&gt;&#xA;&#xA;  &lt;groupId&gt;com.example&lt;/groupId&gt;&#xA;  &lt;artifactId&gt;parent&lt;/artifactId&gt;&#xA;  &lt;version&gt;1.0.00-SNAPSHOT&lt;/version&gt;&#xA;&#xA;  ...&#xA;&#xA;  &lt;dependencies&gt;&#xA;&#xA;    &lt;dependency&gt;&#xA;      &lt;groupId&gt;org.slf4j&lt;/groupId&gt;&#xA;      &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;&#xA;      &lt;version&gt;1.7.7&lt;/version&gt;&#xA;    &lt;/dependency&gt;&#xA;&#xA;    &lt;dependency&gt;&#xA;      &lt;groupId&gt;junit&lt;/groupId&gt;&#xA;      &lt;artifactId&gt;junit&lt;/artifactId&gt;&#xA;      &lt;version&gt;4.11&lt;/version&gt;&#xA;      &lt;scope&gt;test&lt;/scope&gt;&#xA;    &lt;/dependency&gt;            &#xA;&#xA;  &lt;/dependencies&gt;&#xA;&#xA;  &lt;dependencyManagement&gt;&#xA;&#xA;    &lt;dependency&gt;&#xA;      &lt;groupId&gt;commons-lang&lt;/groupId&gt;&#xA;      &lt;artifactId&gt;commons-lang&lt;/artifactId&gt;&#xA;      &lt;version&gt;2.6&lt;/version&gt;&#xA;    &lt;/dependency&gt;        &#xA;&#xA;    &lt;dependency&gt;&#xA;      &lt;groupId&gt;commons-collections&lt;/groupId&gt;&#xA;      &lt;artifactId&gt;commons-collections&lt;/artifactId&gt;&#xA;      &lt;version&gt;2.1&lt;/version&gt;&#xA;    &lt;/dependency&gt;&#xA;&#xA;  &lt;/dependencyManagement&gt;&#xA;&#xA;  &lt;plugins&gt;&#xA;&#xA;    &lt;plugin&gt;&#xA;      &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;&#xA;      &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;&#xA;      &lt;version&gt;3.1&lt;/version&gt;&#xA;      &lt;configuration&gt;&#xA;        &lt;source&gt;1.8&lt;/source&gt;&#xA;        &lt;target&gt;1.8&lt;/target&gt;&#xA;      &lt;/configuration&gt;&#xA;    &lt;/plugin&gt;&#xA;&#xA;  &lt;plugins&gt;&#xA;&#xA;  &lt;pluginManagement&gt;&#xA;&#xA;    &lt;plugins&gt;&#xA;&#xA;      &lt;plugin&gt;&#xA;        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;&#xA;        &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;&#xA;        &lt;version&gt;2.4&lt;/version&gt;&#xA;        &lt;configuration&gt;&#xA;          &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt;&#xA;          &lt;descriptors&gt;&#xA;            &lt;descriptor&gt;src/main/assembly/assembly.xml&lt;/descriptor&gt;&#xA;          &lt;/descriptors&gt;&#xA;        &lt;/configuration&gt;&#xA;        &lt;executions&gt;&#xA;          &lt;execution&gt;&#xA;            &lt;id&gt;make-assembly&lt;/id&gt;&#xA;            &lt;phase&gt;package&lt;/phase&gt;&#xA;            &lt;goals&gt;&#xA;              &lt;goal&gt;single&lt;/goal&gt;&#xA;            &lt;/goals&gt;&#xA;          &lt;/execution&gt;&#xA;        &lt;/executions&gt;&#xA;      &lt;/plugin&gt;&#xA;&#xA;    &lt;/plugins&gt;&#xA;&#xA;  &lt;/pluginManagement&gt;&#xA;&#xA;&lt;/project&gt;&#xA;</code></pre>&#xA;&#xA;<p>And the module might look like this:</p>&#xA;&#xA;<pre><code>&lt;project&gt;&#xA;&#xA;  &lt;parent&gt;&#xA;    &lt;groupId&gt;com.example&lt;/groupId&gt;&#xA;    &lt;artifactId&gt;parent&lt;/artifactId&gt;&#xA;    &lt;version&gt;1.0.00-SNAPSHOT&lt;/version&gt;&#xA;  &lt;/parent&gt;&#xA;&#xA;  &lt;groupId&gt;com.example&lt;/groupId&gt;&#xA;  &lt;artifactId&gt;module&lt;/artifactId&gt;&#xA;  &lt;version&gt;1.0.00-SNAPSHOT&lt;/version&gt;&#xA;&#xA;  &lt;dependencies&gt;&#xA;&#xA;    &lt;dependency&gt;&#xA;      &lt;groupId&gt;commons-lang&lt;/groupId&gt;&#xA;      &lt;artifactId&gt;commons-lang&lt;/artifactId&gt;          &#xA;    &lt;/dependency&gt;        &#xA;&#xA;  &lt;/dependencies&gt;&#xA;&#xA;  &lt;plugins&gt;&#xA;&#xA;    &lt;plugin&gt;&#xA;      &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;&#xA;      &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;&#xA;    &lt;/plugin&gt;&#xA;&#xA;  &lt;/plugins&gt;&#xA;&#xA;&lt;/project&gt;&#xA;</code></pre>&#xA;&#xA;<p>The module would:</p>&#xA;&#xA;<ul>&#xA;<li>have dependencies on <code>org.slf4j:slf4j-api:1.7.7:compile</code>, <code>junit:junit:4.11:test</code> and <code>commons-lang:commons-lang:2.6:compile</code>.</li>&#xA;<li>has the plugin <code>org.apache.maven.plugins:maven-assembly-plugin:2.4</code></li>&#xA;</ul>&#xA;"
44216383,44216244,112079,2017-05-27T11:37:50,"<p>These options are unrelated, Server JRE is a specific packaging for servers, to avoid having to install a full JDK with the associated security risks. Oracle explains this on the download page:</p>&#xA;&#xA;<blockquote>&#xA;  <p>The Server JRE includes tools for JVM monitoring and tools commonly required for server applications, but does not include browser integration (the Java plug-in).</p>&#xA;</blockquote>&#xA;&#xA;<p>You should do 2, as explained above, <strong>and</strong> maybe 1, which optimizes the JVM for server workloads, because of performance advantages. However, have a look at <a href=""http://docs.oracle.com/javase/8/docs/technotes/tools/windows/java.html"" rel=""nofollow noreferrer"">http://docs.oracle.com/javase/8/docs/technotes/tools/windows/java.html</a>: on the 64 bits JVM the server VM is the only one available. </p>&#xA;"
29439190,22987482,703951,2015-04-03T20:24:33,"<p>Sure, you can use HapiJS, any other web framework, or even just plain old node libraries.</p>&#xA;&#xA;<p>Microservices are just about structuring an application (or applications) in smaller bits, rather than a monolithic app.  They are not a silver bullet or anything complicated, in fact they are generally quite simple.  </p>&#xA;&#xA;<p>HapiJS's role would be to expose data over HTTP, which should be fine unless you are building an extremely performance sensitive app.</p>&#xA;&#xA;<p>Of course, one thing that microservices enables is the use of different technologies.  You could try out different web frameworks for each service and see which you prefer - they are all capable of doing the same things.</p>&#xA;"
48568566,48549944,9296335,2018-02-01T17:31:11,"<p>So I finally figured out how to do this. It's a bit of hack:. </p>&#xA;&#xA;<ol>&#xA;<li>Download the JSON Key file for the Service Account. </li>&#xA;<li>Add the JSON file to the Function source. </li>&#xA;<li>Install the google-auth-library NPM module. </li>&#xA;<li><p>Modify the request to use the client from google-auth-library</p>&#xA;&#xA;<pre><code>const keys = require('./VirtualAssistantCred.json');&#xA;</code></pre>&#xA;&#xA;<p>const {auth} = require('google-auth-library');&#xA;  const client = auth.fromJSON(keys);&#xA;  client.scopes = ['<a href=""https://www.googleapis.com/auth/cloud-platform"" rel=""nofollow noreferrer"">https://www.googleapis.com/auth/cloud-platform</a>'];&#xA;  client.authorize().then(function () {&#xA;      const url = <code>https://cloudfunctions.googleapis.com/v1beta2/projects/${config.PROJECT_ID}/locations/${config.LOCATION_ID}/functions/${functionName}</code>;&#xA;      client.request({url}, function(err, res) {&#xA;        if (err) {&#xA;          console.log(<code>Error: ${err}</code>);&#xA;          return cb(true, <code>Service, ${functionName} doesn't exist, returned ${res}</code>)&#xA;        } else { &#xA;          console.log(<code>RESPONSE: ${JSON.stringify(res.data)}</code>);&#xA;          console.log(<code>Found Service at ${res.data.httpsTrigger.url}</code>);&#xA;          return cb(false, res.data.httpsTrigger.url); &#xA;        }&#xA;      });&#xA;    }&#xA;  );</p></li>&#xA;</ol>&#xA;"
52059171,52057828,654031,2018-08-28T13:30:17,"<p>A microservice is simply a concept. You won't find a ""Microservice"" template in Visual Studio. Generally, you're going to be implementing a REST API. A microservice doesn't <em>have</em> to be a REST API, but they most normally are.</p>&#xA;&#xA;<p>You also generally won't just be making a microservice, but rather microservice<em>s</em>. One of the core tenants of a microservice is that it should deal with just one discrete unit of functionality. Unless your application does just one very boring thing, you'll need multiple microservices. For example, for an ecommerce site, you might have a user service, a cart services, a checkout service, an order service, etc. </p>&#xA;&#xA;<p>To coordinate the efforts of all these microservices, it's also typical to implement an API gateway. The application will work with the gateway only, and the gateway will proxy out the requests to each individual microservice to get the information or do the work that the application requires. In a sense, it acts as a conductor would, coordinating all the individual instruments to create the harmony.</p>&#xA;&#xA;<p>Long and short, most likely what you want is one or likely more ASP.NET Core API project(s). You'll create controllers and actions on those controllers, where the latter of which will effectively become your endpoints, i.e. the functional routes your API exposes to do work. Since a microservice architecture is desired, these API project(s) should remain small and razor-focused, potentially only each working with just one entity class or maybe a very narrow slice of app functionality that involves multiple entities. You should strive to keep them as lightweight as possible, minimizing the amount of middleware and external libraries involved. When it comes to deployment, it's most typical to use containerization - Docker is a popular choice for that.</p>&#xA;"
42492093,42491958,5897423,2017-02-27T17:29:31,<p>You can use the middleware to authenticate the users from your MongoDB and once the user is authenticated you can generate the oAuth token. This token can be used for referencing the chat related data in your MySql database. All service calls to MySql database should be passed via oAuth middleware to check for valid token and then do the next messaging process.</p>&#xA;
37337183,37336871,2686927,2016-05-20T02:51:48,"<p>As long as I know it is not possible. As you have 2 services, and maybe more in the future, it seems you are using a distributed microservices architecture and therefore you need to register your services for them to know where the other service is. For that I recommend <a href=""https://github.com/coreos/etcd"" rel=""nofollow"">etcd</a> it is really pretty easy to use.&#xA;A nice fellow also written a <a href=""https://github.com/jplana/python-etcd"" rel=""nofollow"">python-client</a> for it!</p>&#xA;"
27790540,27752201,3477712,2015-01-06T01:02:42,"<p>Microservices in the current context they are being referred to today are intended to be stand alone, a service with no dependence on another service (Data stores as well, dedicated to the microservice).</p>&#xA;&#xA;<p>It is a good idea even if not going the microservices route to divide your application into smaller modules/services/packages etc.. This will allow for more maintainable code. </p>&#xA;"
27825884,27790905,3477712,2015-01-07T18:12:10,<p>In my case I can only derive related items from the service itself. My goal is to abstract the related items to the point that any number of services can be related to a service and only need to lookup ID's or links. One thought was an @ElementCollection named related with a join of the entity ID of the service.  Then in the @Embedded have a relLink field and a relatedID field.  Then in the repository do a findby to find the relLink and relatedID. </p>&#xA;&#xA;<p>The hope is to keep it abstracted enough to essentially mimic a Many to Many setup.</p>&#xA;
36460278,36415988,3477712,2016-04-06T19:09:22,"<p>Take a look at Spring Cloud Sleuth or something like a Redis appender to send to an ELK stack then use the ELK stack for your needs.</p>&#xA;&#xA;<p><a href=""https://github.com/ryantenney/log4j-redis-appender"" rel=""nofollow"">Redi Appender - Github Repo</a></p>&#xA;&#xA;<p><a href=""http://cloud.spring.io/spring-cloud-sleuth/"" rel=""nofollow"">Spring Cloud Sleuth Project</a></p>&#xA;"
31153961,31141688,3477712,2015-07-01T06:25:59,"<p>We are currently using <a href=""http://aws.amazon.com/ecs/"" rel=""nofollow"">Amazon ECS</a> to accomplish exactly what you are talking about trying to achieve.  You can define your Docker Container as a Task definition and then Create an ECS Service which will handle number of instances, scaling, etc.</p>&#xA;&#xA;<p>One thing to note is Amazon mentions the word container a lot in the documentation.  They may be talking about the EC2 instance used for the cluster for your docker instances/containers.</p>&#xA;"
31154138,31097306,3477712,2015-07-01T06:37:43,"<p>We currently use AWS and Spring Cloud / Spring Boot.  Our stance is instances that require more resources receive their own EC2 instance.  Services that can be deployed using Docker are deployed to <a href=""http://aws.amazon.com/ecs/"" rel=""nofollow"">Amazon ECS</a>.</p>&#xA;&#xA;<p>All of our deployments are fat jars using embedded Tomcat. </p>&#xA;&#xA;<p>Not knowing the size of what you are calling a service makes this answer a little complicated. If you are worried about Tomcat's overhead why are you running these services in Docker containers? Ease of deployment?  </p>&#xA;&#xA;<p>If you do go the route of deploying multiple services to the same Tomcat instance, I would not define your setup as a microservices based configuration.</p>&#xA;"
25156052,22513893,3477712,2014-08-06T08:53:33,"<p>I just started learning microservices so this is by no means a perfect answer.  It is my opinion based on my current understanding though.</p>&#xA;&#xA;<p>If you are creating microservices based on events then a message broker will play a huge role.  Lets say you have a service called CreateUserService which is only responsible for gathering the data required to create a user not persisting the data.  This data is then published to a queue. </p>&#xA;&#xA;<p>Subscribers to the createUser queue DuplicateUserService, UserDataStore etc...  Can then react accordingly to the data within each service.  </p>&#xA;&#xA;<p>In the end the client receives data back from another service with relevant information about the attempted event. </p>&#xA;"
30093348,29924248,3477712,2015-05-07T06:21:41,"<p>When you start trying this, I would pay very close attention to the bandwidth charges.  AWS as an example you are ok if you are in the same region.  Bandwidth between services will not cost much if anything.  Lets say you use AWS and Google Cloud.  Now you will be paying for the bandwidth between the 2 providers.</p>&#xA;&#xA;<p>As a suggestion I would look at Docker as a possible solution to your problem/concern of vendor lock in.  </p>&#xA;&#xA;<p>You would be restricted to providers that support docker but in theory you could migrate quickly between providers easily since your application would be abstracted from each cloud providers architecture. </p>&#xA;&#xA;<p>Performance, will take a hit with anything leaving the providers data center.  I suppose with some investigation you might try researching providers that use a common internet exchange.  This would help minimize a few hops at least.</p>&#xA;"
43493563,43476908,4596432,2017-04-19T10:49:18,"<p>To make the jump into the <em>real</em> micro-services world is not trivial. It's not about plumbing some APIs, but a radical change in architecture thinking that, well, at the beginning will make you a bit uncomfortable (e.g. every service with its own database) :)</p>&#xA;&#xA;<p>The best book I have read so far about micro-services is <a href=""https://www.manning.com/books/the-tao-of-microservices"" rel=""nofollow noreferrer"">The Tao of Microservices</a>, by Richard Rodger the author of Seneca himself. It exposes very well the shift from monolithic and object-oriented software towards micro-services.</p>&#xA;&#xA;<p>I have personally struggled a bit with Seneca because of the average quality of documentation (inconsistencies, etc...). I would rather recommend <a href=""https://hemerajs.github.io/hemera/"" rel=""nofollow noreferrer"">Hemera</a>, which took its inspiration from the message-pattern approach from Seneca, but is better documented and <em>much more</em> <strong>production-ready</strong>.</p>&#xA;"
43408686,41609091,4596432,2017-04-14T09:17:17,"<p>If you use Seneca I strongly recommend to read <a href=""https://www.manning.com/books/the-tao-of-microservices"" rel=""nofollow noreferrer"">The Tao of Microservices</a>, by <a href=""http://www.richardrodger.com/"" rel=""nofollow noreferrer"">Richard Rodger</a> the author of Seneca himself.</p>&#xA;&#xA;<p>He addresses directly your question this way (Chapter 3 Section 15):</p>&#xA;&#xA;<blockquote>&#xA;  <p>Bandwith matters. </p>&#xA;  &#xA;  <p>The networked nature of microservices systems means that they are very vulnerable to bandwidth limitations. Even if you start out with a plentiful supply, you must adopt a mentality of scarcity. Misbehaving microservices can easily cause an internally generated denial-of-service attack. <strong>Keep your messages small and lean</strong>. Do not use them to send much actual data, <strong>send references to bulk data storage</strong> instead. [...]</p>&#xA;  &#xA;  <p>To send images between services, do not send the image binary data, send a URL pointing the image.</p>&#xA;</blockquote>&#xA;&#xA;<p>Back to your specific case, you should use a service that allows you to store/retrieve files, and pass only the URL of the files in the messages between your Seneca services. It is not trivial to build such a system in a pure distributed fashion, so I would rather use AWS S3 or equivalent. </p>&#xA;"
45552269,45551966,6244560,2017-08-07T17:12:12,"<p>On production, never use docker or docker compose alone. Use an orchestrator (rancher, docker swarm, k8s, ...) and deploy your stack there. Orchestrator will take care of the networking issue. Your container can link each other, so you can access them directly by a name (don't care too much about the ip).</p>&#xA;&#xA;<p>On local host, use docker compose to startup your containers and use link. do not use a local port but the name of the link. (if your container A need to access container B on port 1234, then do a link B linked to A with name BBBB and use tcp://BBBB:1234 to access the container from A )</p>&#xA;&#xA;<p>If you really want to bind port to your localhost and use this, access port by your host IP, not localhost.</p>&#xA;"
51525298,51524648,6244560,2018-07-25T18:14:22,"<p>API management can do a small check on your JWT (fail early), BUT your microservices are the only one that can really manage all the security stuff ! </p>&#xA;&#xA;<p>If you set security only on api management it means that someone that can access your network will be able to push request to your API unauthenticated. &#xA;You will not be able to log who do what. And finally, if you need to set some kind of ACL, it will not be possible (When you ask to list orders, you can only list YOUR order).</p>&#xA;&#xA;<p>Perhaps you will think of decoding your JWT on the api management layers and push a header with user name to your backend to prevent all the thing I spoke about above, but I think it is not really a good practice.</p>&#xA;&#xA;<p>First, access to network will means I'm able to be anybody. Then JWT is much more than just a username. For instance, perhaps you use scope on your authentication layers. ( scope read orders / scope modify orders / scope delete orders). This is useful to restrict what an application can do (either at client_id level) or what a user accept to give to the application ( scope share email ...). &#xA;For this JWT on the backoffice is mandatory. </p>&#xA;&#xA;<p>Ok you can do like username and extract data on api management and put specific headers to call backend, but really ? why do specific stuff ? oauth2 with JWT can do this for you.</p>&#xA;"
51491340,51491162,6244560,2018-07-24T05:51:40,"<p>Consul is a service management (discovery, config, ...), not a cache / pub-sub system.</p>&#xA;&#xA;<p>You may want to use a shared cache behind the scene for your use case. You AS1 service authenticate a user, then put in the cache the token. AS2 can retrieve the token in the cache. For that, you can use application like</p>&#xA;&#xA;<ul>&#xA;<li>redis</li>&#xA;<li>hazelcast</li>&#xA;<li>infinispan</li>&#xA;<li>... other stuff like store data in a DB ...</li>&#xA;</ul>&#xA;&#xA;<p>You can also use a pub-sub system and a local cache for each ASx, but you can have issue when on AS restart (cache lost). So from my point of view, shared cache is better.</p>&#xA;"
51427278,51422290,4085968,2018-07-19T16:04:11,"<p>Lets say when a remote service(any service) is started to respond slowly, but a typical application(service which is making call to remote service) will still continue to call that remote service. So short circuited(Threadpooled) method helps you build a Defensive system in this particular case.</p>&#xA;&#xA;<p>As calling service does not know if the remote service is healthy or not and new threads are spawned every time a request comes in. This will cause threads on an already struggling server to be used. </p>&#xA;&#xA;<p>We don’t want this to happen as we need these threads for other remote calls or processes running on our server and we also want to avoid CPU utilization spiking up. so this prevents resources from becoming blocked if latency occurs. Also Bounded thread pool also gives some breathing room for downstream services to recover.</p>&#xA;&#xA;<p>For detail : <a href=""https://github.com/Netflix/Hystrix/wiki/Configuration#ThreadPool"" rel=""nofollow noreferrer"">ThreadPool in Hystrix</a></p>&#xA;"
51475272,51433860,4085968,2018-07-23T09:12:30,"<p>Consider the Hytrix circuit-breaker transition from CLOSED to OPEN. it short-circuits all requests made against that circuit-breaker.</p>&#xA;&#xA;<p>After some amount of time (<code>HystrixCommandProperties.circuitBreakerSleepWindowInMilliseconds()</code>) the next single request is let through &#xA;(this is the HALF-OPEN state). </p>&#xA;&#xA;<p>If the request fails, the circuit-breaker returns to the OPEN state for the duration of the sleep window. &#xA;So, This HALF-OPEN state call determines whether to OPEN or CLOSE circuit.</p>&#xA;&#xA;<p>If the request succeeds, the circuit-breaker transitions to CLOSED and the logic in 1. takes over again. checkout <a href=""https://design.codelytics.io/hystrix/how-it-works"" rel=""nofollow noreferrer"">hystrix working.</a></p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/OAfjY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OAfjY.png"" alt=""Circuit Breaker""></a></p>&#xA;"
51483057,51469118,4085968,2018-07-23T16:05:30,"<p>For programmatically management of <strong>contracts</strong>, if you using spring-cloud stack then you must look into <a href=""https://cloud.spring.io/spring-cloud-contract/"" rel=""nofollow noreferrer"">spring-cloud-contract</a>, by which you can easily keep track of your latest version of contracts for your Rest endpoints and also if any change occurs in your api endpoint, this will help you notify by breaking the contract and failing the test-cases build around it.</p>&#xA;&#xA;<p>Let's say for example, service A's <code>/getThing</code> endpoint has been refactored to return different data then all calling services to this endpoint will fail while build time of your project. </p>&#xA;&#xA;<p>However, this methodology won't facilitate updating the API gateway to adapt to this change as there might different logic you want to perform of every new version of your endpoints.</p>&#xA;&#xA;<p>You can also create Rest Docs snippets using these endpoint contracts. checkout <a href=""http://cloud.spring.io/spring-cloud-contract/single/spring-cloud-contract.html#_generating_spring_rest_docs_snippets_from_the_contracts"" rel=""nofollow noreferrer"">Rest Docs snippets</a>. You can also use <a href=""https://swagger.io/"" rel=""nofollow noreferrer"">swagger</a> for documenting your endpoints.</p>&#xA;&#xA;<p>for NodeJs check <a href=""https://stackoverflow.com/questions/45828892/spring-cloud-contract-support-for-node-js"">here</a>.   </p>&#xA;"
51769891,51767389,4085968,2018-08-09T14:40:25,"<p>This could be because of Ribbon does the retry when there is connection time-out. For details on that check <a href=""http://ryanjbaxter.com/cloud/spring%20cloud/spring/2017/03/15/retrying-http-requests-in-spring-cloud-netflix.html"" rel=""nofollow noreferrer"">spring retry</a> . To fix this issue you can follow below step:- </p>&#xA;&#xA;<ul>&#xA;<li>In <strong>application.properties</strong> of your consumer of csv-converter microservice <code>feignClientName.ribbon.OkToRetryOnAllOperations=false</code> </li>&#xA;</ul>&#xA;&#xA;<p>and you also check more properties here <a href=""https://github.com/spencergibb/spring-cloud-sandbox/blob/master/spring-cloud-sandbox-sample-frontend/src/main/resources/application.yml"" rel=""nofollow noreferrer"">application.yml</a></p>&#xA;"
51241753,51238009,4085968,2018-07-09T09:04:36,"<p>From Design perspective, there is no harm having <code>v1/suppliers/{supplierList}</code> as an endpoint for your service (microservice called supplier service).&#xA;For performance improvement you can do other optimization like <strong>cache</strong> etc. or implement <a href=""http://microservices.io/patterns/data/cqrs.html"" rel=""nofollow noreferrer"">CQRS</a>.</p>&#xA;"
51289878,51285225,4085968,2018-07-11T16:01:49,"<p>If you are looking for any open-source solution for your problem, you can have a look into <a href=""https://www.keycloak.org/"" rel=""nofollow noreferrer"">keyclaok</a>. </p>&#xA;&#xA;<p>Keycloak also got place in <a href=""https://www.thoughtworks.com/radar/platforms/keycloak"" rel=""nofollow noreferrer"">Thoughtworks Technology RADAR</a>. &#xA;It is very promising solution and has LDAP, Multi Tenancy support also. checkout <a href=""https://www.keycloak.org/docs/2.5/server_admin/topics/overview/features.html"" rel=""nofollow noreferrer"">keycloak features</a>. </p>&#xA;&#xA;<p>There is paid solution like <a href=""https://www.forgerock.com/"" rel=""nofollow noreferrer"">ForgeRock</a> is also avaible.</p>&#xA;&#xA;<p>Coming to feedback which you have asked about SOA or microservice way of implementation here (You will get different feedback/advice on this)</p>&#xA;&#xA;<p>It will be better if you have a service to take care of access and authorization management and other to look into user details. If you meant that having different services for different account then note that Having one service for taking care account is still considered as Microservice approach as there is one dedicated service to perform single set of tasks. </p>&#xA;&#xA;<p>You can have User-Service for user information management and a authService to handle access and authorization of users. <a href=""https://stackoverflow.com/questions/44886715/should-the-auth-server-be-combined-with-the-user-service-in-a-microservices-arch"">check</a>.</p>&#xA;"
50780886,50772902,4085968,2018-06-10T05:40:39,"<p>For breaking microservices, it is always preferred to with <a href=""https://en.wikipedia.org/wiki/Domain-driven_design"" rel=""nofollow noreferrer"">Domain-driven design</a>. If in your case, Assessments does not exist without Questions then it totally make sense to have them in same domain.   </p>&#xA;"
50915833,50903364,4085968,2018-06-18T18:56:19,"<p><strong>Orchestration</strong> can be linked with how Orchestra performs i.e when you have control over all the actors in a process - when they're all in one domain of control and you can control the flow of activities. This is ofcourse most often when you're specifying a business process that will be enacted inside one organisation that you have control over.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/U19X0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U19X0.png"" alt=""Orchestration vs. Choreography""></a></p>&#xA;&#xA;<p><strong>Choreography</strong> is a way of specifying how two or more parties - none of which has any control over the other parties' processes, or perhaps any visibility of those processes - can coordinate their activities and processes to share information and value. Use choreography when coordination across domains of control/visibility is required. You can think of choreography, in a simple scenario, as like a network protocol. It dictates acceptable patterns of requests and responses between parties.</p>&#xA;&#xA;<p>You can choose between Orchestration and Choreography in your Microservices based on what fits best for your use case based something similar lines of above mentioned explanation.  </p>&#xA;"
50572185,50562495,4085968,2018-05-28T18:57:26,"<p>When any one of the microservice is down, Interaction between services becomes very critical as <strong>isolation of failure, resilience and fault tolerance</strong> are some of key characteristics for any microservice based architecture.</p>&#xA;&#xA;<p>Totally agreed what @jayant had answered, <strong>in your case Implementing proper fallback mechanism makes more sense and you can implement required logic you wanna write based on use case and dependencies between M1, M2 and M3.&#xA;you can also raise events in your fallback if needed.</strong> </p>&#xA;&#xA;<p>Since you are using new to microservice, you need to know below <strong>common techniques and architecture patterns</strong> for resilience and fault tolerance against the situation which you have raised in your question. And here your are using Spring-Boot, use can easily add Netflix-OSS in your microservices. </p>&#xA;&#xA;<p>Netflix has released <a href=""https://medium.com/netflix-techblog/introducing-hystrix-for-resilience-engineering-13531c1ab362"" rel=""nofollow noreferrer"">Hystrix</a>, a library designed to control points of access to remote systems, services and 3rd party libraries, providing greater tolerance of latency and failure.</p>&#xA;&#xA;<p>It include below important characteristics: </p>&#xA;&#xA;<ul>&#xA;<li><p><strong>Importance of Circuit breaker and Fallback Mechanism:</strong></p>&#xA;&#xA;<p>Hystrix that implements the <a href=""https://martinfowler.com/bliki/CircuitBreaker.html"" rel=""nofollow noreferrer"">circuit breaker pattern</a> which is useful when a &#xA;service failure can cause cascading failure all the way up to the user. &#xA;When calls to a particular service exceed &#xA;<code>circuitBreaker.requestVolumeThreshold</code> (default: 20 requests) and the &#xA;failure percentage is greater than &#xA;<code>circuitBreaker.errorThresholdPercentage</code> (default: >50%) in a rolling &#xA;window defined by <code>metrics.rollingStats.timeInMilliseconds</code> (default: 10 &#xA;seconds), the circuit opens and the call is not made. </p>&#xA;&#xA;<p>In cases of error and an open circuit, a fallback can be provided by the &#xA;developer. Fallbacks may be chained so that the first fallback makes &#xA;some other business call. check out <a href=""https://github.com/Netflix/Hystrix/wiki/How-To-Use#Fallback"" rel=""nofollow noreferrer"">Fallback Implementation of Hystrix </a> </p></li>&#xA;<li><p><strong>Retry:</strong></p>&#xA;&#xA;<p>When a request fails, you may want to have the request be retried &#xA;automatically. <strong><a href=""https://github.com/Netflix/ribbon"" rel=""nofollow noreferrer"">Ribbon</a> does this job for us.</strong>&#xA;In distributed system, a microservices system retry can trigger multiple &#xA;other requests or retries and start a cascading effect</p>&#xA;&#xA;<p><strong>here are some properties to look of Ribbon</strong></p>&#xA;&#xA;<p><code>sample-client.ribbon.MaxAutoRetries=1</code></p>&#xA;&#xA;<h1>Max number of next servers to retry (excluding the first server)</h1>&#xA;&#xA;<p><code>sample-client.ribbon.MaxAutoRetriesNextServer=1</code></p>&#xA;&#xA;<h1>Whether all operations can be retried for this client</h1>&#xA;&#xA;<p><code>sample-client.ribbon.OkToRetryOnAllOperations=true</code></p>&#xA;&#xA;<h1>Interval to refresh the server list from the source</h1>&#xA;&#xA;<p><code>sample-client.ribbon.ServerListRefreshInterval=2000</code></p>&#xA;&#xA;<p>More details :- <a href=""https://github.com/Netflix/ribbon/wiki/Getting-Started#the-properties-file-sample-clientproperties"" rel=""nofollow noreferrer"">ribbon properties</a></p></li>&#xA;<li><p><strong>Bulkhead Pattern:</strong> </p>&#xA;&#xA;<p>In general, the goal of the bulkhead pattern is to avoid faults in one &#xA;part of a system to take the entire system down. <a href=""https://docs.microsoft.com/en-us/azure/architecture/patterns/bulkhead"" rel=""nofollow noreferrer"">bulkhead pattern</a></p>&#xA;&#xA;<p>The bulkhead implementation in Hystrix limits the number of concurrent &#xA;calls to a component. This way, the number of resources (typically &#xA;threads) that is waiting for a reply from the component is limited.</p>&#xA;&#xA;<p><strong>Assume you have a request based, multi threaded application (for example &#xA;a typical web application) that uses three different components, M1, M2, &#xA;and M3. If requests to component M3 starts to hang, eventually all &#xA;request handling threads will hang on waiting for an answer from M3.</strong> </p>&#xA;&#xA;<p>This would make the application entirely non-responsive. If requests to &#xA;M3 is handled slowly we have a similar problem if the load is high &#xA;enough.&#xA;Implementation details can be found <a href=""https://github.com/Netflix/Hystrix/wiki/How-To-Use#CommandThreadPool"" rel=""nofollow noreferrer"">here</a> </p>&#xA;&#xA;<p><strong>So, These are some factors you need to consider while handling microservice Interaction when one of the microservice is down.</strong></p></li>&#xA;</ul>&#xA;"
50403956,50390452,4085968,2018-05-18T04:47:34,"<p>This Problem occurs because Ribbon does the retry when there is connection time-out. For details on that check <a href=""http://ryanjbaxter.com/cloud/spring%20cloud/spring/2017/03/15/retrying-http-requests-in-spring-cloud-netflix.html"" rel=""nofollow noreferrer"">spring retry</a> . To fix this issue you can follow below step :- </p>&#xA;&#xA;<ul>&#xA;<li>In <strong>application.properties</strong> of your Microservice X <code>feignClientName.ribbon.OkToRetryOnAllOperations=false</code> and you also check more properties here <a href=""https://github.com/spencergibb/spring-cloud-sandbox/blob/master/spring-cloud-sandbox-sample-frontend/src/main/resources/application.yml"" rel=""nofollow noreferrer"">application.yml</a></li>&#xA;</ul>&#xA;"
50468340,50454109,4085968,2018-05-22T12:58:22,"<p><strong>It all depends on your service's communication behaviour to choose between REST APIs and Event-Based design Or Both</strong>.</p>&#xA;&#xA;<p>What you do is based on your  requirement you can choose <strong>REST APIs</strong> where you see <strong>synchronous behaviour</strong> between services &#xA;and go with <strong>Event based design</strong> where you find services needs <strong>asynchronous behaviour</strong>, there is no harm combining both also. </p>&#xA;&#xA;<p>Ideally for inter-process communication protocol it is better to go with messaging and for client-service REST APIs are best fitted.&#xA;Check the Communication style in <a href=""http://microservices.io/"" rel=""nofollow noreferrer"">microservices.io</a></p>&#xA;&#xA;<p><strong>REST based Architecture</strong></p>&#xA;&#xA;<ul>&#xA;<li><p>Advantage</p>&#xA;&#xA;<ol>&#xA;<li><p>Request/Response is easy and best fitted when you need synchronous environments.</p></li>&#xA;<li><p>Simpler system since there in no intermediate broker</p></li>&#xA;<li><p>Promotes orchestration i.e Service can take action based on response of other service.</p></li>&#xA;</ol></li>&#xA;<li><p>Drawback</p>&#xA;&#xA;<ol>&#xA;<li><p>Services needs to discover locations of service instances.</p></li>&#xA;<li><p>One to one Mapping between services.</p></li>&#xA;<li><p>Rest used HTTP which is general purpose protocol built on top of TCP/IP which adds enormous amount of overhead when using it to pass messages.</p></li>&#xA;</ol></li>&#xA;</ul>&#xA;&#xA;<p><strong>Event Driven Architecture</strong></p>&#xA;&#xA;<ul>&#xA;<li><p>Advantage</p>&#xA;&#xA;<ol>&#xA;<li><p>Event-driven architectures are appealing to API developers because they function very well in asynchronous environments.</p></li>&#xA;<li><p>Loose coupling since it decouples services as on a event of once service multiple services can take action based on application requirement. it is easy to plug-in any new consumer to producer.</p></li>&#xA;<li><p>Improved availability since the message broker buffers messages until the consumer is able to process them.</p></li>&#xA;</ol></li>&#xA;<li><p>Drawback</p>&#xA;&#xA;<ol>&#xA;<li>Additional complexity of message broker, which must be highly available</li>&#xA;<li>Debugging an event request is not that easy. </li>&#xA;</ol></li>&#xA;</ul>&#xA;"
50495323,50484157,4085968,2018-05-23T18:40:25,"<p>By default Zuul load balancer using the ZoneAwareLoadBalancer from Ribbon. So there is nothing like choosing between Zuul and Ribbon for Load-Balancing, <strong>it's basically Ribbon who is involved in Load-Balancing</strong>. check out <a href=""https://github.com/Netflix/zuul/wiki/Core-Features#load-balancing"" rel=""nofollow noreferrer"">Zuul load-balancing</a></p>&#xA;&#xA;<p>As Ribbon is a client-side load balancer module and is integrated to many http client modules. As an example, Feign and Load-balanced RestTemplate support Ribbon. do check <a href=""https://github.com/Netflix/ribbon/wiki/Working-with-load-balancers"" rel=""nofollow noreferrer"">Ribbon's working with load balancer</a></p>&#xA;&#xA;<p>Regarding Zuul, there is a <code>RibbonRoutingFilter</code> that routes your request to an actual service instance. <code>RibbonRoutingFilter</code> is using Ribbon to choose a server from the list that is given from your configuration or from Eureka. <strong>So if you want to use Zuul as a load-balanced reverse proxy, Zuul needs Ribbon</strong>. </p>&#xA;"
50425947,50417003,4085968,2018-05-19T13:45:09,"<p>Since Microservices architecture are Un-opinionated software design. So you may get different answers on this questions.&#xA;Yes, REST and Event based are two different things but sometime both combined gives design to achieve better flexibility.</p>&#xA;&#xA;<p>Answering to your concerns, <strong>I don't see any harm if REST APIs also subscribe to a queue</strong>  as long as you can maintain both of them i.e changes to message does not have any impact of APIs and you have proper fallback and <a href=""https://en.wikipedia.org/wiki/Eventual_consistency"" rel=""nofollow noreferrer"">Eventual consistency</a> mechanism in place. you can check <a href=""https://softwareengineering.stackexchange.com/questions/331101/relationship-between-restful-uris-and-pubsub-topics"">discussion</a> . There are already few project which tried it such as <a href=""https://github.com/zalando/nakadi"" rel=""nofollow noreferrer"">nakadi</a> and <a href=""https://github.com/eclipse/ponte"" rel=""nofollow noreferrer"">ponte</a>.</p>&#xA;&#xA;<p><strong>So It all depends on your service's communication behaviour to choose between REST APIs and Event-Based design Or Both</strong>.</p>&#xA;&#xA;<p>What you do is based on your  requirement you can choose <strong>REST APIs</strong> where you see <strong>synchronous behaviour</strong> between services &#xA;and go with <strong>Event based design</strong> where you find services needs <strong>asynchronous behaviour</strong>, there is no harm combining both also. </p>&#xA;&#xA;<p>Ideally for inter-process communication protocol it is better to go with messaging and for client-service REST APIs are best fitted.&#xA;Check the Communication style in <a href=""http://microservices.io/"" rel=""nofollow noreferrer"">microservices.io</a></p>&#xA;&#xA;<p><strong>REST based Architecture</strong></p>&#xA;&#xA;<ul>&#xA;<li><p>Advantage</p>&#xA;&#xA;<ol>&#xA;<li><p>Request/Response is easy and best fitted when you need synchronous environments.</p></li>&#xA;<li><p>Simpler system since there in no intermediate broker</p></li>&#xA;<li><p>Promotes orchestration i.e Service can take action based on response of other service.</p></li>&#xA;</ol></li>&#xA;<li><p>Drawback</p>&#xA;&#xA;<ol>&#xA;<li><p>Services needs to discover locations of service instances.</p></li>&#xA;<li><p>One to one Mapping between services.</p></li>&#xA;<li><p>Rest used HTTP which is general purpose protocol built on top of TCP/IP which adds enormous amount of overhead when using it to pass messages.</p></li>&#xA;</ol></li>&#xA;</ul>&#xA;&#xA;<p><strong>Event Driven Architecture</strong></p>&#xA;&#xA;<ul>&#xA;<li><p>Advantage</p>&#xA;&#xA;<ol>&#xA;<li><p>Event-driven architectures are appealing to API developers because they function very well in asynchronous environments.</p></li>&#xA;<li><p>Loose coupling since it decouples services as on a event of once service multiple services can take action based on application requirement. it is easy to plug-in any new consumer to producer.</p></li>&#xA;<li><p>Improved availability since the message broker buffers messages until the consumer is able to process them.</p></li>&#xA;</ol></li>&#xA;<li><p>Drawback</p>&#xA;&#xA;<ol>&#xA;<li>Additional complexity of message broker, which must be highly available</li>&#xA;<li>Debugging an event request is not that easy. </li>&#xA;</ol></li>&#xA;</ul>&#xA;"
49638448,49638004,4085968,2018-04-03T20:15:54,"<p>Firstly, This isn't super dumb question. As we know that Working with Microservices does not any fixed Pattern Or Design Consideration so it always depends on what is perfect for our Architecture. </p>&#xA;&#xA;<p>As far as what is looks based on details you have provided, <strong>There won't be any implications</strong>.Provided as you also mentioned ensuring that only the gateway can call it directly no other origin is allowed access and accessing data doesn't need any kind of Roles of your Application.</p>&#xA;&#xA;<p>For understanding more on Microservices Security you can check <a href=""http://www.grahamlea.com/2015/07/microservices-security-questions/"" rel=""nofollow noreferrer"">Microservice's Security Questions</a>. Hope this information is helpful.</p>&#xA;"
36994200,36954140,1008336,2016-05-03T01:33:30,"<p>This is not supported. MSF4J doesn't claim to be 100% JAXRS compliant, but is a lightweight framework for building microservices.&#xA;I've created the JIRA [1] for this. We will implement this in future release.</p>&#xA;&#xA;<p>[1] - <a href=""https://wso2.org/jira/browse/WMS-83"" rel=""nofollow"">https://wso2.org/jira/browse/WMS-83</a></p>&#xA;"
47655524,47527983,1410512,2017-12-05T14:05:03,"<p>I think your module structure is good. But I would suggest you create a real 'multi module' project (<a href=""https://maven.apache.org/guides/mini/guide-multiple-modules.html"" rel=""nofollow noreferrer"">link</a>). This way, using code from another module will generate a <strong>compile-error</strong>. This will help you to keep your good intentions!</p>&#xA;&#xA;<p>In order to do this, you'll have to split each module in a <strong>private</strong> (implementations) and <strong>public</strong> (api, only interfaces) module (By doing this, you don't need an 'api'-package). &#xA;An implementation module can depend on any public-module, but not a private-module.</p>&#xA;&#xA;<p>If you wire your application together in the private module, with dependency injection, the private modules will have no 'internal' dependencies!&#xA;The private modules will not have any '<strong>compile-time</strong>' dependencies, only  '<strong>runtime</strong>' dependencies.</p>&#xA;&#xA;<p>Here quick module dependency graph: <a href=""https://i.stack.imgur.com/tLLmb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tLLmb.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>I hope you find this usefull!</p>&#xA;&#xA;<p><strong>Edit:</strong>&#xA;You will need an extra module only to bootstrap the application!</p>&#xA;"
50008846,49992237,8359632,2018-04-24T18:46:32,"<p>First, I don't think repeating of go vendor is necessary unless dependency has changed. It should not happen often.</p>&#xA;&#xA;<p>Second, Golang does require you to build/compile, so if you make changes you would have to re-build the service. However, most of the times, you should make changes only to one services to make your life easier. It will make the testing and debugging easier during development.</p>&#xA;&#xA;<p>Lastly, there are a few ways to speed things up with development using docker. I would suggest keep using docker as it will make your devops life easier, although you don't have to. For simplicity, you can move the build step to Docker container. What you have to do is to copy the dir including the dependency into container and build inside the container (or just do <code>go run</code> inside container). This is OK for development. If you would like to have one set of container definition for development and deployment, you can write a script to compile each services, copy the binary into one directory and have docker copy the binary from that directory. A few lines of python script should be enough. This has the added benefits of using docker-compose where you can stand up the whole services with a single click. Then you can also write your integration test launcher using another docker container which will run your test once and exit.  </p>&#xA;"
51951483,51950785,4175568,2018-08-21T14:56:36,"<p>In my opinion - the second option is the one that makes the most sense - you might want to change the Dockerfile when you change the code for the repository, so you can trigger the CI pipeline with a single push. On the CI here you also want to build the containers. Also the Dockerfile is a resource that you often want both DevOps and Dev teams to be able to access and often work on them together.</p>&#xA;&#xA;<p>Additionally if you have CI integration, you would like to trigger the pipeline after each change to the Dockerfile, which would be considerably easier when it's in the same place with the code.</p>&#xA;&#xA;<p>Then you also have all Kubernetes/Helm files in a different (single) repository so you can manage them and combine them when necessary for the different microservices (if you have more complex deployments). In the CI here, you just want linting for the charts/files.</p>&#xA;"
38123573,38121112,3670737,2016-06-30T12:47:39,"<p>You may try to customize the default <code>JwtBearerAuthenticationMiddleware</code> providing to it a custom <code>ISecurityTokenValidator</code>. Your user identity will be automatically set by the middleware, and you may continue using the <code>Authorize</code> attribute inside MVC:</p>&#xA;&#xA;<pre><code>class MyTokenValidator : ISecurityTokenValidator&#xA;{&#xA;    public string AuthenticationScheme { get; }&#xA;&#xA;    public MyTokenValidator(string authenticationScheme)&#xA;    {&#xA;        AuthenticationScheme = authenticationScheme;&#xA;    }&#xA;&#xA;    public bool CanValidateToken =&gt; true;&#xA;&#xA;    public int MaximumTokenSizeInBytes&#xA;    {&#xA;        get&#xA;        {&#xA;            throw new NotImplementedException();&#xA;        }&#xA;        set&#xA;        {&#xA;            throw new NotImplementedException();&#xA;        }&#xA;    }&#xA;&#xA;    public bool CanReadToken(string securityToken) =&gt; true;&#xA;&#xA;    public ClaimsPrincipal ValidateToken(string securityToken, TokenValidationParameters validationParameters, out SecurityToken validatedToken)&#xA;    {&#xA;        validatedToken = null;&#xA;&#xA;        //your logic here&#xA;        var response = GetResponseFromMyAuthServer(securityToken);&#xA;        //assuming response will contain info about the user&#xA;&#xA;        if(response == null || response.IsError)&#xA;            throw new SecurityTokenException(""invalid"");&#xA;&#xA;        //create your identity by generating its claims&#xA;        var claims = new[]&#xA;        {&#xA;            new Claim(ClaimTypes.NameIdentifier, response.UserId),&#xA;            new Claim(ClaimTypes.Email, response.Email),&#xA;            new Claim(ClaimsIdentity.DefaultNameClaimType, response.UserName),&#xA;        };&#xA;&#xA;        return new ClaimsPrincipal(new ClaimsIdentity(claims, AuthenticationScheme));&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>And in your startup class:</p>&#xA;&#xA;<pre><code>var options = new JwtBearerOptions();&#xA;options.SecurityTokenValidators.Clear();&#xA;options.SecurityTokenValidators.Add(new MyTokenValidator(options.AuthenticationScheme));&#xA;&#xA;app.UseJwtBearerAuthentication(options);&#xA;&#xA;//the rest of your code here&#xA;app.UseMvc();&#xA;</code></pre>&#xA;&#xA;<p>You may need to further refine this approach, but this way you can achieve what you need by delegating all the validation to the remote endpoint.</p>&#xA;"
45084339,45080886,4799185,2017-07-13T14:56:54,"<p>If your schema changes, you still need to restart the Rails processes. The reason is that ActiveRecord caches column data during startup.</p>&#xA;"
44728124,44065186,5784273,2017-06-23T18:47:37,"<p>Writing compensation logic within Hystrix fallback is dangerous because of no persistence involved.</p>&#xA;&#xA;<p>This approach doesn't offer any resiliency. ACID guarantee from the database is not enough here because of external parties involved, and the Hystrix fallback will not guard you from anything that's not part of your code. </p>&#xA;&#xA;<p>For example, if your solution experiences outage (say, power outage or a simple <code>kill -9</code>) after payment completion, you will lose both the order and the compensation logic, meaning order will be paid for, but not present in the database.</p>&#xA;&#xA;<p>A more resilient approach would involve any popular message broker for event-driven delivery and some deduplication in processing logic to ensure exactly-once quality of service when the events get redelivered after an outage.</p>&#xA;"
51281914,51272560,4413905,2018-07-11T09:29:07,<p><strong>RESOLVED</strong></p>&#xA;&#xA;<p>The cause was logging aspect. I realized a lot of threads waiting on: </p>&#xA;&#xA;<pre><code>sun.misc.Unsafe.park(Unsafe.java:-2) native&#xA;    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)&#xA;        java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)&#xA;        java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)&#xA;        java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)&#xA;        java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209)&#xA;        java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285)&#xA;        ch.qos.logback.core.OutputStreamAppender.writeBytes(OutputStreamAppender.java:197)&#xA;        ch.qos.logback.core.OutputStreamAppender.subAppend(OutputStreamAppender.java:231)&#xA;        ch.qos.logback.core.OutputStreamAppender.append(OutputStreamAppender.java:102)&#xA;        ch.qos.logback.core.UnsynchronizedAppenderBase.doAppend(UnsynchronizedAppenderBase.java:84)&#xA;        ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51)&#xA;        ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270)&#xA;        ch.qos.logback.classic.Logger.callAppenders(Logger.java:257)&#xA;        ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421)&#xA;        ch.qos.logback.classic.Logger.filterAndLog_2(Logger.java:414)&#xA;        ch.qos.logback.classic.Logger.debug(Logger.java:490) &#xA;</code></pre>&#xA;
48511013,46476437,1341905,2018-01-29T22:23:32,"<ol>&#xA;<li><p>In conjunction with a <a href=""https://cloud.google.com/appengine/docs/standard/python/config/dispatchref"" rel=""nofollow noreferrer"">dispatch.yaml</a> directive to handle routing, you can use GCE as a gateway to multiple microservices running as services in a given project.</p></li>&#xA;<li><p>You can call the services directly or proxy through a server.</p></li>&#xA;</ol>&#xA;"
42107357,42073576,4423889,2017-02-08T07:42:45,"<p>During the investigation, I found that there are a few approaches:</p>&#xA;&#xA;<ol>&#xA;<li><p><strong>Client-side Service Discovery</strong> - suppose you have consul and it knows all about available servers and their statuses, at the client&#xA;you should write a service layer, which can call consul's API, fetch&#xA;healthy servers, and then do one more http request to needed server.&#xA;(Of course it can be a bit smarter and has an ability e.g. cache&#xA;healthy servers etc.).</p></li>&#xA;<li><p><strong>Server-side Service Discovery (load balancer)</strong> - an additional layer above the consul - it can be haproxy or nginx, and it will forward requests to&#xA;    needed server. (From the front-end side you can use consul dns names&#xA;    or docker container dns names).</p></li>&#xA;<li><p><strong>Server-side Service Discovery (API Gateway)</strong> - and the last one, you can write one more microservice which will handle all requests and proxy them to the needed servers after checking their statuses in the consul.</p></li>&#xA;</ol>&#xA;&#xA;<p>But now there is one more question - which approach should you use? - I think it very depends on the project complexity, server load, and count of microservices.</p>&#xA;&#xA;<p>IMHO if you have a few microservices and low server load, you can use any of them, but in any other cases I think it's better to choose 2th approach.</p>&#xA;"
47272817,47271002,3390417,2017-11-13T20:15:04,"<p>One alternative approach, since you are using <em>Spring Boot</em> would be to do the following:</p>&#xA;&#xA;<ol>&#xA;<li>First annotation your <code>@SpringBootApplication</code> class with <a href=""https://docs.spring.io/spring-data/gemfire/docs/current/api/org/springframework/data/gemfire/transaction/config/EnableGemfireCacheTransactions.html"" rel=""nofollow noreferrer""><code>@EnableGemfireCacheTransactions</code></a>...</li>&#xA;</ol>&#xA;&#xA;<p>Example:</p>&#xA;&#xA;<pre><code>@SpringBootApplication&#xA;@EnableGemfireCacheTransactions&#xA;@EnableGemfireRepositories&#xA;class YourSpringBootApplication { &#xA;&#xA;    public static void main(String[] args) {&#xA;        SpringApplication.run(YourSpringBootApplication.class, args);&#xA;    }&#xA;&#xA;    ...&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>The <code>@EnableGemfireCacheTransactions</code> annotation enables <em>Spring Data GemFire's</em> <a href=""https://docs.spring.io/spring-data/gemfire/docs/current/api/org/springframework/data/gemfire/transaction/GemfireTransactionManager.html"" rel=""nofollow noreferrer""><code>GemfireTransactionManager</code></a>, which integrates GemFire's <a href=""http://gemfire-91-javadocs.docs.pivotal.io/org/apache/geode/cache/CacheTransactionManager.html"" rel=""nofollow noreferrer""><code>CacheTransactionManager</code></a> with <a href=""https://docs.spring.io/spring/docs/current/spring-framework-reference/data-access.html#transaction"" rel=""nofollow noreferrer""><em>Spring Transaction Management</em> infrastructure</a> which then allows you to do this...</p>&#xA;&#xA;<ol start=""2"">&#xA;<li><p>Now, just annotate your <code>@Service</code> application component transactional service methods with core <em>Spring's</em> <code>@Transactional</code> annotation, like so...</p>&#xA;&#xA;<p>@Service&#xA;class YourBoxReceiverTransferService {</p>&#xA;&#xA;<pre><code>@Transactional&#xA;public &lt;return-type&gt; update(ReceiveContext receiveContext, &#xA;        TransferContext transferContext {&#xA;&#xA;    ...&#xA;    receiveContextRepository.save(receiveContext);&#xA;    transferContextRepository.save(transferContext);&#xA;    ...&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>}</p></li>&#xA;</ol>&#xA;&#xA;<p>As you can see here, I also used <a href=""https://docs.spring.io/spring-data/gemfire/docs/current/reference/html/#gemfire-repositories"" rel=""nofollow noreferrer""><em>Spring Data (GemFire's) Repository</em></a> infrastructure to manage the persistence operations (e.g. CRUD), which will be used appropriately in the transactional scoped-context setup by <em>Spring</em>.</p>&#xA;&#xA;<p>2 advantages with the <em>Spring</em> approach, over using GemFire's public API, which unnecessarily couples you to GemFire (a definite code smell, particularly in a <em>Spring</em> context), is...</p>&#xA;&#xA;<ol>&#xA;<li><p>You don't have to place a bunch of boilerplate, crap code in to your application components, which does <strong>not</strong> belong there!</p></li>&#xA;<li><p>Using <em>Spring's Transaction Management</em> infrastructure, it is extremely easy to change your <em>Transaction Management Strategy</em>, such as by switching from GemFire's local-only cache transactions, to say, Global, <strong>JTA-based Transactions</strong> if the need every arises (such as, oh, well, now I need to send a message over a JMS message queue after the GemFire Region's and Cassandra BOX Table are updated to notify some downstream process that the Receiver/Transfer context has been updated).  With <em>Spring's Transaction Management</em> infrastructure, you <strong>do not</strong> need to change a single line of application code to change transaction management strategies (such as local to global, or global to local, etc).</p></li>&#xA;</ol>&#xA;&#xA;<p>Hope this helps!</p>&#xA;&#xA;<p>-John</p>&#xA;"
51555535,51463258,3652817,2018-07-27T10:01:48,"<p>It depends on what you're doing, but I would say mostly it's OK to do this. This is an established microservices pattern called ""Composite UI"". See this for details: <a href=""https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/architect-microservice-container-applications/microservice-based-composite-ui-shape-layout"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/architect-microservice-container-applications/microservice-based-composite-ui-shape-layout</a></p>&#xA;&#xA;<p>If your microservices are using the CQRS pattern, (while still not wrong) you <em>may</em> be missing an opportunity to compose a view-specific ""view model"". However, if you're composing\showing data from multiple domains I would say that it's still better to just call multiple microservices to retrieve the data you need.</p>&#xA;&#xA;<p>The only problem you <em>may</em> be introducing if you're not careful, is introducing projection logic (which is not THAT bad) or business logic (very very bad) in your client code if you're doing any processing on the data you receive in order to display it. Composite UI is meant to server a UI with clearly separated sections.</p>&#xA;"
31868288,31845342,2867409,2015-08-07T01:23:08,"<p>Great question.</p>&#xA;&#xA;<p>First and foremost, make sure that <code>spring-cloud-starter-config</code> is on the class path of your applications that want to use remote configuration from a config service.</p>&#xA;&#xA;<p><a href=""http://cloud.spring.io/spring-cloud-config/spring-cloud-config.html#_client_side_usage"" rel=""nofollow"">http://cloud.spring.io/spring-cloud-config/spring-cloud-config.html#_client_side_usage</a></p>&#xA;&#xA;<p>The best way to understand whether or not the config service is correctly serving the environment configurations for an application is to enable health check.</p>&#xA;&#xA;<p>On your config service configuration, make sure to enable the following for one of your applications. I've added a health check for the <code>movie</code> service, with a label of <code>master</code> (indicating to use the master branch of my git repository).</p>&#xA;&#xA;<pre><code>spring:&#xA;  cloud:&#xA;    config:&#xA;      server:&#xA;        git:&#xA;          uri: https://github.com/kbastani/spring-boot-microservice-config&#xA;        health:&#xA;          repositories:&#xA;            movie:&#xA;              label: master &#xA;</code></pre>&#xA;&#xA;<p>Now all that I need to make sure of is that my git repository has a configuration available for my application with the name <code>movie</code>. The name of this configuration could be <code>movie.{properties|yml}</code>. I've chosen to use yaml: <a href=""https://github.com/kbastani/spring-boot-microservice-config/blob/master/movie.yml"" rel=""nofollow"">https://github.com/kbastani/spring-boot-microservice-config/blob/master/movie.yml</a></p>&#xA;&#xA;<p>Now after you've started your config service, you can run a health check to see if the remote repository is being used.</p>&#xA;&#xA;<p><code>$ curl http://localhost:8888/health</code></p>&#xA;&#xA;<p>This will return the following response:</p>&#xA;&#xA;<pre><code>{&#xA;  ""status"" : ""UP"",&#xA;  ""configServer"" : {&#xA;    ""status"" : ""UP"",&#xA;    ""repositories"" : [ {&#xA;      ""sources"" : [ ""https://github.com/kbastani/spring-boot-microservice-config/movie.yml"", ""https://github.com/kbastani/spring-boot-microservice-config/application.yml"" ],&#xA;      ""name"" : ""movie"",&#xA;      ""profiles"" : [ ""default"" ],&#xA;      ""label"" : ""master""&#xA;    } ]&#xA;  },&#xA;  ""discoveryComposite"" : {&#xA;    ""description"" : ""Spring Cloud Eureka Discovery Client"",&#xA;    ""status"" : ""UP"",&#xA;    ""discoveryClient"" : {&#xA;      ""description"" : ""Spring Cloud Eureka Discovery Client"",&#xA;      ""status"" : ""UP"",&#xA;      ""services"" : [ ""configserver"" ]&#xA;    }&#xA;  },&#xA;  ""diskSpace"" : {&#xA;    ""status"" : ""UP"",&#xA;    ""total"" : 498954403840,&#xA;    ""free"" : 445484142592,&#xA;    ""threshold"" : 10485760&#xA;  },&#xA;  ""hystrix"" : {&#xA;    ""status"" : ""UP""&#xA;  }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Now in the movie service, make sure the following configuration is set in <code>bootstrap.yml</code>.</p>&#xA;&#xA;<pre><code>spring:&#xA;  application:&#xA;    name: movie&#xA;  profiles:&#xA;    active: default&#xA;  cloud:&#xA;    config:&#xA;      uri: http://localhost:8888&#xA;    failFast: true&#xA;</code></pre>&#xA;&#xA;<p>Now start your movie service, making sure first that the config service is running and available at <code>http://localhost:8888</code>, and the remote configuration will be used for the specified profile.</p>&#xA;"
38790888,38786207,1172945,2016-08-05T13:48:14,"<p>Write your custom exception mapper and register it. You can customize responses.</p>&#xA;&#xA;<p>Complete example is <a href=""http://www.codingpedia.org/ama/error-handling-in-rest-api-with-jersey/"" rel=""nofollow"">here</a></p>&#xA;&#xA;<pre><code>public class GenericExceptionMapper implements ExceptionMapper&lt;Throwable&gt; {&#xA;&#xA;    @Override&#xA;    public Response toResponse(Throwable ex) {&#xA;        return Response.status(500).entity(YOUR_RETURN_OBJ_HERE).build();&#xA;    }&#xA;&#xA;}&#xA;</code></pre>&#xA;"
47521336,47479663,1256914,2017-11-27T23:09:45,"<p><a href=""https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events"" rel=""nofollow noreferrer"">Server-sent Events</a> (SSEs) allow your server to push new information to your browser, and hence look ideal to me for this purpose. They work over HTTP and there is <a href=""https://www.w3schools.com/html/html5_serversentevents.asp"" rel=""nofollow noreferrer"">good support for all browsers</a> except for IE.</p>&#xA;&#xA;<p>So the new process could look as follows:</p>&#xA;&#xA;<ol>&#xA;<li>Client send request to server, which initiates the lookup and potentially responds with the topic the browser needs to subscribe to (in case that's unique per lookup)</li>&#xA;<li>Server does its thing and sends updates as it processes new content. See how the beauty of this is that you could inform your client about partial updates.</li>&#xA;</ol>&#xA;&#xA;<p>If SSEs is not an option to you, you could leverage good old Websockets for bi-directional communication, but for such a simple endeavor, it might be too much technology to solve the problem.</p>&#xA;&#xA;<p>A third alternative, especially if you are talking amongst services (no web or mobile clients on the other side) is to use web-hooks, so that the interested party would expose and listen on a specific endpoint, that the publisher (the server that does the processing) would write updates to.</p>&#xA;&#xA;<p>Hope this is useful.</p>&#xA;"
45661246,45661006,30674,2017-08-13T14:09:32,"<p>Ok, so <code>Monolith</code> solutions are the old way of basically having ONE project in a single solution which has <em>all</em> the code in there.</p>&#xA;&#xA;<p>So lets say you're doing a website.</p>&#xA;&#xA;<p>This means you would create a single Solution with a single Project and all the database calls (persistence), logic (business logic/services) and finally figuring out how to display that calculated data (presentation) are all mixed in , in a chaotic way in that single project. Sometimes people have tried to split the concerns into folders, but usually it's a large mess. This makes support/maintenance of the application a nightmare. If you wish to make a single change to the website/application, <em>the entire application will go offline/restart</em>.</p>&#xA;&#xA;<p>vs</p>&#xA;&#xA;<p><code>n-tier / n-layered</code> solutions/applications. This is where we have multiple projects (usually) in a solution which separates the concerns of our application in to more bite-sized components. This enables us to keep the problem space to a single area making it waaay easier to maintain and support. This also enables easier <strong>reuse</strong> of your various components/projects/dll's into various <em>other</em> subsystems of your application. It's way better than the old monolith architecture pattern. But, if you wish to make a single change to the website/application, <em>the entire application will go offline/restart</em> still.</p>&#xA;&#xA;<p>Finally, we have <code>microservices</code>. This is a more modern concept and continues on with the evolution of <code>monolith -&gt; n tier -&gt; microservices</code>. This is when we split up our application concerns into individual applications so that when one microservice needs to be updated, then <em>entire appilication</em> hasn't come to a stop. Sure, the <em>part</em> of the application that has a dependency on the microservice might stop/be affected, but it's possible that the <em>entire</em> app is not.</p>&#xA;&#xA;<p>Lets use an example:</p>&#xA;&#xA;<p>I have a website that sells Pets (cats/dogs/etc).&#xA;I might split this website up into separate microservice mini websites:</p>&#xA;&#xA;<ul>&#xA;<li>authentication</li>&#xA;<li>administration/backend management (think: stuff only an admin can see)</li>&#xA;<li>public website</li>&#xA;<li>animal inventory</li>&#xA;<li>shopping cart</li>&#xA;</ul>&#xA;&#xA;<p>So each of those are a single website, like the the n-tiered architecture'd application. So it would have a presentation layer (the MVC website). some database project and some basic services.</p>&#xA;&#xA;<p>Now each of the 4 microservices (mini websites) all do that.</p>&#xA;&#xA;<p>Now, you need to update some stuff with your administration section of the website. You take that offline and the main website stays up. People can still browse and buy animals.</p>&#xA;&#xA;<p>So yes, it's a nice thing to implement microservices <em>if you application is large enough that it has areas</em> you might want to segment. It does add some more complexity but it also brings about it's own advantages.</p>&#xA;&#xA;<p>And yes, your microservices should follow the n-tiered pattern <em>if you application</em> isn't some silly hello-world app or some Research Project.</p>&#xA;"
42642604,42641804,1091026,2017-03-07T07:30:30,"<p>You can refer to AWS API Gateway for clues on how to implement authentication for REST APIs.&#xA;<a href=""https://aws.amazon.com/api-gateway/faqs/#security"" rel=""nofollow noreferrer"">https://aws.amazon.com/api-gateway/faqs/#security</a></p>&#xA;&#xA;<p>Summary:</p>&#xA;&#xA;<ol>&#xA;<li>Access Token</li>&#xA;<li>Custom Authentication</li>&#xA;<li>Enable CORS</li>&#xA;<li>Client side SSL certificate based authentication</li>&#xA;</ol>&#xA;"
51099288,51099034,2931410,2018-06-29T10:06:43,"<p>I think the problem is with <code>@Id</code> in embedded class. We can not use in an embedded class. Try removing that? If you can remove it, try using <code>@EmbeddedId</code> if you just need an id field.</p>&#xA;"
36185419,36157778,2675922,2016-03-23T17:57:48,"<p>I ran into these same frustrating issues myself and resolved them.</p>&#xA;&#xA;<p>A couple of things to check</p>&#xA;&#xA;<p>1) Make sure your ASP.NET 5 web api project is not referencing any X64 libraries. </p>&#xA;&#xA;<p>2) Port Clash.. when deploying your ASPNET 5 web api to the Cluster, make sure there are no 2 websites that are using the same port.  You can change the port inside the [ASP.NET 5 project]/PackageRoot/ServiceManifest.xml.. towards the bottom</p>&#xA;&#xA;<p>**3) ""wrap"" folder madness!  Verify that your wrap folder (at the root solution folder) only contains your .net 4.5.1 class libraries.  My issues went away when I deleted the ""Newtonsoft.Json"" folder inside the wrap folder.  You will then have to run dnu restore on the solution so it will recreate the project.lock.json files</p>&#xA;&#xA;<p>4) Make sure nothing is blowing up inside your startup.cs class.  Run the web api locally just to make sure its loading; its normal that the service proxy class will fail to load.</p>&#xA;"
45998284,45820257,8509906,2017-09-01T10:30:55,<p>You may need to write your custom router strategy with <code>Spring Cloud Netflix</code> which can deliver your request to different microservice according to your rule . </p>&#xA;
47334344,47329630,56018,2017-11-16T16:13:26,"<p>The simple MassTransit examples are just that, the absolute simplest examples of interacting with queues.</p>&#xA;&#xA;<ol>&#xA;<li>RabbitMQ is your message broker.  It is hosted separately.</li>&#xA;<li>MassTransit is a development framework that makes it much easier to interact with RabbitMQ (or Azure Service Bus) by abstracting away the implementation-specific ""plumbing.""</li>&#xA;<li>You write any number of .NET services that either publish messages to a queue, or subscribe to queues.</li>&#xA;</ol>&#xA;"
51908780,51908605,6872018,2018-08-18T12:41:10,"<p>I am still not aware how its working yet. But I changed the below entry of ""gateway-service.yml"" from the ""config-server"" module and it started working.</p>&#xA;&#xA;<p>I changed </p>&#xA;&#xA;<pre><code>spring:&#xA;  cloud:&#xA;    gateway:&#xA;      discovery:&#xA;        locator:&#xA;          enabled: true&#xA;</code></pre>&#xA;&#xA;<p>to</p>&#xA;&#xA;<pre><code>spring:&#xA;  cloud:&#xA;    gateway:&#xA;      discovery:&#xA;        locator:&#xA;          lowerCaseServiceId: true&#xA;</code></pre>&#xA;&#xA;<p><strong>I found some good links to help to understand this issue more -</strong> </p>&#xA;&#xA;<ol>&#xA;<li><p><a href=""https://github.com/spring-cloud/spring-cloud-gateway/issues/344"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-gateway/issues/344</a> </p></li>&#xA;<li><p><a href=""https://github.com/spring-cloud/spring-cloud-gateway/issues/123"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-gateway/issues/123</a></p></li>&#xA;</ol>&#xA;"
51918508,51918410,6872018,2018-08-19T14:12:09,"<p>I was able to figure out this issue. While building the whole source code, I checked the maven debug log closely and found the below error.</p>&#xA;&#xA;<pre><code>[INFO] --- maven-compiler-plugin:3.7.0:compile (default-compile) @ monitoring ---&#xA;[INFO] Changes detected - recompiling the module!&#xA;[INFO] Compiling 1 source file to C:\Users\pc\Desktop\My_MicroServices\PPP\PiggyMetrics-master\monitoring\target\classes&#xA;[ERROR] error reading C:\Users\pc\.m2\repository\io\projectreactor\ipc\reactor-netty\0.7.8.RELEASE\reactor-netty-0.7.8.RELEASE.jar; invalid LOC header (bad signature)&#xA;[ERROR] error reading C:\Users\pc\.m2\repository\org\springframework\spring-messaging\5.0.7.RELEASE\spring-messaging-5.0.7.RELEASE.jar; invalid LOC header (bad signature)&#xA;[INFO]&#xA;[INFO] --- maven-resources-plugin:3.0.1:testResources (default-testResources) @ monitoring ---&#xA;[INFO] Using 'UTF-8' encoding to copy filtered resources.&#xA;[INFO] Copying 1 resource&#xA;[INFO]&#xA;[INFO] --- maven-compiler-plugin:3.7.0:testCompile (default-testCompile) @ monitoring ---&#xA;[INFO] Changes detected - recompiling the module!&#xA;[INFO] Compiling 1 source file to C:\Users\pc\Desktop\My_MicroServices\PPP\PiggyMetrics-master\monitoring\target\test-classes&#xA;[ERROR] error reading C:\Users\pc\.m2\repository\io\projectreactor\ipc\reactor-netty\0.7.8.RELEASE\reactor-netty-0.7.8.RELEASE.jar; invalid LOC header (bad signature)&#xA;[ERROR] error reading C:\Users\pc\.m2\repository\org\springframework\spring-messaging\5.0.7.RELEASE\spring-messaging-5.0.7.RELEASE.jar; invalid LOC header (bad signature)&#xA;</code></pre>&#xA;&#xA;<p>I deleted <code>""spring-messaging-5.0.7.RELEASE.jar""</code> and <code>""reactor-netty-0.7.8.RELEASE.jar""</code> from <code>.m2</code> and rebuild the project. This time no build compiling error and even able to make the monitoring service up.</p>&#xA;&#xA;<p>Done !</p>&#xA;"
37397777,29704842,2745810,2016-05-23T18:12:34,"<p>The <a href=""https://github.com/bazaarvoice/maven-process-plugin"" rel=""nofollow"">process-exec-maven-plugin</a> could be helpful as allows to start multiple java processes at <strong>pre-integration-test</strong> phase (as standard Spring Boot apps), and it automatically takes care of shutting them down in the <strong>post-integration-test</strong> phase. </p>&#xA;&#xA;<p><strong>NOTE:</strong> The integration test should be run at the <strong>integration-test</strong> phase for that the <strong>maven-failsafe-plugin</strong> should be configured with <strong>spring-boot-maven-plugin</strong> <a href=""http://docs.spring.io/spring-boot/docs/current-SNAPSHOT/maven-plugin/usage.html"" rel=""nofollow"">see</a>. &#xA;Then to run our integration tests <strong>verify</strong> or higher maven Lifecycle should be targeted, because the <strong>integration-test</strong> phase is in fact located between  <strong>package</strong> and <strong>verify</strong> Lifecycles <a href=""https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html"" rel=""nofollow"">see Default Lifecycles</a>.</p>&#xA;&#xA;<p>The following maven (pom.xml) configuration worked for me:</p>&#xA;&#xA;<pre><code>&lt;build&gt;&#xA;    &lt;plugins&gt;&#xA;        &lt;plugin&gt;&#xA;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&#xA;            &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;&#xA;            &lt;version&gt;1.3.5.RELEASE&lt;/version&gt;&#xA;            &lt;executions&gt;                    &#xA;                &lt;execution&gt;&#xA;                    &lt;id&gt;pre-integration-test&lt;/id&gt;&#xA;                    &lt;goals&gt;&#xA;                        &lt;goal&gt;start&lt;/goal&gt;&#xA;                    &lt;/goals&gt;&#xA;                    &lt;configuration&gt;&#xA;                        &lt;skip&gt;${integration-tests.skip}&lt;/skip&gt;&#xA;                    &lt;/configuration&gt;&#xA;                &lt;/execution&gt;&#xA;                &lt;execution&gt;&#xA;                    &lt;id&gt;post-integration-test&lt;/id&gt;&#xA;                    &lt;goals&gt;&#xA;                        &lt;goal&gt;stop&lt;/goal&gt;&#xA;                    &lt;/goals&gt;&#xA;                    &lt;configuration&gt;&#xA;                        &lt;skip&gt;${integration-tests.skip}&lt;/skip&gt;&#xA;                    &lt;/configuration&gt;&#xA;                &lt;/execution&gt;&#xA;            &lt;/executions&gt;&#xA;        &lt;/plugin&gt;           &#xA;&#xA;        &lt;plugin&gt;&#xA;            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;&#xA;            &lt;artifactId&gt;maven-failsafe-plugin&lt;/artifactId&gt;&#xA;            &lt;version&gt;2.19.1&lt;/version&gt;&#xA;            &lt;configuration&gt;&#xA;                &lt;skip&gt;${integration-tests.skip}&lt;/skip&gt;                  &#xA;                &lt;includes&gt;&#xA;                    &lt;include&gt;**/*IT.java&lt;/include&gt;&#xA;                &lt;/includes&gt;&#xA;            &lt;/configuration&gt;&#xA;            &lt;executions&gt;&#xA;                &lt;execution&gt;&#xA;                    &lt;goals&gt;&#xA;                        &lt;goal&gt;integration-test&lt;/goal&gt;&#xA;                        &lt;goal&gt;verify&lt;/goal&gt;&#xA;                    &lt;/goals&gt;                        &#xA;                &lt;/execution&gt;                    &#xA;            &lt;/executions&gt;&#xA;        &lt;/plugin&gt;&#xA;&#xA;        &lt;plugin&gt;&#xA;            &lt;groupId&gt;com.bazaarvoice.maven.plugins&lt;/groupId&gt;&#xA;            &lt;artifactId&gt;process-exec-maven-plugin&lt;/artifactId&gt;&#xA;            &lt;version&gt;0.7&lt;/version&gt;&#xA;            &lt;executions&gt;                    &#xA;                &lt;execution&gt;&#xA;                    &lt;id&gt;switchboard-process&lt;/id&gt;&#xA;                    &lt;phase&gt;pre-integration-test&lt;/phase&gt;&#xA;                    &lt;goals&gt;&#xA;                        &lt;goal&gt;start&lt;/goal&gt;&#xA;                    &lt;/goals&gt;&#xA;                    &lt;configuration&gt;&#xA;                        &lt;name&gt;accounts-service&lt;/name&gt;&#xA;                        &lt;workingDir&gt;/../../micro-service&lt;/workingDir&gt;&#xA;                        &lt;waitForInterrupt&gt;false&lt;/waitForInterrupt&gt;                          &#xA;                        &lt;arguments&gt;&#xA;                            &lt;argument&gt;java&lt;/argument&gt;&#xA;                            &lt;argument&gt;-jar&lt;/argument&gt;&#xA;                            &lt;argument&gt;${basedir}/../micro-service/target/micro-service-${project.version}-exec.jar&lt;/argument&gt;&#xA;                        &lt;/arguments&gt;&#xA;                    &lt;/configuration&gt;&#xA;                &lt;/execution&gt;&#xA;                &lt;!--Stop all processes in reverse order--&gt;&#xA;                &lt;execution&gt;&#xA;                    &lt;id&gt;stop-all&lt;/id&gt;&#xA;                    &lt;phase&gt;post-integration-test&lt;/phase&gt;&#xA;                    &lt;goals&gt;&#xA;                        &lt;goal&gt;stop-all&lt;/goal&gt;&#xA;                    &lt;/goals&gt;&#xA;                &lt;/execution&gt;&#xA;            &lt;/executions&gt;&#xA;        &lt;/plugin&gt;&#xA;    &lt;/plugins&gt;&#xA;&lt;/build&gt;&#xA;</code></pre>&#xA;&#xA;<p>Having an Integration Test class (<strong>WebServerIT</strong>) in <strong>test.java</strong> folder:</p>&#xA;&#xA;<pre><code>import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.status;&#xA;import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.content;&#xA;&#xA;@RunWith(SpringJUnit4ClassRunner.class)&#xA;@SpringApplicationConfiguration(classes = WebServerApp.class)&#xA;@WebIntegrationTest(""server.port:0"")&#xA;public class WebServerIT {&#xA;&#xA;    @Autowired&#xA;    private WebApplicationContext webServerAppContext;&#xA;&#xA;    private MockMvc webServerMockMvc;&#xA;&#xA;    @Before&#xA;    public void setUp() {&#xA;        System.out.println(""the test is set up"");&#xA;        webServerMockMvc = MockMvcBuilders.webAppContextSetup(webServerAppContext).build();&#xA;    }&#xA;&#xA;    /**&#xA;     * This test calls the WebServer's endpoint (/accounts/123456789) which in turn calls the micro-service rest api &#xA;     * which is started using the process-exec-maven-plugin, otherwise the test would fail.&#xA;     */&#xA;    @Test&#xA;    public void testWebServerInteractionWithMicroService() throws Exception {&#xA;        this.webServerMockMvc.perform(get(""/accounts/123456789""))&#xA;                .andExpect(status().isOk());&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
48479181,48473966,7337938,2018-01-27T18:35:45,"<p>One of the best approaches is the <strong>OAuth</strong> delegation protocol with JSON token <strong>JWT</strong>  </p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/hpHKm.png"" rel=""nofollow noreferrer"">Authentication in micro-services architecture</a></p>&#xA;&#xA;<ol>&#xA;<li>the user send his credentials to the OAuth server</li>&#xA;<li>The server Checks the user's information (from LDAP server for example), then gives him an access token </li>&#xA;<li>the user send his request with the access token to the API Gateway </li>&#xA;<li>the API Gateway extracts out the access_token from the request, then he will talks to the Token Exchange endpoint to validate it and then issues a JWT</li>&#xA;<li>this JWT That contains all the necessarily information about the user will be sent to the micro-service.</li>&#xA;<li>the micro-service also should verify the validity of the token by talking to the token exchange endpoint.</li>&#xA;<li>when the token is checked, the micro-service can start its job.</li>&#xA;</ol>&#xA;&#xA;<p>I think this link will be useful for you <a href=""https://medium.facilelogin.com/securing-microservices-with-oauth-2-0-jwt-and-xacml-d03770a9a838"" rel=""nofollow noreferrer"">Securing Microservices</a> </p>&#xA;"
50574522,50552293,784594,2018-05-28T23:34:35,"<p>There's a Payara Micro maven archetype available in Maven Central repository. Here's how to use it to generate a project: <a href=""https://github.com/payara/ecosystem-maven/tree/master/payara-micro-maven-archetype"" rel=""nofollow noreferrer"">https://github.com/payara/ecosystem-maven/tree/master/payara-micro-maven-archetype</a></p>&#xA;&#xA;<p>You just need to set <code>version.javaee</code> to <code>8.0</code>, <code>version.payara.micro</code> to <code>5.181</code> and also set version of the <code>payara-micro-maven-plugin</code> to 1.0.0, which is the last one available in Maven Central right now.</p>&#xA;&#xA;<p>However, the generated project is fairly simple - it only configures the payara-micro-maven-plugin and contains a simple html page. There's no other generator for Payara Micro yet. However any Java EE 8 project generator would work because you don't need any special configuration to build and run a microservice with Payara Micro. You can simply run the final WAR with Payara Micro from command line or you can generate an executable JAR. Payara Micro Maven plugin only makes it easier to do this from within a maven build but it's not required.</p>&#xA;"
43434704,43382402,1070291,2017-04-16T07:09:18,"<p>Opaque by-value tokens are really intended for the following purposes:</p>&#xA;&#xA;<ul>&#xA;<li>Reducing token size to a minimum (for client efficiency)</li>&#xA;<li>Hiding the details of what claims a user has as they dont need to know</li>&#xA;<li>Forcing a lookup of the claims on each request (so you can revoke/change tokens instantly)</li>&#xA;</ul>&#xA;&#xA;<p>You are on an internal network so payload size isnt as big of an issue and you probably don't care about leaking claims to other services. If your claims arent changing often then opaque tokens probably arent needed. This means that your service simply needs to request an maintain a by-value token to access internal resources.</p>&#xA;&#xA;<p>Thats not too bad. </p>&#xA;&#xA;<p>If you do need to convert by ref to by value on every request, or you want to simplify the auth loop for consumers a proxy approach is best. This would intercept requests to your service and replace the by-ref token (or perhaps apikey) with a by-value token. The advantage here would be that you have more detailed control over usage of by-value tokens, and your clients dont need to care about your internal security infrastructure. </p>&#xA;&#xA;<p>This approach adds more overhead in exchange for more control. Its also fine to call through this from your internal services.</p>&#xA;&#xA;<p>I wrote a bit about <a href=""http://blog.staticvoid.co.nz/2016/11/16/the_authentication_proxy"" rel=""nofollow noreferrer"">the authentication proxy</a> pattern on my blog.</p>&#xA;"
43434849,43418403,1070291,2017-04-16T07:30:50,"<p>If your microservice is publishing an interface you want to be especially careful about how you deal with changes. Sharing a communication library is a particularly dangerous way of coupling these two services togeather especially as it gives the illusion of consistancy between services.</p>&#xA;&#xA;<p>If you want to share an interface you have to do at least the following (IMO):</p>&#xA;&#xA;<ul>&#xA;<li>Version your microservice, <strong>and support all previous version of your transport payloads</strong>. (you want to do this anyway)</li>&#xA;<li>Ship the actual transport contracts as a versioned package (so that the consumer is not forced to use the latest contracts) I think in java you might use maven for this (but im not a java person)</li>&#xA;</ul>&#xA;&#xA;<p>This is really important as it means two things are possible</p>&#xA;&#xA;<ul>&#xA;<li>A service can change its interface at will without needing to worry about downstream code dependancies (assuming it follows versioning rules)</li>&#xA;<li>A consumer has the choice about when to update its communication wrappers, and is never overtly compelled to build and ship a new version in response to a change in one of its dependancies.</li>&#xA;</ul>&#xA;&#xA;<p>For some more info on versioning check out my blog. <a href=""http://blog.staticvoid.co.nz/2017/3/22/microservice_versioning;_how_to_make_breaking_changes_without_breaking_stuff"" rel=""nofollow noreferrer"">Microservice versioning; How to make breaking changes without breaking stuff</a></p>&#xA;"
43589761,29888108,1070291,2017-04-24T13:41:33,"<p>I know this is super old but I couldnt help but answer for the sake of currency.</p>&#xA;&#xA;<p>There are two ways of communicating an API contract with other .net services which I find particularly useful.</p>&#xA;&#xA;<ul>&#xA;<li>Ship a nuget package with the contracts (Interfaces describing responses) and possibly some call logic to methodise your api calls</li>&#xA;<li>Use swagger to describe your api (seems to have come out as the API description winner, <a href=""https://github.com/domaindrivendev/Swashbuckle"" rel=""nofollow noreferrer"">Swashbuckle</a> makes it seamless in .net) and then either hand code the bits you need at the caller or use a codegen</li>&#xA;</ul>&#xA;&#xA;<p>I quite often do both, swagger is also nice for documentation and other language compatibility, and its good practice to be formal about your contracts and backward compatibility.</p>&#xA;"
43590429,43559197,1070291,2017-04-24T14:12:18,"<p>Yes you will need to share a key in order for JWT to function securely/correctly.</p>&#xA;&#xA;<p>What I would recommend is using a public-private key signing method and pass by value JWT. This will then mean you get a private signing key which only your gateway needs to know and a public verification key.</p>&#xA;&#xA;<p>You can then distribute your verification key to all your microservices. This can either be something you do via deployment, or your microservices can use some kind of refresh cycle and publish your signing key along with the gateway. The former is more secure, the later better at self healing.</p>&#xA;&#xA;<p>This might be useful: <a href=""https://mkjwk.org/"" rel=""nofollow noreferrer"">JWK</a>.</p>&#xA;"
44360596,44355294,1070291,2017-06-05T01:38:32,"<p>I have no idea how spotify does this internally.</p>&#xA;&#xA;<p>Just at a guess though from the way he was talking about it, I'm not sure these microservices were storing any data at all. I got the feeling that they were essentially a presentation layer on top of something else (perhaps an internal service layer).</p>&#xA;&#xA;<p>If a microservice can have multiple versions active, and it is also the owner of some data then one of two things are probably happening:</p>&#xA;&#xA;<ul>&#xA;<li>Schema consistancy is applied at the service level, if a change is made to the schema it must be updated in all historic versions. In the case of multiple versions being deployed independantly this would nessecitate a deploy of all versions on a schema change (he didnt talk like this was the case, but this is how I've always done versioning)</li>&#xA;<li>Each version owns an independant copy of the data. To do this you really need to have a pub/sub model in the system and perform all modifications as a result of some kind of message. (to me this seems pretty inefficient unless its very low churn, but perhaps at that scale it might make sense, there is also a problem with how to prime a new version with data)</li>&#xA;</ul>&#xA;&#xA;<p>I dont think running a schema and never changing it would really work long term. Things change and changes are nessacary. One of the biggest benifits of microservies/SOA (IMO) is that change is easier as the domains are small and containted. You can do things like completely change the storage mechanism (perhaps even to a new type of storage) reasonably safely and quickly as the volume of code is low.</p>&#xA;&#xA;<p>More thoughts on microservice versioning in my article, <a href=""http://blog.staticvoid.co.nz/2017/microservice_versioning;_how_to_make_breaking_changes_without_breaking_stuff/"" rel=""nofollow noreferrer"">Microservice versioning; How to make breaking changes without breaking stuff</a></p>&#xA;"
45358216,45311236,614355,2017-07-27T18:14:52,"<p>Typically microservices talk to each other. Thats the whole point. With Eureka discovery in place you simply call the microservice by name instead of the FQDN which we normally would use without microservice.</p>&#xA;&#xA;<p>For e.g. your <code>book-service</code> will call the <code>author-service</code> like this&#xA;<code>http://author-service/authors</code></p>&#xA;&#xA;<p>full example here <a href=""https://spring.io/blog/2015/01/20/microservice-registration-and-discovery-with-spring-cloud-and-netflix-s-eureka"" rel=""nofollow noreferrer"">https://spring.io/blog/2015/01/20/microservice-registration-and-discovery-with-spring-cloud-and-netflix-s-eureka</a></p>&#xA;&#xA;<p>Please don't forget that JHipster is an opinionated framework based off of Spring Cloud so you can find most of this stuff by searching Spring docs.</p>&#xA;"
45542189,45526675,2798291,2017-08-07T08:11:09,"<p>The reason to use a non-blocking HTTP-Client is to prevent too much CPU from being used on thread-switching. If you already solve that problem by limiting the amount of background threads, then non-blocking IO won't provide any noticeable benefits.</p>&#xA;&#xA;<p>There is another problem with your setup: it is very vulnerable to DDOS attacks (intentional or accidental ones). If a someone calls your service very often, it will internally create a huge work-load that will keep the service busy for a long time. You will definitely need to limit the background task queue (which is a supported feature of the Executor class) and return 503 (or equivalent) if there are too many pending tasks.</p>&#xA;"
43168867,43167750,2277331,2017-04-02T13:26:04,"<p>You can hold an <a href=""https://en.wikipedia.org/wiki/AVL_tree"" rel=""nofollow noreferrer"">AVL tree</a>, the insertion and deletions operation taken O(logn) time on such data structure. each time you need to update a player: remove from tree, change score, insert to tree. </p>&#xA;&#xA;<p>This is the exact trade-off you are looking for, all the operations takes O(logn) and since you need all the operations (lookup and update - delete\insert) this is the best match for you. The memory consumption is O(n) btw.</p>&#xA;"
33758450,33726653,1238739,2015-11-17T13:42:04,"<p>I have been investigating this topic a lot as well (to be applied to my work for <a href=""http://particular.net/NServiceBus"" rel=""noreferrer"">NServiceBus</a> and <a href=""http://www.messagehandler.net/"" rel=""noreferrer"">MessageHandler</a>) and would like to provide my thoughts on the matter. However I haven't determined what the best model is yet.</p>&#xA;&#xA;<p>If you disregard the practical implementation with ServiceFabric I would categorize the proposed approach in the following order when it comes to reliability:</p>&#xA;&#xA;<ul>&#xA;<li>C) The store and forward model is probably the best of the 3 models when it comes to interservice communication, all services can work independently from each other and are in no way subject to networking outages (at the downside of added latency)</li>&#xA;<li>A) Input queue per service: Each service free from impact by network outages for it's own work. However when it wishes to send messages to another service it may be impacted by network outages and needs retry built in to accomodate for this.</li>&#xA;<li>B) Output queue per service: Is probably the least of the 3 models as each service is directly dependent on a resource of the others, this results in to much dependency on network availability between the nodes.</li>&#xA;</ul>&#xA;&#xA;<p>If you look at it from a simplicity point of view, I would categorize them the following way</p>&#xA;&#xA;<ul>&#xA;<li>A) Input queue per service: As the message source needs to actively route messages to a given destination queue, it is fairly simpel to implement business processes or workflows (what I assume your pipeline is going to represent) using a routing pattern (either static routing or dynamic f.e. using a routing slip pattern</li>&#xA;<li>C) Store and forward: Again routing is an explicit part of your implementation, so both static and dynamic routing patterns are possible, however the practical implemenation is harder as you need to build and manage a messagepump that transfers messages from the transfer queue (output) to the destination queue and the associated need to flow context from the message source into the message pump. (Shameless plug: NServiceBus is a framework that can take away the complexity for you and make this scenario as simple as A)</li>&#xA;<li>B) Output queue per service: Each service needs to be setup to explicitly read from another's queue, this approach would only allow static routing as the routing rules are embedded in where you read from only (this severely limit you from a functional perspective)</li>&#xA;</ul>&#xA;&#xA;<p>If we take ServiceFabric's implementation details into account, then I assume you want to make use of the IReliableQueue implementation? This implementation has some shortcomings though, that make me wonder if these patterns can actually be implemented properly on ServiceFabric's native storage infrastructure.</p>&#xA;&#xA;<ol>&#xA;<li>The storage infrastructure is only available on Statefull services, so Stateless services (like Rest API's or other protocol termination gateway's) cannot be part of the pipeline (usually you want one of these as an entry point)</li>&#xA;<li>Only 1 thread can access a reliable queue at the same time, so it is impossible to write and read from the same queue at the same time. This severely limits throughput of the queue.</li>&#xA;<li>Accessing a reliable queue requires a local transaction, but these transactions are limited to a single partition. So it's also impossible to scale out your statefull services to create a competing consumer pattern.</li>&#xA;</ol>&#xA;&#xA;<p>Given these shortcomings, I'm still inclined to use another type of queueing infrastructure for SF Services instead of SF's persistence model, for example Azure Service Bus or Azure Storage Queues (Which NserviceBus allows as well).</p>&#xA;&#xA;<p>In short, I'll support both A and C, with a slight preference for C, but I'm not convinced about using reliable queues as an implementation until these shortcomings have been resolved.</p>&#xA;"
46263488,46263443,1838804,2017-09-17T11:17:25,"<p>there are many solution that can help you in this problem , redis have good integration with spring and you can easily use it for sloving your problem , you can read more about it in this <a href=""http://caseyscarborough.com/blog/2014/12/18/caching-data-in-spring-using-redis/"" rel=""nofollow noreferrer"">link</a></p>&#xA;&#xA;<p>also there solutions likes <a href=""http://www.hazelcast.com"" rel=""nofollow noreferrer"">Hazelcast</a> , it's a datagrid and more than a simple key-value cache. </p>&#xA;"
41832706,41832273,4478420,2017-01-24T16:01:02,"<p>For me the usage graphs worked once I installed <a href=""https://github.com/kubernetes/heapster"" rel=""nofollow noreferrer"">heapster</a> as an addon. Heapster requires an influxdb as data sink for the metric storage. Luckily you can deploy all those easily in k8s with the following definitions in the <code>kube-system</code> namespace (tested it with k8s <a href=""https://github.com/kubernetes/kubernetes/releases/tag/v1.4.6"" rel=""nofollow noreferrer"">1.4.6</a>): </p>&#xA;&#xA;<p><strong>heapster-service.yml:</strong></p>&#xA;&#xA;<pre><code>apiVersion: v1&#xA;kind: Service&#xA;metadata:&#xA;  labels:&#xA;    task: monitoring&#xA;    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)&#xA;    # If you are NOT using this as an addon, you should comment out this line.&#xA;    kubernetes.io/cluster-service: 'true'&#xA;    kubernetes.io/name: Heapster&#xA;  name: heapster&#xA;  namespace: kube-system&#xA;spec:&#xA;  ports:&#xA;  - port: 80&#xA;    targetPort: 8082&#xA;  selector:&#xA;    k8s-app: heapster&#xA;</code></pre>&#xA;&#xA;<p><strong>heapster-deployment.yml:</strong></p>&#xA;&#xA;<pre><code>apiVersion: extensions/v1beta1&#xA;kind: Deployment&#xA;metadata:&#xA;  name: heapster&#xA;  namespace: kube-system&#xA;spec:&#xA;  replicas: 1&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        task: monitoring&#xA;        k8s-app: heapster&#xA;        version: v6&#xA;    spec:&#xA;      containers:&#xA;      - name: heapster&#xA;        image: kubernetes/heapster:canary&#xA;        imagePullPolicy: Always&#xA;        command:&#xA;        - /heapster&#xA;        - --source=kubernetes:https://kubernetes.default&#xA;        - --sink=influxdb:http://monitoring-influxdb:8086&#xA;</code></pre>&#xA;&#xA;<p><strong>influxdb-service.yml:</strong></p>&#xA;&#xA;<pre><code>apiVersion: v1&#xA;kind: Service&#xA;metadata:&#xA;  labels:&#xA;    task: monitoring&#xA;    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)&#xA;    # If you are NOT using this as an addon, you should comment out this line.&#xA;    kubernetes.io/cluster-service: 'true'&#xA;    kubernetes.io/name: monitoring-influxdb&#xA;  name: monitoring-influxdb&#xA;  namespace: kube-system&#xA;spec:&#xA;  # type: NodePort&#xA;  ports:&#xA;  - name: api &#xA;    port: 8086&#xA;    targetPort: 8086&#xA;  selector:&#xA;    k8s-app: influxdb&#xA;</code></pre>&#xA;&#xA;<p><strong>infuxdb-deployment.yml:</strong></p>&#xA;&#xA;<pre><code>apiVersion: extensions/v1beta1&#xA;kind: Deployment&#xA;metadata:&#xA;  name: monitoring-influxdb&#xA;  namespace: kube-system&#xA;spec:&#xA;  replicas: 1&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        task: monitoring&#xA;        k8s-app: influxdb&#xA;    spec:&#xA;      volumes:&#xA;      - name: influxdb-storage&#xA;        emptyDir: {}&#xA;      containers:&#xA;      - name: influxdb&#xA;        image: kubernetes/heapster_influxdb:v0.6&#xA;        resources:&#xA;          requests:&#xA;            memory: ""256M""&#xA;            cpu: ""0.1""&#xA;          limits:&#xA;            memory: ""1G""&#xA;            cpu: ""1.0""&#xA;        volumeMounts:&#xA;        - mountPath: /data&#xA;          name: influxdb-storage&#xA;</code></pre>&#xA;"
42323669,36144330,1812436,2017-02-19T04:34:35,"<p>Investigate Kubernetes and/or Mesos, and you'll never look back. They're tailor-made for what you're looking to do. The two components you should focus on are:</p>&#xA;&#xA;<ol>&#xA;<li><p><strong>Service Discovery:</strong> This allows inter-dependent services (micro-service ""A"" calls ""B"") to ""find"" each other. It's typically done using DNS, but with registration features on top of it that handle what happens as instances are scaled.</p></li>&#xA;<li><p><strong>Scheduling:</strong> In Docker-land, scheduling isn't about CRON jobs, it means how containers are scaled and ""packed"" into servers in various ways to maximize efficient usage of available resources.</p></li>&#xA;</ol>&#xA;&#xA;<p>There are actually dozens of options here: Docker Swarm, Rancher, etc. are also competing alternatives. Many cloud vendors like Amazon also offer dedicated services (such as ECS) with these features. But Kubernetes and Mesos are emerging as standard choices, so you'd be in good company if you at least start there.</p>&#xA;"
49836561,49836268,6337748,2018-04-14T21:56:41,"<p>There are multiple solutions available for this problem. One of the solution is <a href=""https://dzone.com/articles/security-in-microservices"" rel=""noreferrer"">API Gateway pattern</a>.</p>&#xA;&#xA;<ol>&#xA;<li>First request goes to API gateway</li>&#xA;<li>API Gateway authenticates &amp; authroizes the request </li>&#xA;<li>Authentication is stored on cache database such as Redis, Memcache etc with expiry time on it</li>&#xA;<li>Saved access token is returned to client</li>&#xA;<li>Client can use the saved access token in the subsequent calls for the some time span (i.e. until the token is valid) </li>&#xA;<li>Once the token is expired, the API gateway will authenticate and share the new token to client</li>&#xA;<li>This solution will reduce the need to authenticate each request and improves the performance</li>&#xA;</ol>&#xA;&#xA;<p>API Gateway is the single entry point for all the services. So, you may not need separate cache for each service.</p>&#xA;&#xA;<p>Refer the diagram in this <a href=""http://microservices.io/patterns/apigateway.html"" rel=""noreferrer"">page</a>.</p>&#xA;"
41824988,41824300,7457236,2017-01-24T09:51:52,"<p>You want to have a server that listens to the incoming API calls, e.g. a HttpListener.</p>&#xA;&#xA;<p>Inside the handler of incoming requests, you peek into the request and decide where the API call needs to be relayed.</p>&#xA;&#xA;<p>Then you use something like a HttpClient to make another request to the actual API endpoint (mimicking the original request as closely as possible) and you relay its response back to the user.</p>&#xA;&#xA;<p>It should all be in the listener's request handler, and the response to the original request is the response from the real API.</p>&#xA;&#xA;<p>See MSDN docs on HttpListener.</p>&#xA;&#xA;<p>Also a good read: <a href=""https://stackoverflow.com/questions/9034721/handling-multiple-requests-with-c-sharp-httplistener"">Handling multiple requests with C# HttpListener</a></p>&#xA;"
48463220,48437842,5495508,2018-01-26T14:21:28,"<p>Your instinct is correct - Rest-Assured and mountebank solve different problems. In fact, both were used in conjunction on the first project to use mountebank.</p>&#xA;&#xA;<p>Rest-Assured provides a nice domain specific language for testing RESTful services. It's basically a wrapper over HTTP calls and JSON parsing that simplifies your test workflow for you when you want to test a REST service over the wire.</p>&#xA;&#xA;<p>In a microservices architecture, that REST service you're testing likely makes call to other downstream services, which can introduce non-determinism into your test suite. That's where mountebank comes in. It can provide a test double for that downstream service, allowing your first set of behavioral tests to be deterministic. It's effectively stubbing over the wire.</p>&#xA;&#xA;<p>The two tools can work nicely together: Rest-Assured calls your REST service, and mountebank stubs its downstream dependencies.</p>&#xA;"
47249382,47202936,832725,2017-11-12T13:30:35,"<p>With friends a put together a site: <a href=""https://trunkbaseddevelopment.com"" rel=""nofollow noreferrer"">https://trunkbaseddevelopment.com</a> that has a lot of the alternate choices, but you were after case studies. Here's one - <a href=""https://devops.paulhammant.com/bofa-platform-case-study/"" rel=""nofollow noreferrer"">https://devops.paulhammant.com/bofa-platform-case-study/</a>. That was ten years ago, and a bunch of my work is explaining that to corporates, and which steps to take first on a multi-month series of steps to CD.</p>&#xA;"
42962337,38863716,442837,2017-03-22T20:45:06,<p>We use something like </p>&#xA;&#xA;<pre><code>api/suggestion/:id PUT     // for updating the entire resource&#xA;api/suggestion/:id/views   //for updating a portion of the resource&#xA;</code></pre>&#xA;&#xA;<p>1) the services map requests to commands.  the command can simply contain a subset of the fields in the entity.&#xA;2) and 3) you need to read the docs and CQRS design.  The architecture is foremost for dealing with highly concurrent updates to mutable state</p>&#xA;
50422104,36129008,4441045,2018-05-19T05:29:39,"<p>Looks like the 'web' related functionality is now moved into module 'seneca-web' along with separate adapter for express. I got the below modified version to work.</p>&#xA;&#xA;<pre><code>""use strict"";&#xA;&#xA;const express = require('express');&#xA;const app = express();&#xA;const seneca = require('seneca')({ log: 'silent' });&#xA;const web = require('seneca-web');&#xA;&#xA;let routes = [{&#xA;  prefix: '/api',&#xA;  pin: 'role:api,cmd:*',&#xA;  map: {&#xA;    getData: {&#xA;      GET: true&#xA;    }&#xA;  }&#xA;}];&#xA;&#xA;let config = {&#xA;  context: app,&#xA;  routes: routes,&#xA;  adapter: require('seneca-web-adapter-express')&#xA;};&#xA;&#xA;seneca.add('role:api,cmd:getData', getData);&#xA;seneca.use(web, config);&#xA;&#xA;function getData(arg, done){&#xA;    done(null, {foo: 'bar'});&#xA;}&#xA;&#xA;seneca.ready(() =&gt; {&#xA;  app.listen(3002, () =&gt; {&#xA;    console.log('listening on port 3002');&#xA;  });&#xA;});&#xA;</code></pre>&#xA;"
41750133,41749456,7138517,2017-01-19T19:30:30,"<p>There's plenty of stuff available for Ruby as well, to give you a couple of examples there are</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://github.com/ondra-m/ruby-spark"" rel=""nofollow noreferrer"">Ruby-Spark</a> for Apache Spark,</li>&#xA;<li><a href=""https://github.com/josephmisiti/awesome-machine-learning#ruby"" rel=""nofollow noreferrer"">Big list of machine learning libraries for Ruby</a></li>&#xA;<li>I can recommend <a href=""https://github.com/jekyll/classifier-reborn"" rel=""nofollow noreferrer"">Classifier-Reborn</a> for text classification</li>&#xA;</ul>&#xA;&#xA;<p>And others.</p>&#xA;&#xA;<p>As to calling other languages from Ruby, that's also doable but especially if you need to do it on every request it can be slow. Using Python directly for your entire software might be a better option especially if good performance is required.</p>&#xA;"
34421297,33315965,5705686,2015-12-22T17:24:33,"<p>You will likely want to use batching to reduce the number downstream calls.  Instead of sending a single user through the observable, you will want to send the batch.</p>&#xA;"
50036590,49922136,2366166,2018-04-26T06:53:55,"<p>If someone comes across to this situation, what happen to me is when i build the new package with release mode, the new dll file was not getting attached. So when i check the .nupkg(change its file type to zip, extract and see) file, that .dll file was missing. That was the cause. So i tried once again and it double checked and then pushed to nuget. and tried to restore the package in project.json file by changing the version number, and the problem got resolved. </p>&#xA;&#xA;<p>@LexLi this is the solution which i was looking for.</p>&#xA;"
42849284,42846703,4614870,2017-03-17T04:33:39,"<p>Yes, if I read correctly, you are describing microservice architecture. You need to read up on this. There is no ""right"" or ""wrong"" way to do this; it is highly dependent upon your situation. The first link references all of the technologies you are interested in.</p>&#xA;&#xA;<ul>&#xA;<li><a href=""http://callistaenterprise.se/blogg/teknik/2015/04/10/building-microservices-with-spring-cloud-and-netflix-oss-part-1/"" rel=""nofollow noreferrer"">http://callistaenterprise.se/blogg/teknik/2015/04/10/building-microservices-with-spring-cloud-and-netflix-oss-part-1/</a></li>&#xA;<li><a href=""https://spring.io/blog/2015/07/14/microservices-with-spring"" rel=""nofollow noreferrer"">https://spring.io/blog/2015/07/14/microservices-with-spring</a></li>&#xA;<li><a href=""http://microservices.io/"" rel=""nofollow noreferrer"">http://microservices.io/</a></li>&#xA;</ul>&#xA;"
46968837,46949073,159446,2017-10-27T06:34:08,"<blockquote>&#xA;  <p>I am working on a monolith system... My goal is to set a new path how project should evolve. At first I was wondering if I could move towards microservices based architecture.</p>&#xA;</blockquote>&#xA;&#xA;<p>In what ways do you need to evolve the project? Will it be mostly bugfixes, adding features, improving performance and/or scalability? Do you anticipate other developers collaborating in the future? Are you currently having maintenance issues? The answers to these questions (and many more) should be considered in guiding your choices.</p>&#xA;&#xA;<p>You seem to be doing your homework around the pros and cons of a microservice architecture, so if you haven't asked yourself why you're even doing this in the first place, now would be good time to do so.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Maybe there are other approaches I should consider?</p>&#xA;</blockquote>&#xA;&#xA;<p>There's always the good old don't-break-what's-going ;)</p>&#xA;"
51568933,51558885,2459094,2018-07-28T07:00:48,"<p>Your initial idea was the one I would go with.  Having 150 different workflows with 10K tasks each leads to a fully dynamic and unmanageable scenario.  On the one hand you say that each task is just a simple gRPC but at the same time you mention that the page-level tasks are really complex to encapsulate behind a single task and there are external dependencies that may cause flow bottlenecks measured in hours.</p>&#xA;&#xA;<p>If I were you I'd redesign the solution and transfer the page-level reporting to a different layer.  For example creating a service that would do all these complex calculations would be a better option than trying to implement this in Airflow.  This way you could probably cut down the number of page level tasks significantly.  </p>&#xA;&#xA;<p>Regarding your specific questions:</p>&#xA;&#xA;<ul>&#xA;<li>Airflow is case agnostic - every scenario can be perfect depending on the &#xA;design. Oozie is really old-school and cumbersome and lacks the plethora of &#xA;integration features that Airfow offers.  Luigi I haven't used.</li>&#xA;<li>As mentioned earlier, this approach is unpredictable and unmanageable at the same time.  I foresee mayhem :)</li>&#xA;<li>Getting a hanging UI is a great indicator that you should revisit your implementation design.  But the UI should be your #1 concern - how can you monitor and manage 10,000 tasks in a single workflow?  Correct - you can't.  And multiply that by 150.</li>&#xA;<li>I read an article a while ago from a company where they experienced issues by scaling out using Celery and they decided to scale up instead and run many scheduler processes in parallel on the same VM.  Not quite sure if this is a setup that would significantly benefit your scenario.</li>&#xA;</ul>&#xA;&#xA;<p>If I were you I'd have a single workflow for all 150 sites.  I'd create a subdag for each website (btw there is no mention of the word 'unstable' in the <a href=""https://airflow.apache.org/concepts.html#subdags"" rel=""nofollow noreferrer"">official docs</a>) and try to offload complex calculation operations to a different layer in order to cut down on the number of page level tasks as much as possible.</p>&#xA;"
46975976,46975781,563158,2017-10-27T13:15:59,"<p>There are several layers that you need to pass.</p>&#xA;&#xA;<p>First, you need to containerize each microservice into its own container image. Usually you will use Docker for that.</p>&#xA;&#xA;<p>Each app will have a separate Dockerfile which you will use to create docker images which you will push to docker image registries which will be pulled by whoever wants to run your app.</p>&#xA;&#xA;<p>As for the codebase organisation, you can have one repository with all your microservices, but you will need multiple Dockerfiles to create images for all of them.</p>&#xA;&#xA;<p>Then you need to orchestrate your containers created from your images (run them). If you want your app to run on one host you may get away with Docker compose, which lets you define which containers are run in what order in single yaml manifest file. Docs are here: <a href=""https://docs.docker.com/compose/"" rel=""nofollow noreferrer"">https://docs.docker.com/compose/</a></p>&#xA;&#xA;<p>If you want to run your app in a kubernetes cluster, you will want to create a k8s Deployment (<a href=""https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</a>) which will run your containers as pods on your cluster nodes.</p>&#xA;"
40180357,39215533,1284054,2016-10-21T15:24:41,"<p>You can set a comma-separated list of configurations you want to load using <code>spring.cloud.config.name</code> property:</p>&#xA;&#xA;<pre><code>spring.cloud.config.name: jmsclient,productclient,orderclient&#xA;</code></pre>&#xA;"
45611769,30800908,5810390,2017-08-10T10:55:21,"<p>I found implementing my client in Dropwizard a little bit challenging as well. So I would like to contribute just in case anyone encounters this problem.&#xA;This is a client in Dropwizard (v1.0.5) that invokes a POST web service using Multipart. The client is accessed via web service as well using GET.</p>&#xA;&#xA;<p>Dependencies in my pom.xml:</p>&#xA;&#xA;<pre><code>&lt;dependencies&gt;&#xA;&lt;dependency&gt;&#xA;    &lt;groupId&gt;io.dropwizard&lt;/groupId&gt;&#xA;    &lt;artifactId&gt;dropwizard-core&lt;/artifactId&gt;&#xA;    &lt;version&gt;${dropwizard.version}&lt;/version&gt;&#xA;&lt;/dependency&gt;&#xA;&lt;dependency&gt;&#xA;    &lt;groupId&gt;io.dropwizard&lt;/groupId&gt;&#xA;    &lt;artifactId&gt;dropwizard-assets&lt;/artifactId&gt;&#xA;    &lt;version&gt;${dropwizard.version}&lt;/version&gt;&#xA;&lt;/dependency&gt;&#xA;&lt;dependency&gt;&#xA;    &lt;groupId&gt;io.dropwizard&lt;/groupId&gt;&#xA;    &lt;artifactId&gt;dropwizard-forms&lt;/artifactId&gt;&#xA;    &lt;version&gt;${dropwizard.version}&lt;/version&gt;&#xA;&lt;/dependency&gt;&#xA;&lt;dependency&gt;&#xA;    &lt;groupId&gt;io.dropwizard&lt;/groupId&gt;&#xA;    &lt;artifactId&gt;dropwizard-client&lt;/artifactId&gt;&#xA;    &lt;version&gt;${dropwizard.version}&lt;/version&gt;&#xA;&lt;/dependency&gt;&#xA;&#xA;&lt;/dependencies&gt;&#xA;</code></pre>&#xA;&#xA;<p>Here my Dropwizard Application (Client2PostApplication.java):</p>&#xA;&#xA;<pre><code>    public class Client2PostApplication extends Application&lt;Client2PostConfiguration&gt; {&#xA;    public static void main(String[] args) throws Exception {&#xA;        new Client2PostApplication().run(args);&#xA;    }&#xA;&#xA;    @Override&#xA;    public void initialize(Bootstrap&lt;Client2PostConfiguration&gt; bootstrap) {&#xA;        bootstrap.addBundle(new MultiPartBundle());&#xA;    }&#xA;&#xA;    @Override&#xA;    public void run(Client2PostConfiguration configuration,&#xA;                Environment environment) throws Exception {&#xA;&#xA;        environment.jersey().register(MultiPartFeature.class);&#xA;        JerseyClientConfiguration conf = configuration.getJerseyClientConfiguration();&#xA;&#xA;        conf.setChunkedEncodingEnabled(false);&#xA;&#xA;        final Client client = new JerseyClientBuilder(environment).using(conf).build(getName());&#xA;        environment.jersey().register(new Client2Post(client));&#xA;        environment.jersey().register(new MyPostResource());       &#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Here my Configuration (Client2PostConfiguration.java):</p>&#xA;&#xA;<pre><code>public class Client2PostConfiguration extends Configuration {&#xA;&#xA;    @Valid&#xA;    @NotNull&#xA;    private JerseyClientConfiguration jerseyClient = new JerseyClientConfiguration();&#xA;&#xA;    @JsonProperty(""jerseyClient"")&#xA;    public JerseyClientConfiguration getJerseyClientConfiguration() {&#xA;        return jerseyClient;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>And now, the post web service (MyPostResource.java):</p>&#xA;&#xA;<pre><code>@Path(""/testpost"")&#xA;&#xA;public class MyPostResource {&#xA;&#xA;    public MyPostResource() {&#xA;&#xA;    }&#xA;&#xA;    @POST&#xA;    @Consumes(MediaType.MULTIPART_FORM_DATA)&#xA;    @Timed&#xA;    public String test(&#xA;        @FormDataParam(""foo"") String testData) throws IOException {&#xA;        return testData;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>And finally, the client (Client2Post.java):</p>&#xA;&#xA;<pre><code>@Produces(MediaType.TEXT_PLAIN)&#xA;@Path(""/client"")&#xA;public class Client2Post {&#xA;&#xA;    private Client client;&#xA;&#xA;    public Client2Post(Client client) {&#xA;        this.client = client;&#xA;    }&#xA;&#xA;    @GET&#xA;    @Path(""/test"")&#xA;    public String testPost() {&#xA;&#xA;    final Invocation.Builder request = client.target(""http://localhost:8080/testpost"").register(MultiPartFeature.class).request();&#xA;&#xA;    final FormDataMultiPart entity = new FormDataMultiPart()&#xA;            .field(""foo"", ""bar"");&#xA;&#xA;    final String response = request.post(Entity.entity(entity, entity.getMediaType()), String.class);&#xA;&#xA;    return response;&#xA;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>The complete source code can be downloaded from <a href=""https://github.com/esparig/dwclient2post"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;"
45472930,45426194,240342,2017-08-03T00:23:11,"<p>You can use <a href=""http://json-schema.org/"" rel=""nofollow noreferrer"">JSON Schema</a>.&#xA;There is more explanatory info <a href=""https://spacetelescope.github.io/understanding-json-schema/"" rel=""nofollow noreferrer"">available</a> also.</p>&#xA;"
42563014,41893494,7582422,2017-03-02T18:21:35,"<p>They do fundamentally different things. Docker's swarm mode routing mesh is at layer 3/4 --- if you have something running on port X in one container, you can have every container listen on port X and route that traffic to the container that's actually using it.</p>&#xA;&#xA;<p>Linkerd's service mesh operates at layer 5/7. It does request-level failure and latency handling, load balancing across instances, and logical routing (""service a"" => ""datacenter 1, prod cluster, version 1.2 of service a"")</p>&#xA;&#xA;<p>You can use the two in conjunction.</p>&#xA;"
45977929,45928269,248534,2017-08-31T09:26:44,"<p>Consul is another service, and I wouldn't deploy it inside the container of my microservice. In a large-scale scenario, I'd deploy several Consul containers: some would run the agent under Server mode (think of them as Masters), and some would run it under Client mode (think of them as Slaves).</p>&#xA;&#xA;<p>I wouldn't deploy the agents running under client mode as part of my application's containers, because:</p>&#xA;&#xA;<ol>&#xA;<li>Isolating them means that they are stopped individually. Putting them together means that whenever I'd stop my application's container due to a version upgrade or a failure, I'd be needlessly stopping the Consul agent running therein. Same goes the other way around: stopping the Consul agent would stop my running application. This unneeded coupling isn't beneficial.</li>&#xA;<li>Isolating them means they can be scaled separately. I may need to scale my microservice and deploy more instances of it. If the container also contains Consul client agents, then scaling my microservice would end up scaling Consul as well. Or the other way around: I may need to scale Consul without scaling my microservice.</li>&#xA;<li>Isolating them is easier in terms of Docker container images. I can keep using the official Consul image and upgrade without much hassle. Putting Consul and my microservice together would mean that upgrading Consul requires me to modify the container image by myself.</li>&#xA;</ol>&#xA;"
48374296,44203398,2741462,2018-01-22T03:25:23,"<p>Need to increase the ""GB MEMORY PER INSTANCE"". The default value is too low for some cases.</p>&#xA;&#xA;<p>For Example: <code>manifest.yml</code></p>&#xA;&#xA;<pre><code>applications:&#xA;- name: uaa-service&#xA;  memory: 2048M&#xA;  ....&#xA;</code></pre>&#xA;"
48778432,48763985,8645590,2018-02-14T01:43:17,"<blockquote>&#xA;  <p>Can I choose this EC2 + Docker for deployment of my microservice for actual production environment?</p>&#xA;</blockquote>&#xA;&#xA;<p>Yes, this is totally possible, although I suggest using <strong>kubernetes</strong> as the container-orchestrator as it manages the lifecycle of the containers for you:</p>&#xA;&#xA;<ol>&#xA;<li><a href=""https://kubernetes.io/docs/getting-started-guides/aws/"" rel=""nofollow noreferrer"">Running Kubernetes on AWS EC2</a></li>&#xA;<li><a href=""https://aws.amazon.com/blogs/aws/amazon-elastic-container-service-for-kubernetes/"" rel=""nofollow noreferrer"">Amazon Elastic Container Service for Kubernetes</a></li>&#xA;<li><a href=""https://aws.amazon.com/blogs/compute/kubernetes-clusters-aws-kops/"" rel=""nofollow noreferrer"">Manage Kubernetes Clusters on AWS Using Kops</a></li>&#xA;<li><a href=""https://aws.amazon.com/eks/"" rel=""nofollow noreferrer"">Amazon EKS</a></li>&#xA;</ol>&#xA;"
46957478,46949073,4128470,2017-10-26T14:51:00,"<p>Based on my experience with working in Microservices for last few years, it seems like an overkill in current scenario but pays off in long-term.</p>&#xA;&#xA;<p>Based on the information stated above, my thoughts are:</p>&#xA;&#xA;<ul>&#xA;<li><strong>Code Structure -</strong> Microservices Architecture (MSA) applying in above context means not separating DAO, Business Logic etc. rather is more on the designing system as per business functions. For example, if it is an eCommerce application, then you can shipping, cart, search as separate services, which can further be divided into smaller services. Read it more about domain-driven design <a href=""https://en.wikipedia.org/wiki/Domain-driven_design"" rel=""nofollow noreferrer"">here</a>.</li>&#xA;<li><strong>Deployment Unit -</strong> Keeping microservices apps as an independent deployment unit is a key principle. Hence, keep a vertical slice of the application and package them as Docker Image with Application Code, App Server (if any), Database and OS (Linux etc.)</li>&#xA;<li><p><strong>Communication -</strong> With MSA, communication between services become a key and hence general practice is to remain with the message-oriented approach for communication (read about the <a href=""https://www.oreilly.com/ideas/reactive-programming-vs-reactive-systems"" rel=""nofollow noreferrer"">reactive system and reactive programming</a> for more insight).</p></li>&#xA;<li><p><strong>PaaS Solution -</strong> There are multiple PaaS solutions available, which you can apply so that you don't need to worry about all the other aspects like container management, container orchestration, auto-scaling, configuration management, log management and monitoring etc. See following PaaS solutions: </p></li>&#xA;<li><p><a href=""https://www.nanoscale.io/"" rel=""nofollow noreferrer"">https://www.nanoscale.io/</a> by TIBCO</p></li>&#xA;<li><p><a href=""https://fabric8.io/"" rel=""nofollow noreferrer"">https://fabric8.io/</a> - by RedHat</p></li>&#xA;<li><p><a href=""https://openshift.io"" rel=""nofollow noreferrer"">https://openshift.io</a> - by RedHat</p></li>&#xA;<li><p><strong>Cloud Vendor Platforms -</strong> AWS, Azure &amp; Google Cloud all of them have specific support for Microservices App from the deployment perspective, which we can use as an alternative solution if you don't want to deploy PaaS solution in your organization.</p></li>&#xA;</ul>&#xA;&#xA;<p>Hope these pointers will have in understanding the overall landscape so that you can structure your architecture for future need.</p>&#xA;"
46443292,46432262,4128470,2017-09-27T08:44:15,"<p>Firstly, Microservices are fine-grained services and as per design, they should be stateless. This helps them to scale as processes without any overhead of statefulness.&#xA;My recommendations are:</p>&#xA;&#xA;<ul>&#xA;<li>Leverage reactive microservices approach to use events (in the form of messages) as means of communications between Product &amp; Checkout Service. For example - an item added to cart will generate an event and Checkout service can process that event by listening to the queue. Read more about reactive microservices here: <a href=""http://www.oreilly.com/programming/free/reactive-microservices-architecture-orm.csp"" rel=""nofollow noreferrer"">Reactive Microservices</a> </li>&#xA;<li>Both Product &amp; Checkout service should have its own database for storing data, which is another underlying principle of microservices to keep datastore separate (even though data is being stored at multiple places - DRY principle does not apply here)</li>&#xA;<li>In terms of communication, you can use REST interface between services to call each other (with the help of any API gateway or similar solution for security or any mediation)</li>&#xA;<li>You can also introduce ShoppingCart Service, which can have its own datastore to store cart objects. Essentially, all services having its own datastore and shared state should be kept minimum and only to be accessed using REST interface between services.</li>&#xA;<li>You can use a caching solution for shared session data such as Couchbase, Redis etc. </li>&#xA;</ul>&#xA;&#xA;<p>See sample implementation by Kbastani here: <a href=""https://github.com/kbastani/spring-cloud-microservice-example"" rel=""nofollow noreferrer"">Sample Microservices Code</a></p>&#xA;"
50889163,50876475,3737815,2018-06-16T15:21:26,"<p>You can enable <code>spring-retry</code> (included in pom.xml in a default JHipster gateway) by setting <code>zuul.retryable: true</code> in application.yml.  Then any failed GET requests will be attempted again.  If you also want other HTTP methods such as POST to be attempted more than once, set <code>ribbon.OkToRetryOnAllOperations: true</code></p>&#xA;&#xA;<p><a href=""http://cloud.spring.io/spring-cloud-static/Dalston.SR1/#retrying-failed-requests"" rel=""nofollow noreferrer"">http://cloud.spring.io/spring-cloud-static/Dalston.SR1/#retrying-failed-requests</a></p>&#xA;"
40939981,40938088,3083223,2016-12-02T19:59:22,"<p>If you're looking at using a Docker Swarm you don't need to worry about the DNS configurations as it's already handled by the overlay network. Let's say you have three services:</p>&#xA;&#xA;<p>A&#xA;B&#xA;C</p>&#xA;&#xA;<p>A is your DB, B might be the first service to collect data, and C to recieve that data and update the database (A)</p>&#xA;&#xA;<pre><code>docker network create \&#xA;  --driver overlay \&#xA;  --subnet 10.9.9.0/24 \&#xA;  youroverlaynetwork&#xA;&#xA;docker service create --network youroverlaynetwork --name A&#xA;docker service create --network youroverlaynetwork --name B&#xA;docker service create --network youroverlaynetwork --name C&#xA;</code></pre>&#xA;&#xA;<p>Once all the services are created <strong>they can refer to each other directly by name</strong></p>&#xA;&#xA;<p>These requests are load balanced against all replicas of the container on that overlay network. So A can always get an IP for B by referencing ""<a href=""http://b"">http://b</a>"" or just by calling hostname B. </p>&#xA;&#xA;<p>When you're dealing with load balancing in Docker, a swarm service is <em>already</em> load balanced internally. Once you've defined a service to listen on port 8018, all swarm hosts, will listen on port 8018 and mesh route that to a container in round robin fashion. </p>&#xA;&#xA;<p>It is still, however, best practice to have an application load balancer sit in front of the hosts in the event of host failure. </p>&#xA;"
48612735,47793065,2103767,2018-02-04T20:33:30,"<p>Instead of outright suggesting AGAINST this idea mainly due to separate stack requirements I will lay out the trade offs and provide some restrictions which will make this possible. </p>&#xA;&#xA;<blockquote>&#xA;  <p>the page consists of several components which would be running autonomously</p>&#xA;</blockquote>&#xA;&#xA;<p>We all know this is offered out of box in Angular components with clear demarcation of inputs and output. </p>&#xA;&#xA;<p>Small CAVEAT: When/If you pass objects for <code>@Input</code> and emit event objects with <code>@Output()</code> interacting components must agree on a defined interface upfront.</p>&#xA;&#xA;<p>Workaround: Create another TypeScript project which just defines these artifacts. All other ""component projects"" would depend on a specific version of this.</p>&#xA;&#xA;<blockquote>&#xA;  <p>each component is managed by one dev-team</p>&#xA;</blockquote>&#xA;&#xA;<p>Dev Teams can distribute the components just like other Angular projects in the opensource are doing. They can publish their artifacts to some npm repository. To develop attributable components I recommend you refer to <a href=""https://github.com/angular/material2"" rel=""noreferrer"">Angular Material2</a> which may be overwhelming or you may use something like <a href=""https://github.com/yogeshgadge/ngx-library-builder"" rel=""noreferrer"">ngx-library-builder</a> (based on <a href=""https://github.com/filipesilva/angular-quickstart-lib"" rel=""noreferrer"">Angular Team Member filipesilva/angular-quickstart-lib </a>) that each component team uses.</p>&#xA;&#xA;<p>CAVEAT: Till this date angular team does not have a quick component library sharing project setup as evident in Angular CLI. But numerous developers have created some sort of library builders to fill the gap in Angular CLI.</p>&#xA;&#xA;<blockquote>&#xA;  <p>each team can change, update and deploy their components without breaking components of other teams</p>&#xA;</blockquote>&#xA;&#xA;<p>Have your main project pull all the components and perform a periodic/change triggered build/deploy on some CI server. This is essentially producing AOT production builds with all the latest component releases. &#xA;As an added bonus you can have some abstract e2e tests built to do some automated integration testing ensuring side effects of one component does not break other components.</p>&#xA;&#xA;<p>CAVEAT: It will be difficult to govern how each team develops the components i.e. they are doing optimal usage and disposition of memory, CPU, and other resources. e.g. what if one team starts creating subscriptions and does not remove them. Using some static code analysis can be useful but you will not have this source code available at this time - unless they also publish their source code.</p>&#xA;&#xA;<blockquote>&#xA;  <p>each team chooses its own toolstack</p>&#xA;</blockquote>&#xA;&#xA;<p>This is a complete deal breaker unless if you mean ""toolstack"" as in developer tools such as IDEs and some of the ""devDependencies"". Although certain parts of ""devDependencies"" of each team must have the same exact versions of angular dev kits such as compilers etc. </p>&#xA;&#xA;<p>At the least each team must use same Angular, RxJS etc.</p>&#xA;&#xA;<p>Most importantly care should be taken that each of the team does not bootstrap any components - only the main projects will have a bootstrap module and that will bootstrap the root component. This will help answer your <code>zone.js</code> issue</p>&#xA;&#xA;<blockquote>&#xA;  <p>Does this work with Angular?</p>&#xA;</blockquote>&#xA;&#xA;<p>If you recognize the limitations and provide governance I suggest go for it. </p>&#xA;"
35918549,35914674,1064151,2016-03-10T13:58:24,"<p>Short answer:&#xA;Yes! That's a good use to a microservice architecture.</p>&#xA;&#xA;<p>Long answer:&#xA;Microservices don't necessarily provide you an inheritence mechanism as in OOP. You should consider microservices as independent ""functions"" which take in an input and respond with an output/action. Any microservice can depend on another to complete its own task.</p>&#xA;&#xA;<p>And then, you ""compose"" necessary microservices in order to achieve the final output/action.</p>&#xA;&#xA;<p>You can have one or few web facing ""frontend"" services that use a mix of few other backend microservices whose ports are not open to the public network.</p>&#xA;&#xA;<p>The drawback with a microservice would be its ""minimum footprint"". The idea with microservices is around some main benefits:</p>&#xA;&#xA;<ul>&#xA;<li>Separate core services so that they can be ""maintained"" independently</li>&#xA;<li>Separate core services so that they can be ""replaced"" independently</li>&#xA;<li>Separate core services so that they can be ""scaled"" independently</li>&#xA;</ul>&#xA;&#xA;<p>But then, each microservice, being a node/meteor app, will have its minimum cpu/ram footprint even when they are just idle and waiting for a connection.</p>&#xA;&#xA;<p>Furthermore, managing a single monolithic app, or just a few ""largish"" services is much easier, from a devops standpoint, than managing tens of individual deployments.</p>&#xA;&#xA;<p>So with all engineering decisions, the right answer would imply some kind of ""balance"".</p>&#xA;&#xA;<p>Edit: reference to inheritence</p>&#xA;&#xA;<p>As per the OP's comment, the microservices can indeed be referenced from a parent code as either functions or classes and be composed (functions) or inherited from (classes) because after all the underlying functionality are DDP endpoints.</p>&#xA;&#xA;<p>If you are using the cluster package from meteorhacks</p>&#xA;&#xA;<pre><code>// create a connection to your microservice&#xA;var someService = Cluster.discoverConnection(""someService"");&#xA;// call a normal meteor method from that service&#xA;var resultFromSomeService = someService.call(""someMethodFromSomeService"");&#xA;</code></pre>&#xA;&#xA;<p>So as with any piece of javascript code, you can wrap the above piece of code in a function or a <a href=""https://developer.mozilla.org/en-US/docs/Web/JavaScript/Inheritance_and_the_prototype_chain"" rel=""nofollow"">class with its constructor and all</a> and inherit from it, exposing its interfaces as you desire.</p>&#xA;"
47055083,47050984,2190807,2017-11-01T12:40:14,"<p>Laravel has officially stopped supporting sessions &amp; views in <code>laravel/lumen</code> framework from version 5.2 and on wards.</p>&#xA;&#xA;<p>But <code>laravel</code> still have a component <code>illuminate/session</code> which can be installed in <code>lumen/framework</code> and we can play around with this.</p>&#xA;&#xA;<p><strong>Step - 1</strong></p>&#xA;&#xA;<p>install <code>illuminate/session</code> using </p>&#xA;&#xA;<p><code>composer require illuminate/session</code></p>&#xA;&#xA;<p><strong>Step - 2</strong></p>&#xA;&#xA;<p>Now goto <code>bootstrap/app.php</code> and add this middleware</p>&#xA;&#xA;<pre><code>$app-&gt;middleware([&#xA;    \Illuminate\Session\Middleware\StartSession::class,&#xA;]);&#xA;</code></pre>&#xA;&#xA;<p><strong>Step - 3</strong></p>&#xA;&#xA;<p>Since <code>Illuminate\Session\SessionManager</code> takes one primitive which is fortunately <code>$app</code>, and it is not type hinted, so we can take advantage of <code>containerBindings</code>.</p>&#xA;&#xA;<p>In <code>bootstrap/app.php</code> add bindings for <code>\Illuminate\Session\SessionManager</code></p>&#xA;&#xA;<pre><code>$app-&gt;bind(\Illuminate\Session\SessionManager::class, function () use ($app) {&#xA;    return new \Illuminate\Session\SessionManager($app);&#xA;});&#xA;</code></pre>&#xA;&#xA;<p><strong>Step - 4</strong></p>&#xA;&#xA;<p>Now add <code>config/session.php</code>, since it is not present in <code>Lumen</code> by default. You can take <code>session.php</code> from <a href=""https://github.com/laravel/laravel/blob/master/config/session.php"" rel=""noreferrer"">Laravel Official Repository</a>.</p>&#xA;&#xA;<p><strong>Step - 5</strong></p>&#xA;&#xA;<p>Add the following line in <code>bootstrap/app.php</code> to load config in container.</p>&#xA;&#xA;<pre><code>$app-&gt;configure('session');&#xA;</code></pre>&#xA;&#xA;<p><strong>Step - 6</strong></p>&#xA;&#xA;<p>Register session service provider</p>&#xA;&#xA;<pre><code>$app-&gt;register(\Illuminate\Session\SessionServiceProvider::class);&#xA;</code></pre>&#xA;&#xA;<p>Because unlike <code>Laravel</code>, you've to tell <code>Lumen</code> to read configuration file.</p>&#xA;&#xA;<p><strong>How to Use</strong></p>&#xA;&#xA;<pre><code>$app-&gt;get('/', function () {&#xA;    app('session')-&gt;put('name', rand());&#xA;&#xA;    dd(app('session')-&gt;all());&#xA;});&#xA;</code></pre>&#xA;"
31621151,31608784,2486743,2015-07-24T23:16:12,"<p>JAX-RS Multipart support is declared as an optional module in WildFly, so the necessary classes to make it work were not included in the application.</p>&#xA;&#xA;<p>Since then Ken Finnigan added a new WildFly Swarm API for the module.</p>&#xA;&#xA;<p>If you build the latest from wildfly-swarm/wildfly-swarm on GitHub you can add the wildfly-swarm-jaxrs-multipart artifact as a dependency to the project and that should resolve the problem.</p>&#xA;"
38518756,38508387,6092247,2016-07-22T05:21:48,"<p><strong>SOA</strong> (Service oriented architecture) as the name suggests that it is combination of services. These services are loosly coupled and interacts with each&#xA;other to provide a solution. Services may be RESTful web service to carry out some function.</p>&#xA;&#xA;<p><strong>Microservices</strong> - Again as the name suggests micro services. The idea behind microservices is that it becomes easier to build and maintain some type of services when they are broken down into smaller, composable pieces that work together. </p>&#xA;&#xA;<p><strong>Webservices</strong> - A service (some functionality like checking seat availability in a train) available on web is known as web service. Such services communicate over the WWW using HTTP.</p>&#xA;&#xA;<p><strong>REST</strong> - It is just an architectural style for designing webservices.</p>&#xA;&#xA;<p>We can think like this:- SOA is having some services. These services are combination of microservices. Those microservices are exposed as webservices and the webservices are build on REST.</p>&#xA;"
45335922,35065875,661464,2017-07-26T19:32:16,"<p>We recently built gRPC-Web (<a href=""https://github.com/improbable-eng/grpc-web"" rel=""noreferrer"">https://github.com/improbable-eng/grpc-web</a>) - a browser client and server wrapper that follows the proposed gRPC-Web protocol. The example in that repo should provide a good starting point.</p>&#xA;&#xA;<p>It requires either a standalone proxy or a wrapper for your gRPC server if you're using Golang. The proxy/wrapper modifies the response to package the trailers in the response body so that they can be read by the browser.</p>&#xA;&#xA;<p>Disclosure: I'm a maintainer of the project.</p>&#xA;"
48981696,48959272,1382872,2018-02-26T04:59:29,"<p>Yes you are absolutely right. At this moment, there is nothing which allows you to share service endpoints between projects. And I strongly feel that this needs to be addressed. </p>&#xA;&#xA;<p>The only workaround that I see for you is to write a small tool which will provision/update your endpoints across all the projects using <a href=""https://www.visualstudio.com/en-us/docs/integrate/api/endpoints/endpoints"" rel=""nofollow noreferrer"">VSTS Rest Apis.</a></p>&#xA;&#xA;<p>There is already a <a href=""https://visualstudio.uservoice.com/forums/330519-visual-studio-team-services/suggestions/11701410-vsts-add-services-on-project-collection-level"" rel=""nofollow noreferrer"">User Voice</a> request for this. </p>&#xA;"
38297545,38297333,138200,2016-07-11T00:11:11,"<p>In most cases, the cost of duplication is offset by the convenience, but you can always consider the shared data as owned by another service.  </p>&#xA;&#xA;<p>If there is only one writer to the ""shared"" data and you access it in a way that allows independent versioning of clients, then you can view the shared data as an unconventionally exposed service.</p>&#xA;&#xA;<p>Example:</p>&#xA;&#xA;<ul>&#xA;<li>Service A owns entity A1, stored in an SQL Server as table A1</li>&#xA;<li>Service B owns entity B1, which requires data from A1 entities</li>&#xA;</ul>&#xA;&#xA;<p>In a classical layout, Service B would access A1 through calls to Service A. </p>&#xA;&#xA;<pre><code>Service B --HTTP--&gt; Service A --SQL--&gt; A1&#xA;</code></pre>&#xA;&#xA;<p>Alternatively, Service A may create a view that allows Service B to access A1 directly.</p>&#xA;&#xA;<pre><code>Service B --SQL--&gt; vwA1_version1 --SQL--&gt; A1&#xA;</code></pre>&#xA;&#xA;<p>When Service A changes field layout, Service A updates <code>vwA1_version1</code> to allow backwards compatibility with old clients and defines <code>vwA1_version2</code> for new clients.</p>&#xA;"
37247782,37247409,4142552,2016-05-16T06:17:11,"<p>Sometimes you just have to get a console and do the work. What's the philosophical point of a migration? To change the database in a replicable/repeatable way, i.e., if you were to re-provision the app or instigate a new development environment. In this case? I'd say screw it, just run a task on production outside a migration.</p>&#xA;&#xA;<p>Maybe, have it make background jobs, so if one fails due to the external service being down, your whole task doesn't restart.</p>&#xA;"
36638726,36638486,1597656,2016-04-15T05:14:49,"<p>The reason why it fails is because <code>yeoman</code> generator fails to install dependencies but fails silently. </p>&#xA;&#xA;<p>If you look inside the <code>package.json</code> you can see the dependencies for <code>seneca-auth</code> and <code>seneca</code> pointing to non existing repo's. Please update those, and run <code>npm install</code> again. </p>&#xA;&#xA;<p><strong>EDIT</strong></p>&#xA;&#xA;<p>I suggest you <strong>not</strong> use this <code>yeoman</code> generator at all. It is outdated and might as well fail on your node version. </p>&#xA;"
51958800,51953129,10257368,2018-08-22T01:42:56,<p>Use the Spring Boot version of 2.0.x...they haven't introduced the Zuul support for 2.1.x</p>&#xA;&#xA;<p>Thanks</p>&#xA;
36576044,36569703,1231064,2016-04-12T14:19:09,"<p>You need to use spring cloud config in native mode, e.g.</p>&#xA;&#xA;<pre><code>spring:&#xA;  cloud:&#xA;    config:&#xA;      server:&#xA;        bootstrap: true&#xA;        native:&#xA;          search-locations: file:///C:/ConfigData&#xA;</code></pre>&#xA;&#xA;<p>See the following link for more information:&#xA;<a href=""http://cloud.spring.io/spring-cloud-config/spring-cloud-config.html#_file_system_backend"" rel=""nofollow"">http://cloud.spring.io/spring-cloud-config/spring-cloud-config.html#_file_system_backend</a></p>&#xA;"
37324653,37289634,1231064,2016-05-19T13:27:20,"<p>I have done this before using a RequestInterceptor as follows:</p>&#xA;&#xA;<pre><code>@Component&#xA;public class MyRequestInterceptor implements RequestInterceptor {&#xA;  @Override&#xA;  public void apply(RequestTemplate template) {&#xA;    template.headers(getHeadersFromWherever());&#xA;  }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>You can find some more useful information here:</p>&#xA;&#xA;<p><a href=""https://github.com/Netflix/feign#user-content-setting-headers-per-target"" rel=""noreferrer"">https://github.com/Netflix/feign#user-content-setting-headers-per-target</a></p>&#xA;"
44889804,44886715,2087640,2017-07-03T15:55:27,"<p>You should keep them separated, oauth is not related to identity management but to authorization delegation.</p>&#xA;&#xA;<p>In oauth2 there are 4 roles (resource server, resource owner, client and authorization server) you are currently asking if the authorization server must be part of one microservice of the resource server which has absolutely no sense.</p>&#xA;&#xA;<p>If I correctly got your case what you name a user corresponds to the resource owner role in oauth2 terminology, some oauth2 flows (e.g. client_credentials) directly allow a client to get an access to the resource server and there will be no users implied in any way.</p>&#xA;"
46317243,46311488,2087640,2017-09-20T08:40:43,"<p>Both approach are not necessary exclusive. Microservices may communicate using a messaging middleware (kafka, AMQP, Akka actors, JMS...) instead of direct http channels. It depends on your constraint (mainly consistency) and deployment policies.</p>&#xA;&#xA;<p>Each choice has his strengths and weakness, I personally advise not to limit yourself to one approach but to use both and choose according to each situation</p>&#xA;&#xA;<p>See <a href=""https://stackoverflow.com/questions/41010290/microservices-rest-vs-messaging"">Microservices: REST vs Messaging</a> and <a href=""https://capgemini.github.io/architecture/is-rest-best-microservices/"" rel=""nofollow noreferrer"">https://capgemini.github.io/architecture/is-rest-best-microservices/</a></p>&#xA;"
50860533,50816609,1892568,2018-06-14T15:02:35,"<p>Your handler should look like:</p>&#xA;&#xA;<pre><code>public class MyCustomHttpHandler implements HttpHandler {&#xA;&#xA;    private HttpHandler next;&#xA;&#xA;    public MyCustomHttpHandler(HttpHandler next) {&#xA;      this.next = next;&#xA;      System.out.println(""MyCustomHttpHandler.java constructed""); // never gets called. How do i register it in Thorntail?&#xA;    }&#xA;&#xA;    @Override&#xA;    public void handleRequest(HttpServerExchange httpServerExchange) throws Exception {&#xA;      // never gets called :/&#xA;      System.out.println(String.format(""HttpHandler next=%s"", next));&#xA;&#xA;      // How do i get next? Is @Inject the way to go?&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>However, at the moment it looks like Thorntail v4 doesn't support the arbitrary addition of HttpHandlers into the chain. Could you please raise an issue here: <a href=""https://github.com/thorntail/thorntail/issues"" rel=""nofollow noreferrer"">https://github.com/thorntail/thorntail/issues</a></p>&#xA;"
50393066,50389344,3607513,2018-05-17T13:45:35,<p>The error was to use the mapped Neo4j port <code>neo4jContainer.getMappedPort(NEO4j_EXPOSED_PORT)</code> in the myserviceContainer instead of the exposed port itself.</p>&#xA;&#xA;<p>Also the @Rule with the RuleChain isn't needed in this case.</p>&#xA;
44195440,44195150,1840857,2017-05-26T06:55:13,"<p>You can have any application run forever using the threads (preferably over thread pool), but you should take utmost care about the GC. So you may have to keep a close eye on the instance management, Heap dumps periodically and fine tune the application. Hope this would help you!</p>&#xA;"
48672554,48671754,1090265,2018-02-07T20:18:53,"<p>After digging around a while, I think the correct answer will be to use <code>GRPC::BadStatus</code>, as described <a href=""https://github.com/grpc/grpc/blob/5253c8f9a899450397a5e46e4923d01ac9a66a27/src/ruby/lib/grpc/errors.rb#L19-L29"" rel=""nofollow noreferrer"">here</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p>BadStatus is an exception class that indicates that an error&#xA;  occurred at either end of a GRPC connection.  When raised, it&#xA;  indicates that a status error should be returned to the other end of a&#xA;  <code>GRPC</code> connection; when caught it means that this end received a status&#xA;  error.</p>&#xA;  &#xA;  <p>There is also subclass of BadStatus in this module for each&#xA;  <code>GRPC</code> status. E.g., the <code>GRPC::Cancelled</code> class corresponds to status&#xA;  <code>CANCELLED</code>.</p>&#xA;  &#xA;  <p>See <a href=""https://github.com/grpc/grpc/blob/master/include/grpc/impl/codegen/status.h"" rel=""nofollow noreferrer"">https://github.com/grpc/grpc/blob/master/include/grpc/impl/codegen/status.h</a> for detailed descriptions of each status code.</p>&#xA;</blockquote>&#xA;&#xA;<p>and so, raising an error like this</p>&#xA;&#xA;<pre><code>raise GRPC::BadStatus.new_status_exception(CANCELLED)&#xA;</code></pre>&#xA;&#xA;<p>should do the trick.</p>&#xA;"
38079681,38071714,6268377,2016-06-28T14:57:52,"<p>Definitely approach #1.</p>&#xA;&#xA;<p>Having your clients talk to multiple GraphQL services (as in approach #2) entirely defeats the purpose of using GraphQL in the first place, which is to provide a schema over your <em>entire</em> application data to allow fetching it in a single roundtrip.</p>&#xA;&#xA;<p>Having a <em>shared nothing</em> architecture might seem reasonable from the microservices perspective, but for your client-side code it is an absolute nightmare, because every time you change one of your microservices, you have to update <strong>all</strong> of your clients. You will definitely regret that.</p>&#xA;&#xA;<p>GraphQL and microservices are a perfect fit, because GraphQL hides the fact that you have a microservice architecture from the clients. From a backend perspective, you want to split everything into microservices, but from a frontend perspective, you would like all your data to come from a single API. Using GraphQL is the best way I know of that lets you do both. It lets you split up your backend into microservices, while still providing a single API to all your application, and allowing joins across data from different services.</p>&#xA;&#xA;<p>If you don't want to use REST for your microservices, you can of course have each of them have its own GraphQL API, but you should still have an API gateway. The reason people use API gateways is to make it more manageable to call microservices from client applications, not because it fits well into the microservices pattern.</p>&#xA;"
45771589,45771237,1422297,2017-08-19T12:34:01,"<p>You can use <a href=""https://martinfowler.com/bliki/CircuitBreaker.html"" rel=""nofollow noreferrer"">Circuit Breaker</a> pattern and tools like <a href=""https://github.com/Netflix/Hystrix"" rel=""nofollow noreferrer"">Hystrix</a> in such a scenario.</p>&#xA;"
45771508,45771097,1422297,2017-08-19T12:25:06,<p>Microservices granularity is a subject of many studies. some practical experience shows dividing microservices based on domain context (DDD practices) can be effective. like what you did in your example however deciding on granularity of services depends on many other things.&#xA;some factor to be considered can be :</p>&#xA;&#xA;<ul>&#xA;<li>size/number of development team.</li>&#xA;<li>hardware/resource consumption</li>&#xA;<li>service location distribution </li>&#xA;<li>number of service communication to handle a request</li>&#xA;<li>service re-usability ( to avoid repetitive work)</li>&#xA;<li>performance</li>&#xA;<li>etc</li>&#xA;</ul>&#xA;&#xA;<p>google about service granularity principle. there are many other factor to consider.</p>&#xA;
51980796,51980596,2932953,2018-08-23T07:50:27,"<p>Based on what you need you can write a simple mapper on your own (with usage of Java reflection API to reduce boilerplate) or you can use such tools as <a href=""http://dozer.sourceforge.net/documentation/about.html"" rel=""nofollow noreferrer"">Dozer</a></p>&#xA;"
45177680,45177029,2933977,2017-07-18T21:48:22,"<p>Try adding <code>-Djava.net.preferIPv4Stack=true</code> to the Docker entry point, i.e.</p>&#xA;&#xA;<pre><code>ENTRYPOINT [""java"", ""-jar"", ""/opt/megasindico-billing-api-swarm.jar"", ""-Djava.net.preferIPv4Stack=true""]&#xA;</code></pre>&#xA;&#xA;<p>And please copy and paste the text of the errors instead of posting screen shots.  No one in the future searching for a similar issue will be able to find it as it's a screen shot.</p>&#xA;"
34664929,34652782,1757145,2016-01-07T20:51:54,"<p>Like @Akira said. If everyone is responsible for one microservice, noone is. </p>&#xA;&#xA;<h2>Shared responsibility</h2>&#xA;&#xA;<p>Every two weeks team which is responsible for given MS will change.&#xA;So according to the date, you will have the owner.</p>&#xA;&#xA;<h2>Align teams according to the product they serve</h2>&#xA;&#xA;<p>Every team is responsible for <code>Product</code>, which is not microservice. It's group of microservices, which together bring value to the business. They work together to accomplish business needs in given domain. You may understand <code>Product</code> as a system, which is constructed with few microservices working together.&#xA;Such team should have domain knowledge about the whole system and should be more effective in supporting MS, which are under their's product.</p>&#xA;&#xA;<h2>Team which is created and destroyed after creating new MS</h2>&#xA;&#xA;<p>Thats kind of management is problematic. If MS isn't just another CMS, but very complex solution it may become even more problematic. People who wrote MS should be responsible for supporting it. If they are to value for the company to become supporters, take someone else, but let them communicate and ask question to the creators of MS. Otherwise MS may be come mess sooner or later. </p>&#xA;&#xA;<h3>Whatever you choose, keep your MS documentation up to date.</h3>&#xA;"
46904188,46892720,840409,2017-10-24T07:05:15,"<p>Adding a dynamic query language will introduce another level of complexity my personal opinion is that you should avoid this unless you really, really need it - i.e. when other people are going to integrate your system a lot. I strongly agree with @plalx 's comment to your post - adding complexity always has its price and that goes both ways.</p>&#xA;&#xA;<p>On the ubiquitous language &amp; terminology mix - you should really avoid duplicate terms in your domain. The ""Profile"" concerning a social media account could be named ""SocialProfile"". So when a ""Profile"" means two different things in two different contexts: try to find a better term for one of those. Sure you can know that a 'Profile' in the context of 'API gateway' is something and at the same time it is something else in that other context however this will not do you good in the long term - try adding new people to the project and explaining how many different things you call 'a Profile'.</p>&#xA;&#xA;<p>On your last question:</p>&#xA;&#xA;<blockquote>&#xA;  <p>In a monolith architecture I could create multiple repositories. One for each use case. In microservices architecture I would need to change microservice responsible for Profile every time other microservices needs new query.</p>&#xA;</blockquote>&#xA;&#xA;<p>It really depends on your architecture - do your micro-services use the same code-base &amp; Repository classes? Are they even in the same name-space? You could have one Repository for each micro-service to use and that is the logical approach since your different micro-services will do different things.&#xA;If you take my advice and find concrete terms for each Entity in your Domain you will not have to wonder around this. :)</p>&#xA;&#xA;<p>And a side-note on REST APIs from my practice:&#xA;  If your API is truly REST you can just use Repositories behind each endpoint. However if you find your API endpoints are not just reading &amp; returning data (or entities) but also 'doing something' then there are 2 things to note:&#xA; - your API is most-probably not REST but plain RPC&#xA; - the objects you use behind endpoints are most-likely Services and not Repositories</p>&#xA;"
48639207,48639206,3336921,2018-02-06T09:09:11,"<p>To start a single microservice on multiple ports.&#xA;    Steps are below.</p>&#xA;&#xA;<pre><code>1. Right click on     project -&gt; Run As.. -&gt; Run Configuration.&#xA;</code></pre>&#xA;&#xA;<p>2.Create New Boot App configuration. &#xA;In Environment tab add ""Server.port =9090"" (** Use any port you like and available :) ). &#xA;Apply and Run. &#xA;This will execute same microservice on same port .&#xA;    For more use above steps 1 &amp; 2.</p>&#xA;&#xA;<pre><code>Danke,&#xA;Rahul&#xA;</code></pre>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/5wz4E.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5wz4E.jpg"" alt=""enter image description here""></a></p>&#xA;"
35315467,35314963,192729,2016-02-10T12:34:43,"<p>When you <strong>opt for not using Flux/Redux</strong>, here is what you do:</p>&#xA;&#xA;<p>Create an <strong>outer component</strong> that should wrap all the other components. This component is also known as a <strong>higher order component</strong> or a <strong>controller view</strong>. This component should use an HTTP library to communicate with your microservices (I personally like <a href=""https://github.com/mzabriskie/axios"" rel=""nofollow"">Axios</a>). I would recommend that you create a client API object that wraps Axios. Your higher order component can reference this client API so it is agnostic of the HTTP library and whatnots. I would also put a reference of this client API on the <code>window</code> object in <code>dev</code> mode so you can do <code>window.clientApi.fetchSomething()</code> in the <code>Chrome console</code> and make debugging easier.</p>&#xA;&#xA;<p>Make all the other components (ChatBox, AvatarBox and NewsStream) <strong>controlled</strong>. If you are not familiar with this concept, it means they receive everything they need through <strong>props</strong> and they avoid keeping state. These components should not call the microservices themselves. This is responsability of the higher order component. In order to be interactive, these components should receive <strong>event handlers</strong> as functions as props.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Is this correct? It will provide clean responsibility managing models, but it gives performance doubts using http requests to load each component's content</p>&#xA;</blockquote>&#xA;&#xA;<p>You can avoid performance issues by <strong>not allowing each component to directly contact the microservices</strong>. If your higher order component compiles all the information needed and make as little as possible HTTP calls, you should be perfectly fine with this approach.</p>&#xA;&#xA;<p>It's generally recommended to use Flux/Redux, but if you opt out, this is how to go about it.</p>&#xA;"
51856096,30213456,8521434,2018-08-15T09:22:25,"<p>Eventual consistency is the key here. </p>&#xA;&#xA;<p>•   One of the services is chosen to become primary handler of the event. &#xA;•   This    service will handle the original event with single commit. &#xA;•   Primary handler will take responsibility for asynchronously communicating the secondary effects to other services. &#xA;•   The primary handler will do the orchestration of other services calls. </p>&#xA;&#xA;<p>The commander is in charge of the distributed transaction and takes control. It knows the instruction to be executed and will coordinate executing them. In most scenarios there will just be two instructions, but it can handle multiple instructions. </p>&#xA;&#xA;<p>The commander takes responsibility of guaranteeing the execution of all instructions, and that means retires. &#xA;When the commander tries to effect the remote update and doesn’t get a response, it has no retry. &#xA;This way the system can be configured to be less prone to failure and it heals itself. </p>&#xA;&#xA;<p>As we have retries we have idempotence. &#xA;Idempotence is the property of being able to do something twice such a way that the end results be the same as if it had been done once only. &#xA;We need idempotence at the remote service or data source so that, in the case where it receives the instruction more than once, it only processes it once. </p>&#xA;&#xA;<p>Eventual consistency&#xA;This solves most of distributed transaction challenges, however we need to consider couple of points here. &#xA;Every failed transaction will be followed by a retry, the amount of attempted retries depends on the context. </p>&#xA;&#xA;<p>Consistency is eventual i.e., while the system is out of consistent state during a retry, for example if a customer has ordered a book, and made a payment and then updates the stock quantity. If the stock update operations fail and assuming that was the last stock available, the book will still be available till the retry operation for the stock updating has succeeded. After the retry is successful your system will be consistent. </p>&#xA;"
34377067,34376576,397786,2015-12-20T01:10:50,"<p>Well, your microservice R can be described as ""receives per-producer batch data and exposes a REST API to list, manage and retrieve this data"". </p>&#xA;&#xA;<p>This service will need to keep this data in a store (have you thought about a database? Postgresql can store up to 2 gig size blobs), and for architectural reasons I would recommend that the contents are not exposed via actual files but via HTTP (did you know that you can retrieve arbitrary parts of a file using HTTP?).</p>&#xA;&#xA;<p>This allows you to stream the files (no need to have the whole file in memory) over the network, have access controls if necessary, logging access and usage times etc.</p>&#xA;"
34411366,34402406,397786,2015-12-22T08:23:28,<p>Just use <code>String acct_id</code> instead of <code>Account acct</code> in your model. (why is the account identifier a String?)</p>&#xA;
46398770,37180375,5120986,2017-09-25T06:21:49,"<p>Add the following within your run method, it will solve this problem</p>&#xA;&#xA;<pre><code>ctx.setSendZuulResponse(false);&#xA;ctx.setResponseStatusCode(401);&#xA;</code></pre>&#xA;"
34842920,34841789,2318810,2016-01-17T19:54:17,"<p>I suggest you to check out <a href=""http://www.kontena.io"" rel=""nofollow"">Kontena</a>. It will solve this kind of problem out of the box. Every service will have an internal DNS that you can use in communication between services. Kontena has also built-in <a href=""http://blog.kontena.io/kontena-load-balancer/"" rel=""nofollow"">load balancer</a> that is very easy to use making it very easy to create and scale micro services.</p>&#xA;&#xA;<p>There are also lot's of built-in features that will help developing containerized applications, like private image registry, VPN access to running services, secrets management, stateful services etc.</p>&#xA;&#xA;<p>Kontena is open source project and the code is visible on <a href=""https://github.com/kontena/kontena"" rel=""nofollow"">Github</a></p>&#xA;"
44986075,44984823,8086746,2017-07-08T12:09:07,"<p>So it transpires that there was a bug in some previous versions of macOS that prevented use of the <code>docker0</code> network. Whilst the bug is fixed in newer versions, Docker support extends to older versions and so Docker for Mac doesn't currently support <code>docker0</code>. See <a href=""https://github.com/moby/moby/issues/22753"" rel=""nofollow noreferrer"">this discussion</a> for details.</p>&#xA;&#xA;<p>The workaround is to create an alias to the loopback interface on the host machine, set the service to listen on either that alias or <code>0.0.0.0</code>, and configure Consul to send the health check GET request to the alias.</p>&#xA;&#xA;<p>To set the alias (choose a private IP address that's not being used for anything else; I chose a class A address but that's irrelevant):</p>&#xA;&#xA;<pre><code>sudo ifconfig lo0 alias 10.200.10.1/24&#xA;</code></pre>&#xA;&#xA;<p>To remove the alias:</p>&#xA;&#xA;<pre><code>sudo ifconfig lo0 -alias 10.200.10.1&#xA;</code></pre>&#xA;&#xA;<p>From the service definition above, the HTTP line should now read:</p>&#xA;&#xA;<pre><code>set http: http://10.200.10.1:35000/health&#xA;</code></pre>&#xA;&#xA;<p>And the HTTP server listening for the health check requests also needs to be listening on either <code>10.200.10.2</code> or <code>0.0.0.0</code>. This latter option is suggested in the discussion but I've only tried it with the alias.</p>&#xA;&#xA;<p>I've updated the title of the question to more accurately reflect the problem, now I know the solution. Hope it helps somebody else too.</p>&#xA;"
47507727,47474453,499930,2017-11-27T09:35:57,"<p>I think there is a confusion here, Microservices should be built around business capability and that means that your Microservice will include both the frontend and the backend. Frontend shouldn't be considered as a microservice.</p>&#xA;&#xA;<p>Frontend could access the backend using sub domain name like <a href=""http://api.example.com"" rel=""nofollow noreferrer"">http://api.example.com</a></p>&#xA;"
47622756,47613318,5229274,2017-12-03T20:08:06,"<p>There are some approaches to solve this situation.</p>&#xA;&#xA;<p>You might create topics for each client request and wait for the reply on the other side, e.g, DriverService would read the request message, fetch all your data and publish it to your client request topic. As soon as you consume the response message, you destroy that topic.</p>&#xA;&#xA;<p>BUT 'temporary' topics might take too long to be delete(if no configuration avoids that, such as delete.topic.enable property) in a request-response interaction, and you need to monitor possible topics overgrowth.</p>&#xA;&#xA;<p>Websocket is another possible solution. Your client would start listening to a specific topic, previously agreed with your server, then in a specific timeout you would wait for the response, when your DriverService  would publish to that specific socket channel.</p>&#xA;&#xA;<p>Spring Boot offers you great starters for Kafka and Websockets. If you are expecting a large amount of transactions, I would go with a mixed strategy, using Kafka to help my backend scale and process all transactions, then would respond to client via Websocket.</p>&#xA;"
35179588,35179495,4379098,2016-02-03T14:21:11,"<p>You can use Spring Cloud and Eureka Server for it.</p>&#xA;&#xA;<p><a href=""https://spring.io/blog/2015/01/20/microservice-registration-and-discovery-with-spring-cloud-and-netflix-s-eureka"" rel=""nofollow"">https://spring.io/blog/2015/01/20/microservice-registration-and-discovery-with-spring-cloud-and-netflix-s-eureka</a></p>&#xA;"
38363541,38352227,6351322,2016-07-14T00:01:06,"<p>It is a API Gateway Pattern, which is commonly used when working with microservices architecture. This pattern hides all services from client allowing client to connect with a API Service. This API Service handle all request while do another stuffs like organize all requisition, for example. </p>&#xA;&#xA;<p>The way this pattern handle requests is a example of Facade Pattern. </p>&#xA;&#xA;<p>I recommend you to read links below to understand how this pattern works properly:&#xA;<a href=""http://microservices.io/patterns/apigateway.html"" rel=""nofollow"">http://microservices.io/patterns/apigateway.html</a></p>&#xA;&#xA;<p><a href=""https://www.nginx.com/blog/building-microservices-using-an-api-gateway/"" rel=""nofollow"">https://www.nginx.com/blog/building-microservices-using-an-api-gateway/</a></p>&#xA;"
29151784,29071226,3667162,2015-03-19T17:57:20,"<p>The API Gateway pattern is a good way to implement a public API in front of a set of microservices: <a href=""http://microservices.io/patterns/apigateway.html"" rel=""nofollow"">http://microservices.io/patterns/apigateway.html</a></p>&#xA;"
43960712,43927492,7994586,2017-05-14T05:36:24,"<p>I believe this is a matter of your testing strategy. If you have a lot of micro-services in your system, it is not wise to always perform end-to-end testing at development time -- it costs you productivity and the set up is usually complex (like what you observed).</p>&#xA;&#xA;<p>You should really think about what is the thing you wanna test. Within one service, it is usually good to decouple core logic and the integration points with other services. Ideally, you should be able to write simple unit tests for your core logic. If you wanna test integration points with other services, use mock library (a quick google search shows this to be promising <a href=""http://spring.io/blog/2007/01/15/unit-testing-with-stubs-and-mocks/"" rel=""nofollow noreferrer"">http://spring.io/blog/2007/01/15/unit-testing-with-stubs-and-mocks/</a>)</p>&#xA;&#xA;<p>If you don't have already, I would highly recommend to set up a separate staging area with all micro-services running. You should perform all your end-to-end testing there, before deploying to production.</p>&#xA;&#xA;<p>This post from Martin Fowler has a more comprehensive take on micro-service testing stratey:</p>&#xA;&#xA;<p><a href=""https://martinfowler.com/articles/microservice-testing"" rel=""nofollow noreferrer"">https://martinfowler.com/articles/microservice-testing</a></p>&#xA;"
43960829,43814764,7994586,2017-05-14T05:59:31,"<p>You should always use golang's native <a href=""https://golang.org/pkg/testing/"" rel=""nofollow noreferrer"">unit testing framework</a> to test each individual service (please, no shell script!). httptest seems fine, but I would argue it is helpful to have finer-grained test boundaries -- you should really have one _test.go for each functional block of your code. Smaller tests are easier to maintain.</p>&#xA;&#xA;<p>In terms of overall integration tests that involve multiple microservices, you shouldn't do them at development time. Set up a staging area and run the tests over there.</p>&#xA;&#xA;<p>My 2 cents.</p>&#xA;"
43960926,43674924,7994586,2017-05-14T06:16:53,"<p>If they are really different code in different repos (but just logically correlated), then file multiple bugs each against one repo. If you use a more advanced issue tracking system (e.g., Jira) instead of github's own (rudimentary) issue tracking, you can create a overarching bug that depends on the individual repo's bugs.</p>&#xA;&#xA;<p>Like Oswin said, I would also suggest thinking about if the code can also be physically shared between those repos. If so, do it. Put it in a separate repo and maybe use git submodule to link them together. Micro-service architecture != not sharing code between micro-services.</p>&#xA;"
42728725,42711116,4852187,2017-03-10T22:24:21,"<p>Well, you could force drop the offending replicas with something like the solution provided to <a href=""https://stackoverflow.com/questions/35765141/removing-application-from-service-fabric-cluster"">this question</a>, but that's usually a bad idea and shouldn't be done in production. </p>&#xA;&#xA;<p>This stuck state usually indicates that the service is having a problem in its shutdown path. Have you debugged this locally? Just creating and deleting the service in a loop until it happens should be enough to show you where it is. </p>&#xA;"
51825020,51726683,5787099,2018-08-13T14:46:16,"<p>There are going to be many ways to build micro-services. I would start by familiarizing yourself with the whitepaper AWS published: <a href=""https://docs.aws.amazon.com/aws-technical-content/latest/microservices-on-aws/introduction.html"" rel=""nofollow noreferrer"">Microservices on AWS</a>, <a href=""https://docs.aws.amazon.com/aws-technical-content/latest/microservices-on-aws/microservices-on-aws.pdf?icmpid=link_from_whitepapers_page"" rel=""nofollow noreferrer"">Whitepaper - PDF version</a>.</p>&#xA;&#xA;<p>In your question you stated: <em>""The issue now is communication between services are via API calls through the API gateway. This feels inefficient and less secure than it can be. Is there a way to make my microservices talk to each other in a more performant and secure manner?""</em></p>&#xA;&#xA;<p>Yes - In fact, the AWS Whitepaper, and <a href=""https://aws.amazon.com/api-gateway/faqs/"" rel=""nofollow noreferrer"">API Gateway FAQ</a> reference the API Gateway as a ""front door"" to your application. The intent of API Gateway is to be used for external services communicating to your AWS services.. not AWS services communicating with each other.</p>&#xA;&#xA;<p>There are several ways AWS resources can communicate with each other to call micro-services. A few are outlined in the whitepaper, and this is another resource I have used: <a href=""https://aws.amazon.com/blogs/compute/better-together-amazon-ecs-and-aws-lambda/"" rel=""nofollow noreferrer"">Better Together: Amazon ECS and AWS Lambda</a>. The services you use will be based on the requirements you have. </p>&#xA;&#xA;<blockquote>&#xA;  <p>By breaking monolithic applications into small microservices, the communication overhead increases because microservices have to talk to each other. In many implementations, REST over HTTP is used as a communication protocol. It is a light-weight protocol, but high volumes can cause issues. In some cases, it might make sense to think about consolidating services that send a lot of messages back and forth. If you find yourself in a situation where you consolidate more and more of your services just to reduce chattiness, you should review your problem domains and your domain model.</p>&#xA;</blockquote>&#xA;&#xA;<p>To my understanding, the root of your problem is routing of requests to micro-services. To maintain the ""<a href=""https://docs.aws.amazon.com/aws-technical-content/latest/microservices-on-aws/characteristics-of-microservices.html"" rel=""nofollow noreferrer"">Characteristics of Microservices</a>"" you should choose a single solution to manage routing. </p>&#xA;&#xA;<h3>API Gateway</h3>&#xA;&#xA;<p>You mentioned using API Gateway as a routing solution. API Gateway can be used for routing... however, if you choose to use API Gateway for routing, you should define your routes explicitly in one level. Why?</p>&#xA;&#xA;<ol>&#xA;<li>Using {proxy+} increases attack surface because it requires routing to be properly handled in another micro-service. </li>&#xA;<li>One of the advantages of defining routes in API Gateway is that your API is self documenting. If you have multiple API gateways it will become colluded.</li>&#xA;</ol>&#xA;&#xA;<p>The downside of this is that it will take time, and you may have to change existing API's that have already been defined. But, you may already be making changes to existing code base to follow micro-services best practices.</p>&#xA;&#xA;<h3>Lambda or other compute resource</h3>&#xA;&#xA;<p>Despite the reasons listed above to use API Gateway for routing, if configured properly another resource can properly handle routing. You can have API Gateway proxy to a Lambda function that has all micro-service routes defined or <a href=""https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-api-gateway-supports-endpoint-integrations-with-private-vpcs/?sc_channel=em&amp;sc_campaign=Launch_2017_reInvent_recap3&amp;sc_medium=em_66769&amp;sc_content=other_la_tier1&amp;sc_country=global&amp;sc_geo=global&amp;sc_category=mult&amp;sc_outcome=other&amp;trk=em_66769&amp;mkt_tok=eyJpIjoiT0dJNU1ETTJNamswTkRZNSIsInQiOiJva3h0anN1ZFlPMFwvSkpzSFJDR2IwTWp0YzBuVnZpcnYrOW4wOVc5cGEyMlVsbG9DVmp3Z1phZURjZjlaNCswXC8wbktXSFh2UDlrWnErR1wvXC9wRTFpMUYwOUNkVUY0RVYyeGZlRzIxYk9ITjkxbEEybmN2OTBGQXJ4M0UzS1FUZHMifQ%3D%3D"" rel=""nofollow noreferrer"">another resource within your VPC</a> with routes defined.</p>&#xA;&#xA;<h3>Result</h3>&#xA;&#xA;<p>What you do depends on your requirements and time. If you already have an API defined somewhere and simply want API Gateway to be used to throttle, monitor, secure, and log requests, then you will have API Gateway as a proxy. If you want to fully benefit from API Gateway, explicitly define each route within it. Both approaches can follow micro-service best practices, however, it is my <strong>opinion</strong> that defining each public API in API Gateway is the best way to align with micro-service architecture. The other answers also do a great job explaining the trade-offs with each approach.</p>&#xA;"
50477929,50415015,5787099,2018-05-22T23:45:10,"<p>You can use API Gateway custom authorizers to validate policies attached to each group.</p>&#xA;&#xA;<p>From AWS <a href=""https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-user-groups.html"" rel=""nofollow noreferrer"">Documentation</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p>You can use groups in a user pool to control permission with Amazon API Gateway. The groups that a user is a member of are included in the ID token provided by a user pool when a user signs in. You can submit those ID tokens with requests to Amazon API Gateway, use a custom authorizer Lambda function to verify the token, and then inspect which groups a user belongs to. </p>&#xA;</blockquote>&#xA;&#xA;<p>Additional references to implement:</p>&#xA;&#xA;<p><a href=""https://aws.amazon.com/blogs/compute/introducing-custom-authorizers-in-amazon-api-gateway/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/compute/introducing-custom-authorizers-in-amazon-api-gateway/</a>&#xA;<a href=""https://aws.amazon.com/blogs/mobile/integrating-amazon-cognito-user-pools-with-api-gateway/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/mobile/integrating-amazon-cognito-user-pools-with-api-gateway/</a></p>&#xA;"
38021363,37555434,427995,2016-06-24T20:11:00,"<p>I don't have the reputation to answer but I am interested in this question for similar reasons.</p>&#xA;&#xA;<p>You might find this question and answer useful on <a href=""https://softwareengineering.stackexchange.com/questions/244728/best-strategy-to-discover-a-web-service-in-a-local-network"">programmers stackexchange</a> which talks about a broadcast approach.</p>&#xA;&#xA;<p>I'm researching etcd and Netflix eureka and trying to understanding if this could be applied on the local LAN.</p>&#xA;"
50629965,50629814,1129436,2018-05-31T17:43:06,"<p>A functional approach.</p>&#xA;&#xA;<pre><code>const { map, uniq } = require('lodash/fp');&#xA;&#xA;const arr = /* you say you already have this */;&#xA;&#xA;const uniqueIds = uniq(map('tagId', arr));&#xA;const objects = await axios.get('http://apihost/tag', { id: uniqueIds });&#xA;const associated = arr.map(({ id, tagId, name }) =&gt; (&#xA;  { id, tagId, name, tag: objects.find(o =&gt; o.id === tagId) };&#xA;));&#xA;</code></pre>&#xA;&#xA;<p>If you want to index (which may avoid an O(N^2) solution)</p>&#xA;&#xA;<pre><code>const byTagId = new Map();&#xA;arr.forEach(o =&gt; byTagId.set(o.tagId, o));&#xA;const objects = await axios.get('http://apihost/tag', { id: byTagId.keys() });&#xA;const associated = arr.map(({ id, tagId, name }) =&gt; (&#xA;  { id, tagId, name, tag: byTagId.get(tagId) }&#xA;));&#xA;</code></pre>&#xA;"
47855635,47837207,8145116,2017-12-17T13:31:34,"<p>Firstly, this is very bad idea to have a separate (per microservice) security model. It should be single always cross-cutting all application, because it can lead to a hell with access management, permissions granting and <strong>mapping between entities in different microservices</strong>.</p>&#xA;&#xA;<p>In second, I assume that you are wrong with understanding <strong><em>how to organize microservices..?</em></strong> You should dedicate the principle of splitting functionality into microservices: by features, by domain, etc. Look at Single Responsibility, DDD and other approaches which helps you to achieve clear behavior of your MS.</p>&#xA;&#xA;<p>So, in best case, you should have to: </p>&#xA;&#xA;<ul>&#xA;<li>Choose right security model <a href=""https://en.wikipedia.org/wiki/Attribute-based_access_control"" rel=""nofollow noreferrer"">ABAC</a> or <a href=""https://en.wikipedia.org/wiki/Role-based_access_control"" rel=""nofollow noreferrer"">RBAC</a> - <em>there are a lot of other options, but looking at your example I guess the ABAC is the best one</em></li>&#xA;<li>Create separate MS for access management - <em>the main responsibility of this MS is a CRUD and assignment of groups/roles/permissions/attributes</em> to the people accounts. </li>&#xA;<li>Create separate MS for providing <strong><em>only permitted</em></strong> health information.</li>&#xA;</ul>&#xA;&#xA;<p>In third, <strong><em>how it works?</em></strong>:</p>&#xA;&#xA;<ol>&#xA;<li>With ABAC you can setup hierarchical roles/permissions (based on groups/attributes) - <em>it helps you to resolve a delegation path of who is permitted to the data</em></li>&#xA;<li>Setup authorization (via auth-MS) and store the list of permissions (in session, cookies, etc)</li>&#xA;<li><p>Check access for a given user for a needed data in health-info-MS. Here we have several options how to do this: </p>&#xA;&#xA;<ul>&#xA;<li><p>If you use memory-grids (hazelcast, coherence), you can easily create filters with predicates based on security attributes. </p></li>&#xA;<li><p>If you're using SQL (hibernate, plain SQL, etc.) you should generate queries to <strong><em>return only permitted data</em></strong> - add security specific criteria to the <strong><em>where clause</em></strong></p></li>&#xA;</ul></li>&#xA;</ol>&#xA;&#xA;<p>Few more details about SQL queries with security check in <strong><em>where</em></strong>: before the SQL execution (if hibernate &amp; spring is easy to do with spring-method-auth hook) you should resolve all permissions assigned to a user - you can do this with call to auth-MS. </p>&#xA;&#xA;<h2>Example</h2>&#xA;&#xA;<p>We created CRUD permissions for <strong>TestResult</strong> entity - VIEW, EDIT, DELETE.</p>&#xA;&#xA;<p>The role DOCTOR can see any TestResults - so, it has VIEW permission</p>&#xA;&#xA;<p>The role PATIENT can see only his/her TestResults</p>&#xA;&#xA;<p>So, you create a business rules which provide the correct <strong><em>where clause</em></strong> for each business role (DOCTOR, PATIENT, LAB, etc.) and at the end the SQL request would be like:</p>&#xA;&#xA;<p>For patient who has assigned VIEW permission:</p>&#xA;&#xA;<pre><code>select * from test_result where id=*patient_id* and 1=1&#xA;</code></pre>&#xA;&#xA;<p>For patient who hasn't assigned VIEW permission:</p>&#xA;&#xA;<pre><code>select * from test_result where id=*patient_id* and 1!=1&#xA;</code></pre>&#xA;&#xA;<p><em>NOTE: In business rules we can add 1=1 or 1!=1 to permit/restrict query result</em></p>&#xA;"
50489906,50397998,4938460,2018-05-23T13:37:51,"<p>Although what you are asking for is possible, and would likely be achieved by passing some <code>sessionID</code> to your frontend and then passing it back with each request, this would violate the principle of a ""stateless service"" and is probably bad practice.</p>&#xA;&#xA;<p>Instead of maintaining user sessions on the backend, try to maintain all session-state on the client side, and have the client frontend interact with the backend only by means of RESTful calls. This will save you enourmous struggle later, when you want to scale your service and realize that tracking all the session IDs across clustered deployments of your service is a nightmare.</p>&#xA;"
48747217,44085454,2912220,2018-02-12T13:05:27,"<p>Best answer to such scenarios is CQRS. Maintain the materialized views of User and product relationship. Meterialized views could be anything which provides low latency in reading like NoSql DBs. Whenever Command updates User or Product by their microservices,the corresponding events should get generated and gets captured by other service managing this materializedView. This service will be used to fetch the query you are interested in.&#xA;   Always keep in mind,Microservices architecture believes in Eventual Consistency which definitely is not a bad thing :) .Hope this answers your question.</p>&#xA;"
48747869,29761872,2912220,2018-02-12T13:40:37,<p>CQRS---Command Query Aggregation Pattern is the answer to thi as per Chris Richardson.&#xA;   Let each microservice update its own data Model and generates the events which will update the materialized view having the required join data from earlier microservices.This MV could be any NoSql DB or Redis or elasticsearch which is query optimized. This techniques leads to Eventual consistency which is definitely not bad and avoids the real time application side joins.&#xA;   Hope this answers.</p>&#xA;
31226614,29731394,1931740,2015-07-05T02:06:15,"<p>I feel that you should merge these two services together to make it stand as a one service. Idea behind micro services architecture is that services should be able to deploy independent of each other and are completely autonomous. Individual services should be able to change/evolve their implementations without impacting their its clients. </p>&#xA;&#xA;<p>If it sounds like two services share the same code base, any change/improvement to it would result in deploying two services which are meant to be independent and deviate from the premise micro services architecture tries to establish.  </p>&#xA;"
46619515,46514002,7769052,2017-10-07T11:14:09,"<p>In <strong>BackendConnectionManager.java</strong>, line 57, you are trying to look up the service <code>java:global/ejb-one/PingFacade!io.project.core.interfaces.PingFacadeRemote</code>, but it seems to not exist. <code>PingFacadeRemote</code> interface is a part of <code>ejb-core</code> module. Yes, it is true that you implement it in <code>ejb-one</code> module, but you registered the interface as a remote (<code>@Remote(PingFacadeRemote.class)</code>, in <strong>PingFacade.java</strong>).</p>&#xA;&#xA;<p>You might try to replace it with <code>java:global/ejb-core/PingFacade!io.project.core.interfaces.PingFacadeRemote</code>.</p>&#xA;"
37088718,35913253,5419228,2016-05-07T12:45:37,"<p>Creating a Micro Service will definitely help you in future because of the following reasons below:</p>&#xA;&#xA;<ol>&#xA;<li><p><strong>Scalability</strong>:&#xA;Micro services can scale independently without disturbing other Micro Services used in the application.</p></li>&#xA;<li><p><strong>Agility</strong>:&#xA;Changes in a micro service can be very quickly, thus accommodating rapidly changing business scenarios.</p></li>&#xA;<li><p><strong>Fault Tolerance</strong>:&#xA;Even if one micro service fails, other micro services can keep servicing thus minimizing impact on the overall application</p></li>&#xA;<li><p><strong>Polyglot persistence</strong>:&#xA;Each micro service can decide its own type of Database mechanism.</p></li>&#xA;</ol>&#xA;&#xA;<p>I have covered advantages of micro services partially here, there are still more.</p>&#xA;&#xA;<p>Thus if you look at all the points above, is definitely worthy creating micro services application</p>&#xA;"
37128799,37084619,5419228,2016-05-10T03:31:50,"<p>Please see below:</p>&#xA;&#xA;<p>1.Create a Wrapper for the tasks meaning create a Task Queue and may be you can also create a service around it.</p>&#xA;&#xA;<p>2.Now when a instance of a particular Micro Service picks up a Task from the Task Queue, Pop that task out from the Queue.In that way you ensure that only 1 instance of a particular Micro Service services that task.</p>&#xA;"
37257840,37257673,5419228,2016-05-16T15:41:25,"<p>There is a Gateway Service called Netflix Zuul(You can also call it as edge Service).Client connect to gateway service which in turn queries Eureka Server to get appropriate Micro Service details.</p>&#xA;&#xA;<p>Hystrix  basically uses fault tolerance mechanism  which can be used in any Micro Service. Its advantage is that, if any API goes down ,it gracefully handles the errors in the Application.</p>&#xA;"
42982748,42981194,1035228,2017-03-23T17:15:22,"<p>Regarding #5, I really don't see that you are somehow committing a horrible violation of principles by merging into one service here (point 5), a kind of “UserManagementService” that takes care of both tasks; even if it can be argued that one MS should perform one task and do it well, these two are intimately related, IMHO. </p>&#xA;&#xA;<p>Another option is to make UserService a sync client of AuthService (load balanced, with circuit breaker, whatever, to ensure availability of one instance), which would be basically the same as injecting the dependency for a password validation in ""normal"" code. </p>&#xA;&#xA;<p>If this is a greenfield thing and you are not stuck with these two services, in general, I would go for a simpler first implementation, and resist the impulse for optimizations, as these tend to be premature, or a unwarranted commitment to the purity of principles/dogmas. You yourself are aware of the complexities you could introduce (2PC) plus you have your requirement there (""so we can't get back to user and tell him to correct the password"").</p>&#xA;"
51574315,51505849,6676444,2018-07-28T18:54:34,<p>i enabled basic authentication for spring boot admin server added the property in microservices </p>&#xA;&#xA;<pre><code>eureka.instance.metadata-map.user.name: &#xA;eureka.instance.metadata-map.user.password:&#xA;</code></pre>&#xA;&#xA;<p>now actuator endpoints are protected by basic authentication</p>&#xA;
46885136,42062199,4795613,2017-10-23T08:42:14,"<p>Reactive Programming is a style of micro-architecture involving intelligent routing and consumption of events.</p>&#xA;&#xA;<p>Reactive is that you can do more with less, specifically you can process higher loads with fewer threads.</p>&#xA;&#xA;<p>Reactive types are not intended to allow you to process your requests or data faster.Their strength lies in their capacity to serve more request concurrently, and to handle operations with latency, such as requesting data from a remote server, more efficiently.</p>&#xA;&#xA;<p>They allow you to provide a better quality of service and a predictable capacity planning by dealing natively with time and latency without consuming more resources. </p>&#xA;&#xA;<p>From<br>&#xA;<a href=""https://blog.redelastic.com/what-is-reactive-programming-bc9fa7f4a7fc"" rel=""nofollow noreferrer"">https://blog.redelastic.com/what-is-reactive-programming-bc9fa7f4a7fc</a>&#xA;<a href=""https://spring.io/blog/2016/06/07/notes-on-reactive-programming-part-i-the-reactive-landscape"" rel=""nofollow noreferrer"">https://spring.io/blog/2016/06/07/notes-on-reactive-programming-part-i-the-reactive-landscape</a>&#xA;<a href=""https://spring.io/blog/2016/07/28/reactive-programming-with-spring-5-0-m1"" rel=""nofollow noreferrer"">https://spring.io/blog/2016/07/28/reactive-programming-with-spring-5-0-m1</a></p>&#xA;"
50005377,42332938,213725,2018-04-24T15:22:58,"<p>Here is a nice summary how logical services differ from physical services: <a href=""https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/architect-microservice-container-applications/logical-versus-physical-architecture"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/architect-microservice-container-applications/logical-versus-physical-architecture</a> </p>&#xA;&#xA;<p>Now, in relation to Service Fabric, Service Fabric applications represent logical services while Service Fabric services represent physical services. To simplify it, a Service Fabric application is a deployment unit, so you would put there multiple services that rely on the same persistent storage or have other inter-dependencies so that you really need to deploy them together. If you have totally independent services, you would put them into different Service Fabric applications.</p>&#xA;"
45168754,45168622,22237,2017-07-18T13:50:10,"<p>I'm not sure if there might be a better solution in terms of structure of the services themselves, for example one of the two services could have an API which returns the constants for the other to use.</p>&#xA;&#xA;<p>But to answer the question you could use an engine, or more likely for simple sharing of constants, a gem. You can create a new gem with <code>bundle gem &lt;GEM NAME&gt;</code> and then add it to the Gemfile of both apps.</p>&#xA;&#xA;<p>You will need to either have a gem server (e.g. <a href=""https://github.com/geminabox/geminabox"" rel=""nofollow noreferrer"">geminabox</a>) or just point directly to your code repo, e.g.</p>&#xA;&#xA;<pre><code>gem 'my-gem', git: 'git@my-server.com:git/my-gem`&#xA;</code></pre>&#xA;&#xA;<p>Personally speaking I'd go with the API returning the constants because you might want to rewrite one service in another language in which case sharing a gem falls down.</p>&#xA;"
25982865,25824957,885981,2014-09-22T20:47:36,"<p>I think there arn't any typical patterns. It depends on your business processes and relations between entities. But there are some general statements.</p>&#xA;&#xA;<ol>&#xA;<li>'Entity' services must be agnostic to all business processes. </li>&#xA;<li>Functional context of 'Entity' services must not overlap.</li>&#xA;<li>There should be no redundant logic in services implementations. </li>&#xA;<li>No need to create new service for each entity if business processes don't require it.</li>&#xA;<li>If there is a strong subordinate relation between entities, thus it's possible to introduce aggregate containing all tied entities and new service operating with it.</li>&#xA;</ol>&#xA;&#xA;<p>You may formally check most of these assertions against your solution.</p>&#xA;&#xA;<p>Moreover, it's always possible to split some service into two new ones. But more effort you make during analysis stage the less likely you will need to split services one time.</p>&#xA;"
51939847,51923607,889053,2018-08-20T23:48:12,"<p>The way I would approach this would be to be a <code>Chain of Responsibilities</code> where each node is responsible for checking a condition or doing a job</p>&#xA;&#xA;<p>For example, your first phase is a logical decision: </p>&#xA;&#xA;<ol>&#xA;<li>""new commit?"" {true|false} D</li>&#xA;<li>If true, ""do stuff"" stop? (visitor ?)</li>&#xA;</ol>&#xA;&#xA;<p>So, each node has the potential to execute a terminating function (which may/may not return results; the functionalist in me says it should). </p>&#xA;&#xA;<p>From a coding perspective, it might look like this:</p>&#xA;&#xA;<pre><code>interface Responsibility&lt;I&gt; {&#xA;    fun apply(context:I) : Unit?&#xA;}&#xA;&#xA;class GitRepo{&#xA;    val files = setOf&lt;URI&gt;()&#xA;    val commiters = setOf&lt;String&gt;()&#xA;    fun hasNewCommit() = true&#xA;&#xA;}&#xA;&#xA;abstract class Chained&lt;I,N&gt;(protected val next:Responsibility&lt;N&gt;) : Responsibility&lt;I&gt; {&#xA;&#xA;&#xA;}&#xA;class CheckCommited(next:Responsibility&lt;Set&lt;URI&gt;&gt;) : Chained&lt;GitRepo,Set&lt;URI&gt;&gt;(next) {&#xA;    override fun apply(gr:GitRepo) = when(gr.hasNewCommit()) {&#xA;        true -&gt; next.apply(gr.files)&#xA;        false -&gt; null&#xA;    }&#xA;}&#xA;class DownLoadFiles(next:Responsibility&lt;Set&lt;URL&gt;&gt;) : Chained&lt;Set&lt;URI&gt;,Set&lt;URL&gt;&gt;(next) {&#xA;    override fun apply(uris:Set&lt;URI&gt;) {&#xA;        next.apply(uris.map { downLoad(it) }.toSet())&#xA;    }&#xA;    fun downLoad(uri:URI): URL {&#xA;        return uri.toURL()&#xA;    }&#xA;}&#xA;&#xA;class NotifyPeople(val people: Set&lt;String&gt;) : Responsibility&lt;Set&lt;URL&gt;&gt; {&#xA;&#xA;    override fun apply(context: Set&lt;URL&gt;): Unit? {&#xA;        people.forEach {  context.forEach { sendMail(it.toString()) }}&#xA;    }&#xA;&#xA;    private fun sendMail(email: String) {&#xA;&#xA;    }&#xA;}&#xA;&#xA;val root = CheckCommited(DownLoadFiles(NotifyPeople(setOf(""joe@company.com"",""suzy@someplace.other""))))&#xA;</code></pre>&#xA;"
38655460,38647186,2463453,2016-07-29T09:38:44,<p>The answer to this question is quite simple:</p>&#xA;&#xA;<ul>&#xA;<li>Don't try to build a distributed monolith: don't build a persistance layer as a microservice.</li>&#xA;<li>Don't try to perform distributed transaction and try to have a functional approach by having a domain bound service.</li>&#xA;<li>You will have to include Hibernate in several microservices. This is a tradeoff you will have to accept.</li>&#xA;</ul>&#xA;
38976381,38974070,2463453,2016-08-16T13:33:28,"<p>This question is not specific to Zookeeper so these principles apply to every implementation (Eureka, Consul, Zookeeper, etc...)</p>&#xA;&#xA;<p>Service registry is the registry where all services register themselves, giving a way to contact them (public API, IP, port).</p>&#xA;&#xA;<p>Service Discovery is the action of retrieving a specific service for your need, basically asking the service registry to get an URL to call.</p>&#xA;"
44695015,44692737,4894173,2017-06-22T09:08:36,"<p>Go through this link:&#xA;<a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-debugging-your-application#debug-a-remote-service-fabric-application"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-debugging-your-application#debug-a-remote-service-fabric-application</a></p>&#xA;&#xA;<p>Thanks,    </p>&#xA;"
51548195,51542629,263902,2018-07-26T22:11:59,"<p>I found the problem.  The existing services depend on the new service, but the new service is not deployed till the existing services are successfully upgraded which they never are as the existing services are waiting on the new (undeployed) service.</p>&#xA;&#xA;<p><a href=""https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-application-upgrade"" rel=""nofollow noreferrer"">Documented here</a></p>&#xA;"
37926534,37693542,1122270,2016-06-20T15:40:42,"<p>Yes, you can certainly do it!</p>&#xA;&#xA;<p>You can terminate https with nginx, and still have the backend operate on either plain http or even https still.  The <a href=""http://nginx.org/r/proxy_pass"" rel=""nofollow""><code>proxy_pass</code></a> directive does support both access schemes for the upstream content.  You can also use the newer TCP stream proxying, if necessary.</p>&#xA;&#xA;<p>There are not that many caveats, really.  It usually just works.</p>&#xA;"
51009783,50935415,5715934,2018-06-24T12:06:08,"<p>Your single question has different separate questions inside it.</p>&#xA;&#xA;<p><strong>(1) Can a micro-services system survive without aid of a message broker system?</strong>   </p>&#xA;&#xA;<p>Yes, it definitely can.</p>&#xA;&#xA;<p><strong>(2) Can a micro-services system with an authentication/authorization (identity server) layer still survive without aid of a message broker system?</strong>   </p>&#xA;&#xA;<p>Yes, Still it can.</p>&#xA;&#xA;<p><strong>(3) Then, why on earth people use message brokers to communicate among micro-services?</strong></p>&#xA;&#xA;<p>Message brokers (Message Queues) is a proven <strong><em>clean</em></strong> way to implement communication (enterprise integration) among sub services. </p>&#xA;&#xA;<p>What does it mean by <strong><em>clean</em></strong>?</p>&#xA;&#xA;<ul>&#xA;<li>Asynchronous Communication (No waiting)</li>&#xA;<li>Can easily scale out different services (No JVM dependencies, can deploy different services in different servers)</li>&#xA;<li>Can manage events centrally (just another pub sub)</li>&#xA;<li>Scalability (Easy to add another service which needs to communicate with some existing services)</li>&#xA;</ul>&#xA;&#xA;<p><strong>(4) Now, what are the other alternatives without using message queues?</strong></p>&#xA;&#xA;<ul>&#xA;<li>Simple re-direction (good alternative, if you hardly try to scale your system further)</li>&#xA;<li><a href=""https://docs.oracle.com/javase/tutorial/sound/SPI-intro.html"" rel=""nofollow noreferrer"">Service Provider Interfacing</a> (SPI) (Again good for small scale, but you are trapped with same JVM sharing limitation)</li>&#xA;</ul>&#xA;"
51115149,51026313,5715934,2018-06-30T13:00:09,"<p>The basic behind url naming is <a href=""https://en.wikipedia.org/wiki/Resource-oriented_architecture"" rel=""nofollow noreferrer"">Resource Oriented Architecture</a> (ROA). i.e. Resource Hierarchy.</p>&#xA;&#xA;<p>Let's take an example from your question. You have a user service. I assume that there you take <code>user</code> as the <code>root resource</code>. </p>&#xA;&#xA;<p>So Let's define what a user looks like. For an example I'll take it like this.</p>&#xA;&#xA;<pre><code>Car&#xA;&#xA;     | - code&#xA;&#xA;     | - name&#xA;&#xA;     | - cars&#xA;</code></pre>&#xA;&#xA;<p>Here I assume that a user can have a <strong>code</strong>, <strong>name</strong> and <strong>some cars</strong> (can be more than one).</p>&#xA;&#xA;<p>And now let's see how we can see a car.</p>&#xA;&#xA;<pre><code>User &#xA;&#xA;     | - number&#xA;&#xA;     | - make&#xA;&#xA;     | - model&#xA;&#xA;     | - year&#xA;</code></pre>&#xA;&#xA;<p>Now you can define a json object for a user like this.</p>&#xA;&#xA;<pre><code>{&#xA;    ""code"": ""001A"",&#xA;    ""name"": ""alice"",&#xA;    ""cars"": [&#xA;        {&#xA;            ""number"": ""ab123"",&#xA;            ""make"": ""toyota"",&#xA;            ""model"": ""corolla"",&#xA;            ""year"": 2015&#xA;        },&#xA;        {&#xA;            ""number"": ""we345"",&#xA;            ""make"": ""nissan"",&#xA;            ""model"": ""sunny"",&#xA;            ""year"": 2017&#xA;        }&#xA;    ]&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>So if I wanna provide an endpoint to retrieve all the users I would give an endpoint like this.</p>&#xA;&#xA;<pre><code>/api/user-service/users&#xA;</code></pre>&#xA;&#xA;<p>Note that <strong><em>users</em></strong> (-- plural), simply because I can have more than one user. It would produce a response like this..</p>&#xA;&#xA;<pre><code>[&#xA;    {&#xA;        ""code"": ""001A"",&#xA;        ""name"": ""alice"",&#xA;        ""cars"": [&#xA;            {&#xA;                ""make"": ""toyota"",&#xA;                ""model"": ""corolla"",&#xA;                ""year"": 2015&#xA;            },&#xA;            {&#xA;                ""make"": ""nissan"",&#xA;                ""model"": ""sunny"",&#xA;                ""year"": 2017&#xA;            }&#xA;        ]&#xA;    },&#xA;    {&#xA;        ""code"": ""001B"",&#xA;        ""name"": ""bob"",&#xA;        ""cars"": [&#xA;            {&#xA;                ""make"": ""toyota"",&#xA;                ""model"": ""yaris"",&#xA;                ""year"": 2016&#xA;            },&#xA;            {&#xA;                ""make"": ""bmw"",&#xA;                ""model"": ""318i"",&#xA;                ""year"": 2017&#xA;            }&#xA;        ]&#xA;    }&#xA;]&#xA;</code></pre>&#xA;&#xA;<p>And if we need an endpoint to retrieve data for a specific user, I would give and endpoint like this.</p>&#xA;&#xA;<pre><code>/api/user-service/users/{user-code}&#xA;</code></pre>&#xA;&#xA;<p>Example</p>&#xA;&#xA;<pre><code>/api/user-service/users/001A&#xA;</code></pre>&#xA;&#xA;<p>So it would produce a response like this.</p>&#xA;&#xA;<pre><code>{&#xA;    ""code"": ""001A"",&#xA;    ""name"": ""alice"",&#xA;    ""cars"": [&#xA;        {&#xA;            ""number"": ""ab123"",&#xA;            ""make"": ""toyota"",&#xA;            ""model"": ""corolla"",&#xA;            ""year"": 2015&#xA;        },&#xA;        {&#xA;            ""number"": ""we345"",&#xA;            ""make"": ""nissan"",&#xA;            ""model"": ""sunny"",&#xA;            ""year"": 2017&#xA;        }&#xA;    ]&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>And if we want an endpoint to retrieve all the cars of a given user, it would be like this.</p>&#xA;&#xA;<pre><code>/api/user-service/users/{user-code}/cars&#xA;</code></pre>&#xA;&#xA;<p>Note that <strong>cars</strong> (--plural)</p>&#xA;&#xA;<p>Example</p>&#xA;&#xA;<pre><code>/api/user-service/users/001A/cars&#xA;</code></pre>&#xA;&#xA;<p>So it would produce a response like this.</p>&#xA;&#xA;<pre><code>[&#xA;    {&#xA;        ""number"": ""ab123"",&#xA;        ""make"": ""toyota"",&#xA;        ""model"": ""corolla"",&#xA;        ""year"": 2015&#xA;    },&#xA;    {&#xA;        ""number"": ""we345"",&#xA;        ""make"": ""nissan"",&#xA;        ""model"": ""sunny"",&#xA;        ""year"": 2017&#xA;    }&#xA;]&#xA;</code></pre>&#xA;&#xA;<p>I hope you get the basic understanding about REST url naming conventions.</p>&#xA;"
51066422,51008734,5715934,2018-06-27T15:22:07,"<p>One thing to mention is that your question is too broad. </p>&#xA;&#xA;<p>Anyway I hope your whole question wraps up asking answers for these 2 questions.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>(1) Caching a response (that was computed for a particular request) and serving that response (without re-computing) over and over again for identical requests that come afterwards.</p>&#xA;&#xA;<p>(2) Caching a computed value (which was computed to serve a particular request) and reuse that value (without computing) to serve subsequent requests which need that value as a part of its computation.</p>&#xA;&#xA;<p>And you need to do it in a multi-node system.</p>&#xA;&#xA;<p>Yes. There are answers for both of your questions.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>(1) <strong><em>HTTP Caching</em></strong></p>&#xA;&#xA;<p>So you doubt about the possibility to tackle the multi node environment. Actually HTTP caching is <strong>only applicable</strong> for <strong><em>intermediate</em></strong> servers(Load Balances,CDNs etc) and browsers(or mobile) <strong>not individual nodes</strong>. </p>&#xA;&#xA;<p>Simply you can configure your caching requirement and serve responses back from cache(intermediate nodes) even before the requests go into an end server node. You might need to add some code to your application as well. </p>&#xA;&#xA;<p>Apart from simple response caching, there are lots of other out of the box features as well. For caching purpose, you would need to use a cache supporting server application (like <a href=""https://www.nginx.com/"" rel=""nofollow noreferrer"">nginx</a>) for your intermediate servers(I hope in your scenario, its your load balancer). Anyway most of them support for <strong>GET</strong> requests by <strong>default</strong>. But there are &#xA;some work around to do to support <strong>POST</strong> as well, depending on the product you select.</p>&#xA;&#xA;<p>And you also need to configure HTTP cache headers as well. It's impossible to mention all the bits and pieces about HTTP caching in a SO answer. Anyway <a href=""https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/http-caching"" rel=""nofollow noreferrer"">this</a> is a really good read about HTTP caching published by <strong>Google</strong>. And there are other web resources too.</p>&#xA;&#xA;<p>(2) <strong><em>A Cache DB</em></strong></p>&#xA;&#xA;<p>You can compute a particular value and store inside a cache db (which is centrally accessible to all the nodes). So that you can add the cache looking up logic to your code before doing a particular computation.</p>&#xA;&#xA;<p>There are lots of in memory caching db applications which serve your requirement.</p>&#xA;&#xA;<p>Ex: <a href=""https://redis.io/"" rel=""nofollow noreferrer"">redis</a>, <a href=""https://hazelcast.com/"" rel=""nofollow noreferrer"">Hazelcast</a></p>&#xA;&#xA;<hr>&#xA;&#xA;<p>I hope this wraps up what you were looking for.</p>&#xA;"
50781129,50779254,5715934,2018-06-10T06:31:42,"<p>Breaking down microservices doesn't have any hard or fast rule. It depends on several factors. In my experience, it mostly relies on these two key factors. </p>&#xA;&#xA;<ul>&#xA;<li>dependencies point of view</li>&#xA;<li>business point of view</li>&#xA;</ul>&#xA;&#xA;<p>Let's revisit our idea of breaking down microservices. We would like to deploy our services as independent modules in such a way, each one's availability is independent. So we are working to achieve the goal of, one service down means only that one is unavailable at that time. Others are available.</p>&#xA;&#xA;<p>Now, let's take two factors, I have mentioned.</p>&#xA;&#xA;<p>(1) <strong><em>Dependencies</em></strong>(models, libraries, dbs etc)</p>&#xA;&#xA;<p>Let's take a simple scenario, just to grasp the idea. Imagine there is an online retailer platform. There we have a set of independent(segregated) db tables to store data of stock related to apparels. And it is in a such way that, it is harder to break down further (ex: different segregation for women clothes, men clothes etc). </p>&#xA;&#xA;<p>So here, when we are going to develop api services, we can take a decision to provide a separate microservice for apparels (since we have a set of independently segregated tables for that section), but not break down any further as sections of apparel (since can't segregate those tables any more). Note that here, since we have a separate db segregation for apparels, any db failure for that segregation will not affect any other service (and vice-versa). And just think what would happen if we broke down services further (separate microservices for women clothes/men clothes etc). Since we have one common db segregation for all, if that common dependency gone down, all the services will go down. So no meaning of trying hard to break further since no gain of availability.</p>&#xA;&#xA;<p>(2) <strong><em>Business</em></strong></p>&#xA;&#xA;<p>I will take the above example here as well. Let's take business operations like client payments. Since it is a critical part of the business, most of the time, we can deduce that we are going to make a separate microservice for payments. Few minutes of unavailability of client payments, can cost the business millions. So we would like to make it as a separate independent microservice and give it a special attention when it comes to deployment.</p>&#xA;&#xA;<p>Finally,since you have mentioned about facebook, I guess they would have these kinds of microservices break down.</p>&#xA;&#xA;<ul>&#xA;<li>Authentication/Authorization</li>&#xA;<li>User Data Management</li>&#xA;<li>Media Content Management</li>&#xA;<li>Messaging</li>&#xA;<li>Calls</li>&#xA;<li>Payments</li>&#xA;<li>Push Notification Service</li>&#xA;<li>Big Data Platform etc.</li>&#xA;</ul>&#xA;"
44065288,44063886,4473822,2017-05-19T08:40:40,"<p>You have many options but I will list 3 of them that I would choose from:</p>&#xA;&#xA;<ol>&#xA;<li>Directly call the other microservice using REST calls (hard code the URL in a property file or create a service for inter service communication).</li>&#xA;<li>Setup microservice architecture with <a href=""http://projects.spring.io/spring-cloud/"" rel=""nofollow noreferrer"">spring-cloud</a> and <a href=""https://cloud.spring.io/spring-cloud-netflix/"" rel=""nofollow noreferrer"">netflix OSS</a></li>&#xA;<li>Setup microservice architecture with <a href=""http://projects.spring.io/spring-cloud/"" rel=""nofollow noreferrer"">spring-cloud</a> and <a href=""https://kubernetes.io/"" rel=""nofollow noreferrer"">kubernetes</a></li>&#xA;</ol>&#xA;"
45952589,36461493,4937438,2017-08-30T05:32:10,"<p>Here are the steps to do it with @ControllerAdvice:</p>&#xA;&#xA;<ol>&#xA;<li>First add a filter of type <code>error</code> and let it be run before the <code>SendErrorFilter</code> in zuul itself.</li>&#xA;<li>Make sure to remove the key associated with the exception from the <code>RequestContext</code> to prevent the <code>SendErrorFilter</code> from executing.</li>&#xA;<li>Use <code>RequestDispatcher</code> to forward the request to the <code>ErrorController</code> -- explained below.</li>&#xA;<li>Add a @RestController class and make it extends <code>AbstractErrorController</code>, and re-throw the exception again (add it in the step of executing your new error filter with (key, exception), get it from the <code>RequestContext</code> in your controller).</li>&#xA;</ol>&#xA;&#xA;<p>The exception will now be caught in your @ControllerAdvice class.</p>&#xA;"
48387453,48209566,1472222,2018-01-22T17:49:48,"<p>You're asking a specific question about a very abstract example. In practice, such questions rarely yield useful results.  </p>&#xA;&#xA;<p>That being said, if your system is not in a state where it can process requests in a reliable and deterministic manner (e.g. a queue component is non-functional), then you should block all requests that rely upon that functionality (i.e. using a 503 - service unavailable). In reality, it wouldn't matter if this was a queue problem or a database problem - the service cannot execute its behavior unless all parts are functioning, so tell clients up-front. If your reliability needs to be higher, you'll have to invest more time in the design to achieve higher reliability.</p>&#xA;"
49407842,48153334,4636126,2018-03-21T13:37:27,"<p>You can integrate Axibase Time Series Database with AWS Route53 health checks via CloudWatch to create a consolidated view of the microservices as described <a href=""https://axibase.com/use-cases/integrations/aws/route53-health-checks/#availability-portal"" rel=""nofollow noreferrer"">here</a>. </p>&#xA;&#xA;<p>Container-based microservices tend to have short lifespans, so we recommend to reduce AWS CloudWatch job frequency to 5 minutes compared to the default 15 minutes. Similarly, given that Health Checks are likely to be created and removed on short notice, it's better to copy their attributes to the database every 5 minutes.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/jtTvN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jtTvN.png"" alt=""ATSD Route 53 portal""></a></p>&#xA;&#xA;<p>Disclaimer: I work for Axibase.</p>&#xA;"
47146544,47143597,8896222,2017-11-06T21:58:05,"<p>I suggest using <code>spring-boot-starter-web</code> instead of <code>tomcat-embed-jasper</code>. You can use multiple starters in your <code>pom.xml</code>.</p>&#xA;&#xA;<pre><code>&lt;dependency&gt;&#xA;  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&#xA;  &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&#xA;&lt;/dependency&gt;&#xA;</code></pre>&#xA;&#xA;<p>See <a href=""http://spring.io/guides/gs/rest-service/"" rel=""nofollow noreferrer"">spring.io</a> for more info.</p>&#xA;"
48707127,48699742,1503710,2018-02-09T13:37:02,"<p>First, there is more than one approach for this common problem.</p>&#xA;&#xA;<p>Here is one popular take:</p>&#xA;&#xA;<p>Divide your world to authentication (you are who you say you are) and authorization (you are permitted to do this action).</p>&#xA;&#xA;<p>As a policy, every service decides on authorization by itself. Leave the authentication to a single point in the system - the authentication gateway - usually combined inside the API gateway. </p>&#xA;&#xA;<p>This gateway forwards requests from clients to the services, after authenticating, with a trusted payload stating that the requester is indeed who they say they are. Its then up to the service to decide whether the request is allowed.</p>&#xA;&#xA;<p>An implementation can be done using several methods. One method is using JWT. </p>&#xA;&#xA;<p>The authenticator creates a JWT after receiving correct credentials, and the client uses this JWT in every request to each service. </p>&#xA;"
31265475,31047564,128734,2015-07-07T09:59:50,"<p>This apparently can't be done easily at the moment.</p>&#xA;&#xA;<p>My research brings me to the conclusion the only way is to use external tools:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://www.google.com/url?q=https%3A%2F%2Fblog.newrelic.com%2F2015%2F05%2F06%2Fservice-maps-ga%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFE1wcDqVmuHRZ2UopQA6NvuukIyQ"" rel=""noreferrer"">New Relic Service Map</a> - Commercial. Probably only diagramming <strong>running</strong> microservice systems, because it's a monitoring system.</li>&#xA;<li><a href=""http://www.google.com/url?q=http%3A%2F%2Fblog.appdynamics.com%2Fdevops%2Fvisualizing-and-tracking-your-microservices%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFCTE8fqtcF4Cf25sVxepT45vLgpA"" rel=""noreferrer"">AppDynamics</a> - Commercial. Probably only diagramming <strong>running</strong> microservice systems, because it's a monitoring system.</li>&#xA;<li><a href=""https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2Fadrianco%2Fspigo&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGtwqU52EJVOzlAWuDNU5DplUysCA"" rel=""noreferrer"">Spigo</a> - Open source. One could parse Spring Integration XML configs, generate input for Spigo, and generate a graph.</li>&#xA;<li><a href=""https://github.com/4finance/micro-infra-spring/wiki/Service-discovery"" rel=""noreferrer"">micro-infra-spring</a> - Open source. For diagramming <strong>running</strong> microservice systems. Requires ZooKeeper. </li>&#xA;</ul>&#xA;"
39590137,39589556,601311,2016-09-20T09:15:48,"<p>I found the root cause to this issue, the <strong>com.fasterxml.jackson.core.jackson-databind</strong> that used in my project was outdated (version 2.1.1). While the eureka client needs minimum 2.5.4 version. </p>&#xA;"
43650469,43378986,4664543,2017-04-27T06:47:12,"<p>OpenLDAP has everything needed to accomplish this. You can lookup a user via its email address and then you can authenticate the user further. Wso2is has everything needed to make this work. It is just simple configuration. Information can be found <a href=""https://docs.wso2.com/display/IS510/Using+Email+Address+as+the+Username"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;"
49973762,49973699,5728991,2018-04-23T05:09:35,"<p>Use a <a href=""https://golang.org/ref/spec#Type_switches"" rel=""nofollow noreferrer"">type switch</a> in the main package:</p>&#xA;&#xA;<pre><code>switch req := network.GetRequestFromBytes(conn).(type) {&#xA;case *network.RequestA:&#xA;   // handle here, req has type *RequestA in this branch&#xA;case *network.RequestB:&#xA;   // handle here, req has type *RequestB in this branch&#xA;default:&#xA;   // handle unknown request type&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>This avoids encoding knowledge of the protocol bytes in the main package.</p>&#xA;&#xA;<p>Because the question specifies that parsing code is in the network package and handler code is in the main package, it's not possible to avoid updating the code in two packages when a new request type is added. Even if the language supported the feature proposed in the question, it would still be necessary to update two packages when a new request type is added.</p>&#xA;"
51036656,51035930,1076703,2018-06-26T06:57:19,"<p>for versioning I can Use bellow any one approach</p>&#xA;&#xA;<p>1.Through a URI path – you include the version number in the URL path of the endpoint, for example, /api/v1/trade.&#xA;2.Through query parameters – you pass the version number as a query parameter with a specified name, for example, /api/trade?version=1.&#xA;3.Through custom HTTP headers – you define a new header that contains the version number in the request.&#xA;4.Through a content negotiation – the version number is included in the “Accept” header together with the accepted content type.<br>&#xA;1) different URL Mapping Something Like bellow &#xA; Version 1:-  </p>&#xA;&#xA;<pre><code>`http://localhost:8080/v1/trade`&#xA;</code></pre>&#xA;&#xA;<p>Version 2:- </p>&#xA;&#xA;<pre><code> `http://localhost:8080/v2/trade`&#xA;&#xA;&#xA;&#xA; @RestController&#xA;public class TradeController {&#xA;&#xA;  @GetMapping(""v1/trade"")&#xA;  public Trade tradeVersionOne() {&#xA;    return new Trade(""123"",""Trade Result"");&#xA;  }&#xA;&#xA;  @GetMapping(""v2/trade"")&#xA;  public Trade tradeVersionTwo() {&#xA;    return new Trade(new RealTimeTrade(""123"", ""Real Time Trade Result""));&#xA;  }&#xA;</code></pre>&#xA;&#xA;<p>2) Query parameters use in URL &#xA; Version 1:- &#xA;    <a href=""http://localhost:8080/trade/param?version=1"" rel=""nofollow noreferrer"">http://localhost:8080/trade/param?version=1</a></p>&#xA;&#xA;<p>Version 2:- &#xA;    <a href=""http://localhost:8080/trade/param?version=2"" rel=""nofollow noreferrer"">http://localhost:8080/trade/param?version=2</a></p>&#xA;&#xA;<pre><code>@RestController&#xA;public class TradeController {&#xA;&#xA;  @GetMapping(""v1/trade"" params = ""version=1"")&#xA;  public Trade tradeVersionOne() {&#xA;    return new Trade(""123"",""Trade Result"");&#xA;  }&#xA;&#xA;  @GetMapping(""v2/trade"" params = ""version=2"")&#xA;  public Trade tradeVersionTwo() {&#xA;    return new Trade(new RealTimeTrade(""123"", ""Real Time Trade Result""));&#xA;  }&#xA; }&#xA;</code></pre>&#xA;"
50835817,50835738,1076703,2018-06-13T11:14:19,<p>divide site into small bucket Module list. then as per the microservice patterns develop the Module one by one / part by part.</p>&#xA;
37235935,37180375,3894751,2016-05-15T08:11:53,"<p>You could try setting <code>setSendZuulResponse(false)</code> in the current context. This should not route the request. You could also call <code>removeRouteHost()</code> from the context, which would achieve the same. You could use<code>setResponseStatusCode</code> to set the 401 status code.</p>&#xA;"
47934098,41262716,5679071,2017-12-21T23:29:57,"<p>The right way to do this with AWS API Gateway would be with the recently launched 'VPC Link' integration, which secures the connection between API Gateway and your backend inside your VPC. </p>&#xA;&#xA;<p><a href=""https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-api-gateway-supports-endpoint-integrations-with-private-vpcs/"" rel=""nofollow noreferrer"">https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-api-gateway-supports-endpoint-integrations-with-private-vpcs/</a></p>&#xA;"
41424000,41400158,7188703,2017-01-02T09:16:42,"<h2>Docker</h2>&#xA;&#xA;<p>Docker containers wrap a piece of software in a complete filesystem that contains everything needed to run: code, runtime, system tools, system libraries – anything that can be installed on a server. This guarantees that the software will always run the same, regardless of its environment.</p>&#xA;&#xA;<p>Containers and virtual machines have similar resource isolation and allocation benefits -- but a different architectural approach allows containers to be more portable and efficient.&#xA;<strong>Virtual machines</strong> include the application, the necessary binaries and libraries, and an entire guest operating system -- all of which can amount to tens of GBs. <strong>Docker containers</strong> include the application and all of its dependencies --but share the kernel with other containers, running as isolated processes in user space on the host operating system. Docker containers are not tied to any specific infrastructure: they run on any computer, on any infrastructure, and in any cloud.</p>&#xA;&#xA;<h2>Bitnami</h2>&#xA;&#xA;<p><a href=""https://bitnami.com/learn_more"" rel=""nofollow noreferrer"" title=""Bitnami"">Bitnami</a> is an app library for server software. You can install your favorite applications on your own servers or run them in the cloud. &#xA;One of the platforms on which to deploy these applications are using Docker Containers. Virtual machines are another technology where applications can be deployed.</p>&#xA;&#xA;<p><a href=""https://bitnami.com/containers"" rel=""nofollow noreferrer"" title=""Bitnami containers"">Bitnami containers</a> give you the latest stable versions of your application stacks, allowing you to focus on coding rather than updating dependencies or outdated libraries. Available as development containers, turnkey application and infrastructure containers, or build your own custom container using Stacksmith.</p>&#xA;"
51934542,51934410,1753823,2018-08-20T16:02:24,<p>add this under your  section at the end of your pom.xml. you may need to add for all the dependencies.</p>&#xA;&#xA;<pre><code>&lt;dependency&gt;&#xA;  &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;&#xA;  &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;&#xA;  &lt;version&gt;what-ever-version-number&lt;/version&gt;&lt;/dependency&gt;&#xA;</code></pre>&#xA;
52048025,52008034,3069919,2018-08-27T23:06:04,"<p>In Amazon Neptune, there is no way to have ACLs for a portion of a graph at the moment. You can have IAM users who have full access to a cluster or no access at all. (Allow All or Deny All). You would need to handle this at application layer. Fine grained access control would be a good feature to have, so you may  want to place a feature request for that (via AWS Forums, for example). </p>&#xA;&#xA;<p>If you rule out access control, and the only thing you need is to make micro services not impact each other, then you can create read replicas, and use that them in your micro services (whether sharing database across micro services is a good choice or not is a separate discussion). Two approaches there are:</p>&#xA;&#xA;<ol>&#xA;<li>Add enough replicas in your cluster and use the cluster-ro (reader) endpoints in your read only micro services. All micro services would share the read replicas, but with DNS round robin.</li>&#xA;<li>Add replicas for various use cases, and then use specific instance endpoints with specific micro services. The micro services would not impact each other, however, a drawback with this approach would be that your instance can get promoted to master in the event of crashes and that may be something that you'd need to handle or be ready for.</li>&#xA;</ol>&#xA;"
51627420,51623293,1235935,2018-08-01T07:15:03,"<p>Anything that you cannot afford to lose (usually the business data) will go in DB and can be parallelly cached in an in-memory DB like Redis that has a cache eviction algorithm inbuilt.</p>&#xA;&#xA;<p>Anything that, if lost, is not a big deal (usually the technical things that are not directly linked with the business data) can go only in an in-memory DB.</p>&#xA;&#xA;<p>Since you are using Spring, you could probably use something like Redis with Spring Data Redis. There are already known Spring solutions (such as <a href=""https://stackoverflow.com/questions/29003786/how-to-disable-redis-caching-at-run-time-if-redis-connection-failed"">this</a>) to fall back on api calls to fetch data from DB if the Redis server goes down. You can also run multiple Redis instances behind Redis Sentinel to provide failover. Redis Cluster provides a way to run a Redis installation where data is automatically sharded across multiple Redis nodes. Also, you can configure Redis to persist the data in file system once daily or so to backup the cache data for disaster recovery. </p>&#xA;"
49881877,49880941,1235935,2018-04-17T15:17:51,"<p>Even when you use Spring Cloud, 100 services do NOT mean 100 servers. In Spring Cloud the packaging unit is Spring Boot application and a single server may host many such Spring Boot applications. If you want, you can containerize the Spring Boot applications and other Spring Cloud infrastructure support components. But that is not Kubernetes.</p>&#xA;&#xA;<p>If you move to Kubernetes, you don't need the infrastructure support services like Zuul, Ribbon etc. because Kubernetes has its own components for service discovery, gateway, load balancer etc. In Kubernetes, the packaging unit is Docker images and one or more Docker containers can be put inside one pod which is the minimal scaling unit. So, Kubernetes has a different set of components to manage the Microservices.</p>&#xA;&#xA;<p>Kubernetes is a different platform than Spring cloud. Both have the same objectives. However, Kubernetes has some additional features like self healing, auto-scaling, rolling updates, compute resource management, deployments etc.</p>&#xA;"
48704042,48699765,1235935,2018-02-09T10:42:31,"<p>The UI shouldn't be bundled with the REST services in a true Microservices based architecture. Because if the UI is bundled with APIs, for every defect fix in the UI, all the APIs need to be rebuilt and deployed. The UI needs to be hosted separately. If the team feels comfortable with Spring Boot, the Angular UI can be bundled in a separate Spring Boot application that doesn't have any API.</p>&#xA;&#xA;<p><strong>Update on 21-Mar-2018</strong>&#xA;I understand that bundling the Angular GUI with Angular Universal in a pm2 server is a better approach.</p>&#xA;"
50147313,50139640,1235935,2018-05-03T05:07:26,"<p>Microservices is an architectural pattern with its own pros and cons. That doesn't mean that everything else (other than Microservices Architecture) is a monolith. There is SOA also and others. So, we need to check the pros and cons of Microservices and if Microservices can offer us (in a project) something that the existing architecture cannot, we may consider implementing Microservices. </p>&#xA;&#xA;<p>So, yes - the microservices need to map with business domains and sub-domains and functionalities so that each of these services can be scaled separately, each have a separate DevOps cycle independent of others.</p>&#xA;&#xA;<p>The negative point is, Microservices architecture is complex with a lot of moving parts. So, it is used only when the advantages outweighs the disadvantages.</p>&#xA;"
47662596,45212459,4916192,2017-12-05T21:01:45,"<p>Micro-service theory says, you shouldn't do that. But I think, from practical point of view, it is often necessary. At least, if you want to ensure some level of governance across the organization. Because, yes, most of the enums are primarily owned by some microservices or bounded contexts. But it doesn't mean, you don't need to use them in others...Take for example mobile banking...You want to open a banking account via mobile application (with its own backend). This mobile app is communicating (via its own backend) (directly or through some messaging layer) with core banking system. &#xA;Your mobile app needs to know the product code(among lot of other things)of an account you want to open, and of course, core banking system needs to know it as well (it is in the domain which masters this information). &#xA;Also, I don't think that sharing enums via library is the only way. You can read the enums dynamically via the REST API , or messages. It is always a trade-off of what you need and what is the cost of doing that. But I don't agree, that not-sharing is always the best option.</p>&#xA;"
44587084,39967784,5127392,2017-06-16T10:33:23,<p>A SOA service is all about componentization on service level.&#xA;A Microservice is all about functional composition on service level.</p>&#xA;&#xA;<p>They are two different solutions for different problems.</p>&#xA;
44588445,41024771,5127392,2017-06-16T11:43:44,"<p>A SOA-service is all about componentization on service level (constructed around a business capability). A Microservice is all about functional composition on service level (input -> processing -> output). A Nanoservice is even smaller than a microservice, and therefore doesn't make any sense.</p>&#xA;&#xA;<p>If you would draw a parallel between services and programming, then SOA can be seen as the component, Microservice as the method and a nanoservice as related lines of code in a method.</p>&#xA;&#xA;<p>An OrderService is a SOA-service that is responsible for order handling, basically a state machine for the lifetime of an order.&#xA;Composing an confirmation e-mail is a microservice.&#xA;Getting data for the confirmation e-mail is a nanoservice.</p>&#xA;"
44779637,39306577,5127392,2017-06-27T11:53:25,"<p>In this example an aggreggate is separated over several microservices. Cascading updates and deletes indicate that <code>Countries</code>, <code>Cities</code> and <code>Streets</code> all belong to the same concept.</p>&#xA;&#xA;<p>In a microservice architecture I should be allowed to shutdown the <code>CitiesService</code> and it will have no business impact. In a well designed microservice architecture, this situation should either never happen or it should not cause any problems. If you run into a situation similar to your example, then you're doing microservices wrong.</p>&#xA;"
46820017,40734086,882912,2017-10-18T22:09:03,"<p>Take a look at the <code>Proxy library for ASP.NET Core</code> at <a href=""https://github.com/aspnet/Proxy"" rel=""nofollow noreferrer"">https://github.com/aspnet/Proxy</a></p>&#xA;&#xA;<p>This library also has support for WebSockets.</p>&#xA;"
44799428,44797253,1350880,2017-06-28T10:03:37,"<p>I would keep things easy and avoid overcomplexity.&#xA;Just persist the WS response time and each instance before  calling again the WS, should check on DB how much time passed since last call.</p>&#xA;"
44060713,44060464,107744,2017-05-19T03:02:07,"<p>Seems like there's something off in the ""simple calculation"" regarding the number of readings per year.</p>&#xA;&#xA;<p>There's 86,400 seconds in 24 hours. That's 8,640 ""ten second intervals"" per day.</p>&#xA;&#xA;<p>Times 365 days per year, that's 3,153,600 ""ten second intervals"" per year.</p>&#xA;&#xA;<p>Times 60 sensors (one reading per sensor each ten seconds), that's 189 million  (189,216,000) readings per year.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>For managing a table with a large number of rows, consider range partitioning on the <code>Time</code> column. For example, by week, or by month.</p>&#xA;&#xA;<p>How much of that <code>VARCHAR(255)</code> do we actually need for identifying a reading/sensor? If we could instead use an <code>INT</code> datatype, that would be only four bytes. And the <code>DATETIME</code> datatype will cost us eight bytes, where a <code>TIMESTAMP</code> datatype would require only four bytes.</p>&#xA;&#xA;<p>If I was going down the route of sharding the table into smaller tables, I'd consider 60 tables, one for each reading. And moving the <code>Id</code>/<code>SensorId</code> values (columns) out of the table, and moving that into the identifier of the table. That would leave us with just <code>Time</code> for the PRIMARY KEY, and save a whole slew of repeated data.</p>&#xA;&#xA;<p>And we could still implement partitioning on each of those table.</p>&#xA;&#xA;<p>But so far we're only talking about inserting rows. What is missing from the discussion and what really matters is how we will be <em>querying</em> the data; what query patterns we need to support.</p>&#xA;&#xA;<p>I'd get a handle on the data structure before monkeying with microservices. If each reading is in a separate table, then that lends itself to sharding those tables across multiple servers. But it wouldn't be transparent to the application. The application layer would need to be aware of that, and make use multiple database connections, using the correct connection for each table.</p>&#xA;"
42008038,42006876,2402272,2017-02-02T16:59:34,"<h2>High-level</h2>&#xA;&#xA;<blockquote>&#xA;  <p><em>Am I on the right track?</em></p>&#xA;</blockquote>&#xA;&#xA;<p>No.  By focusing on an emerging, as-yet loosely defined architectural style as your end objective, you are completely missing the point.  It is well worth considering whether the design and architectural properties considered characteristic of a microservice architecture will be a good fit for your project, but you're turning that backwards.  It is a red flag to me that you claim to be making design decisions ""just for the sake of 'microservices'"".</p>&#xA;&#xA;<h2>Analysis with respect to microservices</h2>&#xA;&#xA;<p>As for whether you are in fact building something in microservices style (for whatever that's worth to you), let's look at microservices characteristics <a href=""https://martinfowler.com/articles/microservices.html"" rel=""nofollow noreferrer"">as described at martinfowler.com</a>:</p>&#xA;&#xA;<h3>Componentization via Services</h3>&#xA;&#xA;<p>As far as I can tell from what you've presented, you are indeed doing this, but all I really have to go on is that you have pieces running in several different, cooperating processes.  That's not all there is to being a component, nor to operating as a service.  Each component should have a well-defined job and a well-defined interface, independent of implementation characteristics to the greatest extent possible.  Components should rely only on each others' defined interfaces to interoperate.</p>&#xA;&#xA;<h3>Organized around Business Capabilities</h3>&#xA;&#xA;<p>What you've presented appears to run directly contrary to this characteristic, being organized around technology layers instead of business capabilities.</p>&#xA;&#xA;<h3>Products not Projects</h3>&#xA;&#xA;<p>This is more a lifecycle management consideration than an architectural one.  It's unclear whether you're operating in this way, but the way you couch your question makes me inclined to guess not.</p>&#xA;&#xA;<h3>Smart endpoints and dumb pipes</h3>&#xA;&#xA;<p>Whether your design has this characteristic is unclear, but it seems likely, given the nature of the components as you are defining them.</p>&#xA;&#xA;<p>To the extent that it might be more appropriate to consider the whole thing as one microservice (see ""Organized around Business Capabilities"" above), the fact that it has a REST interface is a good sign.</p>&#xA;&#xA;<h3>Decentralized Governance</h3>&#xA;&#xA;<p>This is another characteristic that seems a bit larger-scale than your efforts alone.  To the extent that you seem to have mostly free rein to design and build your product, it seems like you are indeed benefiting from decentralized governance.  Inasmuch as you seem to be a one-man team, however, that doesn't really apply internally within the scope of the development effort you are undertaking.</p>&#xA;&#xA;<h3>Decentralized Data Management</h3>&#xA;&#xA;<p>It's unclear whether your plans have this characteristic, or even to what extent it applies to you.</p>&#xA;&#xA;<h3>Infrastructure Automation</h3>&#xA;&#xA;<p>It's again unclear how well this characterizes your intentions, and to what extent it is even within your area of concern.</p>&#xA;&#xA;<h3>Design for failure</h3>&#xA;&#xA;<p>You've said nothing relevant to this characteristic.</p>&#xA;&#xA;<h3>Evolutionary Design</h3>&#xA;&#xA;<p>It's unclear to what extent your proposed architecture exhibits this characteristic.  I'm inclined to guess not very much, though.</p>&#xA;&#xA;<h2>Overall</h2>&#xA;&#xA;<p>What you're describing does not seem to be on the scale to which the term ""microservices architecture"" applies, as Martin Fowler describes it.  There are certainly aspects of this paradigm that you could apply at your scale; some of these you do seem to apply, others you seem not to apply, and others I cannot evaluate with respect to your architecture overview.</p>&#xA;&#xA;<p>I don't think you're well served by trying to bring all these characteristics to your scale, and certainly the extent to which you do so is not a good metric for the quality of your design.  It may be that you want to design your product to work within a larger microservices context, but that puts few constraints on you; indeed, the freedom that offers you is one of the main advantages of the microservices approach.</p>&#xA;"
43228732,43132158,44512,2017-04-05T10:36:41,<p>Here is my solution to apply policy to proxy service:</p>&#xA;&#xA;<ol>&#xA;<li>Create a new API using proxy service's url </li>&#xA;<li>Apply policy to API created in step1</li>&#xA;</ol>&#xA;&#xA;<p>Can anyone confirm this is the correct way?</p>&#xA;
47593412,47474453,8918241,2017-12-01T12:27:48,"<p>In addition to what Hossam posted, I played around with <a href=""https://www.consul.io/"" rel=""nofollow noreferrer"">Consul</a> now. For this purpose, I studied e.g. the following <a href=""http://blog.trifork.com/2016/12/14/service-discovery-using-consul-and-spring-cloud/"" rel=""nofollow noreferrer"">Blog</a>. Using Consul, I can do something like the following:<code>restTemplate.getForObject('http://city-service/rest/city/capital', ...);</code> instead of <code>restTemplate.getForObject('http://localhost:8080/rest/city/capital', ...);</code>. This should also work for other service registries. I still do not not how to do this in e.g. Angular, but perhaps I will create a new question for this.</p>&#xA;"
43378616,43378165,7347425,2017-04-12T20:00:58,"<p>Use Point to Point messaging model and only one consumer will receive a mesage. In pub/sub model all subscribers will be notified.<br>&#xA; Example with <a href=""https://dzone.com/articles/jms-activemq"" rel=""nofollow noreferrer"">ActiveMQ</a>.</p>&#xA;"
43185091,43177818,7347425,2017-04-03T12:48:34,"<p>It is hard to agree with such a statement:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Almost all the successful microservice stories have started with a monolith that got too big and was broken up Almost all the cases where I've heard of a system that was built as a microservice system from scratch, it has ended up in serious trouble.</p>&#xA;</blockquote>&#xA;&#xA;<p>There are microservice stories which did not started as monolith, and on other side there are microservice stories which is successful and which started from scratch.</p>&#xA;&#xA;<p>From my point of view, it is hard to answer to your question, but when you are thinking about architectures you must be aware about pro/cons for different approaches and decide based on your needs.      </p>&#xA;&#xA;<p>For example with monolith approach, horizontal scalability is difficult(vertical is possible), but monitoring and deployment is easier.<br>&#xA;On other side with microservice approach you can scale horizontally, but deployment and monitoring are more complex. Also with microservice approach you must see how microservices will communicate(e.g. broker with producer consumer pattern).</p>&#xA;&#xA;<p>Hope that this helps.</p>&#xA;"
49498566,49488573,7347425,2018-03-26T18:48:36,"<p>Consider Event Sourcing pattern(for example with Kafka as event store) which fits to Microservice architectures.<br>&#xA;UI will publish file to Kafka, then another service can consume file from kafka and store a file. </p>&#xA;&#xA;<p><a href=""http://microservices.io/patterns/data/event-sourcing.html"" rel=""nofollow noreferrer"">Event Sourcing pattern</a>, &#xA;<a href=""http://blog.kontena.io/event-sourcing-microservices-with-kafka/"" rel=""nofollow noreferrer"">Event Sourcing with Kafka</a></p>&#xA;"
35929199,35928639,597473,2016-03-10T23:13:05,"<p>You might use <a href=""https://github.com/qerub/camel-snake-kebab"" rel=""nofollow noreferrer"">camel-snake-kebab</a> library which supports most of the combinations. You can plug it in into most of the JSON libraries for Clojure (cheshire, cli-json, data.json - as mentioned by Elogent) as they usually have an option to provide a function for handling property name mangling.</p>&#xA;&#xA;<p>For example with cheshire:</p>&#xA;&#xA;<p>Generate JSON with camel case property names:</p>&#xA;&#xA;<pre><code>(cheshire.core/generate-string {:my-clojure-key ""abc""} &#xA;                               {:key-fn camel-snake-kebab.core/-&gt;camelCaseString})&#xA;</code></pre>&#xA;&#xA;<p>Result:</p>&#xA;&#xA;<pre><code>{""myClojureKey"":""abc""}&#xA;</code></pre>&#xA;&#xA;<p>Parse JSON to get map with kebab case keys:</p>&#xA;&#xA;<pre><code>(cheshire.core/parse-string ""{\""myClojureKey\"":\""abc\""}"" &#xA;                            camel-snake-kebab.core/-&gt;kebab-case-keyword)&#xA;</code></pre>&#xA;&#xA;<p>Result:</p>&#xA;&#xA;<pre><code>{:my-clojure-key ""abc""}&#xA;</code></pre>&#xA;&#xA;<p>There is also an example for data.json in <a href=""https://github.com/qerub/camel-snake-kebab#with-json"" rel=""nofollow noreferrer"">camel-snake-kebab readme</a>.</p>&#xA;"
32743085,32741333,605153,2015-09-23T15:02:39,"<p>Let me share my opinion. </p>&#xA;&#xA;<p>First of all, if you can keep your application stateless, by all means do so :)&#xA;It will be the best solution in terms of both performance and scalability.</p>&#xA;&#xA;<p>Now, if its impossible, then you should maintain some distributed session management layer.</p>&#xA;&#xA;<p>The gateway responsible for authentication could generate some unique session identifier which could later be used as a key. &#xA;This key could be propagated to all the microservices and be a part of the API or something.</p>&#xA;&#xA;<p>In order to access the session, the microservice could 'get' value by key and work with it.</p>&#xA;&#xA;<p>In terms of implementation: I would take a look on NoSQL solutions. Some of them that can suit your need are:</p>&#xA;&#xA;<ol>&#xA;<li><a href=""https://redislabs.com/"" rel=""noreferrer"">Redis</a>. Take a look on ''hset'' there</li>&#xA;<li><a href=""https://hazelcast.com/"" rel=""noreferrer"">Hazelcast</a>. Its more a in-memory grid but if the solution is java only, you can also implement the required functionality</li>&#xA;<li><a href=""http://memcached.org/"" rel=""noreferrer"">Memcache.d</a>. It will give you an old good map, just distributed :)</li>&#xA;</ol>&#xA;&#xA;<p>There are also other solutions I believe.</p>&#xA;&#xA;<p>Now, the performance is crucial here, otherwise the whole solution will be just too slow. So In my understanding, using an RDBMS would be not be good here, moreover potentially it would be harder to scale it out.</p>&#xA;&#xA;<p>Hope this helps</p>&#xA;"
48607382,48604664,605153,2018-02-04T11:03:01,"<p>In my opinion, the thing that you've asked for is impossible only because each microservice is a dedicated process and it runs inside its own JVM (probably in more than one server). When you create a connection to the database (assuming you use connection pool) its always at the level of a single JVM.</p>&#xA;&#xA;<p>I understand that the chances are that you meant something different but I had to put it on because it directly answers your question</p>&#xA;&#xA;<p>Now, you can share the same database between microservices (the same schema, tables, etc) so that each JVM will have a set of connections opened (in accordance with connection pool definitions). &#xA;However, this is a really bad practice - you don't want to share the databases between microservice. The reason is the cost of change: if you (as a maintainer of microservice A) decide to, say, alter one of the tables, now all microservices will have to support this, and this is not a trivial thing to do.</p>&#xA;&#xA;<p>So, a better approach is to have a service that has a ""sole responsibility"" for your data in some domain. Now, all the services could contact this service and ask for the required data through well-established APIs that should never be broken. In this approach, the cost of change is much ""cheaper"" since only this ""data service"" should be changed in a way that it doesn't break existing APIs. </p>&#xA;&#xA;<p>Now regarding the database connection thing: you usually will have more than one JVM that runs the same microservice (like data microservice) so, it's not that you share connections between them, but rather you share the same way of working with database (because after all its the same code).</p>&#xA;"
51682446,51679363,605153,2018-08-04T04:33:29,"<p>In <a href=""https://stackoverflow.com/questions/24537340/docker-adding-a-file-from-a-parent-directory"">This thread</a> there is a discussion of adding files from the parent directory while building the docker image.</p>&#xA;&#xA;<p>Anyway I don't think its a good approach for this, and probably the following alternative will work better:</p>&#xA;&#xA;<p><strong>An idea:</strong></p>&#xA;&#xA;<p>Do not run ""maven"" from within the docker image building process"", for this simple case this approach will probably work but what if you'll add to setup a ""commons"" module under the root project module, the building image script will become significantly more complicated. In addition, it will be probably hard to keep the number of layers in the resulting image to the minimum.</p>&#xA;&#xA;<p>Consider running the regular mvn package outside docker in the build machine&#xA;When the artifact is ready, Build it just by copying adding the artifact to the desired folder and running the application. </p>&#xA;&#xA;<p>This way you don't need to think about maven dependencies while building docker image, maven knows its dependencies and will be able to build the artifact</p>&#xA;&#xA;<p>Having said that if the requirement to run a build during the image creation process, the core reason of the failure is that dockre build process doesn't allow to access the ""parent"" folder (or in general, any folder outside the folder where the docker file resides). So the following should be possible:</p>&#xA;&#xA;<p>Docker file should present in the root folder (where the root pom.xml resides).&#xA;In order to run the build of only the required module potentially with all its dependencies the following command should work:</p>&#xA;&#xA;<pre><code>COPY &lt;all the filesystem of the project&gt;&#xA;mvn package --projects service --also-make&#xA;</code></pre>&#xA;&#xA;<p>Alternatively, if you have to copy only service, its possible to specify a path to pom that resides in a different location with the help of <code>mvn -f /path/to/pom.xml</code></p>&#xA;"
41807231,33680497,3620833,2017-01-23T13:12:13,"<p>You can check this:&#xA;<a href=""https://spring.io/blog/2015/01/20/microservice-registration-and-discovery-with-spring-cloud-and-netflix-s-eureka"" rel=""nofollow noreferrer"">https://spring.io/blog/2015/01/20/microservice-registration-and-discovery-with-spring-cloud-and-netflix-s-eureka</a> </p>&#xA;&#xA;<p>This example is based on the following services:</p>&#xA;&#xA;<p><strong>Netflix Eureka :</strong></p>&#xA;&#xA;<p>Eureka is a REST (Representational State Transfer) based service that is primarily used in the AWS cloud for locating services for the purpose of load balancing and failover of middle-tier servers.</p>&#xA;&#xA;<p>Eureka is just a Service Registry holding all started service instance severname and port details.</p>&#xA;&#xA;<p><strong>Netflix Zuul :</strong></p>&#xA;&#xA;<p>Zuul is an edge service that provides dynamic routing, monitoring, resiliency, security, and more. </p>&#xA;&#xA;<p>It is the endpoint for microservice clients. All other service instances are secured in secured box whereas Zuul is exposed to clients which intercepts all client requests.</p>&#xA;&#xA;<p><strong>Netflix Ribbon:</strong></p>&#xA;&#xA;<p>Ribbon is a Inter Process Communication (remote procedure calls) library with built in software load balancers. The primary usage model involves REST calls with various serialization scheme support.</p>&#xA;&#xA;<p>Ribbon works in conjunctions with Zuul, uses plenty of algorithms to elect the best performing service node to serve the requests when N number of microservice instances are running.</p>&#xA;&#xA;<p>In simple words, Zuul intercepts client requests and based on the request URL Zuul finds the ip and port of Microservice instance by Using Eureka service discovery. Ribbon elects the best service node to service the request and then from Zuul request is routed to the respective microservice instance.</p>&#xA;&#xA;<p><strong>Advantage from my project perspective:</strong></p>&#xA;&#xA;<p>We can add or remove 'N' number of service nodes based on the network traffic. </p>&#xA;"
49490515,49486062,6708961,2018-03-26T11:35:53,"<p>Unfortunately, there is no way to tell Ignite, which service instance to use, based on the argument of the method being called.</p>&#xA;&#xA;<p>Services are good when you need to store some state in them. Otherwise compute jobs are just as effective. Moreover, compute jobs are used internally to call services' methods.</p>&#xA;&#xA;<p>So, if you don't have any state, then just use the compute jobs. Otherwise you can try injecting needed resources into compute jobs: <a href=""https://apacheignite.readme.io/docs/resource-injection"" rel=""nofollow noreferrer"">https://apacheignite.readme.io/docs/resource-injection</a></p>&#xA;"
51417133,51416552,26849,2018-07-19T07:49:43,"<p>Trying to communicate with the other container won't work with <code>localhost</code>.</p>&#xA;&#xA;<p>You should create a custom bridged network, which will allow you to refer to the containers by name. And there is no need to publish the ports if you are only talking internally.</p>&#xA;&#xA;<pre><code># create network&#xA;docker network create -d bridge mynet&#xA;# container 1&#xA;docker container run --network mynet --name container1 -d image_name&#xA;# container 2&#xA;docker container run --network mynet --name container2 -d some_other_image_name&#xA;</code></pre>&#xA;&#xA;<p>The IP in code snippet can then be replaced with the name of the other container</p>&#xA;&#xA;<pre><code>RestTemplate restTemplate = new RestTemplate();&#xA;ResponseEntity&lt;Boolean&gt; isUp = restTemplate.getForEntity(""http://container2:2002/apis/test"",Boolean.class)&#xA;</code></pre>&#xA;"
51984557,51980596,4615649,2018-08-23T11:17:35,"<p>I recommend mapstruct(faster type-safe and easy to understand ). Java reflection libraries are too slow and and Error could be thrown in Java's execution time. </p>&#xA;&#xA;<p><a href=""http://mapstruct.org"" rel=""nofollow noreferrer"">http://mapstruct.org</a></p>&#xA;"
43058406,43057349,1548513,2017-03-27T23:59:33,"<p>Microservices are a logical concept. what technology you use is irrelevant as long as you adhere to the principles of the architecture. so yes, you can use Windows Services as long as they are <em>microservices</em>.</p>&#xA;"
39118838,39118038,4564449,2016-08-24T08:59:52,<p>If you have control over the docker engine in your CI setup you could execute <code>docker logs [Container_Name]</code> and read out the last line which could be emitted by your application.</p>&#xA;&#xA;<p><code>RESULT=$(docker logs [Container_Name] 2&gt;&amp;1 | grep [Search_String])</code></p>&#xA;&#xA;<p>logs output example:<br>&#xA;<code>Agent pid 13</code><br>&#xA;<code>Enter passphrase (empty for no passphrase): Enter same passphrase again: Identity added: id_rsa (id_rsa)</code><br>&#xA;<code>#host SSH-2.0-OpenSSH_6.6.1p1 Ubuntu-2ubuntu2.6</code><br>&#xA;<code>#host SSH-2.0-OpenSSH_6.6.1p1 Ubuntu-2ubuntu2.6</code></p>&#xA;&#xA;<p>parse specific line:<br>&#xA;<code>RESULT=$(docker logs ssh_jenkins_test 2&gt;&amp;1 | grep Enter)</code></p>&#xA;&#xA;<p>result:<br>&#xA;<code>Enter passphrase (empty for no passphrase): Enter same passphrase again: Identity added: id_rsa (id_rsa)</code></p>&#xA;
38435768,38435582,3407841,2016-07-18T11:38:21,"<p>I started a NodeJS project with a similar structure and it quickly became a mess.  I moved node_modules, common code, and package.json to the project root and used separate startup files so that I could share common code between the projects.  It turned out much easier to deploy and maintain.</p>&#xA;&#xA;<pre><code>root&#xA;|_ socket&#xA;|_ frontend&#xA;|_ email&#xA;|_ node_modules&#xA;|_ package.json&#xA;|_ app_socket.js&#xA;|_ app_frontend.js&#xA;|_ app_email.js&#xA;</code></pre>&#xA;"
44043719,35890054,1638626,2017-05-18T09:24:03,"<p>Give each microservice its own code repository, and add one for the cross-service end-to-end tests.</p>&#xA;&#xA;<p>Inside a microservice's repository, keep everything that relates to that service, from code over tests to documentation and pipeline:</p>&#xA;&#xA;<pre><code>root/&#xA;  app/&#xA;    source-code/&#xA;    unit-tests/ (also: integration-tests, component-tests)&#xA;  acceptance-tests/&#xA;  contract-tests/&#xA;</code></pre>&#xA;&#xA;<p>Keep everything that your build step uses in one folder (here: app), probably with sub-folders to distinguish source code from unit tests, integration tests, and component tests.</p>&#xA;&#xA;<p>Put tests like acceptance tests and contract tests that run in later stages of the delivery pipeline in own folders. This keeps them viually separate. It also simplifies creating separate build/test steps for them, for example by including own pom.xml's when using Maven.</p>&#xA;&#xA;<p>If a developer changes a feature, he will need to change the tests at the exact same time to ensure the two fit together. Keeping code and tests in the same repository keeps the two in sync in a natural way.</p>&#xA;"
44058386,36775802,1638626,2017-05-18T21:57:22,"<p>I assume that reviews are independent from each other and that validating a review therefore requires only that review, and no other reviews.</p>&#xA;&#xA;<p>You don't want to share entities, which rules out things like shared databases, Hadoop clusters or data stores like Redis. You also don't want to duplicate data, thereby ruling out plain file copies or trigger-based replication on database level.</p>&#xA;&#xA;<p>In summary, I'd say your aim should be a stream. Let the Validator request everything from Reviews about Site A, but not in one bulk, but in a stream of single or small packages of reviews.</p>&#xA;&#xA;<p>The Validator can now process the reviews one after the other, at constant memory and processor consumption. To gain performance, you can make multiple instances of the Validator who pull different, disjunct pieces of the stream at the same time. Similarly, you can create multiple instances of the Reviews microservice if one alone wouldn't be able to answer the pull fast enough.</p>&#xA;&#xA;<p>The Validator does not persist this stream, it produces only the errors and stores or sends them somewhere; this should fulfill your no-sharing no-duplication requirements.</p>&#xA;"
31475366,31342583,990946,2015-07-17T11:59:20,"<p>You should have a look at <a href=""http://php.net/manual/en/book.gearman.php"" rel=""noreferrer"">Gearman</a>.</p>&#xA;&#xA;<p>It composes of a <code>client</code> which assigns the jobs, one or more <code>workers</code> which will pick up and execute the jobs and a <code>server</code> which will maintain the list of functions (services) and jobs pending. It will re-assign the jobs if a worker dies.</p>&#xA;"
41382318,40815723,991458,2016-12-29T15:09:17,"<p>I think is not use in each microservice, but in your BFF (backend for frontend) who use microservice. You can find a good implementation and exemple in this book <a href=""https://pragprog.com/book/mnee/release-it"" rel=""nofollow noreferrer"">https://pragprog.com/book/mnee/release-it</a>. Solution with API Gateway is good, see Kong <a href=""https://getkong.org/"" rel=""nofollow noreferrer"">https://getkong.org/</a> for that.</p>&#xA;"
51093519,51093230,2303202,2018-06-29T02:34:52,"<blockquote>&#xA;  <p>since a monorepo can be Gigabytes in size, so we need teams to only clone parts that they care about, not whole project</p>&#xA;</blockquote>&#xA;&#xA;<p>for this you should check the partial a.k.a narrow clone feature which is pretty bleeding edge. I'm not sure if any service currently supports it but if you have own server you probably can sei it up there yourself. See <a href=""https://github.com/git/git/blob/master/Documentation/technical/partial-clone.txt"" rel=""nofollow noreferrer"">https://github.com/git/git/blob/master/Documentation/technical/partial-clone.txt</a></p>&#xA;&#xA;<p>In theory, it could also enable access control - you can fail the fetch request if users asks for a blob it should not have access to. Probably it would require some additional hacking.</p>&#xA;&#xA;<p>otherwise, git does not have the reading access control</p>&#xA;"
40449678,40447582,1916386,2016-11-06T13:07:02,"<p>Don't see any violations here if you are not going to use your domain entities as input parameters to your internal REST services. Use plain old DTO objects as input parameter and then map them to your domain objects.</p>&#xA;&#xA;<p>Also I wouldn't go with API Gateway solution if I were you. I understand you are trying to make your changes transparent for your API clients but the API Gateway adds a redundant step and it might cause performance problems.</p>&#xA;&#xA;<p>I suggest doing the following:</p>&#xA;&#xA;<ol>&#xA;<li>Extract domain logic into reusable libraries, so they can be used by both old and new API.</li>&#xA;<li>Build a new version of your API using the libs from item #1</li>&#xA;<li>Make sure all your new clients are using the new API and promote it among the old ones</li>&#xA;</ol>&#xA;&#xA;<p>Yes, it won't be easy to support both API's for some time but you will get rid of that API Gateway in the long run.</p>&#xA;"
47636539,47527983,3031010,2017-12-04T15:21:17,"<p>Microservices compound by functionality and degree of connectivity.&#xA;I used this approach:</p>&#xA;&#xA;<ul>&#xA;<li>com.company.product:&#xA;&#xA;<ul>&#xA;<li>possible big service:&#xA;&#xA;<ul>&#xA;<li>config</li>&#xA;<li>services</li>&#xA;<li>domain</li>&#xA;<li>etc</li>&#xA;</ul></li>&#xA;<li>possible second big service:&#xA;&#xA;<ul>&#xA;<li>config</li>&#xA;<li>services</li>&#xA;<li>domain</li>&#xA;<li>etc</li>&#xA;</ul></li>&#xA;<li>config</li>&#xA;<li>services // which likely never be separated</li>&#xA;<li>domain // common domain</li>&#xA;<li>etc</li>&#xA;</ul></li>&#xA;</ul>&#xA;&#xA;<p>When split project you analyze new common depencies seeing by packages, exclude common library, copy project for each microservice, delete unneccessary code, perhaps change services implementations (for example, if ""possible big service"" uses ""possible second big service""), configurations and build.&#xA;""Big"" in that context means full-fledged functional implementation of something, what could be horizontally scaled, or need to be microservice for other reasons.</p>&#xA;"
48515185,45869766,4151266,2018-01-30T06:26:36,"<p>This is what I did to get vs 2017 working on windows 10 home with docker-toolbox. You follow this and i guarantee it will work. Note this only applies to windows 10 home which doesn’t support native docker for windows application:</p>&#xA;&#xA;<ol>&#xA;<li><p>Install docker-toolbox on w10 home</p></li>&#xA;<li><p>Run docker QuickStart terminal once to create the docker-machine. It takes a while. So be patient while it assigns the IP address and other things</p></li>&#xA;<li><p>Once it’s done it will show you a command prompt. Type ‘docker-machine ip default’. Note down the ip address as you’re gonna need it later</p></li>&#xA;<li><p>Close the QuickStart terminal window. That was just to initialize the boot2docker.iso image of a tiny Ubuntu linux server into virtualbox app (aka docker-machine aka default vm). If you’re not familiar with virtualization technology or oracle virtualbox stop reading and read up on them first and then start over. But if you do then gladly continue </p></li>&#xA;<li><p>As I mentioned that your docker-machine instance is a Linux vm and therefore it’s obvious that you can only open projects built using .net core technology. Unfortunately for full .net framework you’ll either need to run Windows containers which are only available on windows 10 pro or build your own windows nano server or 2016 server vm on virtualbox and then use and follow steps for native docker for windows on dockers website. From here on the remainder of this answer will be helpful to those wanting to run core projects on Linux vm / docker-machine only</p></li>&#xA;<li><p>Open windows power shell in administrator mode and type ‘docker-machine ls’ to confirm that default vm is running. Can also do ‘docker-machine status default’ and it should return ‘running’</p></li>&#xA;<li><p>Now open virtualbox application which is running your default vm and click on settings. Open “shared drives” tab where you need to make sure ‘c:\Users’ folder on the host machine is mapped/mounted as ‘c/Users’ folder in the vm. Note that this step is super important and missing it will cause a lot of trouble getting it to work successfully</p></li>&#xA;<li><p>Also a quick note that your solution/project/codebase <strong>MUST</strong> be saved under ‘c:\Users\‘ for it to work correctly. This is if you want to use it OOTB. I didn’t wanna waste time trying to mount a folder outside the permitted path. But if you’re the adventurous kind, please, by all means try to figure it out and let us know how you did</p></li>&#xA;<li><p>Now as Quetzalcoatl correctly mentioned vs needs to know about this docker-machine. The only way that happens is if the environment variables are set. Therefore go ahead and run this command ‘docker-machine env deault | Invoke-Expression’ in the powershell window. This is the magic sauce getting vs to behave nicely with docker-machine </p></li>&#xA;<li><p><strike>Go ahead and open vs normally either by dbl clicking your project solution or creating a new project/solution.</strike> In Powershell use the 'start' command to open your existing vs solution or a new vs instance. Pro-tip: if you create a new solution DO NOT select the option of Linux docker at the time of picking the project template type. You can totally add docker support once your solution is all setup and ready to go. Matter of fact leave it unchecked and let vs create your solution. This way you’ll get a chance to build and run your solution in IIS Express or Self-hosted modes to see if your core2.0 even works properly</p></li>&#xA;<li><p>Once satisfied that everything worked and you saw the OOTB homepage now it’s time to add docker support by rt clicking on your project, hovering over Add and then clicking on ‘Add Docker Support’. This will create a new docker project (.dcproj) and add a bunch of docker related files</p></li>&#xA;<li><p>Now I’m not gonna go into the nitty gritty of docker here however you’ll notice that your project is no longer the startup and the newly created docker project is. That’s perfectly normal and intended behavior. It means you’re setup and ready to fire up your app using docker containers. So go ahead and click on the ‘Docker’ button to see your hard work finally pay off. Again be patient as it takes a while to build images and spin up containers but once it’s done the vs will start and attach the debugger </p></li>&#xA;<li><p>Here you’ll once again be disappointed and feel worthless because when the browser opens a new window or tab there’ll be a page unreachable error. The reason is the browser address points to localhost which is not the web server anymore. Your “web server” now is your docker container and therefore you’ll need to replace localhost with the ip address you retrieved above. Port number remains as-is. Once you submit the page you’ll be relieved and ecstatic to see the home page/route work. This should also enable debugging in vs. if for some reason it doesn’t then you may need to delete a folder called .vsdbg in c:\Users\ folder and rerun the application.</p></li>&#xA;</ol>&#xA;"
36645880,36645517,4545506,2016-04-15T11:25:13,"<p>Don't mess your mind with books or library or chapters. Only look your entities: Is ""Draws"" your entity? Then it should be as <code>http://url:port/draws/{gameNo}</code></p>&#xA;&#xA;<p>For Rest API designing, you can read following resources:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""http://shop.oreilly.com/product/9780596805838.do"" rel=""nofollow"">Rest In Practice</a></li>&#xA;<li><a href=""http://martinfowler.com/articles/richardsonMaturityModel.html"" rel=""nofollow"">Richardson Maturity Model</a></li>&#xA;</ul>&#xA;&#xA;<p>I suggest you to have a look on Richardson Maturity Model.</p>&#xA;&#xA;<p>Also I like <a href=""https://developer.github.com/v3"" rel=""nofollow"">Github API</a>. (Old days they used to suggest us to read code to improve our talents, now you can read rest APIs also.)</p>&#xA;"
51136868,50986816,2317794,2018-07-02T13:01:12,"<p>From a more general perspective - on receiving the request you can register a subscriber on the queue in the current request's context (meaning when the request object is in scope) which receives an acknowledgment from responsible services as they finish their jobs (like a state machine which maintains the progress of the total number of operations). When the terminating state is reached it returns the response and removes the listener. I think this will work in any pub/sub style message queue. Here is an overly simplified demo of what I am suggesting.</p>&#xA;&#xA;<pre><code>// a stub for any message queue using the pub sub pattern&#xA;let Q = {&#xA;  pub: (event, data) =&gt; {},&#xA;  sub: (event, handler) =&gt; {}&#xA;}&#xA;// typical express request handler&#xA;let controller = async (req, res) =&gt; {&#xA;  // initiate saga&#xA;  let sagaId = uuid()&#xA;  Q.pub(""saga:register-user"", {&#xA;    username: req.body.username,&#xA;    password: req.body.password,&#xA;    promoCode: req.body.promoCode,&#xA;    sagaId: sagaId&#xA;  })&#xA;  // wait for user to be added &#xA;  let p1 = new Promise((resolve, reject) =&gt; {&#xA;    Q.sub(""user-added"", ack =&gt; {&#xA;      resolve(ack)&#xA;    })&#xA;  })&#xA;  // wait for promo code to be applied&#xA;  let p2 = new Promise((resolve, reject) =&gt; {&#xA;    Q.sub(""promo-applied"", ack =&gt; {&#xA;      resolve(ack)&#xA;    })&#xA;  })&#xA;&#xA;  // wait for both promises to finish successfully&#xA;  try {&#xA;&#xA;    var sagaComplete = await Promise.all([p1, p2])&#xA;    // respond with some transformation of data&#xA;    res.json({success: true, data: sagaComplete})&#xA;&#xA;  } catch (e) {&#xA;    logger.error('saga failed due to reasons')&#xA;    // rollback asynchronously&#xA;    Q.pub('rollback:user-added', {sagaId: sagaId})&#xA;    Q.pub('rollback:promo-applied', {sagaId: sagaId})&#xA;    // respond with appropriate status &#xA;    res.status(500).json({message: 'could not complete saga. Rolling back side effects'})&#xA;  }&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>As you can probably tell, this looks like a general pattern which can be abstracted away into a framework to reduce code duplication and manage cross-cutting concerns. This is what the <a href=""http://microservices.io/patterns/data/saga.html"" rel=""nofollow noreferrer"">saga pattern</a> is essentially about. The client will only wait for as long as it takes to finish the required operations (which is what would happen even if it was all synchronous), plus the added latency due to inter-service communication. Make sure you do not block the thread if you are using an event loop based system like NodeJS or Python Tornado.</p>&#xA;&#xA;<p>Simply using a web-socket based push mechanism doesn't necessarily improve the efficiency or performance of your system. However, it is recommended that you push messages to the client using a socket connection because it makes your architecture more general (even your clients behave like your services do), consistent and allows for better separation of concerns. It will also allow you to independently scale the push-service without worrying about business logic. The saga pattern can be expanded upon to enable rollbacks in case of partial failures, or timeouts and makes your system more manageable.</p>&#xA;"
52113938,52098061,421602,2018-08-31T10:55:12,"<p>This questions is somewhat overly broad and generic, but I'll try to answer it anyway making two assumptions:</p>&#xA;&#xA;<p>If you want to use Spring Boot / Cloud ""in-process"", that is within ODL/ Karaf, then the answer to that would be that such an architecture would make little sense.  Karaf (not ODL) has some Spring support as far as I know, but you'll probably have a hard time to marry that ""nicely"" with ODL...  </p>&#xA;&#xA;<p>The architecture of ODL is that you define YANG models and the RPCs you define in them ""automagically"" get exposed as HTTP REST APIs (via something called RESTCONF), and you can then consume those from other applications.</p>&#xA;&#xA;<p>But if by your question you just mean if you can write a separate new Spring Boot / Cloud application and from that invoke OpenDaylight services via remote RESTCONF, then the answer is that this is certainly possible - and the recommended way to write integrations.</p>&#xA;&#xA;<p>BTW: In this context, you may also be interested in <a href=""https://lighty.io"" rel=""nofollow noreferrer"">https://lighty.io</a> - but if I were you I would very carefully study the license of that project; it's external to OpenDaylight and not free for commercial use AFAIK (but IANAL).</p>&#xA;&#xA;<p>PS: You could have a look at <a href=""https://github.com/vorburger/opendaylight-simple/"" rel=""nofollow noreferrer"">https://github.com/vorburger/opendaylight-simple/</a> for some inspiration as well; but that is a POC which is not ready for consumption by you.</p>&#xA;"
50663225,50663047,6521058,2018-06-03T04:11:40,"<p>No.</p>&#xA;&#xA;<p>Indeed, to work with web server, you would need a war file. But Web server just for receiving and responding network connections. You absolutely can manage connections by your self, with jdk or 3rd party libraries like netty. So you can just build a standalone java application, which actually can be called as a web server.</p>&#xA;"
45813460,45554704,6586594,2017-08-22T09:12:12,<p>Finally i decided to use Kong gateway. Thanks anyway.</p>&#xA;
40034925,40034001,1152738,2016-10-14T04:28:01,"<p>Very broad topic for just one question and answer, but I'll do my best with this scenario.</p>&#xA;&#xA;<p>The concept of microservices is to have independent pieces of the whole where at any given time any of the microservices could go down and other areas wouldn't be affected.  So, if you wanted Stack Overflow (SO) to have the HTML of this question running in a microservice, one way would be to have a QuestionService responsible for taking new questions. This service could process anything it needs to do with the actual question and then post a pub-sub type message that a new question was added. The HtmlService would pick up this message and do all the HTML rending it needed and might need to ask other services for additional data before it can finish its job. It may need to ask a UserService for additional data about the posting user. Once the HtmlService has all the info it needs it could save the HTML into it's own database for persistence. </p>&#xA;&#xA;<p>Now some other user requests from SO to get new questions. The request is routed to the HtmlService and the already generated HTML is served.</p>&#xA;&#xA;<p>Now all questions can still be seen and if the QuestionService or the UserService goes down, all the HTML can still be served. You couldn't add new questions or register new users, but the site as a knowledge base would still work. </p>&#xA;&#xA;<p>I hope this crude example answers your question. Feel free to comment and I'll do my best to go into more detail. </p>&#xA;"
49820030,49818943,2532578,2018-04-13T15:04:23,<p>The microservice hosting needs to be assigned to your tenant otherwise it won't work and the API in that case will return forbidden. So it might be that it is no issue with your user but that your tenant has the feature not activated.</p>&#xA;
45108515,42353292,8309218,2017-07-14T17:17:52,"<p>Reduced coupling or fairly loose coupling still has one thing in common; coupling. In my opinion, coupling to any degree will always create rigid communication patterns that are difficult to maintain and difficult to troubleshoot as a platform grows into a large distributed platform. Isn't the idea behind microservices to allow consumers to engage in ""Permissionless innovation?"" I would suggest that this is only possible by decomposing to microservices that have high cohesion and low coupling and then let the consumer decide how to route, orchestrate or aggregate. </p>&#xA;"
50441327,40574379,8340963,2018-05-21T02:45:00,"<p>If you need to dynamically link to your SNS topic's ARN, you should create an environmental variable within your Lambda resource and reference your topic's ARN, like so:</p>&#xA;&#xA;<p><code>AWSTemplateFormatVersion: '2010-09-09'&#xA;Transform: AWS::Serverless-2016-10-31&#xA;Resources:&#xA;  MyTopic:&#xA;    Type: AWS::SNS::Topic&#xA;  MyLambdaFunction:&#xA;    Type: AWS::Lambda::Function&#xA;    Properties:&#xA;      Handler: index.handler&#xA;      Runtime: nodejs8.10&#xA;      Code: ./path/to/index.js&#xA;      Role: [Role ARN here]&#xA;      MemorySize: 128&#xA;      Timeout: 3&#xA;      Environment:&#xA;        Variables:&#xA;          SNS_TOPIC: !Ref MyTopic&#xA;</code></p>&#xA;&#xA;<p>If multiple stacks need to access your SNS Topic, I suggest creating a separate stack for your SNS topic and then putting the topic ARN in other templates as a parameter, and similarly reference that parameter, like so:</p>&#xA;&#xA;<p><code>AWSTemplateFormatVersion: '2010-09-09'&#xA;Transform: AWS::Serverless-2016-10-31&#xA;Parameters:&#xA;  MyTopicArn:&#xA;    Type: String&#xA;Resources:&#xA;  MyLambdaFunction:&#xA;    Type: AWS::Lambda::Function&#xA;    Properties:&#xA;      Handler: index.handler&#xA;      Runtime: nodejs8.10&#xA;      Code: ./path/to/index.js&#xA;      Role: [Role ARN here]&#xA;      MemorySize: 128&#xA;      Timeout: 3&#xA;      Environment:&#xA;        Variables:&#xA;          SNS_TOPIC: !Ref MyTopicArn&#xA;</code></p>&#xA;&#xA;<p>This way, if you have to replace your SNS topic, you can just update the other stacks' parameters when you deploy them next.</p>&#xA;"
38084850,37793364,3496419,2016-06-28T19:35:26,"<p>In ecs API, The containers defined in a <strong>same task</strong> guarantees <strong>those containers will be deployed on one instance</strong>. If this is the deployment behavior you want, define containers in one 'task'. </p>&#xA;&#xA;<p>If you want to deploy containers across ecs cluster, you should define different task with only one container, so that all your containers can be deployed balancedly on the cluster.</p>&#xA;"
43513842,43512572,2450403,2017-04-20T08:12:50,"<p>I would suggest <strong>Table-Per-Service</strong> will be good approach because when we are using micro-services over the traditional web-service architecture, then it should be more independent &amp; lightweight in nature like just same as plugin or add-on application.</p>&#xA;&#xA;<p><strong>Database-Per-Service</strong> approach will be appropriate when you are having multiple umbrella, but nowadays NOSQL database approach is in fashion.</p>&#xA;&#xA;<p>Hope this will help you to finalize the approach.      </p>&#xA;&#xA;<p>All the best.       </p>&#xA;"
42053609,42053559,1144035,2017-02-05T15:10:15,"<p>This is too long for a comment.</p>&#xA;&#xA;<p>I am assuming that you have a loss-less message queue, where once you get a confirmation for writing data, the queue is guaranteed to have the record.</p>&#xA;&#xA;<p>Basically, you need a loop with a transaction that can roll back or a status in the database.  The pseudo code for a transaction is:</p>&#xA;&#xA;<ul>&#xA;<li>Begin transaction</li>&#xA;<li>Insert into database</li>&#xA;<li>Write to message queue</li>&#xA;<li>When message queue confirms, commit transaction</li>&#xA;</ul>&#xA;&#xA;<p>Personally, I would probably do this with a status:</p>&#xA;&#xA;<ul>&#xA;<li>Insert into database with a status of ""pending"" (or something like that)</li>&#xA;<li>Write to message queue</li>&#xA;<li>When message confirms, change status to ""committed"" (or something like that)</li>&#xA;</ul>&#xA;&#xA;<p>In the case of recovery from failure, you may need to check the message queue to see if any ""pending"" records were actually written to the queue.</p>&#xA;"
44060488,44060464,1144035,2017-05-19T02:30:25,"<p>Your question overall is simply too broad.  However:</p>&#xA;&#xA;<blockquote>&#xA;  <p>we have 3 153 000 records. That is too many records to store into one&#xA;  table (in my opinion)</p>&#xA;</blockquote>&#xA;&#xA;<p>Your opinion is simply wrong.  There is no problem with database tables that store millions (or tens of millions or even hundreds of millions or billions of rows).  You do need to start paying more attention to the structure of the data.</p>&#xA;&#xA;<p>There are two key technologies that can help:</p>&#xA;&#xA;<ul>&#xA;<li>Partitioning (you can learn about it <a href=""https://dev.mysql.com/doc/refman/5.7/en/partitioning.html"" rel=""nofollow noreferrer"">here</a>)</li>&#xA;<li>Indexes</li>&#xA;</ul>&#xA;&#xA;<p>With an update rate of 10 updates/second, you shouldn't have any problem inserting data.</p>&#xA;"
51794775,51794404,3570659,2018-08-10T22:32:36,"<p>Well, maybe this is an opinion based question, but I strongly believe that each microservice should execute its task as <strong>independently</strong> from other systems, as possible. </p>&#xA;&#xA;<p>In your case, you should ask yourself, what happens if an attribute of the data that gets saved should be deleted from SA? Does it have consequences for SB?&#xA;Also notice, that if you heavily write and read the same data source, service SA will cause performance issues to SB and vice versa.</p>&#xA;&#xA;<p>As regards the communication between the microservices <code>REST</code> is <strong>not</strong> your only option; you could also consider using <code>asynchronous communication - Messaging</code> (RabbitMQ, Kafka..), having SA as publisher and SB subscriber. </p>&#xA;&#xA;<p>But the starting point should be: </p>&#xA;&#xA;<blockquote>&#xA;  <p>""Do my SA and SB need to be independently deployable and scalable from&#xA;  each other?""</p>&#xA;</blockquote>&#xA;&#xA;<p>If not, there is definitely <strong>no reason</strong> for two services.</p>&#xA;&#xA;<p>There is a great <a href=""https://www.youtube.com/watch?v=VWefNT8Lb74&amp;t=11s"" rel=""nofollow noreferrer"">Refactoring to System of Systems talk</a> from Oliver Gierke about what you need, mentioning common pitfalls and useful suggestions, that I can only recommend.&#xA;Please, also check the <a href=""http://microservices.io/patterns/data/database-per-service.html"" rel=""nofollow noreferrer"">database-per-service</a> and the other patterns by Chris Richardson. </p>&#xA;"
48926319,48925813,3570659,2018-02-22T11:28:31,"<p>So you have a microservice <strong>Ordering</strong> and a microservice <strong>Pricing</strong>. Both of the microservices need information about the <strong>Product</strong> entity. </p>&#xA;&#xA;<p>You should ask yourself: </p>&#xA;&#xA;<ul>&#xA;<li>Do those two different worlds realize the Product entity in the same way? Both of them need the same information?</li>&#xA;<li>Will the product information change for the same reasons for both of the microservices?</li>&#xA;</ul>&#xA;&#xA;<p>If no (which is likely the case), you have to add an abstract layer between them, so that you are sure that they use the same language.&#xA;If yes, you can keep on sharing the same object. </p>&#xA;&#xA;<p>By the way, these concerns that you have is not a new thing.&#xA;Here is Martin Fowler's article about <a href=""https://martinfowler.com/bliki/BoundedContext.html"" rel=""nofollow noreferrer"">bounded contexts</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>So instead DDD divides up a large system into Bounded Contexts, each&#xA;  of which can have a unified model - essentially a way of structuring&#xA;  MultipleCanonicalModels.</p>&#xA;</blockquote>&#xA;&#xA;<p>keywords for further research: <em>DDD, context map, bounded context, anticorruption layer.</em></p>&#xA;"
49036554,49036468,8485859,2018-02-28T18:49:57,"<p>it may really be either way, but to my liking, I would choose to put it under /Devices/{userId}/devices as you are looking for the devices given the user id. I hope this helps. Have a nice one!</p>&#xA;"
51277940,51277418,5674723,2018-07-11T05:40:36,"<p>The EXPOSE instruction informs Docker that the container listens on the specified network ports at runtime. You can specify whether the port listens on TCP or UDP, and the default is TCP if the protocol is not specified.</p>&#xA;&#xA;<p><code>EXPOSE &lt;port&gt; [&lt;port&gt;/&lt;protocol&gt;...]</code></p>&#xA;&#xA;<p>Example:&#xA;<code>EXPOSE 80/udp</code></p>&#xA;&#xA;<p><a href=""https://docs.docker.com/engine/reference/builder/#arg"" rel=""nofollow noreferrer"">Refer this.</a></p>&#xA;&#xA;<p>Also <a href=""https://stackoverflow.com/a/38058151/5674723"">see this.</a></p>&#xA;"
38477839,38466167,6004963,2016-07-20T09:48:35,"<p>You cannot have 2 or more apps in a single App Engine project but you can have 2 or more modules/services within a single App Engine app. See here for details: </p>&#xA;&#xA;<p><a href=""https://cloud.google.com/appengine/docs/python/modules/converting"" rel=""nofollow"">https://cloud.google.com/appengine/docs/python/modules/converting</a></p>&#xA;"
40446700,40446036,4850916,2016-11-06T06:37:20,"<p>As System Architect I think <code>it is better to have a single unified API domain where we do HTTP (layer 7) routing to reach our endpoints</code>. You can make your system more flexible without any changes for your clients. For example you have a microservice with routes:</p>&#xA;&#xA;<ul>&#xA;<li>api.todos.com/route1</li>&#xA;<li>api.todos.com/route2</li>&#xA;</ul>&#xA;&#xA;<p>In future you can split the microservice by this routes.</p>&#xA;&#xA;<p>But mostly, it depends on what API Gateway will you use. API gateway is single entry point in your system, what proxy request to correct microservice. Also it make auth and cache. More about this microservice's pattern you can read <a href=""http://microservices.io/patterns/apigateway.html"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;"
40650818,40645778,4850916,2016-11-17T09:14:02,<p>Your C service is a bottleneck of your system. Solutions are:</p>&#xA;&#xA;<ol>&#xA;<li>Scale C service. You should have a load balancer and more C service nodes.</li>&#xA;<li>Use cache for C service results.</li>&#xA;<li>Change your architecture with splitting C logic into 2 microservices.</li>&#xA;</ol>&#xA;
45649461,45648115,4850916,2017-08-12T11:00:49,"<p><strong>Don't use <code>require(../../utils/logger)</code>, use npm packages</strong></p>&#xA;&#xA;<p>You should avoid using same files for microservice with symlink or requiring from one folder, because it destroys <code>Loose coupling</code>.</p>&#xA;&#xA;<blockquote>&#xA;  <p><code>Loose coupling</code> is a design goal that seeks to reduce the&#xA;  inter-dependencies between components of a system with the goal of&#xA;  reducing the risk that changes in one component will require changes&#xA;  in any other component. Loose coupling is a much more generic concept&#xA;  intended to increase the flexibility of a system, make it more&#xA;  maintainable, and makes the entire framework more ""stable"".</p>&#xA;</blockquote>&#xA;&#xA;<p>Simply put, you can't have different version of your logger file, but you can have different version of your logger npm package.</p>&#xA;&#xA;<p><strong>Implementation details for using npm modules as reusable components for Node.js microservices</strong>:</p>&#xA;&#xA;<ol>&#xA;<li>Chose naming convention for packages. My advice is <a href=""https://docs.npmjs.com/misc/scope"" rel=""noreferrer"">scoped packages</a>. Example: <code>@vaibhav/logger</code></li>&#xA;<li><p>Chose npm registry. There are such options:</p>&#xA;&#xA;<ul>&#xA;<li>2.1. <a href=""https://www.npmjs.com/pricing"" rel=""noreferrer"">npmjs.com</a> and public packages. It's free, but your packages should have only universal code without any business-valuable details.</li>&#xA;<li>2.2 <a href=""https://www.npmjs.com/pricing"" rel=""noreferrer"">npmjs.com</a> with private packages. It's fast, but not free. </li>&#xA;<li>2.3 <a href=""http://www.verdaccio.org"" rel=""noreferrer"">verdaccio</a> your own npm registry server. It's free simple Node.js solution, which should be install as server in your infrastructure.</li>&#xA;<li>2.4 <a href=""http://www.sonatype.org/nexus/2017/02/14/using-nexus-3-as-your-repository-part-2-npm-packages/"" rel=""noreferrer"">nexus</a>. Universal private registry with npm and docker support.</li>&#xA;</ul></li>&#xA;<li><p>If you use 2.3 or 2.4 solution, then you need to choose ip or link for your server. My advice is use link. Example <code>https://your-registry.com</code></p></li>&#xA;<li>If you use 2.3 or 2.4 solution, the you need to chose install approach in your <a href=""https://docs.npmjs.com/files/npmrc"" rel=""noreferrer"">.npmrc file</a> inside your microservice. There are two options:&#xA;&#xA;<ul>&#xA;<li>install all required packages from your registry. The .npmrc file will looks like <code>registry=https://your-registry.com</code>. Your registry should be able to cache public packages.</li>&#xA;<li>install only your package from your registry, install other packages from public registry. The .npmrc file will looks like <code>@vaibhav:registry=https://mycustomregistry.example.org</code> </li>&#xA;</ul></li>&#xA;<li>Define processes for package development, publishing and updating the package version in microservice <a href=""https://docs.npmjs.com/files/package-lock.json"" rel=""noreferrer"">package-lock.json</a> file. In our projects we define processes in this way:&#xA;&#xA;<ul>&#xA;<li>We use <a href=""https://guides.github.com/introduction/flow/"" rel=""noreferrer"">GitHub flow</a> for <strong>package development</strong>. There are only master branch for publishing and feature branches for development. Master branch can be updated only with pull requests from developers or commits from CI server.</li>&#xA;<li>We use Jenkins as continues integration server for <strong>autoupdating version and publish</strong> after merging pull request. Jenkins runs <a href=""https://docs.npmjs.com/cli/version"" rel=""noreferrer"">npm version</a> command for update version, then publishes new commit to master branch and then publishes to npm registry. Jenkins checks some our rules and use <code>npm version</code> with <code>patch</code> or <code>minor</code> param. Updating major version is breaking change, which we do manualy.</li>&#xA;<li>We don't have 100% automated <strong>process for updating the package version</strong> in microservices. We automate only opening pull requests with new package version in <code>package-lock.json</code> file. Developers should check build status and press merge button manually.</li>&#xA;</ul></li>&#xA;</ol>&#xA;"
44886647,44882977,2788,2017-07-03T13:19:21,"<p>This kind of question is really beyond the scope of Stackoverflow, but it really sounds like you haven't come to understand the pieces of your infrastructure yet. </p>&#xA;&#xA;<p>The Netflix stack (Eureka/Zuul etc) and things like Zipkin, Hystrix and the whole ELK stack only start to make sense when you have really large deployments of many services in multi-site, with many hosts where managing ""by hand"" becomes a real problem, where you have a lot of moving parts in the architecture where something can break and your system still needs to keep running, like a host disconnects or a database node dies.</p>&#xA;&#xA;<p>With 2 hosts and a couple of services it doesn't make sense to introduce all this complexity, it will just overwhelm and confuse you (it already has). If one of your 2 hosts dies even if you're using Eureka and Zuul and it will not save you. The whole system will go down.</p>&#xA;&#xA;<p>Throw out all those latest buzzword libraries (you're not Netflix yet) and just think through a simple architecture where you will run your services say on one host and database on another host (no need for Eureka or Zuul). Think of a shared location for logs and organise a nice, easy to use folder structure to store them so they're easy to find and search with simple command line tools that are much better than Kibana (which is TERRIBLE to look at logs).</p>&#xA;&#xA;<p>Stay simple and only introduce new pieces when you feel it is getting difficult to manage.</p>&#xA;"
52070307,52070247,2788,2018-08-29T05:51:37,"<p>From Eureka <a href=""https://github.com/Netflix/eureka/wiki/Understanding-eureka-client-server-communication#fetch-registry"" rel=""nofollow noreferrer"">documentation</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p><strong>Fetch Registry</strong></p>&#xA;  &#xA;  <p>Eureka clients fetches the registry information from the server and&#xA;  caches it locally. After that, the clients use that information to&#xA;  find other services. This information is updated periodically (every&#xA;  30 seconds) by getting the delta updates between the last fetch cycle&#xA;  and the current one. The delta information is held longer (for about 3&#xA;  mins) in the server, hence the delta fetches may return the same&#xA;  instances again. The Eureka client automatically handles the duplicate&#xA;  information.</p>&#xA;  &#xA;  <p>After getting the deltas, Eureka client reconciles the information&#xA;  with the server by comparing the instance counts returned by the&#xA;  server and if the information does not match for some reason, the&#xA;  whole registry information is fetched again. Eureka server caches the&#xA;  compressed payload of the deltas, whole registry and also per&#xA;  application as well as the uncompressed information of the same. The&#xA;  payload also supports both JSON/XML formats. Eureka client gets the&#xA;  information in compressed JSON format using jersey apache client.</p>&#xA;</blockquote>&#xA;"
49500474,48406730,69348,2018-03-26T20:58:01,"<p>Putting it behind a DNS name with a load balancer across multiple SDS servers is a good setup for reasonable availability. If SDS is down, Envoy will simple not get updated, so it's generally not the most critical failure -- new hosts and services simply won't get added to the cluster/endpoint model in Envoy. </p>&#xA;&#xA;<p>If you want higher availability, you set up multiple clusters. If you add multiple entries to your bootstrap config, Envoy will fail over between them. You can either specify multiple DNS names or multiple IPs.</p>&#xA;&#xA;<p><strong>(My answer after misunderstanding the question below, for posterity)</strong></p>&#xA;&#xA;<blockquote>&#xA;  <p>You can start with a static config or DNS, but you'll probably want to&#xA;  check out a full integration with your service discovery.</p>&#xA;  &#xA;  <p>Check out <a href=""https://www.learnenvoy.io/articles/service-discovery.html"" rel=""nofollow noreferrer"">Service Discovery&#xA;  Integration</a>&#xA;  on LearnEnvoy.io.</p>&#xA;</blockquote>&#xA;"
49687595,49683432,2900196,2018-04-06T07:29:18,"<p>Your microservice can do that :</p>&#xA;&#xA;<ul>&#xA;<li><p>Subscribe a Kafka topic and consume the messages from this topic and process that messages (to do that you need to create a consumer with a <code>group.id</code> in your microservice)</p></li>&#xA;<li><p>Send messages in one or more topics to contact other microservices (to do that you need to create one or more producers in your microservice)</p></li>&#xA;</ul>&#xA;&#xA;<p>I recommend you to read some articles about microservices architectures using Kafka or old fashion JMS compliant system (like ActiveMQ or RabbitMQ for example).</p>&#xA;&#xA;<p>You can begin with this one : <a href=""https://medium.com/@ulymarins/an-introduction-to-apache-kafka-and-microservices-communication-bf0a0966d63"" rel=""nofollow noreferrer"">https://medium.com/@ulymarins/an-introduction-to-apache-kafka-and-microservices-communication-bf0a0966d63</a></p>&#xA;&#xA;<p>Here is an example of scenario :</p>&#xA;&#xA;<ul>&#xA;<li><p>Service A: a restfull endpoint to order a burger menu:</p>&#xA;&#xA;<p><code>POST /v1/order</code></p>&#xA;&#xA;<p><code>{""type"": ""cheeseburger"", ""quantity"": 1, ""ketchup"": true, ""notify"": [""cooker""]}</code></p>&#xA;&#xA;<p>This service use a Kafka producer to send the JSON in a topic called <code>TOPIC_ORDERS</code></p></li>&#xA;<li><p>Service B: consume the messages of the <code>TOPIC_ORDER</code> and send a SMS to the cooker and insert the order in database and produce the created order with ids in another topic <code>CREATED_ORDERS</code></p></li>&#xA;<li><p>Service C: another restfull endpoint used by the cooker when the menu is cooked:</p>&#xA;&#xA;<p><code>PATCH /v1/order/{id}</code></p>&#xA;&#xA;<p><code>{""ready"": true, ""notify"": [""customer""]}</code></p>&#xA;&#xA;<p>This one produce the message in the <code>TOPIC_ORDER</code> in order to call the service B to update the order in database and notify the customer.</p></li>&#xA;<li><p>Service D: consume the messages of <code>CREATED_ORDERS</code> topic to index in a search engine used by some other applications (to make stats for example)</p></li>&#xA;</ul>&#xA;"
49617615,49615519,2900196,2018-04-02T19:33:02,"<p>There's an infinite of possibilities to allow vert.x microservices to communicate between them, each with pros and cons and with more or less relevance depending on the context.</p>&#xA;&#xA;<p>Here is 3 common ways :</p>&#xA;&#xA;<p>1) Using the native vert.x eventBus (&#xA;asynchronous logic) : <a href=""https://vertx.io/docs/vertx-core/java/#event_bus"" rel=""nofollow noreferrer"">https://vertx.io/docs/vertx-core/java/#event_bus</a></p>&#xA;&#xA;<p>2) Using a messages broker system like Apache Kafka (&#xA;sometimes you need persistent message queues with replay mechanisms, which I think is more powerful than the vert.x's event bus, sometimes you need to communicate with multiple languages written microservices and the vert.x event bus is not relevant to do that) or an old fashion JMS compliant system like ActiveMQ, RabbitMQ &amp; cie.</p>&#xA;&#xA;<p>3) Sometimes it's more relevant to expose simple Restfull api, so you can use the <code>vertx-web</code> extension to do that : <a href=""https://vertx.io/docs/vertx-web/java/"" rel=""nofollow noreferrer"">https://vertx.io/docs/vertx-web/java/</a></p>&#xA;"
45719270,45547556,936676,2017-08-16T17:03:20,"<p>I personally would go down a simpler route using a test DNS entry for the green deployment which is then swapped out for the live DNS entry when you have fully verified your green deployment is good.</p>&#xA;&#xA;<p>So what do I mean by this:</p>&#xA;&#xA;<p>You state that your live deployments have the following DNS entries:</p>&#xA;&#xA;<ul>&#xA;<li>a.mydns.com</li>&#xA;<li>b.mydns.com</li>&#xA;</ul>&#xA;&#xA;<p>I would suggest that you create a pattern where each micro-service deployment also gets a test dns entry:</p>&#xA;&#xA;<ul>&#xA;<li>test.a.mydns.com</li>&#xA;<li>test.b.mydns.com</li>&#xA;</ul>&#xA;&#xA;<p>When deploying the ""green"" version of your micro-service, you deploy everything (including the ELB) and map the CNAME of the ELB to the test DNS entry in Route 53. This means you have the green version ready to go, but not being used by your live application. The green version has it's own DNS entry, so you can run your full test-suite against the test.a.mydns.com domain. </p>&#xA;&#xA;<p>If (and only if) the test suite passes, you swap the CNAME entry for a.mydns.com to be the ELB that was created as part of your green deployment. This means that your existing micro-services simply start talking to your green deployment once DNS propagates. If there is an issue, simply reverse the DNS update to the old CNAME entry and you have fully rolled-back.</p>&#xA;&#xA;<p>It requires a little bit of co-ordination here, but you should be able to automate the whole thing with something like Jenkins and the AWS CLI.</p>&#xA;"
50808206,50785058,157924,2018-06-12T01:49:29,"<p><a href=""https://tools.ietf.org/html/rfc6455"" rel=""nofollow noreferrer"">WebSocket</a> is an older standard part of the <a href=""https://en.wikipedia.org/wiki/HTML5"" rel=""nofollow noreferrer"">HTML5 Features</a>, whereas gRPC is a complete remote procedure call mechanism. gRPC uses HTTP/2 under the hood. </p>&#xA;&#xA;<p>That said, comparing <strong>gRPC to WebSockets</strong> is a bit like <strong>apple to oranges</strong>.&#xA;The HTTP/2 vs WebSockets comparison is more interesting in my opinion. Here is a link to one I like: <a href=""https://www.infoq.com/articles/websocket-and-http2-coexist"" rel=""nofollow noreferrer"">Will WebSocket survive HTTP/2?</a></p>&#xA;&#xA;<p>On top of the bare communication protocol itself, gRPC handles a lot more. It adds the whole RPC layer. You define everything in a proto file, and the statically typed (depends on the language) client and server codes are generated. <strong>HTTP/2 is an implementation detail of gRPC</strong>, and actually, it could be implemented using WebSockets, but WebSockets would not add much on top of bare TCP communication (except for the security layer in case of wss://)</p>&#xA;&#xA;<p>WebSockets was more like a workaround for implementing server push back when it was not available in the HTTP/1.1 protocol, and long polling was the only option.</p>&#xA;&#xA;<p><strong>Answering the question:</strong> I do not know of any WebSocket based microservice intercommunication framework, but I do use gRPC for this purpose, and it is awesome ;)</p>&#xA;"
43398103,43384538,1873380,2017-04-13T17:12:05,<p>I doesn't matter <strong>where</strong> so much (but not within the same container as stated earlier in this thread). The important part is that only one microservice has the <strong>ownership</strong> of the data. If more than one microservice needs access to the data they must access it through a api provided by the microservice that owns that data.</p>&#xA;
49197747,49187093,2137316,2018-03-09T16:08:17,"<p>I'm partial to the <code>tymon/jwt-auth</code> package for this, which largely uses 'namshi/jose' under the hood.  As long as the <code>jwt.secret</code> is the same between each system that may need to use the tokens, you should be able to just call <code>JWTAuth::getPayload($token)-&gt;toArray()</code> to decode them.</p>&#xA;&#xA;<p>You do need a user table, but it doesn't have to be <em>the</em> user table already speced out in Laravel.  Any Model that implements the <code>Illuminate\Auth\Authenticatable</code> interface, such as by extending the <code>Illuminate\Foundation\Auth\User</code> model will do.</p>&#xA;&#xA;<p>If you want to inject additional data into the token beyond the user table that the login credentials are being validated against, just add an array as the second parameter to the login attempt:</p>&#xA;&#xA;<pre><code>//The key names here are defined by the return values of methods described in the Authenticatable interface.&#xA;$credentials = [&#xA;    'username' =&gt; 'your_user',&#xA;    'password' =&gt; 'your_password'&#xA;];&#xA;$customClaims = [&#xA;    'someOtherDatapoint' =&gt; 'more_data'&#xA;];&#xA;$token = JWTAuth::attempt($credentials, $customClaims);&#xA;</code></pre>&#xA;&#xA;<p>You could also go directly to the token-creation without authentication:</p>&#xA;&#xA;<pre><code>$user = app(YourUserModel::class)-&gt;first($id);&#xA;$token = JWTAuth::fromUser($user, $customClaims);&#xA;</code></pre>&#xA;&#xA;<p>This is relatively secure, but I'm not sure if it's the best way to communicate encrypted data.  You might want to just use the <code>encrypt()</code> and <code>decrypt()</code> helper functions, that are based off the APP_KEY.  You could also look at Laravel-Passport, <a href=""https://laravel.com/docs/5.6/passport"" rel=""nofollow noreferrer"">https://laravel.com/docs/5.6/passport</a>, for authentication, which uses OAuth2.</p>&#xA;"
29833991,29831699,4573668,2015-04-23T20:46:36,"<p>I agree with Ram's suggestion to retrieve service parameters via <code>VCAP_SERVICES</code>, especially when it specifies information intrinsic to the binding of a service to your application like credentials. However, for more mundane configuration properties (e.g., what languages are supported and where are the translations located? What URL should be used for invoking this REST service?), traditional methods like passing them on the command line, retrieving them from a configuration file, or getting them from environment variables specified by the admin at deployment time are perfectly legitimate.</p>&#xA;&#xA;<p>Pat Mueller's <a href=""https://developer.ibm.com/bluemix/2014/10/14/keeping-secrets-cloud-application-access-credentials-private-data/"" rel=""nofollow"">Keeping secrets – how your cloud application should access credentials and other private data</a> nicely summarizes the options and tradeoffs. Most importantly, he underscores the importance of <strong>not</strong> hardcoding sensitive information, especially given the likelihood that code will be stored in a repository where it's not readily evident who will have access, unlike a deployment script maintained by the system admin.</p>&#xA;"
33933542,31786040,132325,2015-11-26T08:17:07,"<p>Assuming you are using Ribbon as well, I would leave the service IDs as they are. Instead I would take a look at the <a href=""https://github.com/Netflix/ribbon/tree/v2.1.1/ribbon-loadbalancer/src/main/java/com/netflix/loadbalancer""><code>com.netflix.loadbalancer</code></a> package. Canary deployments are essentially load balancing with very specific constraints. You could implement your own <a href=""https://github.com/Netflix/ribbon/blob/v2.1.1/ribbon-loadbalancer/src/main/java/com/netflix/loadbalancer/AbstractLoadBalancerRule.java""><code>AbstractLoadBalancerRule</code></a> that picks servers based on some property you would like to base the routing on. Then add that rule to the configuration of your Zuul instance.</p>&#xA;&#xA;<pre><code>@Configuration&#xA;public class CanaryConfiguration {&#xA;    @Bean public IRule canaryDeploymentRule(IClientConfig config) {&#xA;      CanaryDeploymentRule rule = new CanaryDeploymentRule ();&#xA;      rule.initWithNiwsConfig(config);&#xA;      return rule;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>If you let your services register themselves in Eureka with different service IDs (""simpleservice"" and ""simpleservice-x.y""), I suppose things are bound to become complicated. You would have to extend the discovery client to ignore the version part (""-x.y"", while still being able to deal with ""foo-service"") when retrieving the list of available servers from Eureka and then do some selection process to pick the right one anyway. So I guess things would just become more complex.</p>&#xA;&#xA;<p>This is all based on best guesses, I haven't actually implemented this. I realize that this question is almost 4 months old. So if you have found some other solution in the meantime, it would be great if you could share it in an answer to your own question.</p>&#xA;"
42053983,40786831,138469,2017-02-05T15:45:08,"<p>We are using <a href=""https://github.com/kubernetes/helm"" rel=""nofollow noreferrer"">kubernetes/helm</a> (the latest/incubated version) and a central repository for Helm charts (with references container images built for our component releases).</p>&#xA;&#xA;<p>In other words, the Helm package definitions and its dependencies are separate from the source code and image definitions that make up the several components of our web applications.</p>&#xA;"
44481552,44471051,4006629,2017-06-11T07:49:47,"<p>You can use a PKI signing/verification system. In this approach, your authorization service will sign the JWT using a private key and all the consuming services will need the public key to use the JWT. The public key can be distributed easily - maybe through a config server if you have that in your architecture. Even if someone breaks into one of your services then he will only have a public key and not the private key.</p>&#xA;"
44967535,44965110,6694629,2017-07-07T09:39:44,"<p>Please don't confuse between promises returned from request-promise and async functions.&#xA;async functions can be <strong>await</strong>ed to get a resolved promise and make your code look good.</p>&#xA;&#xA;<p>I believe in letting people resolve their own problems and just guiding them along the way so just to make sure, aren't you getting the return from the resolved promise in this line of yours:</p>&#xA;&#xA;<pre><code>console.log(rs)&#xA;</code></pre>&#xA;&#xA;<p>Moreover, by looking at your snippet, you're returning a response.body from within request-promise's thenable. You cannot catch any response errors from the response body, right?</p>&#xA;&#xA;<p>I'd strongly suggest following a pattern wherein you'd catch errors (where you should) and display proper messages when you do. Wrapping your await call in a try/catch can help catch uncaught errors from request-promise.</p>&#xA;&#xA;<p>Peace!</p>&#xA;"
51627875,51627473,6565093,2018-08-01T07:40:37,"<p>Yml standards dictate us to specify a space after colon(<code>:</code>). Please have a look at the <a href=""http://driver-class-name:oracle.jdbc.driver.OracleDriver"" rel=""nofollow noreferrer"">documentation</a>&#xA;  Change your yml like this and It should work fine.  </p>&#xA;&#xA;<pre><code>spring:&#xA;  profiles: dev&#xA;  datasource:&#xA;     url: jdbc:oracle:XXX:@XXX:XXX/XXX&#xA;     username: XXX&#xA;     password: XXX&#xA;     driver-class-name: oracle.jdbc.driver.OracleDriver&#xA;</code></pre>&#xA;"
47922151,47922096,6565093,2017-12-21T09:48:36,<p>By definition fallback method means a substitution for real method if something were to happen.<br>&#xA;You cannot return different type. It doesn't make any sense.<br>&#xA;Hence method signature has to be exactly same with a few exceptions. </p>&#xA;&#xA;<ol>&#xA;<li>Return types has to be same or covariant  </li>&#xA;<li>Method arguments has to be same. And you can add an extra argument of type <code>Throwable</code> to get the exception on which fallback was called.</li>&#xA;</ol>&#xA;
29821504,29821391,1000933,2015-04-23T11:02:42,"<p>The part after the <code>#</code> will never be sent to the server so in this case you'll always end-up with the same request within the loop. </p>&#xA;&#xA;<p>What you can do is add a custom header in your controller, and make your controller intercept only the code where the header is not present by using the negation of the <code>headers</code> attribute, here's a pseudo code</p>&#xA;&#xA;<pre><code>@RequestMapping(value=[YOUR MAPPING], method=RequestMethod.GET, headers=""!CustomHeader"")&#xA;</code></pre>&#xA;"
42970000,42969532,281829,2017-03-23T07:45:10,"<p>The keyword for your case is 'Service Instance'</p>&#xA;&#xA;<p>You can create a service instance of database server within the environment specific for your application and bind it via application manifest.</p>&#xA;&#xA;<p>e.g.</p>&#xA;&#xA;<pre><code>cf create-service rabbitmq small-plan myapplication-rabbitmq-instance&#xA;</code></pre>&#xA;&#xA;<p>As long as you have a binding to myapplication-rabbitmq-instance in your application manifest it would be preserved/be the same between application deployments within this space.</p>&#xA;&#xA;<p>e.g. in your application manifest:</p>&#xA;&#xA;<pre><code>---&#xA;...&#xA;  services:&#xA;   - myapplication-rabbitmq-instance&#xA;</code></pre>&#xA;&#xA;<p>More on <a href=""https://docs.cloudfoundry.org/devguide/services/"" rel=""nofollow noreferrer"">https://docs.cloudfoundry.org/devguide/services/</a></p>&#xA;"
51472708,51463528,5336549,2018-07-23T06:26:27,"<p>New requirement needs data from cross Domain  </p>&#xA;&#xA;<p>Below are the ways</p>&#xA;&#xA;<ol>&#xA;<li><p>Update the customer Id and Name in every call . Issue is latency as&#xA;there would be multiple round trips </p></li>&#xA;<li><p>Have a cache of all CustomerName with ID in Order Service ( I am&#xA;assuming there a finite customers ).Issue would be , when to refresh&#xA;cache or invalidate cache , For that you may need to expose some&#xA;rest call to invalidate fields. For new customers which are not&#xA;there in cache go and fetch from DB and update cache for future . ) </p></li>&#xA;<li><p>Use CQRS way in which all the needed data( Orders customers etc ..) goes to a separate table . Now in this schema you can create a composite SQL query . This will remove the round trips etc ...</p></li>&#xA;</ol>&#xA;"
52111846,52105225,5336549,2018-08-31T08:53:44,"<p>There is no silver bullet , but below can help you in getting the answer</p>&#xA;&#xA;<p>Calling DB separately</p>&#xA;&#xA;<p>Pros</p>&#xA;&#xA;<ul>&#xA;<li>Complete control over data </li>&#xA;<li>Only one hop for accessing data</li>&#xA;<li>Future changes can be implemented easily without waiting for another&#xA;service's upgrade cycle</li>&#xA;</ul>&#xA;&#xA;<p>Cons </p>&#xA;&#xA;<ul>&#xA;<li>If DB is sharded , need logic in your app to get data</li>&#xA;<li>Duplication of code     </li>&#xA;<li>Maintenance ,now there is more than one place some one have to track about DB queries and impact</li>&#xA;</ul>&#xA;&#xA;<p>Calling via another web service</p>&#xA;&#xA;<p>Pros </p>&#xA;&#xA;<ul>&#xA;<li>Easy Maintenance - all DB related logic at one place (including sharding related issues)</li>&#xA;<li>Easy monitoring of read traffic</li>&#xA;</ul>&#xA;&#xA;<p>Cons</p>&#xA;&#xA;<ul>&#xA;<li>Increase in the hop</li>&#xA;<li>Downtime of this service will impact your service</li>&#xA;</ul>&#xA;"
47164159,47163923,492773,2017-11-07T17:37:52,"<p>I built something similar to what you're asking before. I ran <code>hyperkube</code> manually, which is hardly recommended but did the trick for local development. In my case this was all running in Vagrant for team uniformity.</p>&#xA;&#xA;<pre><code>docker run -d --name=kubelet \&#xA;        --volume=/:/rootfs:ro \&#xA;        --volume=/sys:/sys:ro \&#xA;        --volume=/var/lib/docker/:/var/lib/docker:rw \&#xA;        --volume=/var/lib/kubelet/:/var/lib/kubelet:slave \&#xA;        --volume=/var/run:/var/run:rw \&#xA;        --net=host \&#xA;        --pid=host \&#xA;        --privileged \&#xA;        --restart=always \&#xA;        gcr.io/google_containers/hyperkube-${ARCH}:${K8S_VERSION} \&#xA;        /hyperkube kubelet \&#xA;            --containerized \&#xA;            --hostname-override=127.0.0.1 \&#xA;            --api-servers=http://localhost:8080 \&#xA;            --cluster-dns=10.0.0.10 \&#xA;            --cluster-domain=cluster.local \&#xA;            --allow-privileged --v=2 \&#xA;            --image-gc-high-threshold=50 \&#xA;            --image-gc-low-threshold=40 \&#xA;            --kube-api-qps 1000 --kube-api-burst=2000 \&#xA;            --pod-manifest-path=/etc/kubernetes/manifests&#xA;</code></pre>&#xA;&#xA;<p>On top of this, I had build scripts that would use YAML <a href=""https://mustache.github.io/"" rel=""nofollow noreferrer"">mustache</a> templates that were aware where this was being deployed. When this was being deployed locally, every pod had the source code mounted as a volume so I could auto-reload it.</p>&#xA;&#xA;<p>The same scripts were able to deploy to production thanks to it all being based on mustache templates. I even had multiple configuration files that would apply different template values for different environments.</p>&#xA;&#xA;<p>The build script would prepare the YAML templates, build whatever images it needs to build, apply to Kubernetes and from there it would just auto-reload. It was a semi-nice user experience. My main issue was sluggishness when it come to file updating because it was running inside Docker inside Vagrant. There was no file sharing type that would provide good performance for both client and server <strong>and</strong> allow for file watching (<code>inotify</code> didn't work with most file share types, and NFS/SMB was slow for IDEs).</p>&#xA;&#xA;<p>It was my first Kubernetes experience, so I doubt it's the ""recommended way"", but it worked. There was a lot of scripting involved so there are probably better ways to do this today.</p>&#xA;"
42938882,42934172,1284837,2017-03-21T21:57:10,"<p>Depending on your environment, I see several possible solutions.</p>&#xA;&#xA;<p><strong>1. You can have a config file in Angular</strong></p>&#xA;&#xA;<p>For example, Angular Cli provides 2 files: environment.ts and environment.prod.ts</p>&#xA;&#xA;<p>The content of the file can be:</p>&#xA;&#xA;<pre><code>export const environment = {&#xA;  production: false,&#xA;  apiBaseUrl: 'http://localhost:3000'&#xA;};&#xA;</code></pre>&#xA;&#xA;<p>and the equivalent for production can be the real DNS url</p>&#xA;&#xA;<p>This is provided by Angular CLI but you can do the same without the CLI. The idea here is that the files are chosen at compile time.</p>&#xA;&#xA;<p><strong>2. Always use the same server</strong></p>&#xA;&#xA;<p>You can assume that the hostname will be the same as the one you got your Angular app from:</p>&#xA;&#xA;<pre><code>import { Location } from '@angular/common';&#xA;// Get the hostname&#xA;this.hostname = location.host;&#xA;if (this.hostname.indexOf(':') &gt; 0) {&#xA; this.hostname = this.hostname.substr(0, this.hostname.indexOf(':'));&#xA;}&#xA;// Add a port or a subdomain to get the API url:&#xA;this.apiUrl = 'http://' + this.hostname + ':8080';&#xA;</code></pre>&#xA;&#xA;<p><strong>Variables in compose?</strong></p>&#xA;&#xA;<p>As you said in an earlier comment, the calls are made from the browser, so outside Docker. I don't see an easy way to put this information in the compose file. That said I would be interested to find out.</p>&#xA;&#xA;<p>I hope it helps.</p>&#xA;"
33310482,33308154,4498917,2015-10-23T19:59:40,"<p>As James said - it's not possible currently.</p>&#xA;&#xA;<p>One workaround you could use to ""simulate"" a load balancer would be to set up an application which fronts all of the requests. Bind the route to this application and then include some logic inside it which just redirects traffic to the other services in a 9:1 ratio.</p>&#xA;&#xA;<p>The risks of using this approach are:</p>&#xA;&#xA;<ul>&#xA;<li>Your load-balancer application could theoretically go down</li>&#xA;<li>Your load-balancer application doesn't know the status of the other two services so might route traffic to an offline service instance</li>&#xA;</ul>&#xA;&#xA;<p>You can minimize the first risk by using multiple instances on the ""load balancer"" app.</p>&#xA;"
50754442,41762928,576741,2018-06-08T06:16:17,"<p>I think I should just close this for now, it's been a while. Like I said in the comment above, I have built and used logical counterpart for Feign, Ribbon, with extensible support for service discovery frameworks (currently plugin available for consul.in), with different strategies for load balancing, and declarative API mapping like that of feign interfaces in spring boot, configurable retries, fluent API and support for hystrix. It's a family of node modules: <a href=""https://www.npmjs.com/search?q=Sarathi"" rel=""nofollow noreferrer"">https://www.npmjs.com/search?q=Sarathi</a></p>&#xA;"
40491573,40479926,187365,2016-11-08T16:10:41,"<p>The whole point of Microservice architecture is to isolate functionality in separate distinct bounded contexts.  From you example you've got 2 bounded contexts (schema 1 and schema 2).  You don't share domain objects across those domains.  You may share the Id's, but you are going to have to go back to the microservice that owns the domain object to populate it.   </p>&#xA;"
38254916,38254720,4516837,2016-07-07T20:42:01,"<p>Sure you can.&#xA;Microservice architecture is a concept of having many small interracting components, where each of them performing well defined part of work, but good.</p>&#xA;&#xA;<p>It's extention of the Linux way and the concept of decoupling components.</p>&#xA;&#xA;<p>In your case you can split your service to several smaller services. Each one with own development and deployment cycles, each one with well defined API.</p>&#xA;"
38446944,38440876,786662,2016-07-18T22:29:47,"<p>Do you mean:</p>&#xA;&#xA;<p><code>docker-compose up SERVICE</code> ?</p>&#xA;&#xA;<p>That should start a single service and any linked containers.&#xA;You can also do:</p>&#xA;&#xA;<p><code>docker-compose run SERVICE [COMMAND] [ARGS...]</code> </p>&#xA;&#xA;<p>which might be better for you depending on your needs, has some differences from <code>up</code> in how it's executed. See: <a href=""https://docs.docker.com/compose/reference/run/"" rel=""nofollow"">https://docs.docker.com/compose/reference/run/</a></p>&#xA;"
32249178,32248896,597222,2015-08-27T12:17:38,"<p>The granurality of microservices is always a question and there is no general good answer. Even sometimes it can be reasonable to go with monolith in the end. It really depends what are you trying to achieve.</p>&#xA;&#xA;<p>To your question, microservices usually aim to group up single part of domain (e.g. billing, shipping...). In your example I would say that microservice can be in charge of compression so it makes sense that both archiving and unarchaving can be within this single microservice.</p>&#xA;&#xA;<p>Thinking of microservice as a single function (e.g. untaring) is way too much granular from my point of view. It brings more ""cons"" than ""pros"" - usually more network traffic. Imagine situation that your microservices communicate with each other via HTTP (very common scenario these days) and you would want to create tar.gz archive. One microservice would do tar and the other gzip and there will be unnecessary network traffic that will cost you time/bandwith...</p>&#xA;"
36522884,36519652,6048742,2016-04-09T21:24:47,"<p>I don't know the design of the whole system, but I would let the user microservice send an event when a user is created, updated or deleted. All other microservices which are interested in this event can extract the necessary user data they need. </p>&#xA;&#xA;<p>By using microservices a redundant data cannot be avoided, but each microservice has different view on the data. </p>&#xA;&#xA;<p>Your user microservice may see the user more as identity with complete user data (e.g id, password, first name, last name, email, address, organization, etc.)&#xA;On the other hand, your document generation microservice may see the user as receiver of the document and do not need all user data.</p>&#xA;"
36762806,36744042,6048742,2016-04-21T07:34:49,"<p>Not doing the pagination with SQL can lead to performance problems in case of large databases.</p>&#xA;&#xA;<p>Some solution in between can be applied. I assume that the pagination parameters (offset, page size) are configurable for both services, yours and the external one. </p>&#xA;&#xA;<p>You can implement prefetch logic for both services, lets say the prefetch chunk size can be 100.</p>&#xA;&#xA;<p>The frontend can be served with required page size 10. </p>&#xA;&#xA;<p>If the prefetched chunks do not result in a frontend page size 10, the backend should prefetch another chunk till the fronend can be served with 10 students.</p>&#xA;&#xA;<p>This approach require more logic in backend to calculate the next offsets for prefetching, but if you want performance and pagination solved you must invest some effort.</p>&#xA;"
43573771,43529266,1663462,2017-04-23T16:37:55,<p>This only answers your follow up question (stack ghci within docker). Yes it's possible.</p>&#xA;&#xA;<p>Depending what your service/container is named as (you can determine this with <code>docker ps</code>):</p>&#xA;&#xA;<p>If your container is already running (via <code>docker-compose up</code> / <code>docker run</code>):</p>&#xA;&#xA;<p><code>docker exec -it directoryName_my-service_1 /bin/stack ghci</code></p>&#xA;
45223413,45145109,1676006,2017-07-20T19:13:42,"<p>The answer ended up being a little janky, but it works. Basically, it looks like this:</p>&#xA;&#xA;<pre><code>task runEmu1(type: com.github.psxpaul.task.JavaExecFork) {&#xA;     //execfork stuff&#xA;}&#xA;task runEmu2(type: com.github.psxpaul.task.JavaExecFork) {&#xA;     //execfork stuff&#xA;}&#xA;task runEmus() {&#xA;    dependsOn('runEmu1', 'runEmu2')&#xA;    doLast {&#xA;        ant.input(message: 'Press enter to shut down emulators')&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Runs both emulators in parallel processes, which kill themselves at the end of the build. The build, of course, does not complete until input is provided to <code>ant.input</code>. It monopolizes a terminal window, but I can live with that for now</p>&#xA;"
34098582,34094882,444646,2015-12-04T22:32:03,"<p>It may depend on your workflow. Are you likely to be making changes to multiple services at once?</p>&#xA;&#xA;<p>If you usually only make a change to one or two services at any given time, you can setup your <code>docker-compose.yml</code> to use only <code>image</code> fields (instead of <code>build</code>) , and to not use any host volumes.  With this setup, it doesn't matter where the code is checked out because the <code>docker-compose.yml</code> just uses images available to the docker daemon.</p>&#xA;&#xA;<p>This would require you to have other <code>docker-compose.yml</code> files (or a bash script, makefile, etc) in each repo to build the images for each service. </p>&#xA;&#xA;<p>If you need host volumes for interactive development, you use a <code>docker-compose.override.yml</code> to add them as necessary (but that would be up to the developer to point it at the right path).</p>&#xA;"
38461732,38461294,3787750,2016-07-19T14:44:11,"<p>make a class on both microservices or make a jar of that class and add to both microservices so that they both can access the same data.</p>&#xA;&#xA;<p>Lets say the class is </p>&#xA;&#xA;<pre><code>class TestData{&#xA; private String name;&#xA; private String id;&#xA;&#xA; // getters and setters&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Now you can send data from M1 to M2 as following</p>&#xA;&#xA;<pre><code>    RestTemplate restTemplate = new RestTemplate();&#xA;    HttpHeaders headers = new HttpHeaders();&#xA;    headers.setContentType(MediaType.APPLICATION_JSON);&#xA;    TestData data = new TestData();&#xA;    HttpEntity&lt;?&gt; entity = new HttpEntity&lt;Object&gt;(data,headers);&#xA;    ResponseEntity&lt;Object&gt; responseEntity =    restTemplate.exchange(""url"", HttpMethod.POST, entity, Object.class);&#xA;</code></pre>&#xA;&#xA;<p>In Microservice M2 you can write a controller to get the data and process it as follows</p>&#xA;&#xA;<pre><code>@RequestMapping(value=""/url"",method=RequestMethod.POST)&#xA;public Object do(@RequestBody TestData data){&#xA;  // do something&#xA;  return //something&#xA;}&#xA;</code></pre>&#xA;"
44474418,31165181,1434854,2017-06-10T14:25:07,"<ol>&#xA;<li>Do not use Elastic Beanstalk – use ECS for Docker deployments instead.</li>&#xA;<li><p>To tell Eureka which IP should be used, as described in <a href=""https://github.com/spring-cloud/spring-cloud-netflix/issues/30"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-netflix/issues/30</a>, use following snippet:</p>&#xA;&#xA;<p><code>public EurekaInstanceConfigBean eurekaInstanceConfig() {&#xA;    InetUtilsProperties inetUtilsProperties = new InetUtilsProperties();&#xA;    inetUtilsProperties.setDefaultHostname(EC2MetadataUtils.getLocalHostName());&#xA;    inetUtilsProperties.setDefaultIpAddress(EC2MetadataUtils.getPrivateIpAddress());&#xA;    EurekaInstanceConfigBean config = new EurekaInstanceConfigBean(new InetUtils(inetUtilsProperties));&#xA;    AmazonInfo info = AmazonInfo.Builder.newBuilder().autoBuild(""eureka"");&#xA;    config.setDataCenterInfo(info);&#xA;    info.getMetadata().put(AmazonInfo.MetaDataKey.publicHostname.getName(), EC2MetadataUtils.getLocalHostName());&#xA;    config.setHostname(EC2MetadataUtils.getLocalHostName());&#xA;    config.setIpAddress(EC2MetadataUtils.getPrivateIpAddress());&#xA;    config.setNonSecurePort(port);&#xA;    return config;&#xA;}</code></p></li>&#xA;</ol>&#xA;"
30274963,30267737,1116391,2015-05-16T11:16:44,"<p>Posting this as an answer because it's too long for a comment.</p>&#xA;&#xA;<p>Microservices are self contained components and as such are responsible for their own data. If you want the get to the data you  have to talk to the service API. This applies mainly to different kinds of services (i.e. you don't share a database among services that offer different kinds of business functionality - that's bad practice because you couple services at the heap through the database and it's then easy to couple more things that would normally be done at the API level but it's more convenient to do them through the database => you risk loosing componentization). </p>&#xA;&#xA;<p>But if you have the same kind of service then there are, as you mentioned, two obvious choices: share a database or have each service contain it's own database.</p>&#xA;&#xA;<p>Now you have to ask yourself which solution do you chose:</p>&#xA;&#xA;<ul>&#xA;<li>Are these <code>OrderService</code>s of yours truly capable of working on their own, or do you need to have all the orders in the same database for reporting or access by other applications?</li>&#xA;<li>determine what is your actual bottleneck. Is it the database? If not then share the database. Is it the services? If not then distribute your data.</li>&#xA;<li>need to distribute the data? What are your choices, what are your needs? Do you need to be consistent all the time or eventual consistency is good enough? Do you need to have separate databases and synchronize them manually or does your database installation handle replication and partitioning out of the box? </li>&#xA;<li>etc</li>&#xA;</ul>&#xA;&#xA;<p>What I'm trying to say is that in this kind of situations the answer is: it depends. And something that we tech geeks often forget to do before embarking on such distributed/scalability/architecture journeys is to talk to business. Often business can handle a certain degree of inconsistencies, suboptimal processes or looking up data in more places instead of one (i.e. what you think is important might not necessarily be for business). So talk to them and see what they can tolerate. Might be cheaper to resolve something in an operational way than to invest a lot into trying to build a highly distributable system.</p>&#xA;"
34925858,31573823,595943,2016-01-21T14:01:42,"<p>Option 1 and 2 creates tight coupling which should be avoided as much as possible because you would want to have your services to be independent. So the question becomes:</p>&#xA;&#xA;<p>How do we do this with an event-based architecture?</p>&#xA;&#xA;<ol>&#xA;<li><p>Use events to keep track of licensing information from license service in student service, practically a data duplication. Drawbacks here are: you only have eventual consistency as the data duplication is asynchronous.</p></li>&#xA;<li><p>Use asynchronous events to trigger event chain which ultimately trigger a student creation. From your question, it looks like you already got the idea, but have an issue dealing with UI. You have two possible options here: wait for the student creation (or failure) event with a small amount of timeout, or (event better), make you system completely reactive (use server-client push mechanism for the UI).</p></li>&#xA;</ol>&#xA;"
31097808,26975640,595943,2015-06-28T08:56:22,<p>The problem with how ESB is used is that it creates a coupling between ESB and services by having some business logic built into the ESB. This would makes it more difficult to deploy a single service independently and increasingly making the ESB more complex and difficult to maintain.</p>&#xA;
30969497,22513893,595943,2015-06-21T21:24:53,"<blockquote>&#xA;  <p>What I don't understand is whether people typically try to use messaging for common request/reply scenarios - for example, a user hits their ""profile"" page and part of the data that needs to get rendered on the page is from a user service.</p>&#xA;</blockquote>&#xA;&#xA;<p>The key is to avoid request/reply altogether. Technically this is possible if your whole stack allows async messaging, including for web front-end by using something like a WebSocket in a Single Page Application (SPA). For a practical example you can look into Typesafe's Reactive Maps template (<a href=""https://www.typesafe.com/activator/template/reactive-maps-java"" rel=""nofollow"">https://www.typesafe.com/activator/template/reactive-maps-java</a>).</p>&#xA;"
49725949,46236744,1908967,2018-04-09T05:09:57,"<p><strong>Swagger/OpenAPI 2.0</strong></p>&#xA;&#xA;<p>For Swagger Spec 2.0 (aka OpenAPI Spec 2.0), you can include definitions in the Swagger spec as mentioned by alamar. Swagger Codegen will automatically create model classes for these definitions, even if they are not associated with any paths. I build and maintain a Swagger Spec/Codegen-built <a href=""https://github.com/grokify/go-ringcentral"" rel=""nofollow noreferrer"">SDK in Go for the RingCentral API</a> that has events like this. You can see the auto-generated classes/structs Swagger Codegen builds in the following folder, filtering for the 20 files that end in <code>_event.go</code>. These are currently created using <a href=""https://github.com/swagger-api/swagger-codegen/"" rel=""nofollow noreferrer""><code>swagger-codegen</code> 2.3.1</a>.</p>&#xA;&#xA;<ul>&#xA;<li>Generated files: <a href=""https://github.com/grokify/go-ringcentral/tree/master/client"" rel=""nofollow noreferrer"">https://github.com/grokify/go-ringcentral/tree/master/client</a></li>&#xA;<li>Codegen info: <a href=""https://github.com/grokify/go-ringcentral/tree/master/codegen"" rel=""nofollow noreferrer"">https://github.com/grokify/go-ringcentral/tree/master/codegen</a></li>&#xA;</ul>&#xA;&#xA;<p>If you have multiple event types, having an event property that can distinguish message types you are receiving can help parse it into the correct event class/struct. In Go, you can unmarshal the data twice, once into a generic struct to peek at the event type property, and then a second time for the actual event type.</p>&#xA;&#xA;<p>I also maintain a collection of example events and parsing code in the <a href=""https://github.com/grokify/chathooks"" rel=""nofollow noreferrer"">Chathooks webhook reformatter project</a> you can use for reference. You can see the example events and (hand-rolled) language definitions here:</p>&#xA;&#xA;<ul>&#xA;<li>Examples: <a href=""https://github.com/grokify/chathooks/tree/master/docs/handlers"" rel=""nofollow noreferrer"">https://github.com/grokify/chathooks/tree/master/docs/handlers</a></li>&#xA;<li>Definitions: <a href=""https://github.com/grokify/chathooks/tree/master/src/handlers"" rel=""nofollow noreferrer"">https://github.com/grokify/chathooks/tree/master/src/handlers</a></li>&#xA;</ul>&#xA;&#xA;<p><strong>OpenAPI 3.0</strong></p>&#xA;&#xA;<p>With OpenAPI Spec 3.0, you can support multiple event types in one definition using it's <code>oneOf</code>, <code>anyOf</code>, <code>allOf</code> support. More information is in the OpenAPI 3.0 docs here:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://swagger.io/docs/specification/data-models/oneof-anyof-allof-not/"" rel=""nofollow noreferrer"">https://swagger.io/docs/specification/data-models/oneof-anyof-allof-not/</a></li>&#xA;</ul>&#xA;&#xA;<p>I haven't experimented with this yet because OpenAPI 3.0 spec isn't supported in Swagger Codegen for Go yet. However, they are actively working on Java so if that's your language of choice, you can give it a shot.</p>&#xA;&#xA;<p>You can follow Swagger Codgen support for OpenAPI 3.0 spec in this GitHub issue:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://github.com/swagger-api/swagger-codegen/issues/4669"" rel=""nofollow noreferrer"">https://github.com/swagger-api/swagger-codegen/issues/4669</a></li>&#xA;</ul>&#xA;"
43180406,41824300,3111911,2017-04-03T08:57:47,"<p>This may or may not help but I am currently building an API gateway in .NET core. </p>&#xA;&#xA;<p>You can find it at <a href=""https://github.com/TomPallister/Ocelot"" rel=""noreferrer"">https://github.com/TomPallister/Ocelot</a>.</p>&#xA;&#xA;<p>The code is a little ropey but a few people are working on it now so hopefully we can improve it over time.</p>&#xA;"
45651519,45625886,4695271,2017-08-12T14:47:03,"<p>Depending on the gRPC's <a href=""https://github.com/grpc/grpc/milestones"" rel=""noreferrer"">future roadmap</a>, people will continue migrating to it and letting REST (over HTTP) ""quiet"".</p>&#xA;&#xA;<p>gRPC is <em>more convenient</em> in many ways:</p>&#xA;&#xA;<ul>&#xA;<li>Usually fast (like super-fast)</li>&#xA;<li>(Almost) No ""design dichotomy"" ― what's the right end-point to use, what's the right HTTP verb to use, etc.</li>&#xA;<li>Not dealing with the <em>messy</em> input/response serialization baloney as gRPC deals with the serialization ― more efficient data encoding and HTTP/2 which makes things go faster with multiplexed requests over a single connection and header compression</li>&#xA;<li>Define/Declare your input/response and generate reliable clients for different languages (of course, the ones that are ""supported"", this is a HUGE advantage)</li>&#xA;<li>Formalized set of errors ― this is debatable but so far they are more directly applicable to API use cases than the HTTP status codes</li>&#xA;</ul>&#xA;&#xA;<p>In any case, you will have to deal with all the gRPC troubles also since nothing in this world is infalible, but so far it ""looks better"" than REST ― and has actually proven that.</p>&#xA;&#xA;<p>I think you can have the best of both worlds. In any case gRPC largely follows HTTP semantics (over HTTP/2) but explicitly allow for full-duplex streaming, diverging from typical REST conventions as it uses <em>static paths</em> for performance reasons during call dispatch as parsing call parameters from paths ― query parameters and payload body adds latency and complexity.</p>&#xA;"
42667855,42486386,5288679,2017-03-08T09:44:37,"<p>Don't use #1: when management talk of analyzed reports it don't bother data mismatch between ETL process - obviously you will generate a CEO report after finishing ETL for the day</p>&#xA;&#xA;<p>Don't use #2: this way you'll load transnational system with analytic overhead and dissolve isolation between purpose of two systems (not good for operation and maintenance)</p>&#xA;&#xA;<p>Use #3 as its the best way to fetch <strong>processed results</strong>, Use modern tools like <em>Excel, PowerQuery, PowerBI</em> to allow you to create rich dashboard with speed instead of going into tables and writing APIs.</p>&#xA;"
38291862,38070572,6295784,2016-07-10T12:29:16,"<p>I've already had this problem. What I would do in your case is:</p>&#xA;&#xA;<ul>&#xA;<li><p>Use this new OpenId microservice to authenticate the user and create the access-token. This access token can be a string with the permissions, user_id and the timestamp signed or you can store this info on a database.</p></li>&#xA;<li><p>Then, for every call (to product1 or product2): </p>&#xA;&#xA;<ul>&#xA;<li>I would force the client to send the access-token on the headers. </li>&#xA;<li>Then, when a microservice receives a call (lets say product1), I would send a signed request to the OpenId Microservice to ask if the user is allowed to perform that action.</li>&#xA;</ul></li>&#xA;</ul>&#xA;&#xA;<p>That way, just the OpenId microservice knows how authentication works. So if in a couple of weeks you want to change how authentication works, you just have to change it on the OpenId microservice.</p>&#xA;&#xA;<p>I dont really understand whats the problem with  third-parties. They will get the token and they will be able to perform calls passingit on the Access-token header.</p>&#xA;"
31480191,31088764,476648,2015-07-17T16:12:45,"<p>A Service Bus-based design allows your application to leverage the <a href=""http://screamingtiger.blogspot.ie/2010/04/release-it-chapter-58-decoupling.html"" rel=""nofollow"">decoupling middleware</a> design pattern. You have explicit control in terms of how each Microservice communicates. You can also throttle traffic. However, it really depends on your requirements. Please refer to <a href=""http://insidethecpu.com/2015/07/17/microservices-in-c-part-1-building-and-testing/"" rel=""nofollow"">this tutorial</a> on building and testing Microservices in .NET (C#).</p>&#xA;"
32014151,26331854,476648,2015-08-14T15:58:14,"<p>Follow <a href=""http://insidethecpu.com/2015/05/22/microservices-with-c-and-rabbitmq/"">this link</a> for a step-by-step guide to setting up a simple Microservice framework using the .NET framework, ASP.NET, and RabbitMQ. </p>&#xA;&#xA;<p>The tutorial starts with the core concepts, implements a working application, and scales that application in terms of size and complexity, tackling typical Microservice problems during each step.</p>&#xA;"
42463108,42428045,6116328,2017-02-25T23:58:56,"<p>Ok, what I did was simply to use Zuul (as @Ryan suggested). The thing is, initially, I kind of didn't get the approach for it. This was a petty architectural problem that, after interiorizing @Ryan's suggestion with the two articles I added to the bottom of my question, finally got them all right (hope so). </p>&#xA;&#xA;<p>First: Now that my project is using Zuul (by creating the gateway app as both of my links suggested) and have both Hybrix and Ribbon in its classpath, Zuul must wire them all together and turn into a multifunctional proxy gateway with circuit breaker and load balancing characteristics.</p>&#xA;&#xA;<p>Second: I got rid of that rest template load balancer because: a) the responses where getting the microservice hypermedia because I wasn't sending the port forward header, which pretty much nullify Zuul's proxying capabilities; and b) because since Zuul is acting as a gateway proxy load balancer, I do think is kinda boilerplate to use ribbon again with a rest template.</p>&#xA;&#xA;<p>Now, my app is like this: user -> {[Zuul's gateway -> Spring Data REST microservice (forwarded by Zuul)] &lt;- Eureka}.</p>&#xA;&#xA;<p>I'm gonna take @Ryan's answer as the one since he pointed out exactly what was happening and what in the end I did to solve my problem. Still, I'm also answering since I think further explanation about what I ended up doing could help to another SO user.</p>&#xA;&#xA;<p>If something is wrong about my solution, please comment. </p>&#xA;"
51809369,51808975,1668328,2018-08-12T13:42:54,"<p>There are few problems with the terminology in the question. I think once you get the terminology right, you will understand </p>&#xA;&#xA;<ol>&#xA;<li>ECR is the container registry, i.e docker images are uploaded and pulled from and to there</li>&#xA;<li>ECS is the orchestration platform, i.e it orchestrates multiple containers in a cluster. It decides on each machine what container will be in</li>&#xA;<li>Task definition is an abstraction. You can determine how many containers, what env vars will be passed to the containers, what will be the RUN commands that will be executed once container is started etc.. </li>&#xA;</ol>&#xA;&#xA;<p>For instance you have an API endpoint that needs to be highly redundant, so the task definition will be 10 containers. For offline processing (when we care less about performance) - we can set it with less containers.</p>&#xA;"
47765910,44996290,2986984,2017-12-12T05:32:12,"<p>Consider using a Java bean mapper. See &#xA;<a href=""https://stackoverflow.com/questions/1432764"">any tool for java object to object mapping?</a></p>&#xA;&#xA;<p>Such mappers are flexible and can bridge minor differences between models without requiring the boilerplate code.</p>&#xA;"
38403712,38403001,567016,2016-07-15T19:23:43,"<p>If you are going to use one port, you need to have a way to distinguish between requests for each. If they were previously only identified by port number (instead of, eg, each having a different path or domain), you will have to change the access URL (or header) in some way.</p>&#xA;&#xA;<p>Given that, you can use the npm module, express, to easily distinguish between any number of request types by interpreting the request.</p>&#xA;&#xA;<p>you might have:</p>&#xA;&#xA;<p>service1.com and service2.com both pointing to xxx.xxx.xxx.xxx:2020. You could distinguish these by examining the request object.</p>&#xA;&#xA;<p>Eg,</p>&#xA;&#xA;<pre><code>app.get(/.*/, function(request, response, next){&#xA;switch(request.headers.host){&#xA;    case 'service1.com':&#xA;        //some stuff&#xA;    break;&#xA;    case 'service2.com':&#xA;        //some stuff&#xA;    break;&#xA;}&#xA;});&#xA;</code></pre>&#xA;&#xA;<p>Or, if it was domain.com/service1 and domain.com/service2</p>&#xA;&#xA;<pre><code>app.get(/service1/, function(){/*some stuff*/});&#xA;app.get(/service2/, function(){/*some stuff*/});&#xA;</code></pre>&#xA;"
45938616,42062199,6167785,2017-08-29T12:00:12,<p>Disadvantages</p>&#xA;&#xA;<ul>&#xA;<li>More memory intensive to store streams of data most of the times (since it is based on streams over time).</li>&#xA;<li>Might feel unconventional to learn at start(needs everything to be a stream).</li>&#xA;<li>Most complexities have to be dealt with at the time of declaration of new services.</li>&#xA;<li><p>Lack of good and simple resources to learn.</p></li>&#xA;<li><p>Often confused to be equivalent to Functional Reactive Programming. </p></li>&#xA;</ul>&#xA;
44722693,44722194,8200937,2017-06-23T13:30:17,"<p>If you change the Maven packaging type from <code>jar</code> to <code>war</code> then the distro will be a WAR file rather than an uber JAR. The WAR file can then be deployed to a standard servlet container, like Jetty.</p>&#xA;&#xA;<p>To facilitate bundling both an uber JAR and a WAR you could adopt one of the following approaches:</p>&#xA;&#xA;<ul>&#xA;<li>Define separate bundle modules each of which depends on your 'business' module, one of which would have <code>&lt;packaging&gt;war&lt;/packaging&gt;</code> and the other would have <code>&lt;packaging&gt;jar&lt;/packaging&gt;</code></li>&#xA;<li>Use a Maven profile to set the packaging value, here's <a href=""https://stackoverflow.com/questions/8247720/changing-packaging-based-on-active-profile-in-pom#15134587"">an example</a>.</li>&#xA;</ul>&#xA;"
45934952,45929864,8200937,2017-08-29T09:07:13,"<p>If I understand the question correctly ...</p>&#xA;&#xA;<ul>&#xA;<li>You have an API gateway in which authentication/authorisation is implemented</li>&#xA;<li>On successful negotiation though the API gateway the call is passed on to a core service</li>&#xA;<li>The core services perform some auditing of 'who does what' </li>&#xA;<li>In order to perform this auditing the core services need the identity of the calling user</li>&#xA;</ul>&#xA;&#xA;<p>I think the possible approaches here are:</p>&#xA;&#xA;<ol>&#xA;<li><p>Implement auditing in the API gateway. I suspect this is not a runner because the auditing is likely to be more fine grained than can be implemented in the API gateway. I suspect the most you could audit in the API getway is something like <em>User A invoked Endpoint B</em> whereas you probably want to audit something like <em>User A inserted item {...} at time {...}</em> and this could only be done within a core service.</p></li>&#xA;<li><p>Pass the original caller's credentials through to the core service and let it authenticate again. This will ensure that no unauthenticated calls can reach the core service and would also have the side effect of providing the user identity to the core service which it can then use for auditing. However, if your API gateway is the <strong>only</strong> entrypoint for the core services then authenticating again within a core service only serves to provide the user identity in which case it could be deemed overkill.</p></li>&#xA;<li><p>Pass the authenticated user identity from the API gateway to the core service and let the core service use this in its auditing. If your API gateway is the <strong>only</strong> entrypoint for the core services then there is no need to re-authenticate on the core service and the provision of the authenticated user identity could be deemed part of the core service's API. As for <strong>how</strong> this identity should be propagated from the API gateway to the core service, there are several options depending on the nature of the interop between API gateway and core service. It sounds like these are HTTP calls, if so then a request header would make sense since this state is request scoped. It's possible that you are already propagating some 'horizontal state' (i.e. state which is related to the call but is not a caller supplied parameter) such as a <code>correlationId</code> (which allows you to trace the call though the API getway down into the core service and back again), if so then the authenticated user identify could be added to that state and provided to the core service in the same way.</p></li>&#xA;</ol>&#xA;"
50270588,50270206,9586409,2018-05-10T10:17:33,"<ol>&#xA;<li><p>According to the <a href=""https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md"" rel=""nofollow noreferrer"">documentation of nginx-ingress</a>, the annotation prefix is now <code>nginx.ingress.kubernetes.io</code> and not <code>ingress.kubernetes.io</code> as you used. You can change it with the argument <code>--annotations-prefix</code>.</p></li>&#xA;<li><p>Try to remove the wildcard (<code>*</code>) from your <code>path</code>:</p></li>&#xA;</ol>&#xA;&#xA;<pre><code>- path: /service/service1&#xA;  backend:&#xA;    serviceName: service1&#xA;    servicePort: 8080&#xA;- path: /service/service2&#xA;  backend:&#xA;    serviceName: service2&#xA;    servicePort: 8080&#xA;- path: /&#xA;  backend:&#xA;    serviceName: webapp&#xA;    servicePort: 8080&#xA;</code></pre>&#xA;"
42358795,42356195,4495081,2017-02-21T04:38:33,"<p>Right, services and modules are the same thing. Note the actual url path inside this quote from <a href=""https://cloud.google.com/appengine/docs/python/microservices-on-app-engine#app_engine_services_as_microservices"" rel=""nofollow noreferrer"">App Engine Services as microservices</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p>In an App Engine project, you can deploy multiple microservices as&#xA;  separate <a href=""https://cloud.google.com/appengine/docs/python/modules/"" rel=""nofollow noreferrer"">services</a>, previously known as <em>modules</em> in App Engine.</p>&#xA;</blockquote>&#xA;&#xA;<p>Namespaces are supported by <a href=""https://cloud.google.com/appengine/docs/python/multitenancy/#Python_App_Engine_APIs_that_use_namespaces"" rel=""nofollow noreferrer"">just a few APIs</a> that services can invoke:</p>&#xA;&#xA;<blockquote>&#xA;  <p>App Engine currently supports namespaces in the following APIs:</p>&#xA;  &#xA;  <ul>&#xA;  <li><a href=""https://cloud.google.com/appengine/docs/python/multitenancy/multitenancy#Python_Using_namespaces_with_the_Datastore"" rel=""nofollow noreferrer"">Datastore</a></li>&#xA;  <li><a href=""https://cloud.google.com/appengine/docs/python/multitenancy/multitenancy#Python_Using_namespaces_with_the_Memcache"" rel=""nofollow noreferrer"">Memcache</a></li>&#xA;  <li><a href=""https://cloud.google.com/appengine/docs/python/multitenancy/multitenancy#Python_Using_namespaces_with_the_Task_Queue"" rel=""nofollow noreferrer"">Task queue</a></li>&#xA;  <li><a href=""https://cloud.google.com/appengine/docs/python/multitenancy/multitenancy#Python_Using_namespaces_with_Search"" rel=""nofollow noreferrer"">Search</a></li>&#xA;  </ul>&#xA;</blockquote>&#xA;&#xA;<p>They are really just a way to <strong>separate/slice</strong> (<em>not</em> share) data served by these APIs and can help prevent accidental data leaks across the namespace boundaries. See for example <a href=""https://cloud.google.com/appengine/docs/python/multitenancy/multitenancy"" rel=""nofollow noreferrer"">Implementing Multitenancy Using Namespaces</a>. But note that the protection is only as good as the app code is (data will leak if the code is setting the wrong namespace).</p>&#xA;&#xA;<p>Services do not offer data isolation, they can share data regardless of it being in a namespace or not, by appropriately setting the namespace when invoking the respective API. So it's not placing data in a namespace that <em>makes</em> that data shareable across services.</p>&#xA;&#xA;<p>Project bounds apply to both namespaces and services. But it is possible to configure APIs and projects to allow access even across project boundaries  (or even from outside Google network - see, for example <a href=""https://stackoverflow.com/questions/42051453/how-do-i-use-google-datastore-for-my-web-app-which-is-not-hosted-in-google-app-e"">How do I use Google datastore for my web app which is NOT hosted in google app engine?</a>)</p>&#xA;&#xA;<p>The primary purpose of using services is to obtain code isolation. But it comes with a price - each service has its own instances. Also a bit more difficult: docs and even tools are often a bit behind, many of them assume a single-service GAE app context.</p>&#xA;&#xA;<p>Even though using services to create environments is offered as an alternative in <a href=""https://cloud.google.com/appengine/docs/python/creating-separate-dev-environments"" rel=""nofollow noreferrer"">Naming Developer Environments</a> I would stick, as you mention, to using separate projects for that (to also have data isolation).</p>&#xA;"
42333335,42331661,4495081,2017-02-19T21:51:10,"<p>You can configure resources in one project to allow access to users/apps outside that project. See, for example <a href=""https://cloud.google.com/storage/docs/access-control/create-manage-lists#set-an-acl"" rel=""nofollow noreferrer"">Setting ACLs</a> for how you can allow multiple projects to access a Cloud Storage bucket. Similar cross-projects access can be configured for most if not all Google Cloud resources/services/apps - but you need to check the respective docs for each of them to see the specific details each of them may have.</p>&#xA;&#xA;<p>With this in mind it's really up to you to organize and map your apps and resources into projects.</p>&#xA;"
42361987,42360790,4495081,2017-02-21T08:19:15,"<p>The reason is that there are also several app-level configs, applicable to <strong>all</strong> services/modules:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://cloud.google.com/appengine/docs/python/config/dispatchref"" rel=""nofollow noreferrer"">dispatch.yaml</a></li>&#xA;<li><a href=""https://cloud.google.com/appengine/docs/python/config/indexref"" rel=""nofollow noreferrer"">index.yaml</a></li>&#xA;<li><a href=""https://cloud.google.com/appengine/docs/python/config/queueref"" rel=""nofollow noreferrer"">queue.yaml</a></li>&#xA;<li><a href=""https://cloud.google.com/appengine/docs/python/config/cronref"" rel=""nofollow noreferrer"">cron.yaml</a></li>&#xA;</ul>&#xA;&#xA;<p>Some of these configs can have trouble if not deployed after/together with the <code>default</code> service. And some services may have dependencies on the app-level configs.</p>&#xA;&#xA;<p>The requirement of deploying <code>default</code> first is simply a measure to reduce the risk of initial deployment problems. Subsequent deployments no longer have this restriction (since <code>default</code> is already deployed)</p>&#xA;&#xA;<p>Yes, the <code>default</code> service is mandatory (sort of like a kitchen sink for all kinds of stuff, for example requests not matching any dispatch rule are sent to the <code>default</code> service). So just declare one of your non-web apps the default one (it doesn't matter what the <code>default</code> service actually does).</p>&#xA;&#xA;<p>Somehow related (mostly for the examples): <a href=""https://stackoverflow.com/questions/34110178/can-a-default-module-in-a-google-app-engine-app-be-a-sibling-of-a-non-default-mo"">Can a default service/module in a Google App Engine app be a sibling of a non-default one in terms of folder structure?</a></p>&#xA;"
46314505,46308297,4495081,2017-09-20T06:11:10,"<p>The dispatch file works locally, but only with the local development server, i.e. with the standard environment. </p>&#xA;&#xA;<p>But since you're using Node.js it means you're running in the flexible environment, for which local development means running a server for each of your services, on different ports. Your two <code>app.js</code> seem to be configured for the same port: 8080.</p>&#xA;&#xA;<p>And indeed, you can't really use <code>dispatch.yaml</code> locally in this case, you need some other method to build cross-referencing URLs based on these ports for local development.</p>&#xA;"
38126712,38125926,4495081,2016-06-30T14:53:34,"<p>A monolithic app is really an app with a single module/service (the default one). The <code>app.yaml</code> config for such app (like the one you shown) is really the default module's config file - there is no ""per-app"" such config. The modules are ""tied"" together into one specific app by this line in their respective <code>&lt;module&gt;.yaml</code>:</p>&#xA;&#xA;<pre><code>application: &lt;app_identifier&gt;&#xA;</code></pre>&#xA;&#xA;<p>In a multi-module app each module has its own <code>&lt;module&gt;.yaml</code> config file (the name doesn't have to be <code>app.yaml</code>, it actually has to be different if the module code share the same dir - but I'd stick with the recommended directory structure - one module per app subdirectory).</p>&#xA;&#xA;<p>See this Q&amp;A (which also contains some examples): <a href=""https://stackoverflow.com/questions/34110178/can-a-default-module-in-a-google-app-engine-app-be-a-sibling-of-a-non-default-mo"">Can a default service/module in a Google App Engine app be a sibling of a non-default one in terms of folder structure?</a></p>&#xA;&#xA;<p>The default module name can/should not be set. All other module names are configured in the respective <code>&lt;module&gt;.yaml</code> files, like this:</p>&#xA;&#xA;<pre><code>module: &lt;module_name&gt;&#xA;</code></pre>&#xA;&#xA;<p>All modules in the same app share the same datastore (thus the same <code>index.html</code> file). </p>&#xA;&#xA;<p>Note that other config files are also app-level configs, thus really shared by all modules (or just the default module, which is the one executing the cron service, for example), I'd place them as recommended in the app's top level dir: <code>cron.yaml</code>, <code>dispatch.yaml</code>, <code>queue.yaml</code>.</p>&#xA;"
30457982,30449278,4495081,2015-05-26T12:00:39,"<p>You could keep explicit version mapping for every component you want to include in the release (and possibly other metadata info as needed) in a separate git repo which becomes your <strong>master</strong> SCM control knob. This offers several advantages:</p>&#xA;&#xA;<ul>&#xA;<li>not mixing scripts/code with metadata info (which is error prone)</li>&#xA;<li>you can code your scripts to simply handle the versioning info from this master git repo, no need to change scripts for every release</li>&#xA;<li>you only need to track/tag the master git repo since it contains all the metadata info about all the other components needed in the release - less SCM churn</li>&#xA;<li>you can quickly access relevant metadata info for all components via this single tiny repo, no need to pull the entire set of components (unless you also need to acccess their content specifically)</li>&#xA;<li>you prevent pollution of the components' SCM logs with your particular release info (especially important if these comps are shared with other completely unrelated or 3rd party products which couldn't care less about your particular release cycle).</li>&#xA;</ul>&#xA;&#xA;<p>This doesn't eliminate the release steps you have to do, it just adds some order and can help with the automation.</p>&#xA;"
42400510,42397788,4495081,2017-02-22T19:25:32,"<p>It depends what you mean by ""accessible"".</p>&#xA;&#xA;<p>Yes, the app will have a presence on <code>appspot.com</code>, in the sense that requests can make it to some instance of some version of some service inside your app, based on the <a href=""https://cloud.google.com/appengine/docs/standard/python/how-requests-are-routed"" rel=""nofollow noreferrer"">Routing via URL</a> rules, the most generic ones being:</p>&#xA;&#xA;<blockquote>&#xA;  <p><strong>Sends a request to the named service, version, and instance:</strong></p>&#xA;&#xA;<pre><code>https://instance-dot-version-dot-service-dot-app-id.appspot.com&#xA;http://instance.version.service.my-custom-domain.com&#xA;</code></pre>&#xA;</blockquote>&#xA;&#xA;<p>Also, from <a href=""https://cloud.google.com/appengine/docs/standard/python/how-requests-are-routed#default_service"" rel=""nofollow noreferrer"">Default service</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p>The default service is defined by explicitly giving a service the name&#xA;  ""default,"" or by not including the name parameter in the service's&#xA;  config file. Requests that specify no service or an invalid service&#xA;  are routed to the default service. You can designate a default version&#xA;  for a service, when appropriate, in the <a href=""https://console.cloud.google.com/appengine/versions?_ga=1.27601596.1484861412.1422834776"" rel=""nofollow noreferrer"">Google Cloud Platform Console&#xA;  versions tab</a>.</p>&#xA;</blockquote>&#xA;&#xA;<p>But what your app code responds to such requests is really up to you. Nothing stops, for example, your default service handler from simply returning a 404 or your ""Hello world"" page, for example, if you don't want it to do anything else. As if it wouldn't be there. Yet it still serves the role of the default service.</p>&#xA;"
48014192,48008198,4495081,2017-12-28T20:49:06,"<p>The cron configuration is also an application level configuration, not a module/service level one, which is why when you deploy it for one service it overwrites the previous one from another service.</p>&#xA;&#xA;<p>You need to combine all cron jobs for all your services into a single cron configuration file and deploy that one instead, preferably using the specific cron deployment command, not by uploading it together with a particular service (sometimes that fails for multi service apps).</p>&#xA;&#xA;<p>There are other such app level configurations as well, see <a href=""https://stackoverflow.com/a/42361987/4495081"">https://stackoverflow.com/a/42361987/4495081</a></p>&#xA;"
47995400,47992254,4495081,2017-12-27T16:20:59,"<p>Yes, it is possible to have any mix of environments and languages as separate services inside of the same GAE application. While not explicitly mentioned, it is implied by the code isolation of services. From <a href=""https://cloud.google.com/appengine/docs/standard/python/microservices-on-app-engine#app_engine_services_as_microservices"" rel=""nofollow noreferrer"">App Engine Services as microservices</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Code can be deployed to services independently, and different services can be written in different languages, such as Python, Java, Go, and PHP. </p>&#xA;</blockquote>&#xA;&#xA;<p>You just need to take care of:</p>&#xA;&#xA;<ul>&#xA;<li>deploying the default service first, see <a href=""https://stackoverflow.com/a/42361987/4495081"">https://stackoverflow.com/a/42361987/4495081</a></li>&#xA;<li>deploying the app-level configurations shared by all services: dispatch, cron, queue, etc.</li>&#xA;</ul>&#xA;"
37340094,37336871,4495081,2016-05-20T07:06:04,"<p>It is possible to build cross-module urls that work on both GAE and on the development server using the <code>modules.get_hostname()</code> API. You can find an example in this answer: <a href=""https://stackoverflow.com/a/31145647/4495081"">https://stackoverflow.com/a/31145647/4495081</a></p>&#xA;"
45400267,45400096,2599657,2017-07-30T13:23:23,"<p>In my opinion microservice architecture marries well with DDD</p>&#xA;&#xA;<p>I think you should consider your multi-module project as a ""monolith"" and do your microservice separation based on domain concepts and not on maven projects. </p>&#xA;&#xA;<p>Ex: Do not create a microservice called ""utils"" but rather a microservice called ""accounts"" or ""user-management"" or whatever your domain is. I think without <strong>domain driven development</strong> it kinda loses its usefulness.</p>&#xA;&#xA;<p>It is really easy afterwards to work on different aspects of the <strong>domain</strong> knowing that it is separated by the rest. You should check out <strong>hexagonal architecture</strong> by Alistair Cockburn</p>&#xA;"
49270019,38230495,385257,2018-03-14T04:55:39,"<p>I know it's 1 year late. But for any new visitors.</p>&#xA;&#xA;<p>create a filter.</p>&#xA;&#xA;<pre><code>@Component&#xA;public class AuthenticationFilter extends ZuulFilter {&#xA;    @Override&#xA;    public String filterType() {&#xA;        return ""pre"";&#xA;    }&#xA;&#xA;    @Override&#xA;    public int filterOrder() {&#xA;        return 1;&#xA;    }&#xA;&#xA;    @Override&#xA;    public boolean shouldFilter() {&#xA;        return true;&#xA;    }&#xA;&#xA;    @Override&#xA;    public Object run() {&#xA;        RequestContext ctx = RequestContext.getCurrentContext();&#xA;&#xA;        ctx.addZuulRequestHeader(""userId"", ""123456789"");&#xA;        return null;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Annotate it with @Component, so it will be automatically loaded. Inside run method, use <code>addZuulRequestHeader</code></p>&#xA;"
51899301,51878195,9892073,2018-08-17T16:09:59,"<blockquote>&#xA;  <p>An ExternalName service is a special case of service that does not&#xA;  have selectors and uses DNS names instead.</p>&#xA;</blockquote>&#xA;&#xA;<p>You can find out more about ExternalName service from the official <a href=""https://kubernetes.io/docs/concepts/services-networking/service/#externalname"" rel=""nofollow noreferrer"">Kubernetes documentation</a>:</p>&#xA;&#xA;<p>When  you want to access a service from a different namespace, your yaml could, for <a href=""https://akomljen.com/kubernetes-tips-part-1/"" rel=""nofollow noreferrer"">example</a>, look like this:</p>&#xA;&#xA;<pre><code>kind: Service&#xA;apiVersion: v1&#xA;metadata:&#xA;  name: test-service-1&#xA;  namespace: namespace-a&#xA;spec:&#xA;  type: ExternalName&#xA;  externalName: test-service-2.namespace-b.svc.cluster.local&#xA;  ports:&#xA;  - port: 80&#xA;</code></pre>&#xA;&#xA;<p>As to your Ingress yaml file, please recheck it and make sure it is compliant with the official examples, for <a href=""https://medium.com/@cashisclay/kubernetes-ingress-82aa960f658e"" rel=""nofollow noreferrer"">example</a> this one as it contains some inconsistency:</p>&#xA;&#xA;<pre><code>apiVersion: extensions/v1beta1  &#xA;kind: Ingress  &#xA;metadata:  &#xA;  name: my-ingress  &#xA;spec:  &#xA;  rules:  &#xA;  - host: www.mysite.com  &#xA;    http:  &#xA;      paths:  &#xA;      - backend:  &#xA;          serviceName: website  &#xA;          servicePort: 80  &#xA;  - host: forums.mysite.com  &#xA;    http:  &#xA;      paths:  &#xA;      - path:  &#xA;        backend:  &#xA;          serviceName: forums  &#xA;          servicePort: 80&#xA;</code></pre>&#xA;&#xA;<p>Please also recheck ExternalName yaml as it has TargetPorts and selectors which are not used in this type of <a href=""https://kubernetes.io/docs/concepts/services-networking/service/"" rel=""nofollow noreferrer"">Service</a> and make sure that:</p>&#xA;&#xA;<blockquote>&#xA;  <p>ExternalName Services are available only with <code>kube-dns</code> version 1.7&#xA;  and later.</p>&#xA;</blockquote>&#xA;&#xA;<p>In case you will not succeed, please share the kind of problem you have meet.  </p>&#xA;"
51131298,51123104,4158442,2018-07-02T07:45:47,"<p>tracing in your example would work only for requests made to service-A via HTTP protocol. Istio at the moment traces only HTTP requests. </p>&#xA;&#xA;<p>You can trace event bus messages manually by creating spans inside message producers and receivers.&#xA;IIRC vert.x event bus does not support headers per message:</p>&#xA;&#xA;<p><a href=""https://vertx.io/docs/apidocs/io/vertx/core/eventbus/MessageProducer.html"" rel=""nofollow noreferrer"">https://vertx.io/docs/apidocs/io/vertx/core/eventbus/MessageProducer.html</a>&#xA;<a href=""https://vertx.io/docs/apidocs/io/vertx/core/eventbus/DeliveryOptions.html"" rel=""nofollow noreferrer"">https://vertx.io/docs/apidocs/io/vertx/core/eventbus/DeliveryOptions.html</a></p>&#xA;&#xA;<p>so you would have to encode tracing IDs into the message body and deserialize it correctly on the receiver side.</p>&#xA;"
44275087,44274982,2458858,2017-05-31T03:23:15,<p>There are a couple of ways to do this:</p>&#xA;&#xA;<p>1) Using <code>ClientHttpRequestFactory</code> with <code>RestTemplate</code>:</p>&#xA;&#xA;<pre><code>public RestTemplate restTemplate() {&#xA;    return new RestTemplate(clientHttpRequestFactory());&#xA;}&#xA;&#xA;private ClientHttpRequestFactory clientHttpRequestFactory() {&#xA;    HttpComponentsClientHttpRequestFactory factory = new HttpComponentsClientHttpRequestFactory();&#xA;    factory.setReadTimeout(timeinMillis);&#xA;    factory.setConnectTimeout(timeinMillis);&#xA;    return factory;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>2) Second way is to use callable but I guess you have already explored that solution.</p>&#xA;
35082841,30173267,35306,2016-01-29T11:01:10,"<p>Security as a separate service that you need for every request as you describe it is an extremely bad idea. May I refer you to the basic ideas of modularisation that Parnas described so eloquently in <a href=""http://www.cs.umd.edu/class/spring2003/cmsc838p/Design/criteria.pdf"" rel=""nofollow"">On the Criteria To Be Used in Decomposing Systems into Modules </a>. No coupling also means no cohesion, and  engineering is about finding the sweet spot on that axis. </p>&#xA;&#xA;<p>Contrary to popular belief, micro-services need to be rather large to be able to scale. The limits to scalability are mostly in communications, so they need to be designed to not be chatty. The problem is mostly (unless you're netflix) not bandwidth but delay. </p>&#xA;&#xA;<p>Your security module needs to be closer to your services than a HTTP request, a linked-in module can be fine.</p>&#xA;"
30706767,30649582,35306,2015-06-08T10:29:00,"<p>Well, those statements are wrong. Modularisation is about finding the right combination of low coupling and high cohesion for your context. The user can store a copy of the contact object as long as it is aware that it is only a snapshot of it at a certain moment in time. For many use cases that is fine, and sometimes you just need to ensure that you work with the real contact as known to the responsible domain.</p>&#xA;"
33209702,33202053,35306,2015-10-19T08:19:54,"<p>Literature uses a 5 dimensional model for this:</p>&#xA;&#xA;<ul>&#xA;<li>version (wanting to change)</li>&#xA;<li>status (life cycle: create, test, deploy, retire)</li>&#xA;<li>view (source, deployment, documentation)</li>&#xA;<li>hierarchy (product, microservice)</li>&#xA;<li>variant (largely similar, describing the differences, product families)</li>&#xA;</ul>&#xA;&#xA;<p>Most systems only handle a few of these dimensions. To handle all five, you have to describe (fix) your development process.</p>&#xA;&#xA;<p>When only looking at version plus hierarchy, as you do here with product and microservice versioned separately: product version should change as soon as users of the product can notice something different. You might want to signal that from the microservice versioning by using major-minor numbering: minor should not affect the product version, major should.&#xA;Or you could use version ranges/semantic versioning.</p>&#xA;&#xA;<p>The reference:</p>&#xA;&#xA;<p>Managing design data: the five dimensions of CAD frameworks, configuration management, and product data management.&#xA;van den Hamer, P. Lepoeter, K.&#xA;Philips Res., Eindhoven;</p>&#xA;&#xA;<p>This paper appears in: Proceedings of the IEEE&#xA;Publication Date: Jan 1996&#xA;Volume: 84, Issue: 1&#xA;On page(s): 42-56&#xA;ISSN: 0018-9219&#xA;References Cited: 26&#xA;CODEN: IEEPAD&#xA;INSPEC Accession Number: 5175049&#xA;Digital Object Identifier: 10.1109/5.476025&#xA;Current Version Published: 2002-08-06</p>&#xA;"
49958509,49953536,1615594,2018-04-21T17:41:43,"<p>Your code should retrieve messages but it is instructed to fetch the messages once.</p>&#xA;&#xA;<p>You'd need to put into some loop like such and probably run it on another goroutine if the app needs to run something else concurrently.</p>&#xA;&#xA;<pre><code>go func() {&#xA;  for {&#xA;    receive_resp, err := service.ReceiveMessage(receive_params)&#xA;    if err != nil {&#xA;      log.Println(err)&#xA;    }&#xA;    fmt.Printf(""[Receive message] \n%v \n\n"", receive_resp)&#xA;  }&#xA;}()&#xA;</code></pre>&#xA;&#xA;<p><a href=""https://docs.aws.amazon.com/sdk-for-go/api/service/sqs/#SQS.ReceiveMessage"" rel=""nofollow noreferrer"">Reference on ReceiveMessage</a></p>&#xA;&#xA;<p><a href=""https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html"" rel=""nofollow noreferrer"">Reference on Short vs Long polling</a></p>&#xA;"
42301537,42289439,702954,2017-02-17T15:23:33,"<p>The ""independent"" aspect of microservices is essential here. Basically, you should be able to treat each service as a standalone app, and your UI should not depend on sales/users/products services as ""libs"" (e.g. C# DLL, Java Jars, etc.) but should instead use some kind of API (e.g. REST API) to call each other.</p>&#xA;&#xA;<p>To illustrate the difference, you should not have this :</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/eEqeU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eEqeU.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>But instead you should have something like this.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/eEqeU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9xLyr.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>Now let's assume you are in the wonderful microservices world with REST API between your UI and each of your services.</p>&#xA;&#xA;<p>In Jenkins, it becomes rather simple. For each service (+ main app) you want a job that <strong>compiles, tests, and deploys your service</strong> on the target server.</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/gwCTL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gwCTL.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>As long as your services are independent and if you handle app versionning correctly, you shouldn't need any kind of synchronization between any of your Jenkins jobs.</p>&#xA;&#xA;<p>Of course the build/test/deploy process would be externalized in a common pipeline file so you can reuse it across your different jobs. Just commit a new API endpoint on the sales service before you commit the UI part that uses the new sales endpoint... but I think that just common sense !</p>&#xA;&#xA;<p>If you're new to Jenkins a good start is the <a href=""https://jenkins.io/doc/book/pipeline/"" rel=""nofollow noreferrer"">pipeline documentation</a> !</p>&#xA;"
51061387,51040814,188138,2018-06-27T11:15:54,"<p>I agree with the comments above in preferring duplicating the class code.  I went down the road of distributing shared classes via an internal NuGet server and it didn’t work well.  The different microservice consumers of the JSON started out using the NuGet class, but soon were asking for changes that would break the others.  All consumers eventually ignored the NuGet class and created their own.</p>&#xA;"
51400256,51379968,188138,2018-07-18T10:56:55,"<p>Let’s take this one first.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Is this whole structure a reasonable architecture or am I completely&#xA;  missing something?</p>&#xA;</blockquote>&#xA;&#xA;<p>Nice architecture plan!  I know it feels like there are a lot of moving pieces, but having lots of small pieces instead of one big one is what makes this my favorite pattern.</p>&#xA;&#xA;<blockquote>&#xA;  <p>What is it doing afterwards? Reading the new data from the SQL and&#xA;  triggering an event with it? The query service is listening and&#xA;  storing the same data in its MongoDB? Is it overwriting the old data&#xA;  or also creating a new entry with a version? That seems to be quite&#xA;  redundant? Do I in fact really need the SQL database here?</p>&#xA;</blockquote>&#xA;&#xA;<p>There are 2 logical databases (which can be in the same physical database but for scaling reasons it's best if they are not) in CQRS – the domain model and the read model.  These are very different structures.  The domain model is stored as in any CRUD app with third normal form, etc.  The read model is meant to make data reads blazing fast by custom designing tables that match the data a view needs. There will be a lot of data duplication in these tables. The idea is that it’s more responsive to have a table for each view and update that table in when the domain model changes because there’s nobody sitting at a keyboard waiting for the view to render so it’s OK for the view model data generation to take a little longer.  This results in some wasted CPU cycles because you could update the view model several times before anyone asked for that view, but that’s OK since we were really using up idle time anyway. </p>&#xA;&#xA;<p>When a command updates an aggregate and persists it to the DB, it generates a message for the view side of CQRS to update the view.  There are 2 ways to do this.  The first is to send a message saying “aggregate 83483 needs to be updated” and the view model requeries everything it needs from the domain model and updates the view model.  The other approach is to send a message saying “aggregate 83483 was updated to have the following values:  …” and the read side can update its tables without having to query.  The first approach requires fewer message types but more querying, while the second is the opposite.  You can mix and match these two approaches in the same system. </p>&#xA;&#xA;<p>Since the read side has very different table structures, you need both databases.  On the read side, unless you want the user to be able to see old versions of the appointments, you only have to store the current state of the view so just update existing data.  On the command side, keeping historical state using a version number is a good idea, but can make db size grow.</p>&#xA;&#xA;<blockquote>&#xA;  <p>how can the main app call for data from the query service if one don't&#xA;  want to uses REST?</p>&#xA;</blockquote>&#xA;&#xA;<p>How the request gets to the query side is unimportant, so you can use REST, postback, GraphQL or whatever.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Is that ""event sourcing""?</p>&#xA;</blockquote>&#xA;&#xA;<p>Event Sourcing is when you persist all changes made to all entities.  If the entities are small enough you can persist all properties, but in general events only have changes.  Then to get current state you add up all those changes to see what your entities look like at a certain point in time.  It has nothing to do with the read model – that’s CQRS.  Note that events are not the request from the user to make a change, that’s a message which then is used to create a command.  An event is a record of all fields that changed as a result of the command.   That’s an important distinction because you don’t want to re-run all that business logic when rehydrating an entity or aggregate.</p>&#xA;&#xA;<blockquote>&#xA;  <p>When a client wants to change something but afterwards also show the&#xA;  changed data the only way I see is to trigger the change and than wait&#xA;  (let's say with polling) for the query service to have that data.&#xA;  What's a good way to achieve that? Maybe checking for the existing of&#xA;  the future version number?</p>&#xA;</blockquote>&#xA;&#xA;<p>Showing historical data is a bit sticky.  I would push back on this requirement if you can, but sometimes it’s necessary.  If you must do it, take the standard read model approach and save all changes to a view model table.  If the circumstances are right you can cheat and read historical data directly from the domain model tables, but that’s breaking a CQRS rule.  This is important because one of the advantages of CQRS is its scalability.  You can scale the read side as much as you want if each read instance maintains its own read database, but having to read from the domain model will ruin this.  This is situation dependent so you’ll have to decide on your own, but the best course of action is to try to get that requirement removed.</p>&#xA;&#xA;<p>In terms of timing, CQRS is all about eventual consistency.  The data changes may not show up on the read side for a while (typically fractions of a second but that's enough to cause problems).  If you must show new and old data, you can poll and wait for the proper version number to appear, which is ugly.  There are other alternatives involving result queues in Rabbit, but they are even uglier.</p>&#xA;"
50785093,50771405,188138,2018-06-10T15:13:23,"<p>If time allows, this seems like a perfect opportunity to move to CQRS.  With CQRS you would have two APIs – one for writes and one for reads.  You don’t have the problem of multiple API calls because the API represents business ideas (i.e. use cases), not discrete entity access.  Your front end should only submit one use case at a time, so there is only one API call.  On the server side, your endpoint controller will make as many database reads or writes it needs to do in order to implement the business ideas.</p>&#xA;&#xA;<p>CQRS is an excellent architectural pattern that will grow as large as necessary.  I’ve had to omit a lot of important details in order to summarize it here.  It would be worth the effort to learn about the pattern before you start your refactoring.</p>&#xA;"
50867261,50835738,188138,2018-06-14T23:28:09,"<p>It’s difficult to tell the complexity of the site from the link you posted.  I’ll assume you are doing your own inventory and order management instead of pushing those tasks to a service provider.  I’ll also assume you cannot stop feature development during the migration process.</p>&#xA;&#xA;<p>The most important thing for any legacy migration project is to first get Inversion of Control installed in the legacy code.  This will provide way to separate out the separate concerns that are probably tightly coupled now.  Without IoC, you’re going to have a tough time.  Note that constructor injection is a non-starter when migrating a legacy app.  Instead you’ll have to use the Service Locator pattern and keep constructor or property injection in mind for the future.</p>&#xA;&#xA;<p>Once you have an IoC container in place, you can start looking for seams to break out services.  You will probably find code that naturally lends itself to internal services, so refactor them to be resolved by the Service Locator.  The system logger is a good place to start.</p>&#xA;&#xA;<p>The goal of the first few months is to move to a Service Oriented Architecture where much of the business logic is contained either in the Domain entities or in services.  Don’t stress about getting the perfect architecture right from the start – it’s not possible.  The migration process involves moving from a smelly architecture to architectures with progressively fewer smells, not from monolith directly to modular.  Just get the business logic out of the UI and Controllers, and later you can tweak things for the domain layer or app layer.</p>&#xA;&#xA;<p>Note that repositories are a big focus here also.  As you migrate to services, you must also migrate all DB access to use the repository pattern.  This will open the door to real unit testing.</p>&#xA;&#xA;<p>As you migrate the code to use services, repositories, and IoC, you’ll start to see seams where you can break some features out into APIs.  Create the first API with just a single small feature, and refactor your monolith to use it.  Make it small because it will take a lot of infrastructure and process changes and you want to make as few simultaneous changes as possible.  Once you have that first API separated out, continue migrating more and more features to that API.</p>&#xA;&#xA;<p>BTW, this is a great time to move to a CQRS architecture and DDD strategy.  That first API should be a full Bounded Context and implemented using CQRS.</p>&#xA;&#xA;<p>Good luck!</p>&#xA;&#xA;<p>BTW, I wrote a book on this very thing.  It's .NET focused, but the process holds for any language.  Search for ""Bradley Irby"" on Amazon.</p>&#xA;"
50867350,44394119,188138,2018-06-14T23:41:30,"<p>The main difference between a monolith and a microservice is more about how they are deployed.  A monolith is one large app that must be deployed all-or-nothing.  Microservices are many ""apps"" that work together to achieve their purpose, and each can be deployed separately.  Typically monoliths are more difficult to deploy, and involve more risk since the entire system can crash if they are badly deployed.  For Microservices, each only handles part of the business processing so, in theory, if one is deployed badly only part of the app goes down.</p>&#xA;"
43823135,43821553,4906730,2017-05-06T17:01:15,"<p>There is the <a href=""https://www.npmjs.com/package/npmi"" rel=""nofollow noreferrer"">npmi</a> package which gives an API to npm install.</p>&#xA;&#xA;<p>The logic I would use is: </p>&#xA;&#xA;<ol>&#xA;<li><p>Get the specific package and version from npm (install if is not already installed)</p></li>&#xA;<li><p>Require the package inside nodejs</p></li>&#xA;<li>Run the specified method with the specified parameters</li>&#xA;<li>Return the results to the client</li>&#xA;</ol>&#xA;&#xA;<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">&#xD;&#xA;<div class=""snippet-code"">&#xD;&#xA;<pre class=""snippet-code-js lang-js prettyprint-override""><code>var npmi = require('npmi');&#xD;&#xA;var path = require('path');&#xD;&#xA;&#xD;&#xA;function runModule(moduleName, moduleVersion, moduleMethod, moduleMethodParams) {&#xD;&#xA;&#xD;&#xA;  return new Promise((resolve, reject) =&gt; {&#xD;&#xA;&#xD;&#xA;    var options = {&#xD;&#xA;      name: moduleName, // your module name &#xD;&#xA;      version: moduleVersion, // expected version [default: 'latest'] &#xD;&#xA;      forceInstall: false, // force install if set to true (even if already installed, it will do a reinstall) [default: false] &#xD;&#xA;      npmLoad: { // npm.load(options, callback): this is the ""options"" given to npm.load() &#xD;&#xA;        loglevel: 'silent' // [default: {loglevel: 'silent'}] &#xD;&#xA;      }&#xD;&#xA;    };&#xD;&#xA;    options.path = './' + options.name + ""@"" + options.version,&#xD;&#xA;      npmi(options, function(err, result) {&#xD;&#xA;        if (err) {&#xD;&#xA;          if (err.code === npmi.LOAD_ERR) console.log('npm load error');&#xD;&#xA;          else if (err.code === npmi.INSTALL_ERR) console.log('npm install error');&#xD;&#xA;          console.log(err.message);&#xD;&#xA;          return reject(err)&#xD;&#xA;        }&#xD;&#xA;&#xD;&#xA;        // installed &#xD;&#xA;        console.log(options.name + '@' + options.version + ' installed successfully in ' + path.resolve(options.path));&#xD;&#xA;&#xD;&#xA;        var my_module = require(path.resolve(options.path, ""node_modules"", options.name))&#xD;&#xA;&#xD;&#xA;        console.log(""Running :"", options.name + '@' + options.version)&#xD;&#xA;        console.log(""Method :"", moduleMethod);&#xD;&#xA;        console.log(""With params :"", ...moduleMethodParams)&#xD;&#xA;        resolve(my_module[moduleMethod](...moduleMethodParams))&#xD;&#xA;      });&#xD;&#xA;&#xD;&#xA;  })&#xD;&#xA;&#xD;&#xA;}&#xD;&#xA;&#xD;&#xA;runModule('lodash', '4.10.0', 'fill', [Array(3), 2])&#xD;&#xA;  .then(result =&gt; console.log(""Result :"", result))&#xD;&#xA;&#xD;&#xA;runModule('lodash', '3.10.0', 'fill', [Array(3), 2])&#xD;&#xA;  .then(result =&gt; console.log(""Result :"", result))</code></pre>&#xD;&#xA;</div>&#xD;&#xA;</div>&#xD;&#xA;</p>&#xA;&#xA;<p>You could see now that there are 2 created folders <strong>(lodash@3.10.0 , lodash@4.10.0)</strong> indicating the package name and version.</p>&#xA;&#xA;<p>I have made the assumptions that npm is in path and the server has the permissions to install packages in current directory, also that the <strong>""MODULE_NAME@MODULE_VERSION""</strong> is a valid folder name under the current OS.</p>&#xA;"
49612302,49531349,4120554,2018-04-02T13:31:32,"<p>As far I know <code>opossum</code> does not provide a out of the box solution for your problem. You have to implement some mechanism to cache the latest successful call. In my point of view probably the best way to do it, is having some distributed cache like Redis and cache the latest successful response but make sure to have a temporary entry in Redis you don't want to return old deprecated data.</p>&#xA;"
49613124,49612911,4120554,2018-04-02T14:27:09,"<p>Actually I think you are missing the concept of microservice architecture. Your services must be independent and if they need to communicate with each other they must use a service discovery mechanism that will return a healthy instance of that service. Another pattern of microservices architecture is that every single service must have an endpoint (/health) that returns the health status of the service, having this your service discovery can check if that instance is healthy and return it as a healthy instance..</p>&#xA;&#xA;<p>Microservices is not about technology it's about the concept and implementing the right patterns. Otherwise you will have a chaos architecture :D</p>&#xA;&#xA;<p>If you want to understande the concepts I really recommend this book: <a href=""http://shop.oreilly.com/product/0636920033158.do"" rel=""nofollow noreferrer"">http://shop.oreilly.com/product/0636920033158.do</a></p>&#xA;"
30200678,30116581,261354,2015-05-12T20:19:57,"<p>I believe you are looking for DDP.connect. Basically underneath meteor all calls to the server from the client and all communication from the server to the client use Distributed Data Protocol. (<a href=""https://www.meteor.com/ddp"" rel=""nofollow"">https://www.meteor.com/ddp</a>) As the documentation points out by default a client opens a DDP connection to the server it is loaded from. However, in your case, you'd want to use DDP.connect to connect to other servers for various different tasks, such as a registration services server for RegistrationService. (<a href=""http://docs.meteor.com/#/full/ddp_connect"" rel=""nofollow"">http://docs.meteor.com/#/full/ddp_connect</a>) As a simplified example you'll be looking to do something like this:</p>&#xA;&#xA;<pre><code>if (Meteor.isClient) {&#xA;    var registrationServices = DDP.connect(""http://your.registrationservices.com:3000"");&#xA;&#xA;    Template.registerSomething.events({&#xA;        'click #facebook-login': function(){&#xA;            registrationServices.call('loginWithFacebook', data, function(error, results){ ... }); // registration services points to a different service from your default.&#xA;        }&#xA;    });&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Don't forget that you can also have various DDP.connect's to your various microservices. These are akin to web service connections in other applications.</p>&#xA;"
52036584,51996128,4987371,2018-08-27T09:43:29,"<p>Hi That depends on the project,you keep in mind that we need to use rich models in ddd's approach because the nature of these projects with a domain approach is rich, and we need to use rich domains in those projects, and now in projects that They do not have an ddd’s approach, and I mean data driven projects are. We, too, have Anemic models that answer our work. &#xA;So That depends on the project and the approach taken for that project.&#xA;below link can help you:&#xA;<a href=""https://blog.pragmatists.com/domain-driven-design-vs-anemic-model-how-do-they-differ-ffdee9371a86"" rel=""nofollow noreferrer"">https://blog.pragmatists.com/domain-driven-design-vs-anemic-model-how-do-they-differ-ffdee9371a86</a></p>&#xA;"
28976675,27007353,4009451,2015-03-11T00:45:57,"<p>This is subjective but the following solution worked for me, my team, and our DB team.</p>&#xA;&#xA;<ul>&#xA;<li>At the application layer, Microservices are decomposed to semantic function.&#xA;&#xA;<ul>&#xA;<li>e.g. a <code>Contact</code> service might CRUD contacts (metadata about contacts: names, phone numbers, contact info, etc.)</li>&#xA;<li>e.g. a <code>User</code> service might CRUD users with login credentials, authorization roles, etc.</li>&#xA;<li>e.g. a <code>Payment</code> service might CRUD payments and work under the hood with a 3rd party PCI compliant service like Stripe, etc.</li>&#xA;</ul></li>&#xA;<li>At the DB layer, the tables can be organized however the devs/DBs/devops people want the tables organized</li>&#xA;</ul>&#xA;&#xA;<p>The problem is with cascading and service boundaries: Payments might need a User to know who is making a payment. Instead of modeling your services like this:</p>&#xA;&#xA;<pre><code>interface PaymentService {&#xA;    PaymentInfo makePayment(User user, Payment payment);&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Model it like so:</p>&#xA;&#xA;<pre><code>interface PaymentService {&#xA;    PaymentInfo makePayment(Long userId, Payment payment);&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>This way, entities that belong to other microservices only are <em>referenced</em> inside a particular service by ID, not by object reference. This allows DB tables to have foreign keys all over the place, but at the app layer ""foreign"" entities (that is, entities living in other services) are available via ID. This stops object cascading from growing out of control and cleanly delineates service boundaries.</p>&#xA;&#xA;<p>The problem it does incur is that it requires more network calls. For instance, if I gave each <code>Payment</code> entity a <code>User</code> reference, I could get the user for a particular payment with a single call:</p>&#xA;&#xA;<pre><code>User user = paymentService.getUserForPayment(payment);&#xA;</code></pre>&#xA;&#xA;<p>But using what I'm suggesting here, you'll need two calls:</p>&#xA;&#xA;<pre><code>Long userId = paymentService.getPayment(payment).getUserId();&#xA;User user = userService.getUserById(userId);&#xA;</code></pre>&#xA;&#xA;<p>This may be a deal breaker. But if you're smart and implement caching, and implement well engineered microservices that respond in 50 - 100 ms each call, I have no doubt that these extra network calls can be crafted to <em>not</em> incur latency to the application.</p>&#xA;"
43928863,40574379,2962667,2017-05-12T03:33:34,"<p>As per AWS CloudFormation docs, there are 3 properties available for an SNS topic, and only a change to the third property (i.e. <code>TopicName</code>) would lead to a replacement of an SNS resource (and leading to a generation of a new ARN).</p>&#xA;&#xA;<p>Since you're nuking the resources in CloudFormation stack and then re-creating them (you mentioned that that's how the serverless framework you're using works), you can preserve the SNS topic ARN by specifying the <code>TopicName</code> attribute for the SNS topic in your template (it's only when one doesn't specify a <code>TopicName</code> is when CloudFormation comes up and assigns a random name, otherwise it uses the name that the user specifies).</p>&#xA;&#xA;<p>That being said, your scenario does seem a little unnatural to me. If you aren't already managing subscriptions to that topic via CloudFormation, I'd recommend doing that, and it should automatically create SNS subscriptions on the replaced SNS topic for you, if at all SNS topic ARN changes.</p>&#xA;&#xA;<p>Ref:&#xA;<a href=""http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-sns-topic.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-sns-topic.html</a></p>&#xA;&#xA;<p><a href=""http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sns-subscription.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sns-subscription.html</a></p>&#xA;"
46191547,45085790,8601835,2017-09-13T07:35:30,"<h2>Code</h2>&#xA;&#xA;<pre><code>discoveryClient.getServices().forEach(id -&gt; {&#xA;            discoveryClient.getInstances(id).forEach(instance -&gt; {&#xA;                logger.info(""/hello, host:"" + instance.getHost() + "", service_id:"" + instance.getServiceId());&#xA;            });&#xA;        });&#xA;</code></pre>&#xA;"
51063254,51060417,4151019,2018-06-27T12:49:57,"<p><code>.AddTransientHttpErrorPolicy(...)</code> embeds a specification for you of the <em>what</em> to handle (network failures, 5xx and 408 responses as described <a href=""https://github.com/App-vNext/Polly/wiki/Polly-and-HttpClientFactory#using-addtransienthttperrorpolicy"" rel=""nofollow noreferrer"">in the wiki</a>). You only have to specify the <em>how</em> to handle (eg retry, circuit-breaker).</p>&#xA;&#xA;<p>With <code>.AddPolicyHandler(...)</code>, you specify the whole policy yourself: both <em>what</em> to handle (.<code>Handle&lt;&gt;()</code>, <code>.Or&lt;&gt;()</code>, <code>.OrResult&lt;HttpResponseMessage()</code> etc) and <em>how</em> to handle (eg retry, circuit-breaker).  As shown <a href=""https://github.com/App-vNext/Polly/wiki/Polly-and-HttpClientFactory#using-any-policy-configured-via-the-traditional-polly-syntax"" rel=""nofollow noreferrer"">here in the Polly wiki</a>.</p>&#xA;&#xA;<p>Beyond that, there are no differences in how <code>IHttpClientFactory</code> works with the configured policies.</p>&#xA;"
47580339,44455242,6323947,2017-11-30T18:31:29,"<p>If you want to maintain state (reliable collection) in a service, then you need to use a stateful service.</p>&#xA;"
46392124,46390552,3507435,2017-09-24T16:12:28,"<p>Because your requirements are quite simple you can implement such a gateway by yourself. Here's <a href=""https://github.com/cer/event-sourcing-examples/tree/master/java-spring/api-gateway-service"" rel=""nofollow noreferrer"">an example</a>.</p>&#xA;&#xA;<p>But if you really want to use some Spring solution you can try to use <a href=""http://cloud.spring.io/spring-cloud-netflix/"" rel=""nofollow noreferrer"">Spring Cloud Netflix</a> which is a part of Spring Cloud umbrella project. It includes <strong>router and filter</strong> features which in turn based on <a href=""https://github.com/Netflix/zuul"" rel=""nofollow noreferrer"">Netflix Zuul</a> gateway service.  </p>&#xA;&#xA;<p>Note that this is not a complete standalone application but a library. Therefore you still should create another microservice that would act as  API gateway in your application. To make it a gateway you should just add <code>@EnableZuulProxy</code> annotation to the same class that has <code>@SrpingBootApplication</code> annotation. You can find a very good example <a href=""https://dzone.com/articles/microservice-architecture-with-spring-cloud-and-do"" rel=""nofollow noreferrer"">here</a>.</p>&#xA;&#xA;<p>Please also note that you should somehow inform the gateway about your microservices' addresses for redirection. It can be done in two general ways:</p>&#xA;&#xA;<ol>&#xA;<li>By statically defining the addresses in gateway microservice's configuration;</li>&#xA;<li>By applying <a href=""http://microservices.io/patterns/client-side-discovery.html"" rel=""nofollow noreferrer"">service discovery pattern</a> in conjunction with e.g. <a href=""https://github.com/Netflix/eureka"" rel=""nofollow noreferrer"">Netflix Eureka</a> service registry.</li>&#xA;</ol>&#xA;&#xA;<p>The <em>1st</em> approach is easy and straightforward but is not very well for large number of microservices and/or when microservices' locations can change dynamically (e.g. due to auto-scaling).<br>&#xA;The <em>2nd</em> approach requires additional component - service registry - and needs modification of other microservices (to let them register themselves in the registry). This is quite more complicated approach but is the only possible in case of complex architecture. Simple yet expressive example can be found in the <a href=""https://dzone.com/articles/microservice-architecture-with-spring-cloud-and-do"" rel=""nofollow noreferrer"">same article</a>.</p>&#xA;"
48097317,36832219,7581163,2018-01-04T14:25:33,"<p>Please find below the answer to all your questions - </p>&#xA;&#xA;<ol>&#xA;<li><p>Can all microservices data(cached) be kept under a single instance of redis server? Ans - Yes you can keep all the data under single redis instance, all you need to do is to set that data using different key Name. As redis is basically a Key-Value Database. </p></li>&#xA;<li><p>Should every microservice have its own cache database in redis? Ans - Not required. Just make different key for each microservice. Also please note that you can use colon (:) to make folders in redis, to identify different microservices easily on Redis Desktop Manager.&#xA;Example - Key Name X:Y:Z, here Z is placed in Y folder and Y is in X. SO you will get a folder kind of structure. That would be helpful to differentiate different microservices.</p>&#xA;&#xA;<ol start=""3"">&#xA;<li>How to refresh cache data without setting EXPIRE? Since it would consume more memory. Ans  - You can set data again on the same key if you have any change in Microservice response. That Key  value will get over written in that case. </li>&#xA;</ol></li>&#xA;</ol>&#xA;"
49269429,48984289,1885419,2018-03-14T03:44:19,"<p>So for the people who are looking for a quick solution, here is my nginx file</p>&#xA;&#xA;<pre><code>http {&#xA;&#xA;&#xA;    server {&#xA;&#xA;        listen 80;&#xA;&#xA;        location /test {&#xA;            proxy_pass http://test-api:5000;&#xA;        }&#xA;        location /search {&#xA;            proxy_pass http://search:5000;&#xA;        }&#xA;        location /health-test {&#xA;            proxy_pass http://test-api:5000/health;&#xA;        }&#xA;        location /health-search {&#xA;            proxy_pass http://search:5000/health;&#xA;        }&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
51286494,51280734,9364971,2018-07-11T13:17:54,"<p>You can shutdown the application properly via JMX or HTTP if the endpoint is enabled (add endpoints.shutdown.enabled=true to your application.properties file).</p>&#xA;&#xA;<p>/shutdown - Lets the application be gracefully shutdown. (not enabled by default).</p>&#xA;&#xA;<p>From the <a href=""https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/"" rel=""nofollow noreferrer"">Spring boot documentation</a></p>&#xA;"
30423183,30422184,588267,2015-05-24T11:51:24,"<p>This is a very open question but I will try to illustrate why Elixir/Erlang may be the best platform out there for developing distributed systems (regardless if you are working with microservices).</p>&#xA;&#xA;<p>First, let's start with some background. The Erlang VM and its standard library were designed upfront for building distributed systems and this really shows up. As far as I know, it is the only runtime and VM used widely in production designed upfront for this use case.</p>&#xA;&#xA;<h2>Applications</h2>&#xA;&#xA;<p>For example, you have already hinted at ""applications"". In Erlang/Elixir, code is packaged inside applications which:</p>&#xA;&#xA;<ol>&#xA;<li>are started and stopped as unit. Starting and stopping your system is a matter of starting all applications in it</li>&#xA;<li>provide a unified directory structure and configuration API (which is not XML!). If you have already worked with and configured an OTP application, you know how to work with any other one</li>&#xA;<li>contains your application supervision tree, with all processes (by process I mean ""VM processes"" which are lightweight threads of computation) and their state</li>&#xA;</ol>&#xA;&#xA;<p>The impact of this design is huge. It means that Elixir developers, when writing applications have a more explicit approach to:</p>&#xA;&#xA;<ol>&#xA;<li>how their code is started and stopped</li>&#xA;<li>what are the processes that make part of an application and therefore what is the application state</li>&#xA;<li>how those process will react and be affected in case of crashes or when something goes wrong</li>&#xA;</ol>&#xA;&#xA;<p>Not only that, the tooling around this abstraction is great. If you have Elixir installed, open up ""iex"" and type: <code>:observer.start()</code>. Besides showing information and graphs about your live system, you can kill random processes, see their memory usage, state and more. Here is an example of running this in a Phoenix application:</p>&#xA;&#xA;<p><img src=""https://i.stack.imgur.com/7j9BR.png"" alt=""Observer running with a Phoenix application""></p>&#xA;&#xA;<p>The difference here is that Applications and Processes give you an <strong>abstraction to reason about your code in production</strong>. Many languages provides packages, objects and modules mostly for code organization with no reflection on the runtime system. If you have a class attribute or a singleton object: how can you reason about the entities that can manipulate it? If you have a memory leak or a bottleneck, how can you find the entity responsible for it?</p>&#xA;&#xA;<p>If you ask anyone running a distributed system, that's the kind of insight that they want, and with Erlang/Elixir you have that as the building block.</p>&#xA;&#xA;<h2>Communication</h2>&#xA;&#xA;<p>All of this is just the beginning really. When building a distributed system, you need to choose a communication protocol and the data serializer. A lot of people choose HTTP and JSON which, when you think about it, is a very verbose and expensive combination for performing what is really RPC calls.</p>&#xA;&#xA;<p>With Erlang/Elixir, you already have a communication protocol and a serialization mechanism out of the box. If you want to have two machines communicating with each other, you only need to give them names, ensure they have the same secret, and you are done. </p>&#xA;&#xA;<p>Jamie talked about this at Erlang Factory 2015 and how they were able to leverage this to build a game platform: <a href=""https://www.youtube.com/watch?v=_i6n-eWiVn4"" rel=""noreferrer"">https://www.youtube.com/watch?v=_i6n-eWiVn4</a></p>&#xA;&#xA;<p>If you want to use HTTP and JSON, that is fine too and libraries like Plug and frameworks like Phoenix will guarantee you are productive here too.</p>&#xA;&#xA;<h2>Microservices</h2>&#xA;&#xA;<p>So far I haven't talked about microservices. That's because, up to this point, they don't really matter. You are already designing your system and nodes around very tiny processes that are isolated. Call them nanoservices if you'd like to!</p>&#xA;&#xA;<p>Not only that, they are also packaged into applications, which group them as entities that can be started and stopped as unit. If you have applications A, B and C, and then you want to deploy them as [A, B] + [C] or [A] + [B] + [C], you will have very little trouble in doing so due to their inherent design. Or, even better, if you want to avoid adding the complexity of microservices deployments into your system upfront, you can just deploy them altogether in the same node.</p>&#xA;&#xA;<p>And, at the end of the day, if you are running all of this using the Erlang Distributed Protocol, you can run them in different nodes and they will be able to reach other as long as you refer to them by <code>{:node@network, :name}</code> instead of <code>:name</code>.</p>&#xA;&#xA;<p>I could go further but I hope I have convinced you at this point. :)</p>&#xA;"
33475967,33465577,4332,2015-11-02T10:55:53,"<p>There are several solutions.</p>&#xA;&#xA;<p>First, <strong>DO NOT</strong> put your secrets into the image. That's just a bad idea, as you've realized. If you don't add your secrets at build time, you have to do it at run-time. This leaves us with a few options:</p>&#xA;&#xA;<ul>&#xA;<li><p>Use environment variables as suggested by the <a href=""http://12factor.net/"" rel=""noreferrer"">12 Factor App</a>. You will then need to write a script that will populate the config files with values of these variables when the container starts up. This works, but I don't really like it, as environment variables are easily leaked (they can be seen in linked containers and <code>docker inspect</code> and are often included in bug reports). Also see <a href=""https://github.com/conjurinc/summon"" rel=""noreferrer"">Summon</a>.</p></li>&#xA;<li><p>Use volumes. Just mount the config file with the secrets at run-time. This works, but does mean you have a file with the secrets lying about on the host. This gets more complicated when you don't know on which host your container will run, such as when using frameworks like Swarm and Mesos.</p></li>&#xA;<li><p>Use a secure k/v store such as <a href=""https://hashicorp.com/blog/vault.html"" rel=""noreferrer"">Vault</a>/<a href=""https://github.com/square/keywhiz"" rel=""noreferrer"">Keywhiz</a>. As you point out, you will need to do some scripting to get the values into the application (as with env vars). You also need to authenticate to the k/v store somehow (you may want to look at the volume drivers for <a href=""https://github.com/calavera/docker-volume-keywhiz"" rel=""noreferrer"">Keywhiz</a> and <a href=""https://github.com/calavera/docker-volume-vault"" rel=""noreferrer"">Vault</a>, or use a one-use token passed via an env var).</p></li>&#xA;</ul>&#xA;&#xA;<p>Kubernetes already has <a href=""http://kubernetes.io/v1.0/docs/user-guide/secrets.html"" rel=""noreferrer"">fairly advanced support for secrets</a>, and I would expect to see other frameworks adopt their own solutions.</p>&#xA;"
33479036,33478131,4332,2015-11-02T13:44:07,"<p>In general you'll find that most containers use entrypoint scripts to configure applications by populating configuration files using environment variables. For an advanced example of this approach see the <a href=""https://github.com/docker-library/wordpress/blob/master/apache/docker-entrypoint.sh"" rel=""nofollow"">entrypoint script</a> for the <a href=""https://hub.docker.com/_/wordpress/"" rel=""nofollow"">Wordpress official image</a>.</p>&#xA;&#xA;<p>Because this is a common pattern, Jason Wilder created the <a href=""https://github.com/jwilder/dockerize"" rel=""nofollow"">dockerize</a> project to help automate the process.</p>&#xA;"
49616741,49606124,728812,2018-04-02T18:32:41,"<p>There are several patterns you could use.</p>&#xA;&#xA;<p>For once there is still the possibility of MS sharing databases, as long as it is within their bounded context.&#xA;As only beyond the translation layer there should be no sharing on the data layer.</p>&#xA;&#xA;<p>Alternatively, you could see Neo4j as the database powering certain microservices that utilize the graph, e.g. recommendation, fraud etc. That database could then be populated by domain-events in an event-sourcing architecture.</p>&#xA;"
43011321,39622142,5712620,2017-03-25T00:52:29,"<p><a href=""http://www.reactivemanifesto.org/"" rel=""nofollow noreferrer"">Reactive Programing</a> can not be just resumed in design patterns. There are <em>many</em> considerations about systems architecture, dev ops and so to have in mind when you are designing high performance and availability systems.</p>&#xA;&#xA;<p>Specifically, about resiliency, you should be thinking, for example, in:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://www.docker.com/"" rel=""nofollow noreferrer"">Containerization</a></li>&#xA;<li><a href=""https://kubernetes.io/"" rel=""nofollow noreferrer"">Services Orchestration</a></li>&#xA;<li><a href=""https://mesos.github.io/chronos/"" rel=""nofollow noreferrer"">Fault Tolerant Jobs</a></li>&#xA;<li><a href=""https://kafka.apache.org/"" rel=""nofollow noreferrer"">Pub/Sub Model</a></li>&#xA;<li>And looots of other things :)</li>&#xA;</ul>&#xA;"
35369492,26616962,617964,2016-02-12T18:14:37,"<p>Dumb pipes simply means point to point connections. The end points do all the work and any complexity is taken out of the mechanism connecting them. I think people talk about ESBs in this conversation because dumb pipes (point to point connections) are a terrible idea in an enterprise setting (and in many others). ESBs are not 'dumb pipes'. ESBs are pretty much a good definition of very intelligent pipes. And they help get control of the incredibly hairy mess that point to point connections create whenever you have more than a few services that need to talk to each other. </p>&#xA;&#xA;<p>WSO2 has just created a set of good webinars on microservices, and they talk about this concept. But even they shy away from using the concept of dumb pipes. </p>&#xA;&#xA;<p>Now dumb pipes can make sense if the services are used purely in an ad hoc basis but not when trying to manage complex enterprise systems. Setting up multiple network connections for every service is the least of it.</p>&#xA;"
37739680,37739538,816620,2016-06-10T04:03:39,"<p>Assuming your microservice is its own http server (if not, then please explain more about its architecture) and assuming your API is designed such that you can easily identify which routes go to which microservice without specifying every single possible route, then you should be able to create one route handler for an entire microservice (or at worst a very small number of route handlers for the entire microservice).</p>&#xA;&#xA;<p>For example, if all requests that start with <code>/api/foo</code> go to the <code>foo</code> microservice, then you should be able to have a single route handler that catches <code>/api/foo/*</code> and proxies that to the microservice.  If you have common middleware for all requests (such as middleware that runs or verifies an authentication process), that can be in the stack before the proxy route handlers so it will be invoked for all requests that go to the microservice.</p>&#xA;&#xA;<p>If you don't have a 1-to-1 mapping between incoming API calls and microservice APIs, then you have to create a mapping between the two.  Depending upon the level of mismatch, you may be able to do this with a table-driven approach where you specify what matches with what in a table and then one piece of generic code processes all the definitions in the table.</p>&#xA;"
51380173,51368809,5492716,2018-07-17T11:24:24,"<p>I think, you will have to change default Zone in client property from <strong>/EurekaServer</strong> to <strong>/eureka</strong></p>&#xA;&#xA;<pre><code>eureka:&#xA;     client: &#xA;      serviceUrl:&#xA;       defaultZone: http://localhost:8080/eureka&#xA;</code></pre>&#xA;&#xA;<p><strong>""/eureka"" is the rest endpoint to register services to eureka registry.</strong> &#xA;Make the changes and try, it should work.</p>&#xA;&#xA;<p>In addition to this, if you are willing to change the UI dashboard url you should use eureka.dashboard.path property.</p>&#xA;"
50709096,50651256,3664876,2018-06-05T21:19:57,"<p>From a purely technical perspective, the smaller the microservice the easier it can be developed quicker (Agile), iterated on quicker (Lean), and deployed more frequently (Continuous Delivery). But on the modeling side, it is important to avoid creating services that are too small. According to Vaughn Vernon (Author of IDDD Book), we cannot arbitrarily reduce the size of a bounded context because its optimal size is determined by the business context (domain). Our technical need for the size of a service can sometimes be different (smaller) from what DDD modeling can facilitate. This is probably why Sam Newman, very carefully, called bounded context analysis an excellent start, but not the sole prescription for how to size microservices. Bounded contexts are a great start.</p>&#xA;"
29888762,29888108,61164,2015-04-27T06:35:19,"<p>REST services do not have a schema or WSDL type of function to define the format of the service.  This is what makes them more light weight compared to traditional web services.  </p>&#xA;&#xA;<p>There is something called WADL, or Web Application Description Language, but this is not really a standard, and isn't widely supported.  It's also quite controversial as there are many that feel it's not needed.</p>&#xA;&#xA;<p><a href=""http://en.wikipedia.org/wiki/Web_Application_Description_Language"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/Web_Application_Description_Language</a></p>&#xA;&#xA;<p>Also see this discussion on Programmers</p>&#xA;&#xA;<p><a href=""https://softwareengineering.stackexchange.com/a/133693/4368"">https://softwareengineering.stackexchange.com/a/133693/4368</a></p>&#xA;"
47581262,47544877,8908524,2017-11-30T19:33:39,"<p>You should not mix the concepts. An API Gateway is hiding your infrastructure from your client. It can be a single frontend for many services, in the ""Backends for frontends"" sense. It can also be responsible for many other things, such as authentication.</p>&#xA;&#xA;<p>A web socket server can sit in parallel to your API Gateway. Another domain or another port. Let's say you use a web socket server like <a href=""http://nchan.io"" rel=""nofollow noreferrer"">http://nchan.io</a>. The events from your application go through your message broker or whatever messaging integration pattern you use. A consumer can pick up this events and publish them through the Nchan server. Clients (for example Browsers) connect to the Nchan server and will be informed about the events.</p>&#xA;"
42732552,42648060,5834477,2017-03-11T07:22:27,<p>Setting <code>management.security.enabled=false</code> in the <code>application.properties</code> will disable the security on the endpoints.</p>&#xA;
35419378,35409492,5834477,2016-02-15T21:25:32,"<p>Either you use the netflix-eureka-client without spring-cloud and have to configure all by yourself (which means duplicating EurekaDiscoveryClientConfiguration)</p>&#xA;&#xA;<p>Or you could run a sidecar service. The sidecar includes a zuul-proxy which would proxy the services discovered by eureka. Have a look int the <a href=""http://projects.spring.io/spring-cloud/spring-cloud.html#_polyglot_support_with_sidecar"" rel=""nofollow"" title=""sping cloud documentation"">Spring Cloud Docs - Polyglot support with Sidecar</a></p>&#xA;"
50219506,50176793,1849837,2018-05-07T17:25:43,"<p>I was successfully doing such kind of test. You need test instance of RabbitMQ, test exchange to send messages to  and test queue to connect to receive messages.</p>&#xA;&#xA;<p><strong>Do not mock everything!</strong></p>&#xA;&#xA;<p>But, with test consumer, producer and test instance of rabbitMQ there is no actual production code in that test.</p>&#xA;&#xA;<p><strong>use test rabbitMQ instance and real aplication</strong></p>&#xA;&#xA;<p>In order to have meaniningfull test I would use test RabbitMQ instance, exchange and queue, but leave real application (producer and consumer).</p>&#xA;&#xA;<p>I would implement following scenario</p>&#xA;&#xA;<ol>&#xA;<li><p>when test application does something that test message to rabbitMQ</p></li>&#xA;<li><p>then number of received messages in rabbitMQ is increased then</p></li>&#xA;<li><p>application does something that it should do upon receiving messages</p></li>&#xA;</ol>&#xA;&#xA;<p>Steps 1 and 3 are application-specific. Your application sends messages to rabbitMQ based on some external event (HTTP message received? timer event?). You could reproduce such condition in your test, so application will send message (to test rabbitMQ instance).</p>&#xA;&#xA;<p>Same story for verifying application action upon receiving message. Application should do something observable upon receiving messages.&#xA;If application makes HTTP call- then you can mock that HTTP endpoint and verify received messages. If application saves messages to the database- you could pool database to look for your message.</p>&#xA;&#xA;<p><strong>use rabbitMQ monitoring API</strong></p>&#xA;&#xA;<p>Step 2 can be implemented using RabbitMQ monitoring API (there are methods to see number of messages received and consumed from queue <a href=""https://www.rabbitmq.com/monitoring.html#rabbitmq-metrics"" rel=""noreferrer"">https://www.rabbitmq.com/monitoring.html#rabbitmq-metrics</a>)</p>&#xA;&#xA;<p><strong>consider using spring boot to have health checks</strong></p>&#xA;&#xA;<p>If you are java-based and then using Spring Boot will significantly simpify your problem. You will automatically get health check for your rabbitMQ connection!</p>&#xA;&#xA;<p>See <a href=""https://spring.io/guides/gs/messaging-rabbitmq/"" rel=""noreferrer"">https://spring.io/guides/gs/messaging-rabbitmq/</a> for tutorial how to connect to RabbitMQ using Spring boot.&#xA;Spring boot application exposes health information (using HTTP endpoint /health) for every attached external resource (database, messaging, jms, etc)&#xA;See <a href=""https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html#_auto_configured_healthindicators"" rel=""noreferrer"">https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html#_auto_configured_healthindicators</a> for details.</p>&#xA;&#xA;<p>If connection to rabbitMQ is down then health check (done by org.springframework.boot.actuate.amqp.RabbitHealthIndicator) will return HTTP code 4xx and meaninfull json message in JSON body.</p>&#xA;&#xA;<p>You do not have to do anything particular to have that health check- just using org.springframework.boot:spring-boot-starter-amqp as maven/gradle dependency is enough.</p>&#xA;&#xA;<p><strong>CI test- from src/test directory</strong></p>&#xA;&#xA;<p>I have written such test (that connect to external test  instance of RabbitMQ) using integration tests, in src/test directory. If using Spring Boot it is easiest to do that using test profile, and having details of connection to test RabbitMQ instance in application-test.properties (production could use production profile, and application-production.properties file with production instance of RabbitMQ).</p>&#xA;&#xA;<p>In simplest case (just verify connection to rabbitMQ) all you need is to start application normally and validate /health endpoint.</p>&#xA;&#xA;<p>In this case I would do following CI steps</p>&#xA;&#xA;<ul>&#xA;<li>one that builds (gradle build)</li>&#xA;<li>one that run unit tests (tests without any external dependenices)</li>&#xA;<li>one that run integration tests</li>&#xA;</ul>&#xA;&#xA;<p><strong>CI test- external</strong></p>&#xA;&#xA;<p>Above described approach could also be done for application deployed to test environment (and connected to test rabbitMQ instance). As soon as application starts, you can check /health endpoint to make sure it is connected to rabbitMQ instance.</p>&#xA;&#xA;<p>If you make your application send message to rabbitMQ, then you could observe rabbbitMQ metrics (using rabbitMQ monitoring API) and observe external effects of message being consumed by application.</p>&#xA;&#xA;<p>For such test you need to start and deploy your application from CI befor starting tests.</p>&#xA;&#xA;<p>for that scenario I would do following CI steps</p>&#xA;&#xA;<ul>&#xA;<li>step that  that builds app</li>&#xA;<li>steps that run all tests in src/test directory (unit, integration)</li>&#xA;<li>step that deploys app to test environment, or starts dockerized application</li>&#xA;<li>step that runs external tests</li>&#xA;<li>for dockerized environment, step that stops docker containers</li>&#xA;</ul>&#xA;&#xA;<p><strong>Consider dockerized enevironment</strong></p>&#xA;&#xA;<p>For external test you could run your application along with test RabbitMQ instance in Docker. You will need two docker containers.</p>&#xA;&#xA;<ul>&#xA;<li>one with application</li>&#xA;<li>one with rabbitMQ . There is official docker image for rabbitmq <a href=""https://hub.docker.com/_/rabbitmq/"" rel=""noreferrer"">https://hub.docker.com/_/rabbitmq/</a> and it is really easy to use</li>&#xA;</ul>&#xA;&#xA;<p>To run those two images, it is most reasonable to write docker-compose file.</p>&#xA;"
49638459,49638004,1849837,2018-04-03T20:16:34,<p>It is fine as long as you are fine with non-logged user to be able to browse code of any part of the UI. Using security you are able to block access to some part of the UI if user is not logged or if he misses required role.</p>&#xA;
44879695,41010290,7617773,2017-07-03T07:11:29,"<p>REST API presumes use of HTTP only. it is quite stone age technology and does not accept async. messaging. To plugin messaging there, I would consider  WebSockets Gateways&#xA;-sorry for eventually dummy statements</p>&#xA;"
37130938,36701111,944109,2016-05-10T06:37:27,"<p>Have you looked at <a href=""http://stytex.de/blog/2016/03/25/jhipster3-microservice-tutorial/"" rel=""nofollow"">http://stytex.de/blog/2016/03/25/jhipster3-microservice-tutorial/</a> Part 2: inter-service communication section. It walks you through a specific example of how it is achieved </p>&#xA;"
51809524,51808975,1802221,2018-08-12T14:01:34,"<p>In Addition to @gCoh's answer, to answer your question - <code>Is it best practice to create one Cluster per microservice, or one Cluster for all the microservices, but one Service per microserivce?</code></p>&#xA;&#xA;<p>It depends, What we have done is - Logically grouped the services as per their functionality and created separate clusters out of them.</p>&#xA;"
51810218,51810147,1802221,2018-08-12T15:26:05,<p>You can create it as <code>module</code> and that way you can have multiple modules in the same window.</p>&#xA;
35267050,35264620,3572717,2016-02-08T10:13:25,<p>The problem was that a method in Feign interface cannot have more than one 'general' argument. you can have as many header arguments as you want but not more than one as body. Since @RequestBody doesn't do anything it is regarded not as a header but as another variable in addition to the HttpServletRequest request variable.</p>&#xA;&#xA;<p>So I had to change my business logic to have only one parameter.</p>&#xA;
33309141,33308154,4631789,2015-10-23T18:30:02,"<p>This is not currently possible in Cloud Foundry, the open-source platform Bluemix is based upon. </p>&#xA;&#xA;<p>The only work-around is to have nine instances of the old service and one of the new. The round-robin load-balancer will on average then distribute 10% of traffic to the new instance.</p>&#xA;"
40889846,40884616,6084589,2016-11-30T13:38:47,"<p>The issue you are describing is the fundamental difference between messaging (stateful connections) and http-based services (stateless). A stateful consumer can process messages, b/c the broker knows the connection is active. This is also known as a ""push"" semantic. HTTP-based services are ""pull"". WebSockets provide a level of ""push"" available to web-browsers, but in the end you are really just doing STOMP or MQTT over WebSockets. </p>&#xA;&#xA;<p>If you are doing a web application, look to web sockets. If it is a backend application go JMS+Openwire.</p>&#xA;"
28501714,28500066,1240557,2015-02-13T14:24:18,"<p>The problem is that Jenkins <a href=""https://wiki.jenkins-ci.org/display/JENKINS/Spawning+processes+from+build"">doesn't handle spawning child process from builds very well</a>. Workaround suggested by @Steve in the comment (<code>nohup</code>ing) didn't change the behaviour in my case, but a simple workaround was to <em>schedule</em> app's start by using the <code>at</code> unix command:</p>&#xA;&#xA;<pre><code>&gt; echo ""mvn spring-boot:run"" | at now + 1 minutes&#xA;</code></pre>&#xA;&#xA;<p>This way Jenkins successfully completes the job without timing out.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>If you end up running your application from a <code>.jar</code> file via <code>java -jar app.jar</code> be aware that <a href=""https://github.com/spring-projects/spring-boot/issues/1106"">Boot breaks if the .jar file is overwritten</a>, you'll need to make sure the application is stopped before copying the artifact. If you're using <code>ApplicationPidListener</code> you can verify that the application is running (and stop it if it is) by adding execution of this command:</p>&#xA;&#xA;<pre><code>&gt; test -f application.pid &amp;&amp; xargs kill &lt; application.pid || echo 'App was not running, nothing to stop'&#xA;</code></pre>&#xA;"
29831700,29831699,3666413,2015-04-23T18:36:16,"<p>Hard coding the URL or any credentials in the source is bad practice. In Bluemix, you can pass these credentials to your application using the VCAP_SERVICES environment variable. This is same environment variable that holds credentials for Bluemix services.</p>&#xA;&#xA;<p>You are essentially creating your own service visible to your org and space by creating a <strong>user-provided service</strong>. </p>&#xA;&#xA;<p>Create a new service that provides the url to consumers of this service: </p>&#xA;&#xA;<pre><code>$ cf cups myOrdersAppService -p ""url""&#xA;url&gt; myOrdersApp12345.mybluemix.net/api&#xA;Creating user provided service myOrdersApp in org **** / space ****&#xA;OK&#xA;</code></pre>&#xA;&#xA;<p>And then binding this service to the application that wants this “url” information</p>&#xA;&#xA;<pre><code>$ cf bind-service myOtherApplication myOrdersAppService&#xA;</code></pre>&#xA;&#xA;<p>myOtherApplication can how get the url by parsing the VCAP_SERVICES environment variable. For example, in PHP:</p>&#xA;&#xA;<pre><code>$services = getenv(""VCAP_SERVICES"");&#xA;$services_json = json_decode($services, true);&#xA;&#xA;for ($i = 0; $i &lt; sizeof($services_json[""user-provided""]); $i++){&#xA;    if ($services_json[""user-provided""][$i][""name""] == ""myOrdersApp""){&#xA;        $ordersHost = $services_json[""user-provided""][$i][""credentials""][""url""];&#xA;        $response = http_get($ordersHost);&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>You can use this same method to share credentials of any external services (like databases). When using the microservice architecture, this feature becomes extremely useful. </p>&#xA;&#xA;<pre><code>$ cf cups myExternalDB -p ""host, username, password""&#xA;host&gt; 123.123.123.123&#xA;username&gt; myusername&#xA;password&gt; mypassw0rd&#xA;</code></pre>&#xA;&#xA;<p>For more details, check out this doc: <a href=""http://docs.cloudfoundry.org/devguide/services/user-provided.html"" rel=""nofollow"">http://docs.cloudfoundry.org/devguide/services/user-provided.html</a></p>&#xA;"
41019645,41018981,809198,2016-12-07T14:12:33,"<p>This is a good question and one I think a lot of people are still thinking through as far as best practices are concerned.  The answer really depends on your needs.  There are several ways to crack this nut but these are the two I'm using right now:</p>&#xA;&#xA;<ul>&#xA;<li>Running the database in the typical manner on dedicated machine(s) with replication, etc</li>&#xA;<li>I am currently experimenting with running the database as a service on a Docker Swarm cluster with the data persisted across the cluster with GlusterFS&#xA;&#xA;<ul>&#xA;<li>I have three machines in the cluster labeled as database machines</li>&#xA;<li>These database machines all run a GlusterFS container providing the GlusterFS capabilities&#xA;&#xA;<ul>&#xA;<li>When the database service is started I map the GlusterFS share into the container and specify that the service should only run on a machine labeled as a database node.  With this setup it doesn't matter which node the database service starts on and if a machine fails the database service is automatically migrated to another node labeled as a database node.  The GlusterFS replication of data ensures the integrity of the persisted data.</li>&#xA;</ul></li>&#xA;</ul></li>&#xA;</ul>&#xA;&#xA;<p>As mentioned, it is my understanding that there is still a lot of experimentation going on with this and 'best practices' are not entirely established.  Those best practices will ultimately depend on your needs and risk tolerances.  </p>&#xA;"
46501586,46057423,3114478,2017-09-30T09:49:57,"<p>The biggest problem for us has been that legacy data is on the LAN, and our microservices are in the public cloud (in an attempt to avoid a ""new legacy"" hybrid cloud future).</p>&#xA;&#xA;<p>Oracle Goldengate for Big Data can push change records as JSON to Kafka/Confluent. There's also the option to write your own handlers. You can find a lot of our PoC code in github.</p>&#xA;&#xA;<p>As time has gone by, it became apparent the number of feeds was going to end up in the 300+ range, and we're now considering a data virtualisation + caching approach rather than pushing the legacy data to the cloud apps</p>&#xA;"
37539152,37521011,1413614,2016-05-31T07:13:30,"<p>You have to push your logs to a centralized logging application. &#xA;You can use <a href=""https://www.elastic.co/webinars/introduction-elk-stack"" rel=""nofollow"">Elastic Stack</a>, <a href=""http://www.splunk.com/en_us/solutions/solution-areas/log-management.html"" rel=""nofollow"">Splunk</a>, <a href=""https://www.loggly.com/blog/tools-and-techniques-for-logging-microservices/"" rel=""nofollow"">Loggly</a> or other log management systems.</p>&#xA;"
38400318,38389706,190446,2016-07-15T15:50:11,"<p>Golang is great to use for microservices, but I would say there is not so big difference of managing golang or other languages microservices.&#xA;What I would say is golang specific:</p>&#xA;&#xA;<ul>&#xA;<li>you don't need to install anything on servers since golang is compiled to single library</li>&#xA;<li>you can take advantage of std lib golang rpc package and gob binary decoding, instead of usin 3rd party solution (gorpc, protocol buffer etc)</li>&#xA;</ul>&#xA;&#xA;<p>Other than that you need to use your own judgement. There is plenty of ways of doing one things in microservices world; one day you will implement solution A but when after 3 month you will see that its better to do B, do that.</p>&#xA;&#xA;<p>In internet, there is so much reading about microservices. I will recommend you 2 good resurces: <a href=""https://books.google.co.uk/books/about/Building_Microservices.html?id=RDl4BgAAQBAJ&amp;source=kp_cover&amp;redir_esc=y&amp;hl=en"" rel=""nofollow"">https://books.google.co.uk/books/about/Building_Microservices.html?id=RDl4BgAAQBAJ&amp;source=kp_cover&amp;redir_esc=y&amp;hl=en</a></p>&#xA;&#xA;<p>And article: <a href=""http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html"" rel=""nofollow"">http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html</a></p>&#xA;&#xA;<p>Remember, microservices are not a golden bullet, they often can help making application easier to maintain and grow, but from the other side require lot of additional work, consequence in specifying API contracts and strong devops culture.</p>&#xA;"
48021163,47837207,7873775,2017-12-29T10:30:30,"<p>I came to the following generic solution.</p>&#xA;&#xA;<ol>&#xA;<li>ACL security model is used. Each object in the system has associated set of permissions. Permissions defines who and what actions can perform on the object.</li>&#xA;<li>Microservices are responsible for entity-level authorization and filter objects in responses based on permissions of the objects.</li>&#xA;<li>Central Access Control Service is responsible for the creation, update, and deletion of permissions for all objects in the system. Access Control Service database is the primary store of objects' permissions.</li>&#xA;<li>Permissions stored in microservices databases are synchronized with Access Control Service database using <a href=""https://martinfowler.com/articles/201701-event-driven.html#Event-carriedStateTransfer"" rel=""nofollow noreferrer"">event-carried state transfer</a>. Every time, permissions are changed an event is sent to the message broker. Microservices can subscribe to these events to synchronize permissions.</li>&#xA;<li>API Gateway can be used as the additional protection layer. API Gateway can call Access Control Service directly (RPC) to check response objects' permissions or load recently revoked permissions.</li>&#xA;</ol>&#xA;&#xA;<p>Design features:</p>&#xA;&#xA;<ol>&#xA;<li>A way to uniquely identify each object in the system is required (e.g. UUID).</li>&#xA;<li>Permissions synchronization in microservices are eventual consistent. In case of partitioning between message broker and microservice permissions will not be synchronized. It may be a problem with revocation of the permissions. The solution to this problem is a separate topic.</li>&#xA;</ol>&#xA;"
34553201,34498033,4215791,2016-01-01T02:58:00,"<p>Have you tried creating a <a href=""http://kubernetes.io/v1.1/docs/user-guide/namespaces.html"" rel=""nofollow"">namespace</a>?</p>&#xA;"
40454286,38922420,7111919,2016-11-06T20:39:56,"<p>Our findings have been the same - configuration must be externalized and the notion of ""code as config"" through the lessons learned from implementing time consuming Puppet/Chef systems.</p>&#xA;&#xA;<p>We're building a microservices and API orchestration system at LunchBadger. We also uses git - but it's encapsulated into our system that we provide as a service, because we want configuration to be externalized AND dovetail into whatever CI/CD pipeline infrastructure you may have or are looking to adopt. We also provide visualization on top of microservices and APIs so that you can get an idea of the topology of your once monolithic app in the form of many microservices.</p>&#xA;"
40454375,35875489,7111919,2016-11-06T20:47:51,"<p>No - it's not a prerequisite, even if you have a client app that consumes them - but imho it's a good idea.</p>&#xA;&#xA;<p>Microservices architecture is really nothing new as far as concept. Break up your monolithic app into many components that can iterate quickly at their own pace of development.  Cloud native tooling and best practices have brought on this new notion of ""microservices"" with technology like containers and best practices like code as config.</p>&#xA;&#xA;<p>For client consumption across a broad spectrum of client capabilities, microservices are more easily and practically managed with a gateway because the gateway is a piece of infrastructure that acts as a control point for QoS concerns such as security, throttling, caching, payload pagination, lightweight transformation, tracebacks through header injection, etc.</p>&#xA;&#xA;<p>It's not necessary, but practically speaking if you have one already baked into your microservices development process as part of a cloud ready infrastructure - you address these things during development consideration and not in ""big bang"" fashion after the fact - a huge burden in the past for devop/ops folks to get their head around.  It's such a burden, that that's why we've started our own solution that marries microservices development integration with management under ones cloud native platform and have a market for it</p>&#xA;"
40445003,40233978,7111919,2016-11-06T01:07:46,"<p>There really isn't a notion of what tiers are included in a microservice itself, however generally speaking, when referring to microservices most of the time you're referring to some back end based app.  That begin said - there are companies out there - two that I've talked to directly that are building ""microapps"" - basically the backend service, facade and a portlet like interface that can be plugged into a federated portal through a layer of orchestration.</p>&#xA;&#xA;<p>For the Java folks - think fully encapsulated JSR-268 like components that are run as independent processes running within a portal itself. </p>&#xA;"
37896888,37864491,1805551,2016-06-18T12:23:04,"<p>i think that microservices must use queues in order to communicate with one and other. </p>&#xA;&#xA;<p>the frontend can generate the call but the backend must save the logic for which queues need to be notified once an action has taken place.</p>&#xA;&#xA;<p>i think that RabbitMq is a great tool for those type of design. for example: </p>&#xA;&#xA;<ul>&#xA;<li>user pass stage 1 at checkout&#xA;&#xA;<ul>&#xA;<li>send - userid , email to emailqueue_standby Queue</li>&#xA;<li>send - userid , items to emailcontent_standby Queue</li>&#xA;<li>send - userid , paymentdata to paymentchecking Queue</li>&#xA;</ul></li>&#xA;<li>user complete stage 2 at checkout&#xA;&#xA;<ul>&#xA;<li>consume msg from emailqueue_standby Queue -> activate code that send email</li>&#xA;<li>consume msg from paymentchecking Queue -> activate code check the payment data -> post reply to approve_payment_data -> value user_id , Deal_id</li>&#xA;<li>consume msg from approve_payment_data Queue -> if user payment is approved continue , else stop back to stage 1.</li>&#xA;</ul></li>&#xA;</ul>&#xA;&#xA;<p>so this flow may allow you to update many stages at once without code blocking and also allow to distribute the load between the backend servers.</p>&#xA;"
36690164,36681094,6207983,2016-04-18T09:31:09,"<p>Generally you should not expose all your services to the internet. It is better to have <a href=""http://microservices.io/patterns/apigateway.html"" rel=""nofollow noreferrer"">Gateways</a> as entry points (as you mentioned) to your services. Then you do not need to increase the complexity of the services.<br>&#xA;Therefore my opinion is to go with the first answer you gave to your question and configure IDS at the entry point of the Microservice architecture. This way you detect anything triggering your services and keep the services focused on their service.</p>&#xA;&#xA;<p>A good read for Microservices patterns is either Chris Richardsons <a href=""http://microservices.io/"" rel=""nofollow noreferrer"">microservices.io</a> or <a href=""https://martinfowler.com/tags/microservices.html"" rel=""nofollow noreferrer"">Martin Fowler's blog</a> articles on Microservices.</p>&#xA;"
41319275,40586946,1491439,2016-12-25T07:02:57,"<p>Check <a href=""https://github.com/openshift/origin/releases"" rel=""nofollow noreferrer"">https://github.com/openshift/origin/releases</a> for the list of currently avalable releases.</p>&#xA;&#xA;<p>Your maven build is looking for an older release:&#xA;<a href=""https://github.com/openshift/origin/releases/download/v1.3.1/openshift-origin-client-tools-v1.3.1-dad658de7465ba8a234a4fb40b5b446a45a4cee1-mac.zip"" rel=""nofollow noreferrer"">https://github.com/openshift/origin/releases/download/v1.3.1/openshift-origin-client-tools-v1.3.1-dad658de7465ba8a234a4fb40b5b446a45a4cee1-mac.zip</a></p>&#xA;&#xA;<p>You can try to override the dependency in your builds pom and use the currently available version:&#xA;openshift-origin-client-tools-v1.3.2-ac1d579-mac.zip</p>&#xA;"
50900484,50893043,9170927,2018-06-17T21:08:19,"<p>Maybe <a href=""http://scs-architecture.org/index.html"" rel=""nofollow noreferrer"">Self-Contained Systems (SCS)</a> is what you are looking for. Each Service provides its own UI which may be part of the whole.</p>&#xA;&#xA;<p>A brute force (and not that beautiful) integration approach would be separate Angular Apps presented in iframes.&#xA;Sadly I have too little experience on that topic to help you more.</p>&#xA;"
51428040,51422793,1585136,2018-07-19T16:48:01,"<p>The use of orgs &amp; spaces here is irrelevant to an app being public/private.  Orgs &amp; spaces are for internally organizing your apps and limiting access to those through the cf cli.  You can use whatever structure makes sense for your team &amp; company.</p>&#xA;&#xA;<p>For making an app public/private, it all depends on the use of routes.  If you want public access to an app, you bind a public route to the app (i.e. not an internal route).  If you do not want an app to be public either bind an <a href=""https://docs.cloudfoundry.org/devguide/deploy-apps/routes-domains.html#internal-routes"" rel=""nofollow noreferrer"">internal route</a> or don't bind a route at all.  If you bind an internal route, you can use <a href=""https://docs.cloudfoundry.org/concepts/understand-cf-networking.html#service-discovery"" rel=""nofollow noreferrer"">the platform's DNS-based discovery</a> or bring your own, like Eureka or Consul.  If you don't bind a route you would communicate through a service, like a message broker.</p>&#xA;&#xA;<p>You can even control the traffic between two apps on the container-to-container network via <a href=""https://docs.cloudfoundry.org/concepts/understand-cf-networking.html#policies"" rel=""nofollow noreferrer"">policies</a>.  This allows you to allow or restrict traffic based on type &amp; port.</p>&#xA;"
46096291,46067821,1585136,2017-09-07T12:24:28,"<p>There is nothing specific to Cloud Foundry about how you would receive an uploaded file in a web application.  Since you mentioned using Java, I would suggest checking out this post.</p>&#xA;&#xA;<p><a href=""https://stackoverflow.com/questions/2422468/how-to-upload-files-to-server-using-jsp-servlet#2424824"">How to upload files to server using JSP/Servlet?</a></p>&#xA;&#xA;<p>The only thing you need to keep in mind that's specific to Cloud Foundry is that the filesystem in Cloud Foundry is ephemeral.  It behaves like a normal filesystem and you can write files to it, however, the lifetime of the filesystem is equal to the lifetime of your application instance.  That means restarts, restages or anything else that would cause the application container to be recreated will destroy the file system.  </p>&#xA;&#xA;<p>In short, you can use the file system for caching or temporary files but anything that you want to retain should be stored elsewhere.</p>&#xA;&#xA;<p><a href=""https://docs.cloudfoundry.org/devguide/deploy-apps/prepare-to-deploy.html#filesystem"" rel=""nofollow noreferrer"">https://docs.cloudfoundry.org/devguide/deploy-apps/prepare-to-deploy.html#filesystem</a></p>&#xA;"
50140722,50122851,1585136,2018-05-02T17:51:10,"<blockquote>&#xA;  <p>Having that in mind, is it possible to use a client side load balancer such as Ribbon in a PaaS cloud, so that the clients of your app will reach out directly to the instances running your app, not to the load balancer? If yes, what are the benefits?</p>&#xA;</blockquote>&#xA;&#xA;<p>You certainly could.  If you're using the route mapped to your app on PCF, you'd just have a client side load balancer with one server in it so it's not really adding any benefit.</p>&#xA;&#xA;<p>Where you'd see more benefit is if you're using a client side load balancer and Cloud Foundry's Container to Container networking.  With C2C, you can talk directly to other apps.  The registry is needed in this case, so you can locate your service apps.</p>&#xA;&#xA;<p>This blog post walks through a basic example of using SCS &amp; C2C (it's for PWS, but should work on any PCF environment with C2C).</p>&#xA;&#xA;<p><a href=""https://content.pivotal.io/blog/building-spring-microservices-with-cloud-foundrys-new-container-networking-stack"" rel=""nofollow noreferrer"">https://content.pivotal.io/blog/building-spring-microservices-with-cloud-foundrys-new-container-networking-stack</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>One more related question, if all my services follow the same naming convention e.g. myapp-service and therefore are available under <a href=""https://myapp-service.cfapps.io"" rel=""nofollow noreferrer"">https://myapp-service.cfapps.io</a> is there any benefit of setting up a Service Discovery service (e.g. Eureka) in a PaaS cloud?</p>&#xA;</blockquote>&#xA;&#xA;<p>I think the question is how does your app find <code>myapp-service.cfapps.io</code>?  If you have one app that's consuming one service, it's not too hard to have the app configured with the URL or perhaps to pass it in through an env variable or even a user provided, bound service.  Problem solved.</p>&#xA;&#xA;<p>When you start to get lots of services, where ""lots"" is an arbitrarily large number, this get's to be a pain to manage.  When you hit that point, using a service registry will make life easier.  Not to say you can't use a service registry with the one app / one service scenario, it's just probably not as big of an advantage.</p>&#xA;&#xA;<p>The other advantage of using a service registry is with C2C networking, as I mentioned above.  In that case, you need something to locate your services on the container network.  The SCS registry will do that.</p>&#xA;&#xA;<p>For what it's worth, if you're on a pretty recent version of CF, there is actually platform provided service discovery via DNS.  If available, this could also be used to locate your service instances.  See here for details.</p>&#xA;&#xA;<p><a href=""https://www.cloudfoundry.org/blog/polyglot-service-discovery-container-networking-cloud-foundry/"" rel=""nofollow noreferrer"">https://www.cloudfoundry.org/blog/polyglot-service-discovery-container-networking-cloud-foundry/</a></p>&#xA;"
51026338,50986816,2045168,2018-06-25T14:52:05,"<p>Unfortunately, I believe you'll likely have to use either long polling or web-sockets to accomplish something like this. You need to ""push"" something to the user, or keep the http request open until something comes back.</p>&#xA;&#xA;<p>For handling getting the data back to the actual user, you could use something like <a href=""https://socket.io"" rel=""nofollow noreferrer"">socket.io</a>. When a user connects, socket.io creates an id. Anytime a user connects, you map the userid to the id socket.io gives you.&#xA;Once each request has a userid attached to it, you can emit the result back to the correct client. The flow would be something like this:</p>&#xA;&#xA;<p>web requests order (POST with data and userId)</p>&#xA;&#xA;<p>ui service places order on queue (this order should have userId)</p>&#xA;&#xA;<p>x number of services work on order (passing userId along each time)</p>&#xA;&#xA;<p>ui service consumes from topic. At some point, data appears on the topic. The data it consumes has the userId, the ui service looks up the map to figure out which socket to emit to. </p>&#xA;&#xA;<p>Whatever code is running on your UI would need to also be event-driven, so it would deal with a push of data without the context of the original request. You could use something like <a href=""https://redux.js.org"" rel=""nofollow noreferrer"">redux</a> for this. Essentially, you'd have the server creating redux actions on the client, it works pretty well!</p>&#xA;&#xA;<p>Hope this helps.</p>&#xA;"
40591419,40585401,411632,2016-11-14T14:44:53,"<p>Finally, RabbitMQ already provides a solution to solve my scenario: <a href=""https://www.rabbitmq.com/tutorials/tutorial-six-python.html"" rel=""nofollow noreferrer"">Remote Procedure Calls tutorial</a>.</p>&#xA;&#xA;<p>At the end of the day, a message can have <strong>correlation id</strong> and a <strong>reply to</strong> fields:</p>&#xA;&#xA;<ul>&#xA;<li><p><strong>Correlation id</strong>. An arbitrary value used to uniquely-identify an operation across many exchanges/queues. </p></li>&#xA;<li><p><strong>Reply to</strong>. An arbitrary <code>string</code> to define on which exchange or queue you want to put the result of a given operation. The target exchange/queue will be the <em>callback</em> information that will be received by the operation.</p></li>&#xA;</ul>&#xA;&#xA;<p>Thus, <strong>Service A</strong> sends a message to a given exchange with a <strong>correlation id</strong> and a <strong>reply to</strong> value. It gets subscribed to <strong>reply to</strong> exchange. When <strong>Service B</strong> processes the message it publishes a message to the <em>reply to exchange</em> and <strong>Service A</strong> receives the whole result.</p>&#xA;&#xA;<p>BTW, I still have some concerns about this model, because you need to receive callback messages from other operations while you wait for the result, and whenever you consume a message that's not marked with desired <strong>correlation id</strong>, you simply don't process it and you wait for the next one, and so on, until you get what you want.</p>&#xA;&#xA;<p>Maybe it might be a good idea that <em>callback messages</em> would be queued <a href=""https://www.rabbitmq.com/ttl.html"" rel=""nofollow noreferrer"">with a <em>time-to-live</em></a> so if who started an operation isn't waiting for the callback anymore, a given callback message would be automatically dropped.</p>&#xA;&#xA;<p>Another approach would be throwing the callback information as a document or record in a SQL/NoSQL database, and poll the database with an indexed query <em>by callback identifier</em>... </p>&#xA;&#xA;<p>If I find some other approach using RabbitMQ I'll update my answer. </p>&#xA;"
40720762,40719381,411632,2016-11-21T12:56:51,"<p>After some research, I feel that this should be a <em>good practice</em>:</p>&#xA;&#xA;<ul>&#xA;<li>There should be a single exchange. For example <strong>searchrequest</strong>.</li>&#xA;<li>Two queues, one for <em>incoming</em> requests and other for <em>responses</em>.</li>&#xA;<li>The whole single exchange should route message from requests or responses queue based on a given routing key. </li>&#xA;<li>When some service does a search requests, sends a message to <strong>searchrequest</strong> exchange and <strong>request</strong> routing key. When search indexing service creates a response, sends it to <strong>searchrequest</strong> exchange too but it publishes the response message with <strong>response</strong> routing key. </li>&#xA;</ul>&#xA;&#xA;<p>At the end of the day, using routing keys to publish messages to same queue on my particular case doesn't seem to be <em>natural</em>. It feels hacky. </p>&#xA;"
35379622,35379246,1726192,2016-02-13T12:06:24,"<p>Django basic idea is to couple the entire app functionality together, but this does not hold in your case. It's a style and opinion question, here is what I did in a similar situation.</p>&#xA;&#xA;<p>Split the app functionality into two project lairs:</p>&#xA;&#xA;<pre><code>mysite&#xA;   |&#xA;   | - db_access&#xA;   |         | --- app1&#xA;   |                 | ---- models.py&#xA;   |                 | ---- db_api.py&#xA;   |         | --- app2&#xA;   |                 | ---- models.py&#xA;   |                 | ---- db_api.py&#xA;   | - service&#xA;   |         | --- app1&#xA;   |                 | ---- urls.py&#xA;   |                 | ---- views.py&#xA;   |         | --- app2&#xA;   |                 | ---- urls.py&#xA;   |                 | ---- views.py&#xA;</code></pre>&#xA;&#xA;<p>The db_access part has the models, and <code>db_api.py</code> has the queries, get objects etc, so you don't query the models, but the db_api. </p>&#xA;&#xA;<p>Instead of</p>&#xA;&#xA;<pre><code>item = app1.models.Items.objects.get(user=request.user)&#xA;</code></pre>&#xA;&#xA;<p>Use</p>&#xA;&#xA;<pre><code>item = app1.db_api.get_first_item(user=request.user)&#xA;</code></pre>&#xA;&#xA;<p>This style lets you share the db &amp; models access together, while each service consumes what it needs, and then use it for API, website etc. If a service consumes some data in a way that other services don't (and will not) use, then put this query in the service code, otherwise in the db_api.py.</p>&#xA;&#xA;<p>This is more like a traditional laired application, but it works.</p>&#xA;&#xA;<p>Another thing, the same project can use <strong>two</strong> git repositories, one for the db_access (which all services pull), and one for the specific service. So each django project is actually the db_access repo, and the service repo - wsgi doesn't care, of course, if the project code comes from two repositories.</p>&#xA;"
40048703,35065875,105456,2016-10-14T17:19:27,"<p><a href=""https://github.com/tmc/grpc-websocket-proxy"" rel=""noreferrer"">https://github.com/tmc/grpc-websocket-proxy</a> sounds like it may meet your needs. This translates json over web sockets to grpc (layer on top of grpc-gateway).</p>&#xA;"
30995617,30991404,41968,2015-06-23T06:43:59,"<p>The issue as it seems  transitive dependency of the dependency is resulting with two different versions of metrics-core. The best thing to do would be to used the right library dependency so that you end up with a single version of this library. Please use <a href=""https://github.com/jrudolph/sbt-dependency-graph"" rel=""nofollow"">https://github.com/jrudolph/sbt-dependency-graph</a> , if it is difficult to figure out dependencies.</p>&#xA;&#xA;<p>If it is not possible to get to a single version then you would most likely to go down <a href=""http://www.scala-sbt.org/0.13/docs/Library-Management.html#Exclude+Transitive+Dependencies"" rel=""nofollow"">exclude route</a> . I assume, this only work, if there is compatibility between the all required versions.</p>&#xA;"
50713860,40467382,1555696,2018-06-06T06:47:38,"<p>Whenever you have to implement <strong>MicroServices</strong> in Rails, then I prefer to put your authentication and authorization (role based permissions) using <strong>JWT</strong> <strong>(<a href=""https://jwt.io/"" rel=""nofollow noreferrer"">JSON Web Token</a>)</strong>. Because in <strong>MicroServices</strong>, there are multiple different projects which are deployed on different servers and communicating with each other through APIs and you require only one API Gateway, where user provides the login credentials and its should work for all different projects. I wont prefer devise because it creates a session after successful login which is Stateful, while JWT is Stateless.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Statelessness means that every HTTP request happens in complete isolation. When the client makes an HTTP request, it includes all information necessary for the server to fulfill that request. The server never relies on information from previous requests. If that information was important, the client would have sent it again in this request.</p>&#xA;</blockquote>&#xA;&#xA;<p>In case of <a href=""https://jwt.io"" rel=""nofollow noreferrer"">JWT</a>, each request comes with a token something like <code>""eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJleHAiOjE0NjUwOTYxMzMsInN1YiI6MX0.e9yeOf_Ik8UBE2dKlNpMu2s6AzxvzcGxw2mVj9vUjYI""</code> which will contain all required info in its payload for login. Please refer <a href=""https://restfulapi.net/statelessness"" rel=""nofollow noreferrer"">https://restfulapi.net/statelessness</a>. </p>&#xA;&#xA;<p>The token should also include the role or permissions (e.g. admin role) of user and based on role it should fetch the data and its relatively faster than Stateful requests. Because in case of Stateful requests (as happens in traditional Web Apps), it stores the session_id in cookies and sends the session_id with request. So on server side, first it fetches its user info and check whether its valid user, then fetch its role and then after successful authentication and authorization, it fetches requested data. While in case of JWT, since the role and username comes within token itself which would be decoded on server side, and directly fetches the requested data from DB. JWT (or Statelessness) helps in scaling the APIs to millions of concurrent users by deploying it to multiple servers. Any server can handle any request because there is no session related dependency.</p>&#xA;&#xA;<p>Difference between Stateful and Stateless, please refer <a href=""https://restfulapi.net/statelessness"" rel=""nofollow noreferrer"">https://restfulapi.net/statelessness</a> and <a href=""https://nordicapis.com/defining-stateful-vs-stateless-web-services"" rel=""nofollow noreferrer"">https://nordicapis.com/defining-stateful-vs-stateless-web-services</a>. </p>&#xA;&#xA;<p>For more info about the implementation, please refer <a href=""http://pacuna.io/2016/06/03/rails-and-jwt"" rel=""nofollow noreferrer"">http://pacuna.io/2016/06/03/rails-and-jwt</a> and <a href=""https://github.com/nsarno/knock"" rel=""nofollow noreferrer"">https://github.com/nsarno/knock</a>. </p>&#xA;"
51555275,37915326,773616,2018-07-27T09:48:26,"<p>While using the same database for multiple services may be possible, it should be avoided as it'll create a higher coupling between services than is desirable. E.g. a database downtime will affect all services with sharing but only a single one if each service has their own one.</p>&#xA;&#xA;<p>To avoid a ""distributed monolith"" of services that do synchronous calls to each other (e.g. using REST), you could work with a streaming based approach. Each service would publish a change event whenever its data changes, and other services can subscribe to these streams. So they can react to data changes relevant to them, e.g. by storing a local version of the data (in a representation suited to their needs, e.g. just columns they are interested int) in their own database. That way they can provide their functionality, also if other services aren't available for some time. Naturally, such architecture employs semantics of eventual consistency, but usually that's inevitable in distributed systems anyways.</p>&#xA;&#xA;<p>One way to set up such data streams is change data capture CDC, which will trail the databases log files (e.g. the binlog in MySQL) and publish corresponding events for each INSERT, UPDATE and DELETE. One open source CDC tool is <a href=""http://debezium.io/"" rel=""nofollow noreferrer"">Debezium</a> which comes with connectors for MySQL, Postgres, MongoDB as well as (work-in-progress as of now) Oracle and SQL Server. It can be used with Apache Kafka as the streaming backbone or as library within your Java applications, allowing you to stream data changes into other streaming layers such as Pulsar or Kinesis with just a bit of code. One nice advantage of using persistent topics for the change events, e.g. with Kafka, is that new services can come up and re-read the entire change stream (depending on the topic's retention policy) or just get the current state of each record to do an initial seed of their local database.</p>&#xA;&#xA;<p>(Disclaimer: I'm the lead of Debezium)</p>&#xA;"
40095263,36265833,2604529,2016-10-17T20:38:07,"<p>Also tried hard to figure out the best scheme for versioning</p>&#xA;&#xA;<p>Things to consider: </p>&#xA;&#xA;<pre><code> 1. CI&#xA; 2. Git Branching model&#xA; 3. Compatibility&#xA; 4. Routing&#xA;</code></pre>&#xA;&#xA;<p>So far I use modified SemVer <strong>X.Y.Z</strong></p>&#xA;&#xA;<p><strong>Z</strong> for minor fixes and improvements</p>&#xA;&#xA;<p><strong>X,Y</strong> for label routing</p>&#xA;&#xA;<p><strong>Y</strong> for new features</p>&#xA;&#xA;<p><strong>X</strong> is modified if changes breaks protocol between services</p>&#xA;&#xA;<p>Release: ""2.0.x"",&#xA;Version: ""2.0.0"",</p>&#xA;&#xA;<p>Folder structure</p>&#xA;&#xA;<pre><code>[Service name] -&gt; [Release] -&gt; [Service]&#xA;</code></pre>&#xA;"
39253046,35289988,5882097,2016-08-31T15:00:39,"<p>Multiple options here, hence please provide more details. e.g. Are you able to use ready-made Authorization server implementation (open source)? What technology are you based on?</p>&#xA;&#xA;<p>I was able to easily integrate IdentityServer (<a href=""https://github.com/IdentityServer/IdentityServer3"" rel=""nofollow"">https://github.com/IdentityServer/IdentityServer3</a>), and plug it into own ""users"" service with simple implementation of a few interfaces. It may also handle the DB hardwork for you (store all data around for OAuth 2.0, like clients with secrets, auth codes etc.). IdentityServer allows you to provide your own link to have the 'register' operation available for users, you may also have ability for Administrator to accept or reject registrations, so only accepted users will be able to log in.</p>&#xA;&#xA;<p>In general - Implementing Authorization Service as it is required by RFC for OAuth2.0 (see <a href=""https://tools.ietf.org/html/rfc6749"" rel=""nofollow"">https://tools.ietf.org/html/rfc6749</a> for details) is never piece of cake. Rather use proven solution when needed.</p>&#xA;&#xA;<p>Regards!</p>&#xA;"
46663624,46660853,6747889,2017-10-10T09:47:15,"<p>You should use docker-compose. Best way to manage releases/versions and builds is to make own repository for dedicated docker images(nexus is an example).</p>&#xA;&#xA;<p>In docker-compose you can describe all your infrastructure, create services, network, connecting services to communicate other services, so I think you should go this way to create nice developmnet and production build flow for your microservice application</p>&#xA;&#xA;<p>For cassandra and other  known services you can find prefered images on <code>https://hub.docker.com</code>.</p>&#xA;&#xA;<p>In each microservice you should have Dockerfile, then in main directory of your solution you can create <code>docker-compose.yml</code> file with services definitions.</p>&#xA;&#xA;<p>You can build your microservices in docker container too. Read more about ""Java application build flow with docker"" in google.</p>&#xA;&#xA;<p>All about docker compose you can find here: <code>https://docs.docker.com/compose/</code></p>&#xA;&#xA;<p>All about docker swarm you can find here: <code>https://docs.docker.com/engine/swarm/</code></p>&#xA;"
35001828,34586306,70386,2016-01-25T20:34:07,"<p>ODATA do fit micro services. However, micro services are not a good fit for odata. What I mean is that there is really nothing that stops you from exposing OData in a micro service.</p>&#xA;&#xA;<p>However, by doing so you typically expose a large set of the inner data structure in the micro service. That would in turn increase the coupling between different services. By doing so, you make it harder to change a service due to dependencies.</p>&#xA;&#xA;<p>My own personal rule of thumb is to expose as small API as possible from each service. And the data structures that I expose are not the same as the internal ones. They might be flattened or a union between data in different internal entities.</p>&#xA;&#xA;<p>My reasoning is: If you are going to create separate services, try to separate them as much as possible. Else you are just building a monolith that happens to run in a couple of different windows services.</p>&#xA;"
36896502,36896418,70386,2016-04-27T17:11:12,"<p>Every time you have something that isn't activated by a HTTP request?</p>&#xA;&#xA;<p>To get a hint, run <code>services.msc</code> from <code>WIN+R</code> (run program). Go through the services and try to see if all could be hosted within IIS instead.</p>&#xA;&#xA;<p>If not: Well, there is your answer.</p>&#xA;&#xA;<blockquote>&#xA;  <p>I do believe we can skate by the reboot issue with our centralized monitoring service, as well as the IIS ""never go down"" module. I've chosen to leave the service as a Windows Service while I automate his deployment, however I'm not completely convinced it's worth the inconvenience of install -> attach debugger -> make change -> start/stop -> attach debugger -> flail when debugger fails because service locks a file, etc</p>&#xA;</blockquote>&#xA;&#xA;<p>imho it's much more fragile to depend on a monitoring service and a always on module when you have stuff for that already built into windows (monitoring = windows event log). </p>&#xA;&#xA;<p>As for debugging windows services, they are easy to debug with a small adaptation in  Program.cs. I've written about it here: <a href=""http://blog.gauffin.org/2011/09/05/an-easier-way-to-debug-windows-services/"" rel=""nofollow"">http://blog.gauffin.org/2011/09/05/an-easier-way-to-debug-windows-services/</a></p>&#xA;"
34810240,34794630,70386,2016-01-15T11:38:31,"<p><strong><em>Edit</em></strong></p>&#xA;&#xA;<p><em>This answer is about the Gateway &lt;-> Micro service communication. The user should of course be properly authenticated when the App talks with the gateway</em></p>&#xA;&#xA;<p><strong><em>end edit</em></strong></p>&#xA;&#xA;<p>First of all, the micro services should not be reachable from internet. They should only be accessible from the gateway (which can be clustered).</p>&#xA;&#xA;<p>Second, you do need to be able to identify the current user. You can do it by passing the <code>UserId</code> as a HTTP header. Create a WebApi filter which takes that header and creates a custom <code>IPrincipal</code> from it.</p>&#xA;&#xA;<p>Finally you need some way to make sure that the request comes from the gateway or another micro service. An easy way to do that is to use HMAC authentication on a token. </p>&#xA;&#xA;<p>Store the key in the <code>web.config</code> for each service and the gateway. Then just send a token with each request (which you can authenticate using a WebApi authentication filter)</p>&#xA;&#xA;<p>To generate a hash, use the <code>HMACSHA256</code> class in .NET:</p>&#xA;&#xA;<pre><code>private static string CreateToken(string message, string secret)&#xA;{&#xA;    secret = secret ?? """";&#xA;    var keyByte = Encoding.ASCII.GetBytes(secret);&#xA;    var messageBytes = Encoding.ASCII.GetBytes(message);&#xA;    using (var hasher = new HMACSHA256(keyByte))&#xA;    {&#xA;        var hashmessage = hasher.ComputeHash(messageBytes);&#xA;        return Convert.ToBase64String(hashmessage);&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>So in your <code>MicroServiceClient</code> you would do something like this:</p>&#xA;&#xA;<pre><code>var hash = CreateToken(userId.ToString(), mySharedSecret);&#xA;var myHttpRequest = HttpRequest.Create(""yourUrl"");&#xA;myHttpRequest.AddHeader(""UserId"", userId);&#xA;myHttpRequest.AddHeader(""UserIdToken"", hash);&#xA;//send request..&#xA;</code></pre>&#xA;&#xA;<p>And in the micro service you create a filter like:</p>&#xA;&#xA;<pre><code>public class TokenAuthenticationFilterAttribute : Attribute, IAuthenticationFilter&#xA;{&#xA;    protected string SharedSecret&#xA;    {&#xA;        get { return ConfigurationManager.AppSettings[""SharedSecret""]; }&#xA;    }&#xA;&#xA;    public async Task AuthenticateAsync(HttpAuthenticationContext context, CancellationToken cancellationToken)&#xA;    {&#xA;        await Task.Run(() =&gt;&#xA;        {&#xA;            var userId = context.Request.Headers.GetValues(""UserId"").FirstOrDefault();&#xA;            if (userId == null)&#xA;            {&#xA;                context.ErrorResult = new StatusCodeResult(HttpStatusCode.Forbidden, context.Request);&#xA;                return;&#xA;            }&#xA;&#xA;            var userIdToken = context.Request.Headers.GetValues(""UserIdToken"").FirstOrDefault();&#xA;            if (userIdToken == null)&#xA;            {&#xA;                context.ErrorResult = new StatusCodeResult(HttpStatusCode.Forbidden, context.Request);&#xA;                return;&#xA;            }&#xA;&#xA;            var token = CreateToken(userId, SharedSecret);&#xA;            if (token != userIdToken)&#xA;            {&#xA;                context.ErrorResult = new StatusCodeResult(HttpStatusCode.Forbidden, context.Request);&#xA;                return;&#xA;            }&#xA;&#xA;&#xA;            var principal = new GenericPrincipal(new GenericIdentity(userId, ""CustomIdentification""),&#xA;                new[] {""ServiceRole""});&#xA;            context.Principal = principal;&#xA;        });&#xA;    }&#xA;&#xA;    public async Task ChallengeAsync(HttpAuthenticationChallengeContext context, CancellationToken cancellationToken)&#xA;    {&#xA;    }&#xA;&#xA;    public bool AllowMultiple&#xA;    {&#xA;        get { return false; }&#xA;    }&#xA;&#xA;    private static string CreateToken(string message, string secret)&#xA;    {&#xA;        secret = secret ?? """";&#xA;        var keyByte = Encoding.ASCII.GetBytes(secret);&#xA;        var messageBytes = Encoding.ASCII.GetBytes(message);&#xA;        using (var hasher = new HMACSHA256(keyByte))&#xA;        {&#xA;            var hashmessage = hasher.ComputeHash(messageBytes);&#xA;            return Convert.ToBase64String(hashmessage);&#xA;        }&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
28936852,28930710,70386,2015-03-09T07:16:37,"<p>Any solution attempting to divide the load upon different items in the same collection (like orders) are doomed to fail. The reason is that if you got a high rate of transactions flowing you'll have to start doing one of the following things:</p>&#xA;&#xA;<ol>&#xA;<li>let nodes to talk each other (<code>hey guys, are anyone working with this?</code>)</li>&#xA;<li>Divide the ID generation into segments (node a creates ID 1-1000, node B 1001-1999) etc and then just let them deal with their own segment</li>&#xA;<li>dynamically divide a collection into segments (and let each node handle a segment.</li>&#xA;</ol>&#xA;&#xA;<p>so what's wrong with those approaches?</p>&#xA;&#xA;<p>The first approach is simply replicating transactions in a database. Unless you can spend a large amount of time optimizing the strategy it's better to rely on transactions.</p>&#xA;&#xA;<p>The second two options will decrease performance as you have to dynamically route messages upon ids and also change the strategy at run-time to also include newly inserted messages. It will fail eventually.</p>&#xA;&#xA;<h2>Solutions</h2>&#xA;&#xA;<p>Here are two solutions that you can also combine.</p>&#xA;&#xA;<h2>Retry automatically</h2>&#xA;&#xA;<p>Instead you have an entry point somewhere that reads from the message queue.</p>&#xA;&#xA;<p>In it you have something like this:</p>&#xA;&#xA;<pre><code>while (true)&#xA;{&#xA;    var message = queue.Read();&#xA;    Process(message);&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>What you could do instead to get very simple fault tolerance is to retry upon failure:</p>&#xA;&#xA;<pre><code>while (true)&#xA;{&#xA;    for (i = 0; i &lt; 3; i++)&#xA;    {&#xA;       try&#xA;       {&#xA;            var message = queue.Read();&#xA;            Process(message);&#xA;            break; //exit for loop&#xA;       }&#xA;       catch (Exception ex)&#xA;       {&#xA;           //log&#xA;           //no throw = for loop runs the next attempt&#xA;       }&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>You could of course just catch db exceptions (or rather transaction failures) to just replay those messages.</p>&#xA;&#xA;<h2>Micro services</h2>&#xA;&#xA;<p>I know, Micro service is a buzz word. But in this case it's a great solution. Instead of having a monolithic core which processes all messages, divide the application in smaller parts. Or in your case just deactivate the processing of certain types of messages. </p>&#xA;&#xA;<p>If you have five nodes running your application you can make sure that Node A receives messages related to orders, node B receives messages related to shipping etc.</p>&#xA;&#xA;<p>By doing so you can still horizontally scale your application, you get no conflicts and it requires little effort (a few more message queues and reconfigure each node).</p>&#xA;"
51944527,51923607,70386,2018-08-21T08:22:52,"<p>That aren't microservices. That's pico services and imho they are way to small to motivate all the extra maintenance and administration that a microservice require. In his excellent talk from <a href=""https://www.youtube.com/watch?v=wgdBVIX9ifA"" rel=""nofollow noreferrer"">GOTO Conference 2014</a> Martin Fowler recommends that a couple of full time developers is recommended per Microservice. If you don't have at least one that works full time with the service, then it's way to small.</p>&#xA;&#xA;<p>To keep it simple you could use a message based architecture instead, where each internal service subscribes on messages from other internal services.</p>&#xA;&#xA;<p>For instance when a download is complete you could send a <code>FileDownloaded</code> event from the <code>FileDownloader</code> and to which the <code>FileProcessor</code> have subscribed on. On that way there is no tight coupling between them either.</p>&#xA;&#xA;<p>As for messaging libraries, you could for instance use my <a href=""https://github.com/jgauffin/dotnetcqs"" rel=""nofollow noreferrer"">messaging library</a></p>&#xA;"
39942425,39941660,3740914,2016-10-09T10:16:37,"<p>Try using Feign - it is a declarative REST client. It does not require any boilerplate that you mentioned. Checkout spring-cloud-netflix documentation for more details. In short, your REST client would look like this:</p>&#xA;&#xA;<pre><code>@FeignClient(name = ""service-name"", path = ""/base-path"")&#xA;public interface MyClient{&#xA;&#xA;    @RequestMapping(method = RequestMethod.GET, value = ""/greeting"")&#xA;    String getGreeting();&#xA;&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Invoking <strong>getGreeting</strong> method would result in sending GET request to a service named <strong>service-name</strong> and url <strong>/base-path/greeting</strong></p>&#xA;"
46441915,38071714,3568626,2017-09-27T07:30:13,"<p>For approach #2, in fact that's the way I choose, because it's much easier than maintaining the annoying API gateway manually. With this way you can develop your services independently. Make life much easier :P </p>&#xA;&#xA;<p>There are some great tools to combine schemas into one, e.g. <a href=""https://github.com/AEB-labs/graphql-weaver"" rel=""nofollow noreferrer"">graphql-weaver</a> and apollo's <a href=""https://github.com/apollographql/graphql-tools/pull/382"" rel=""nofollow noreferrer"">graphql-tools</a>, I'm using <code>graphql-weaver</code>, it's easy to use and works great.</p>&#xA;"
50144589,50139640,9601266,2018-05-02T22:43:16,"<p>From my experience, the most obvious advantage of a microservice is the ability to scale horizontally. User analysis takes to long? Just add 10 more workers. Done. Remove then. No need to add more RAM/CPU/whatever to your already costly server that runs your monolith.</p>&#xA;&#xA;<p>Do not plan ahead an try to separate ClientManager microservice - this should be just a class.</p>&#xA;&#xA;<p>You are thinking about migrating to microservices for a reason. Something is using up too much resources. <strong>Find the most problematic process that slows everything down, and create microservice for it.</strong> It can be for example report generation, user creation, data agregation. Start with planning the API. It will state clearly, what are responsibilities it will have and how much resources it will use. When you know what it should do, <strong>name it properly</strong>.</p>&#xA;&#xA;<p>Agile software methodologies are your greatest friend in this process. Take the processes one by one. <strong>Experiment, iterate and evaluate.</strong> With time, it will be obvious how the microservices should do.</p>&#xA;&#xA;<p>There is also a hot topic on how to organize code with microservices - I lean towards a <strong>monorepo</strong> - a single repository with all the code. </p>&#xA;&#xA;<ul>&#xA;<li>Pros: One pull request for many services, easy utils sharing, common dependencies, common deployment procedure and easier automation.</li>&#xA;<li>Cons: You can easily break the API contract and do too much work within one microservice (meaning, it can take other services responsiblity.)</li>&#xA;</ul>&#xA;"
50155005,50154865,9601266,2018-05-03T12:22:27,"<p>Heartbeats are simple requests to microservice's API. Usually, for REST APIs you send <code>GET /healthcheck</code>, that should respond with HTTP status 200. If the request fails or takes awfully long, it means that microservice is dead or hanged, and load balancer should not direct any requests to it.</p>&#xA;"
38511669,38511443,5017586,2016-07-21T18:21:41,"<p>Amazon SQS is a message queue service used by distributed applications to exchange messages through a polling model, and can be used to decouple sending and receiving components—without requiring each component to be concurrently available. By using Amazon SNS and Amazon SQS together, messages can be delivered to applications that require immediate notification of an event, and also persisted in an Amazon SQS queue for other applications to process at a later time.</p>&#xA;&#xA;<p>Untill you don't want to decouple sending and receiving components and just want to achieve your use case in the question it will work in both case SNS- Lambda and SNS - SQS - Lambda.</p>&#xA;"
45765864,45755580,2203890,2017-08-18T22:35:54,"<p>Every service in the microservice architecture should be isolated which means that the team in charge of that service is able to put continuous deployment in practice without need to deploy other services. In practice, IMHO I think that we can use two approaches using our favourite build tool such as <code>maven</code> or <code>gradle</code>:</p>&#xA;&#xA;<ul>&#xA;<li><p>Monoproject: domain, repositories, services and controllers are all in the same project.</p></li>&#xA;<li><p>Multiproject: domain, repositories, services and controllers can be grouped in different modules. i.e domain and repositories are in repository module and services in another module with the same name and controllers in front module.</p></li>&#xA;</ul>&#xA;&#xA;<p>But, doesn't matter which approach you use the project (mono or multi) should represent one service. </p>&#xA;"
46784703,46783912,56050,2017-10-17T07:28:21,"<p>I just figured it out, but I'll leave it up for the benefit of anyone else having the same problem.</p>&#xA;&#xA;<p>Turns out you can't instantiate the <code>Server</code> in the constructor. You <em>must</em> do it in the <code>init</code> method. (No idea why, though—perhaps the thread pools are not set up yet when the constructor runs.)</p>&#xA;&#xA;<p><strong>TestService.scala</strong></p>&#xA;&#xA;<pre class=""lang-scala prettyprint-override""><code>class TestServiceImpl extends Service with TestService {&#xA;    var server: Server = _&#xA;&#xA;    def cancel(ctx: ServiceContext) = {&#xA;        server.stop()&#xA;        server.join()&#xA;    }&#xA;&#xA;    def init(ctx: ServiceContext) = {&#xA;        println(""TestServiceImpl#init"")&#xA;        server = new Server(8090)&#xA;    }&#xA;&#xA;    def execute(ctx: ServiceContext) = {&#xA;        println(""TestServiceImpl#execute"")&#xA;        server.start()&#xA;    }&#xA;&#xA;    def test() = {&#xA;        println(""Tested"")&#xA;    }&#xA;}&#xA;</code></pre>&#xA;"
46803713,46803321,1237490,2017-10-18T06:00:45,"<p>Apart from <strong>Scaling</strong> as an advantage of micro-services, it also provides you flexiblity to choose polyglot architecture i.e (using the right programming language,framework,database for right job).</p>&#xA;&#xA;<p>If you use spring sessions(which off-course provides session replication accross nodes), internally it uses redis/gemfire/hazelcast as replicated session store, but you will have to stick to one programming language &amp; framework for all your services i.e Java &amp; Spring resp.(You can offcourse write your own implementation in other language to read from session store, but its re-inventing the wheels) And this will take away Benefit of Ployglot Architecture.</p>&#xA;&#xA;<p>So typically in microservices architecture, you have a token-service(and it should be able scale individually) implementation to generate tokens(aka sessionIds) which are used for Authentication &amp; Authorization in each service and you should try to avoid storing the session information. It will also help avoiding ""<strong>Single point of Failure</strong>"". </p>&#xA;"
50940159,50935415,2692083,2018-06-20T04:19:41,"<p>Messages or events are use for async communication and microservices in this case are just a small piece of code that react to new messages in certain way. You can have a mechanism in place to send messages from service to another but that will just not be efficient or clean. So we do need a message broker in place whose responsibility is to manage events/messages.&#xA;You cannot in my opinion, live without a message broker </p>&#xA;"
51035991,51026313,2692083,2018-06-26T06:13:01,<p>It is a good practice to name the service as /api/serviceName/resource&#xA;Having said that your problem is the service name. One thing we used to do was add Service to the name. Say UserService or OrderService. I have also seen name as oms (for Order Management Service stand in) ims etc.&#xA;You can have order-management/order or user-management/user.</p>&#xA;&#xA;<p>If it is an internal api you can also go ahead with some code names for your services.</p>&#xA;&#xA;<p>Or you can just let go of the service name. Your routing logic might swell but for external clients they really don't care whom they are talking to. You can keep /api/order or /api/user </p>&#xA;
48537764,48536106,2692083,2018-01-31T08:35:19,"<p>To start with microservices in spring boot, you can go through the <a href=""https://spring.io/guides/gs/rest-service/"" rel=""nofollow noreferrer"">this</a> tutorial.&#xA;This should help you get started with Rest services.&#xA;Later you can cover topics related to data modules.</p>&#xA;&#xA;<p>Once you get hold of the concept of how app are developed and how they execute you can go to aws and pick up a ec2 to setup the container (like tomcat) and deploy you app on it. They are multiple ways you can deploy your app on it.</p>&#xA;&#xA;<p>You can then explore <a href=""https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html"" rel=""nofollow noreferrer"">Elastic beanstalk</a> or <a href=""https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html"" rel=""nofollow noreferrer"">multi containers</a>( similar to docker)</p>&#xA;&#xA;<p>You can read <a href=""https://www.infoq.com/articles/boot-microservices"" rel=""nofollow noreferrer"">this</a> as well</p>&#xA;&#xA;<p>There are just too many things to cover here. Microservices are collection of many services and you will need to provide many helper modules to help you deploy and manage these services.</p>&#xA;"
49350736,49349235,2692083,2018-03-18T17:16:43,"<p>Multiple microservices accessing the database is not recommended. Here you have the case where each of the service needs to be triggered, then they update the data and then some how call the next service. </p>&#xA;&#xA;<p>You really need a mechanism to orchestrate the services. A workflow engine might fit the bill.</p>&#xA;&#xA;<p>I would however suggest an event driven system. I might be going beyond with a limited knowledge of the data that you have. Have one service that gives you basic crud on data and other services that have logic to change the data (I would at this point would like to ask why you want different services to change the state, if its a biz req, its fine) Once you get the data written just create an event to which services can subscribe and react to it.</p>&#xA;&#xA;<p>This will allow you to easily add more states to your pipeline in future.&#xA;You will need a service to manage the event queue.</p>&#xA;&#xA;<p>As far as logging the state of the event was concerned it can be done easily by logging the events. </p>&#xA;&#xA;<p>If you opt for workflow route you may use Amazon SWF or Camunda or really there quite a few options out there.&#xA;If going for the event route you need to look into event driven system in mciroservies. </p>&#xA;"
49379038,49377817,2692083,2018-03-20T08:02:43,"<p>Security needs to be handled at multiple layers and as such its a really broad topic. I will however share some pointers which you can explore further.</p>&#xA;&#xA;<p>First thing first any security comes at a cost. And it's a trade off that you need to do. </p>&#xA;&#xA;<p>If you can ensure that services are available only to the other services and API gateway, then you can delegate application layer security to API gateway and strip the security headers at API gateway itself and continue to have free communication between services. It is like creating restricted zone with ip restrictions (or other means on from where can service be accessed), and api gateway or reverse proxy handling all the external traffic. This will allow you to concentrate on few services as far as security is concerned. Point that you should note here is that you will be losing on authorization part as well but you can retain it if you want to.</p>&#xA;&#xA;<p>If you are using AWS you need to look into security groups and VPN etc to set up a secure layer.</p>&#xA;&#xA;<p>A part of security is also to ensure the service is accessible all the time and is not susceptible to DDOS. API gateways do have a means of safeguarding against such threats.</p>&#xA;"
49036737,49036468,2692083,2018-02-28T19:00:46,"<p>You are requesting a resource from a service, resource being a device and service being a device service.&#xA;From a rest standpoint, you are looking for a resource and your service is providing various methods to manipulate that resource.</p>&#xA;&#xA;<p>The following url can be used.</p>&#xA;&#xA;<pre><code> [GET] ../device?user_id=xyz&#xA;</code></pre>&#xA;&#xA;<p>And device information can be fetched via <em>../device/{device_id}</em></p>&#xA;&#xA;<p>Having said that, if you had one service that is providing for both user and device data than the following would have made sense.</p>&#xA;&#xA;<pre><code>[GET] ../user/{userId}/device&#xA;</code></pre>&#xA;&#xA;<p>Do note that this is just a naming convention and you can pick what suits best for you, thing is pick one and hold onto it.&#xA;When exposing the api consistency is more important.</p>&#xA;"
49054858,49051663,2692083,2018-03-01T16:54:38,"<blockquote>&#xA;  <p>What are API Gateway responsibilities?</p>&#xA;</blockquote>&#xA;&#xA;<p>API gateway could be understood as one service that sits in front of all other services and let you expose these services to the client. &#xA;In doing so it allowing all the traffic to pass through itself and it can, therefore, do lot many things like</p>&#xA;&#xA;<ul>&#xA;<li>Security</li>&#xA;<li>Logging</li>&#xA;<li>Routing</li>&#xA;<li>Versioning</li>&#xA;<li>Transforming</li>&#xA;</ul>&#xA;&#xA;<p>You have an incoming request with headers and you need to pass it along to downstream service that will actually be processing the request. API gateway can do pretty much everything in between like those mentioned above.</p>&#xA;&#xA;<blockquote>&#xA;  <p>What are Identity microservice responsibilities?</p>&#xA;</blockquote>&#xA;&#xA;<p>Identity is the set of data that help you identify the user uniquely. In your case, you have Active Directory which has identity information of all your employees. Similarly, you can keep information about the customer is one such service. Identity should be solely responsible for the basic demographic info of the user that helps you identify him. Alongside this, such a service may need to provide for the security around using the identity.</p>&#xA;&#xA;<p>Identity can have roles and the services when communicating with each other need to pass along the information that tells the downstream service about the identity and some means by which this service can verify the identity information being passed. </p>&#xA;&#xA;<p>This is where OAuth comes into play, and if you add identity, identity provider and authorization you get something called OIDC or OpenID Connect </p>&#xA;&#xA;<p>With this, you should be able to define roles for each of your identities and then let the individual service decide what a particular identity with a particular role can do or cannot do.</p>&#xA;&#xA;<blockquote>&#xA;  <p>How to identify if given user is a customer, an employee or an admin?</p>&#xA;</blockquote>&#xA;&#xA;<p>Well, you can use the role to identify (if you can add roles to your identities) or just let your identity service answer this for you.&#xA;Pass the userId and let the service tell you what kind of user it is based on where this user data was sourced.&#xA;It is little difficult to answer this unless I have more insight.</p>&#xA;"
51942238,51942114,2692083,2018-08-21T05:40:19,<p>It is because of Separation of concerns. &#xA;In Controller which is primarily interested in handling incoming http request and responding back to that request. We are worried about things related to handling stuff related to a given communication channel.</p>&#xA;&#xA;<p>You can expose an rest api as well soap api or you may have various formats int which you would want to share the data. Biz logic as such does not care about how you are communicating this data to end users. So you take it out and keep in one common place that only deals with biz logic while the controller class just calls this. You can then have a rest controller and soap controller answering request via same piece of biz logic code.</p>&#xA;&#xA;<p>What you do in controller is validate the request call the service and handle exception in way you want it to be exposed to the caller.</p>&#xA;
51997398,51988439,2692083,2018-08-24T04:15:40,"<p>When you are working you split your application down into services based on certain guidelines( be it biz driven or logic entities being together) and this will lead to similar problem as you have mentioned.&#xA;My take on that is when you working on your service you want to take control of what your view of that service is. User service is free to define task as they seem fit and you may use User as is or you can change user model as suited for Task service. Important thing is you get to decide (ofcourse aligned to the User service). They are source of truth for user and you are for Task. </p>&#xA;&#xA;<p>Your view of User is built from what ever User service says but you can structure it way you want to.&#xA;So you do have to define User model but it not tightly coupled. Important thing for you is to make sure that you never share your model of user out to the world. Always the reference.</p>&#xA;&#xA;<p>User service is free to add anything to the user model ( and remove provided others are not using and if they are, User service is breaking a contract)&#xA;And you are free to pick and choose from the User Response you get from user service.</p>&#xA;&#xA;<p>Thats how two are decoupled.&#xA;If you talk about code redundancy, where in different services have to create a user model and make api calls and handle failures, well thats a different issue, and you can mitigate it by asking services to expose packages with models and basics calls.</p>&#xA;"
52024043,52023726,2692083,2018-08-26T07:18:11,"<p>The approach you mentioned aligns well with the use case of API Gateway.&#xA;And it is not really just a proxy. There are way too many things you can do around that, but perhaps the most important thing is security and abstraction.&#xA;By allowing frontend to access the services directly you are exposing all your services to the internet and you have to put in mechanism to tackle vulnerabilities in each one of them. It make sense to do it in one place.</p>&#xA;&#xA;<p>Second is discovery and maintainability of endpoints which is a better with such a solution</p>&#xA;&#xA;<p>This may add one more hop but what you could do with this in place make it worth.</p>&#xA;&#xA;<p>Logging, routing, downtime/error handling all can be managed well if you are not exposing services directly.</p>&#xA;&#xA;<p>Another point with respect to Auth service, it may lie behind your proxy or could be the only other service exposed. There are some benefits in either of those approaches but since its not what you asked I will trim it to this.</p>&#xA;&#xA;<p>Having said that, in the end it all really depends on your needs and your use cases. If security is not a prime concern or if you are working more around POC. But just consider the pros and cons of having one</p>&#xA;"
52083082,52081388,2692083,2018-08-29T17:23:26,"<p>To start with I will say there is still another option and this is strip auth token in the api gateway itself and if need be bloat the header with the user context and pass it to downstream. We tend to do this as we consider any service that you access after the api gateway to part of our secured zone. we put all our defenses before or at api gateway. This allow service to talk to each other freely. However we also loose authorization bit here. If you can live with that this works best as it reduce some overhead and this is what I tend to use most often.</p>&#xA;&#xA;<p>However before this I used to get JWT token from the UI and it was verified by our node layer (BFF) and in turn exchange for what we used to call a node access token. Essentially we have a user at the node level for which we used to generate this token and pass along. This token has authorization for pretty much everything (which happened gradually and ultimately we end up giving up authorization for downstream services) &#xA;It helped us because we could ensure that user access token has little use for our downstream services, and if the call is made to downstream directly with user token it will be discarded. Downside to generating our own token was that we ended up giving access to all the services and all the apis to this one token.</p>&#xA;&#xA;<p>If you pass the token from UI to the downstream, pros and cons are some what reversed. Also giving proper roles to the user becomes difficult. </p>&#xA;"
52126206,52126065,2692083,2018-09-01T08:07:05,"<p>Yes, we use RestTemplate to make api calls to our apis and have made each service expose a set of jars that defines the model, basic api calls on that model handling retries and stuff and expose each of them aligned with the api version.</p>&#xA;&#xA;<p>Other services when import these and use them. Say they want user data they use make a function call to the method getUser and which in turn makes the api call fetches the data and respond back.</p>&#xA;&#xA;<p>We try not to make breaking changes in apis and incrementally release new api version and new jar to be used by other system.</p>&#xA;&#xA;<p>Note this is completely optional and its up to the service if they want to make use of the jars or if they want to make an api call directly.</p>&#xA;"
52125468,52125150,2692083,2018-09-01T06:22:21,"<p>I cannot answer how Amaozon does it, nor I think anyone could on a public forum I can tell you how I think this could be managed.&#xA;So you have to take lock on your inventory if you want to make sure you precisely map inventory to an order. If you intend to do that, question will be where you take lock. When an item gets added to the cart, when user goes for payment or when payment is done. But the problem with lock is that it will make you system slow.&#xA;So that is something you should avoid.</p>&#xA;&#xA;<p>Rest all the options you have already covered in your question and it boils down to tradeoffs.</p>&#xA;&#xA;<p>First point, user experience will suffer and you also need to incur the cost of the transaction.</p>&#xA;&#xA;<p>Second option ask you to be ready to undersell or oversell.&#xA;When you keep reserves, you are basically saying that I will be underselling. This can also backfire because say you decide to reserve 5 items but you get 20 concurrent request foir checkout and payment, you will be back to the square one. But it can help in most scenarios, given you are willing to take a hit.</p>&#xA;&#xA;<p>Doing inventory check at checkout can help you get better resolution on inventory but it will not help when you literally have last item in inventory and 10 people doing a checkout on it. Read calls even for two such request coincides you will give them inventory and back to square one.</p>&#xA;&#xA;<p>So what I do in such scenarios, is&#xA;1. My inventory goes not as number but enum i.e critical, low, med, high, very high&#xA;Depending on some analytics we configure inventory check. For high and very high we will not do any check and book the item. for critical and we take the lock. (not exactly a db lock but we reserve the inventory for them), for low and medium we check the inventory and proceed if we have enough. All these values are configurable and help us mitigate the scenarios we have.</p>&#xA;&#xA;<p>Another thing that we are trying is to distribute inventory to inventory brokers and assign inventory broker to some set of services to see this inventory. Even if we reserve the inventory on one broker others can continue selling freely. And there brokers regularly update the inventory master about the status of inventory. Its like Inventory master has 50 items, it distributes 5 each to all ten. After 10 mins they come back and if they need more inventory they ask for it, if they have left over (in case of failure) they drop back inventory to the master for it to be assigned to others.</p>&#xA;&#xA;<p>The above approach will not help you resolve the issue precisely but it gives you certain degree of freedom as to how you can manage the inventory.</p>&#xA;"
51999321,51999232,2692083,2018-08-24T07:18:32,"<p>You are saying to postman (for that matter any client) that your api is producing json and instead you are returning just a string.</p>&#xA;&#xA;<p>Check your header you will have a field called </p>&#xA;&#xA;<pre><code>Content-Type → application/json;charset=UTF-8&#xA;</code></pre>&#xA;&#xA;<p>All you need to do is ensure you are sending valid json or remove that header entry so clients do not try to read it as json.&#xA;Or just to check you are getting right data by changing format from json to text in postman</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/BoA1q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BoA1q.png"" alt=""enter image description here""></a></p>&#xA;"
51781892,51779798,2692083,2018-08-10T08:08:09,"<p>In general, database access is restricted across the services, and also keeping information which you do not own, tends to be a tiresome process as you will always be in fight to sync this piece of data with its intended source of truth.</p>&#xA;&#xA;<p>So the only option that you have is what you have already mentioned in the question.</p>&#xA;&#xA;<p>You end up with a horrible query. </p>&#xA;&#xA;<p>But is it horrible when you write it or will it lag in performance?</p>&#xA;&#xA;<p>It depends, yes if you are using MySQL, you can always perform joins over sub queries</p>&#xA;&#xA;<p>But joins have there own cost.</p>&#xA;&#xA;<p>And even than it will be ok to check if sub queries in these cases gives you expected performance.</p>&#xA;&#xA;<p>If its feasible you can explore other databases which could be really optimized for queries like these.</p>&#xA;&#xA;<p>Or worst case scenario, you can copy the data but you have to put in lot effort to ensure they remain in sync.</p>&#xA;&#xA;<p>Most of the choices are not straight forward and there will be trade offs that you need to take call on.</p>&#xA;"
51861631,51861095,2692083,2018-08-15T15:20:01,"<p>Kafka is product that provides you with an scalable implementation of a queue.&#xA;CQRS is a pattern which talks about how you can separate calls that update databases from those which just reads stuff from DB.</p>&#xA;&#xA;<p>Both of them have no direct association with microservices, which itself just talks about one of way of writing your software. It gives basic guidelines on how to break your software into pieces that might help you develop, deploy and scale them easily.</p>&#xA;&#xA;<p>To answer you question, you can make use of both of them in microservices or you can use them even if you do not have microservices. </p>&#xA;"
51869516,51868586,2692083,2018-08-16T04:14:48,<p>When you writing microservices it is advised to not share databases. That is there because it provides for easy scaling and provides each to services to have their own say when it comes to their set of data. It also enables one of these services to take a call on how data is to be kept and can change it at their will. This gives services flexibility.</p>&#xA;&#xA;<p>Even if your schema is not changing you can be sure of the fact that one service when throttled will not impact the others.</p>&#xA;&#xA;<p>If all your crud calls are channeled through a rest service you do have a service layer in front of a database and yes you can do that under microservices guidelines</p>&#xA;&#xA;<p>I will still not call the approach an anti pattern. Its just that the collective experience says that its better not to talk to one databases for some of the reasons I mentioned above. </p>&#xA;
51915110,51911797,2692083,2018-08-19T06:30:22,"<p>Microservices do tend to go out of control sooner than later. With so many services floating around, you need to think of deployment and monitoring strategies ahead of time.</p>&#xA;&#xA;<p>Both of these are not an easy problem, but you have quite a few tools available at your disposal. </p>&#xA;&#xA;<p>Start with CI/CD. Search around it and you will find a way around. One option is to make use of Jenkins for Blue/Green deployments&#xA;In this case jenkins will be one central place where you manage your deployments (but this is just an example, we do have quite a lot of tools build around this that may help you better based on your needs)</p>&#xA;&#xA;<p>Other part of this problem lies in when where you tend to deploy stuff? Different cloud providers have their own specific ways of handling microservices and it depends on your host really. But one alternative is to make use containers.</p>&#xA;&#xA;<p>If you go with raw containers like dockers directly you will have to take care of mapping ports (if they are deployed on same host machine) but then you can use abstraction on top of this like if you are on AWS then you can consider ECS or docker swarms or I personally prefer Kubernetes. You do not need to worry about the ports on which they are and can directly talk to your service over a load balancer. There is lot that is missing in here and you really need to pick one such tool and dig deep, but there are options out there for you to explore.</p>&#xA;&#xA;<p>Next is monitoring, if you are going with kubernetes, you do get lot of monitoring tools out of the box that will help you access the service logs query them etc. But you also need to make sure that from development perspective you do provide for correlations id's, api metrics, response times, because you will need them to debug issues when its comes to microservices specially one related with latencies. If you are not on kubernetes you can still get all these features added but individually, like ELK stack for log monitoring (as you do not want to go to each service to check for logs), zipkin for tracking , API gateway and loadbalancers for service discovery and talking to containers.</p>&#xA;&#xA;<p>Hope this helps you get started.</p>&#xA;"
50902694,50902528,2692083,2018-06-18T04:29:44,"<p>With the services we tend to ask one question who is source of truth?&#xA;In your case user adds item to the cart and there is service which keep tracks of what items the user has added (it may just have a itemid stored)&#xA;When this used to moves to checkout, there will be a checkout service which will then ask the cart service about the items in the users cart, apply in the cart logic. &#xA;Thing to note is checkout service knows and care about the checkout process and it has no idea where to get the data of the items. Its just calls the right service and get the stuff it wants and apply the logic.&#xA;For checkout to payment you pass along userid cartid and other info and payment can make use of these information to bloat the information as it sees fit and return a response back to checkout which may trigger an order service.</p>&#xA;&#xA;<p>So if you see data is always available with one service and when ever you have a situation where you need data, instead of making a db call you make a service call&#xA;(Service responsibility is to give you this data with low latency and may be pulling in logic to cache or whatever)</p>&#xA;&#xA;<p>Another point with respect to data is source of truth. For order service, which is called often we tend to keep a copy of all the information related to the order in it (Again we do that, their may be better approaches) and in doing so often during a return flow question which system to trust.You may query an order service to get an address on which order is supposed to be shipped, but this address might have been deleted by the user.</p>&#xA;&#xA;<p>This is where Single source of truth comes into play. This is little tricky, for delivery service source of truth for delivery address is what it gets from order service and not the user service (however order service picked up the details from user service at time of placing orders)&#xA;At the same time, during return flow we consider the prices as stored in order service (again a snapshot of what was there during the time order was placed) not necessarily make a call to product service, however for payments we talk to payment service directly to check amount we have taken from the user (There may be multiple inward and outward flows)</p>&#xA;&#xA;<p>So bottom line is</p>&#xA;&#xA;<ul>&#xA;<li>Have one database exposed via one service, and let other service connect to db via this service</li>&#xA;<li>Read more about Single Source of Truth. We decided on certain contracts like who is the SSOT for whom (I do not necessarily agree with this approach but it works well for us)</li>&#xA;</ul>&#xA;"
50852714,50849415,2692083,2018-06-14T08:15:43,"<p>First question is why you need a new version?&#xA;Has your contract change or is there some change in internal logic and if you really need to expose a new version. Next question to ask is how long you need to support the old version and how many versions you intend to have.&#xA;For a mature api and you may make use of approach 2, with smaller footprint if possible. For others you would be in better situation to expose v2 via same service and work around it.&#xA;Code replication is a factor but depends on what you changing. If its all about change in contract you can try to have same biz logic and make it to work with both new and old contracts.</p>&#xA;&#xA;<p>Here are some links that you may find useful</p>&#xA;&#xA;<p><a href=""https://www.mnot.net/blog/2012/12/04/api-evolution"" rel=""nofollow noreferrer"">https://www.mnot.net/blog/2012/12/04/api-evolution</a></p>&#xA;&#xA;<p><a href=""https://stackoverflow.com/questions/389169/best-practices-for-api-versioning"">Best practices for API versioning?</a></p>&#xA;"
50888802,50887887,2692083,2018-06-16T14:30:56,"<p>When using kong as an API Gateway (or for that matter any gateway) we tend to put it at the point where external clients talk to your service. It is a means to discover the individual services. And kong can do good enough job to validate such request. </p>&#xA;&#xA;<p>For the calls you make to other services from within your set of microservices, you may allow for the free passage by means of directly invoking the service. Challenge in that case will be how the services will discover each other. (One way is to rely on DNS entries. We used to do that but later moved to kubernetes and started using their service discovery), and restrict all the incoming traffic to a given service from outside world. So they can only get in via gateway (and thats where we have all the security)</p>&#xA;&#xA;<p>The reason behind the above philosophy is that we trust the services we have created (This may or may not be true for you and if its not then you need to route all your traffic via an api gateway and consider your APIs as just another client and they need to get hold of access token to proceed further or may be have another service discovery for internal traffic)</p>&#xA;&#xA;<p>Or you may write a custom plugin in kong that filters out all the traffic that originates from within your subnet and validates everything else.</p>&#xA;"
50717704,50700178,2692083,2018-06-06T10:06:41,"<p>You can strip of the authentication at the gateway and there is nothing wrong in doing so. There is a slight overhead on the gateway and this will not be a problem if </p>&#xA;&#xA;<ol>&#xA;<li>you intend to make all your resources secure.</li>&#xA;<li>you make sure that any call that reaches the the resource service is from a secure zone i.e request should not come directly to service as it will not have any means to authenticate.</li>&#xA;<li>No Authorization. JWT tokens also has vital info about the roles which help application decide on the authorization.&#xA;If it is ok for you to loose that bit of info, then thats fine.</li>&#xA;</ol>&#xA;&#xA;<p>However you have one place to handle authentication and if you strip the token from the call, depending on the number of hops this call has to make this removal of token may help you.</p>&#xA;&#xA;<p>On the other hand II option gives you freedom that all your services are individually secured. If you want some of the resources of some of the service to be available anonymously you can get that as well.&#xA;You also have control over authorization bit.</p>&#xA;&#xA;<p>Its all about trade offs. But I prefer the second approach as I have more freedom.</p>&#xA;&#xA;<p>Having said that, you really don't need to make a call to auth server to verify the JWT. JWT tokens can be verified independently if you have the public key of signing authority.</p>&#xA;&#xA;<p>Also when requesting for the resource, if token is invalid response code should be 401 and if token is valid Principal is not authorized to access the resource, response should be 403.</p>&#xA;&#xA;<p>API gateway IMO should not have anything to do with Authorization (authentication may be) as it is something which is decided by the service and vary from service to service and resource to resource and should be left for the services to take care of. </p>&#xA;"
48679762,48667874,2692083,2018-02-08T07:21:24,"<p>This is an expected behaviour when making use of tokens.Tokens once created need no further check(other than verifying the signature) and this is perhaps why they are good as they minimize the validation call to Authentication Provider each and every time a service is invoked.</p>&#xA;&#xA;<p>Tokens should be created for the smallest of duration with the TTL being as small as possible for you. In microservices this time can be kept to as small couple of minutes to may be five minutes (Every five minutes you can refresh your token). This should be enough for most applications as long as you do due diligence to keep the token safe when passing from one service to another </p>&#xA;&#xA;<p>If however its critical for you to make sure that no unintended use of tokens is their, you should rest that responsibility with Authentication Provider itself. Anyone else in between do not have the authority to validate the token. </p>&#xA;"
48758911,48731168,2692083,2018-02-13T03:20:32,"<p>Polls Module is as of now seems to be responsible for creating a polls, conducting polls, managing poll session (given that you might have more than one question to answer in your poll) and keeping track of results.</p>&#xA;&#xA;<p>Alright, you might have just one db to keep all of these things but they have separate concerns and (if you scale this up) different biz teams managing it. So I would suggest you can split polls alongs those lines. (They might be helpful when you start scaling it up)</p>&#xA;"
45679551,45673863,2692083,2017-08-14T17:07:23,"<p>I can suggest to look into spring security and various options that it provides.&#xA;If however you are looking from microservices standpoint, you are on a right path with access tokens and as suggested <a href=""https://jwt.io/"" rel=""nofollow noreferrer"">JWT</a> will do good.&#xA;No to get a JWT you can opt for another small auth service using say <a href=""https://github.com/pac4j/spring-security-pac4j"" rel=""nofollow noreferrer"">Pac4j</a> or similar tools out there.</p>&#xA;&#xA;<p>I prefer to use <a href=""http://www.keycloak.org/"" rel=""nofollow noreferrer"">keycloak</a> initially&#xA;Its a open source IAM&#xA;There are several examples that you can find as to how to get started with Keycloak and Spring boot&#xA;But here is the <a href=""https://github.com/XMansion/Kingcross"" rel=""nofollow noreferrer"">one</a> that I wrote some time back .</p>&#xA;&#xA;<p>--&#xA;Just to add that for microservices it makes sense to use AccessTokens as you are not required to keep user creds and pass it along every service and also services can verify the tokens all by themselves without making an expensive network call to your auth service.</p>&#xA;"
48962438,48961984,2692083,2018-02-24T11:40:53,"<p>I will prefer the option 1. Apart from the HTTP overhead, this will also lead your system in an inconsistent state. Service 1 might be working on new values but service 2 will be on old.</p>&#xA;&#xA;<p>Since this is a distributed system that we are talking about, I am willing to take a risk with availability. </p>&#xA;&#xA;<p>Have a configuration service that allows you to plan your config changes. Instead of saying change the value of A from x to y, you say change from x to y at time t. This t allows you to consistently propagate changes to all your system.You need to put in effort to understand what the min value of t should be for you set of services, how will you make all services acknowledge the changes and make them at the right time and how will you manage the new services that come up in between.</p>&#xA;&#xA;<p>Another approach is use Spring Cloud Config (or something similar). It ask the service to register with the centralised config service and make refresh call to all the services to update config. Limitation being not all configs could be refreshed and if you are behind the LB you still need to handle ways to make sure all instances gets updated. </p>&#xA;"
49000560,49000291,2692083,2018-02-27T03:18:57,"<p>You need to ask this question as to what define a service. Reading a chunks of data from a while, does this come under a service?&#xA;When I think of separating my services I think along multiple lines like what this module needs to do. Who all will be using it, what all dependencies do I have, how I need to scale it up in future and above all. Which business team will be taking care of it. I tend to divide the modules based on the answers I get to these questions.</p>&#xA;&#xA;<p>Here in your case I see this as less of a service and more of a utility function that can be put in a jar and shared across. A new service will be more along a line of say reporting service reading legacy excel files to create reports or migrating service which uses a utility to read excel.</p>&#xA;&#xA;<p>Also there is no final answer you need to keep questioning your design unless you are happy with it. </p>&#xA;"
37428398,37427976,212211,2016-05-25T05:22:16,"<p>If micro-services do need to talk to each other, generally its via REST as you have noticed.However microservice purists may be against such dependencies. That apart, service discovery is enabled by publishing available endpoints on to a service registry - <a href=""https://github.com/coreos/etcd"" rel=""nofollow"">etcd</a> in case of CloudFoundry. Once endpoint is registered, various instances of a given service can register themselves to the registry using a POST operation. Client will need to know only about the published end point and not the individual service instance's end point. This is <a href=""http://microservices.io/patterns/self-registration.html"" rel=""nofollow"">self-registration.</a> Client will either communicate to a load balancer such as ELB, which looks up service registry or client should be aware of the service registry.</p>&#xA;&#xA;<p>For (2), there should not be such a hard dependency between micro-services as per micro-service definition, if one is designing such a coupled set of services that indicates some imminent issues such as orchestrating and synchronizing. If such dependencies do emerge, you will have rely on service registries, health-checks and circuit-breakers for fall-back.</p>&#xA;"
31103500,31044380,934899,2015-06-28T18:50:48,"<p>there is a tutorial and description of authentication flow in microservices in our activator, it's in Scala — <a href=""http://www.typesafe.com/activator/template/reactive-microservices"" rel=""nofollow"">http://www.typesafe.com/activator/template/reactive-microservices</a> (source: <a href=""https://github.com/theiterators/reactive-microservices"" rel=""nofollow"">https://github.com/theiterators/reactive-microservices</a>)</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>The basic idea is: You need to validate authenticity of the auth token for every request.&#xA;You can either:</p>&#xA;&#xA;<p>— do it in a proxy (the gateway)</p>&#xA;&#xA;<p>— do it inside the billing microservice</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>What we tend to do is: to validate Auth-Token inside every client-facing microservices. </p>&#xA;&#xA;<p>— We keep the Auth-Token to User information inside Redis instance.</p>&#xA;&#xA;<p>— The client-facing service asks the redis instance if this token is valid</p>&#xA;&#xA;<p>— Redis returns some JSON string that we can use as a user-data for further authorization.</p>&#xA;&#xA;<p>So the server side flow looks like this</p>&#xA;&#xA;<pre><code>get(""projects/"" / Segment) { projectName =&gt;&#xA;  getHeader(""Auth-Token"") { authToken =&gt;&#xA;    Redis.get(""auth:token:#{authToken}"").map { userJson =&gt;&#xA;      if(userJson(""projects"").include(projectName)) {&#xA;        ...processSth...&#xA;        Ok&#xA;      } else {&#xA;        Unauthorized&#xA;      }&#xA;    }&#xA;  }&#xA;}&#xA;</code></pre>&#xA;"
36281348,35113957,1853171,2016-03-29T10:12:37,"<p>This is our approach for anyone else having same problem:</p>&#xA;&#xA;<p>Now each of our projects has a docker-compose which can be run standalone. We have another project called 'development-kit' which clones needed projects and store them in a directory. We can run our projects using command similiar to:</p>&#xA;&#xA;<pre><code>python controller.py --run projectA projectB&#xA;</code></pre>&#xA;&#xA;<p>It runs each project using docker-compose up command. Then when all projects are up and running, it starts adding all other projects main docker's IP to other projects by adding them to the /etc/hosts ips using these commands:</p>&#xA;&#xA;<pre><code># getting contaier id of projectA and projectB&#xA;CIDA = commands.getoutput(""docker-compose ps -q %s"" % projectA)&#xA;CIDB = commands.getoutput(""docker-compose ps -q %s"" % projectB)&#xA;# getting ip of container projectA&#xA;IPA = commands.getoutput(""docker inspect --format '{{ .NetworkSettings.IPAddress }}' %s"" % CIDA)&#xA;</code></pre>&#xA;&#xA;<p>Now for sending requests from projectB to projectA we only need to define projectA IP as ""projectA"" in projectB's settings.</p>&#xA;"
48251134,48251083,150771,2018-01-14T15:31:02,"<p>Lambda probably isn't the best tool for you if you're looking to deploy a Boot application that has multiple endpoints. Lambda functions have a <em>single</em> entry point and API Gateway maps a URL to that entry point. You would need to create multiple Lambda functions, each with their own Spring Boot application (they are isolated from each other) in order to create your REST API endpoints.</p>&#xA;&#xA;<p>If you're looking to run a Spring Boot app on a platform that doesn't require you to manage servers, the Elastic Container Service is probably a better option (or something like Heroku if you can go outside the AWS ecosystem).</p>&#xA;"
41344628,41285879,362483,2016-12-27T11:48:58,"<p>I think using the CQRS pattern, you will have events as well and corresponding event handlers. As you are using RabbitMQ for asynchronous communication between command and query then any message put on specific channel on RabbitMQ, can be listened by a callback method</p>&#xA;&#xA;<p>Receiving messages from the queue is more complex. It works by subscribing a callback function to a queue. Whenever we receive a message, this callback function is called by the Pika library.</p>&#xA;"
41344713,41262716,362483,2016-12-27T11:54:39,"<p>Generally, such kind of situation are handled by implementing proper OAuth server wherein only your API gateway will handle the token validation. Any direct call to microservice will not have proper token exchange and hence requests will be aborted.</p>&#xA;&#xA;<p>In case, you have deployed your micro-services on any cloud then you can acheive this by exposing routes to only API gateway.&#xA;And yes, firewall blocking, IP whitelisting are some of the other ways in restricting the access to your microservices.</p>&#xA;"
41327882,41270649,362483,2016-12-26T07:30:15,"<p>As you have landscape of different microservices, wherein each microservice has its own business context and is accessible by REST endponit. In this case, your client need not to aware about each microservice and thus API gateway comes into picture, using which you can have one entry point to all microservices landscape.</p>&#xA;&#xA;<p>There are different API gateway solutions available like you said apigee, apiman etc. These framework gives some basic implementation of features required in API gateways like request throttling, monitization of request calls, authentication handle, centralized security etc.</p>&#xA;&#xA;<p>Netflix's Zuul provides the filters which you need to implement yourself. So, if you are using Zuul, you have to implement all the features which you want to put in your API gateway by yourself.</p>&#xA;&#xA;<p>I hope this explanation helps!!!</p>&#xA;"
51915247,51868586,362483,2018-08-19T06:50:47,"<p>I have a different opinion here and I'll call this approach as anti-pattern. Few of the key principles which are getting violated here:</p>&#xA;&#xA;<ol>&#xA;<li>Each MS should have it's own bounded context, here if all are sharing the same data set then their business boundaries are blurred.</li>&#xA;<li>DB is single point of failure if DB goes down, all your services goes down.</li>&#xA;<li>If you try to scale single service and spawn multiple instances, it will impact DB performance and eventually, will effect the performance of other microservices.</li>&#xA;</ol>&#xA;&#xA;<p>Solution,..IMO,</p>&#xA;&#xA;<ol>&#xA;<li>First analyze if you have a case for MSA, if your data is very tightly coupled and dependent on each other then you don't need this architecture.</li>&#xA;<li>Explore CQRS pattern, you might like to have different DB for read and write and synchronize them via event pattern.</li>&#xA;</ol>&#xA;&#xA;<p>Hope this helps!!</p>&#xA;"
39465714,39044130,2068211,2016-09-13T08:43:20,<p>If you like Spring Boot then I'd stick with it really. One of the whole points of microservices is for teams to use the technology that suits their needs and that they are happy with.</p>&#xA;&#xA;<p>IMHO the Java EE Microprofile is focussed on helping help folks who know or like Java EE to move to microservices reusing their Java EE knowledge; rather than having to relearn how to do things on Spring Boot. e.g. so they can reuse CDI if they prefer that to Spring DI</p>&#xA;
40613609,40586946,2068211,2016-11-15T15:25:36,<p>I wonder what the output of this command is:</p>&#xA;&#xA;<blockquote>&#xA;  <p>gofabric8 version</p>&#xA;</blockquote>&#xA;&#xA;<p>The binary is in ~/.fabric8/bin if its not on your $PATH.</p>&#xA;&#xA;<p>I wonder if its just that your gofabric8 binary is old. e.g. we have a 0.4.105 version now. </p>&#xA;&#xA;<p>If your gofabric8 binary is old; its sounding like the auto upgrading isn't working correctly; so try deleting ~/.fabric8/bin/gofabric8 and re-running your mvn command</p>&#xA;
47641445,47641231,168179,2017-12-04T20:28:04,"<p>Ok, I figured it out.  The documentation is wrong.  In the template, you're not supposed to wrap the placeholder in quotes.  However, there's a huge caveat:</p>&#xA;&#xA;<blockquote>&#xA;  <p>If you use a placeholder whose value you don't want wrapped in quotes, <strong><em>the template file will no longer be valid JSON</em></strong>, meaning you can't edit it in a JSON editor.  You have to edit it in a plain-text editor.</p>&#xA;</blockquote>&#xA;&#xA;<p>That's what stumped me. I thought the templates had to be valid JSON. They don't.  They <strong>become</strong> valid JSON after the substitutions are performed, but they don't have to be beforehand.</p>&#xA;&#xA;<p>Anyway, hope this helps others!</p>&#xA;"
43543116,43532494,2210035,2017-04-21T12:47:35,<p>It is common to combine ehcache with terracotta to allow distributed caching among nodes. </p>&#xA;&#xA;<p>Regards </p>&#xA;
41074619,41036545,2153459,2016-12-10T10:44:06,"<p>Neither of these are very nice, but what you could do, is split the structure into two parts - a structured part which is deserialised by your microservice, and a separate 'additionalFields' field which contains your other JSON, then you can modify the JSON inside this field without changing <code>Task</code>. You could either add nested JSON as a <code>String</code>.</p>&#xA;&#xA;<pre><code>public class Task {&#xA;   public double operand1;&#xA;   public double operand2;&#xA;   public double multiplicationResult;&#xA;   public String additionalFields&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Or, you could add a <code>Map&lt;String, Object&gt;</code> which would allow you to add key-value pairs, but again, you'd lose the type safety:</p>&#xA;&#xA;<pre><code>public class Task {&#xA;   public double operand1;&#xA;   public double operand2;&#xA;   public double multiplicationResult;&#xA;   public Map&lt;String, Object&gt; additionalFields&#xA;}&#xA;</code></pre>&#xA;"
45453138,45453061,1531124,2017-08-02T06:34:01,"<p>The main point is: a <strong>web service</strong> is (as the name announces) something that you would expect to ""deal"" with HTTP - in other words it does something in the context of the world wide <em>web</em>.</p>&#xA;&#xA;<p>Whereas a microservice is not subject to the WWW context. At its core a microservice is supposed to provide one specific service - but there is no restriction to the http protocol for example.</p>&#xA;&#xA;<p>Often a microservice implements a restful ""web service"" - but it doesn't have to be that way! </p>&#xA;&#xA;<p>Beyond that, microservices are often meant as <em>counter model</em> to a huge <strong>monolithic</strong> application that serves many different kind of requests.</p>&#xA;"
51889585,51889526,1531124,2018-08-17T06:31:10,"<p>First of all, that comment is fully correct: micro-services by itself do not help with latency. Ideally, they only (well, mainly) communicate with other services by using them as service, thus adding a potential penalty for network/inter-process calls.</p>&#xA;&#xA;<p>But the important thing to understand: the idea is not that a micro-service depends on 15 different services that it needs to do its job. You should rather look at them as independent units, which are <em>designed</em> to allow horizontal scaling (by simply adding ""more"" instances).</p>&#xA;&#xA;<p>Therefore the key element is to actually define a micro-service <em>architecture</em>. Just turning a monolith into a distributed system of services, while keeping unnecessary coupling will not achieve that. </p>&#xA;"
39722080,39721791,1531124,2016-09-27T10:17:41,"<p>First, some thought on your model: are you sure that an Employee works for exactly <strong>one</strong> company? And that this company may never change?</p>&#xA;&#xA;<p>For your real question, I don't see a real problem; so let me quote from ""<a href=""https://www.infoq.com/minibooks/domain-driven-design-quickly"" rel=""nofollow"">DDD quickly</a>"": </p>&#xA;&#xA;<p><em>There are times when the construction of an object is more complex</em>. &#xA;...</p>&#xA;&#xA;<p><em>It seems appropriate to use a special Factory class which is given the task of creating the entire Aggregate, and which will contain the rules, the constraints and the invariants which have to be enforced for the Aggregate to be valid. The objects will remain simple and will serve their specific purpose without the clutter of complex construction logic.</em></p>&#xA;&#xA;<p>One solution might be add another layer - one that does that lookup for you; and calls your buildEmployee() method directly with the UUID of the company.</p>&#xA;"
48984673,48984610,1531124,2018-02-26T08:58:21,"<p>Micro services are a ""design"" pattern that guides <em>how</em> you implement functionality. ""Web services"" on the other hand focus on how customers consume services. </p>&#xA;&#xA;<p>In that sense, these two concepts are completely orthogonal. You can provide a REST / SOAP interface to your clients - and internally, this REST endpoint is implemented as micro service ... or as just one thing within a huge monolith.</p>&#xA;"
45301780,45301310,1531124,2017-07-25T11:22:12,"<p>As this is on the brisk of asking for opinion, let's answer about the facts at hand.</p>&#xA;&#xA;<p>The core thing to understand: any <em>concept</em> should never be seen as unalterable <strong>law</strong>. You don't follow rules because they are rules, but because they provide <em>helpful guidance</em> regarding your design decisions. </p>&#xA;&#xA;<p>In that sense this is about balancing. When you can really <strong>abstract</strong> common infrastructure elements, then it is <em>good practice</em> to avoid code duplication where possible. </p>&#xA;&#xA;<p>So instead of creating more and more copies of the same class, you rather turn that into an ""internal library"" and have your service instances use that. The downside here is of course that <strong>all</strong> services using this library now have a dependency on it. </p>&#xA;&#xA;<p>Coming from there, you would strive to really provide a <em>framework</em>. Meaning: base, common parts are implemented only once - but in a fashion that allows the different services to configure/extend/enhance the framework where necessary. </p>&#xA;"
45351270,45351222,1531124,2017-07-27T12:47:47,"<p>""Simple"": when there are two services that should be using something <strong>common</strong> - then the answer is move this code into some form of <em>library</em> and have both services depend on it. </p>&#xA;&#xA;<p>Anything else is most likely a <em>bad</em> idea. The whole idea of micro-services is that service A does <em>not</em> in any form depend on B. And you do <strong>not</strong> want to get into the reflection game and somehow access internals of another service through some sort of backdoor. </p>&#xA;&#xA;<p>As some comment makes clear: using a library helps to avoid <em>code duplication</em>. The alternative is to <strong>willfully</strong> copy the ""common"" parts from service B into service A. This is an option, too.</p>&#xA;&#xA;<p>In that sense: you either re-factor the common parts into a library - or you copy them. Both approaches have their pros and cons. You have to determine what matters most in your environment.</p>&#xA;"
26506839,26491425,1348596,2014-10-22T11:52:01,"<p>I think is because it gives you a great deal of flexibility when the time to scale up your app comes. If you use tomcat you'll have to copy your .war and drop it inside another tomcat and then load balance your requests to either of them.</p>&#xA;&#xA;<p>Instead If your app has a self contained http server, you colud just run another instance in another port and forget about all that tomcat stuff. You would still have to load balance your requests to either of your app instances, but seems more straight forward.</p>&#xA;"
49320013,49260669,3719412,2018-03-16T11:44:58,<p>You can use whatever you want and in your particular case I am pretty sure you won't see any drawbacks. From my point of view only difference with rest is mostly semantic -some people may also argue about cacheability but I don't think so- </p>&#xA;&#xA;<p>Apart from rest/rpc creating microservices without any actual domain could cause a maintenance issue in the long run as you totally depend on some other microservices data whenever a change required in their side you may also need change this microservice. That is why I don't recommend those kind of calculation services unless you have a valid scalability requirement.      </p>&#xA;
49320250,49198914,3719412,2018-03-16T11:58:23,"<p>It sounds like what you need is a discovery service like eureka. &#xA;What it does basically discovering/providing services as you need in your case. You can check out the details from <a href=""https://spring.io/guides/gs/service-registration-and-discovery/"" rel=""nofollow noreferrer"">spring-eureka</a></p>&#xA;"
32933013,32778803,3719412,2015-10-04T11:36:11,"<p>Yeah I think it can be counted as microservice architecture in a way. But there will be just one microservice that is authentication service that will be used by other applications. You can search ""single sign on"" to implement it. There is <a href=""http://jasig.github.io/cas/4.1.x/index.html"" rel=""nofollow"">CAS</a> for that in java world. </p>&#xA;"
32933621,32470907,3719412,2015-10-04T12:46:07,"<pre><code>1.Will this not be a huge maintenance issue in terms of backups, restores etc?&#xA;</code></pre>&#xA;&#xA;<p>From your view yes it will. I mean at the end of day you will not have just one database server to backup but tens or hundreds of them. But mostly people -at least that is what we do - is using a cloud database service to get rid of all these maintenance effort. </p>&#xA;&#xA;<pre><code>2.How is the initial data populated into these stores ? Are there any best practices around this ? Organisations are bound to have huge volumes of customer or product data &amp; they will most likely be mastered in other systems.&#xA;</code></pre>&#xA;&#xA;<p>I am not sure if there is a best way but we created a client to read the data from legacy system then convert and split it into the parts for each microservices and push them to those microservices by consuming their services. We used message queues to to be sure about health of migration.</p>&#xA;&#xA;<pre><code>3.How does this approach of multiple data stores impact the 'omni-channel' approach where it implies getting a single view of all data? Organizations might have had data consolidation initiatives going on to achieve the same.&#xA;</code></pre>&#xA;&#xA;<p>Well I don't know what ""omni-channel"" is so I can't answer that.</p>&#xA;&#xA;<p>Lastly you were mentioning about logical entities shared between services. The real hardest part about implementing microservices is defining what each service will provide. And while doing that you should carefully examine data needs for each services and those services should share as little as possible like only entity ids etc. At least that is what we are doing.</p>&#xA;"
33933246,33926707,3719412,2015-11-26T07:58:54,<p>First of all IMHO you should NOT share those domain objects between your microservices by a jar. Actually you should not share your domain objects in any way. It can make maintainability and deployment process a nightmare for you. Instead you can have kind of data transfer objects(DTO) in your microservices. </p>&#xA;&#xA;<p>The best portable and healthy way for communication could be using some interoperable serialization mechanism like using json or xml.</p>&#xA;&#xA;<p>And lastly microservices can be difficult to manage for the long run if you don't really need it. So using it just because it sounds cleaner could be not a good idea at all. Of course if you have a complicated and big scale project then  you can go for it.</p>&#xA;
33853689,33805449,3719412,2015-11-22T10:16:55,<p>There are two completely separate layer in your case.&#xA;One for application servers and another one for database.&#xA;If you really need a scalable system -I think you need because you are mentioning about load balancing- then you should remove all the states out from you application . &#xA;For example you should not use layer 2 caching in your application instance instead you should use some external service like redis or memcache. </p>&#xA;&#xA;<p>And you should use just one master database instance for writes and another replicate waiting for failover. To do that we are using Amazon RDS MultiAZ instances. There is just one master database which is replicated to another instance. In case of crash or something the second database is automatically set as master in a couple of seconds.  </p>&#xA;
33853757,33798965,3719412,2015-11-22T10:26:56,"<p>We are using a redundant read-only data on passive side and message queues for communication for those kind of scenarios. I mean having a user state table in your other service side and update it as the messages comes from your user service. It won't effect availability,performance and scalability of your OtherService. Dependency will be almost zero. The only possible problematic side of that approach could be the freshness of your user state data. By freshness problem I am talking about milliseconds generally. If that freshness problem is that important in your case then your best bet could be combining two services.</p>&#xA;"
49602824,49600833,3719412,2018-04-01T21:32:33,<p>The performance will be depending on so many other factors before/with the number of rest calls. It is really hard to say anything beforehand. </p>&#xA;&#xA;<p>But according to my previous experience if you need to provide such a search functionality for a autocomplete feature you may most likely need elastic search for that. </p>&#xA;&#xA;<p>That means instead of sending a request to each and every service for each autocomplete request you should have an index to search in (partial or maybe all data you need to show in your frontend) and search that index first to find the corresponding items then ask for the remaining/full data to other services if necessary. Details are totally depends on your requirements.</p>&#xA;&#xA;<p>Of course if you go that way you should also implement a data population/syncronization mechanism to elastic search. </p>&#xA;
49355304,49273441,2353652,2018-03-19T03:16:38,"<p>Why we need to cache the HTTP headers?&#xA;Normally, only GET responses are valuable to be cached on the client.</p>&#xA;"
34132879,34094882,205557,2015-12-07T11:49:52,"<p>I've had similar problem with workflow (with exception that I didn't use forks). Ultimately my requirements came down to 1 thing:</p>&#xA;&#xA;<ul>&#xA;<li>As a developer, I want to run 1 command <code>project bootstrap</code> to bootstrap my environment</li>&#xA;</ul>&#xA;&#xA;<hr>&#xA;&#xA;<p>I would suggest to do few things to optimize the workflow, which worked incredibly well for me:</p>&#xA;&#xA;<ul>&#xA;<li><p>store list of services in JSON file, where each service has ""url"", ""fork_url"", ""name"" and other common attributes required for docker-compose to understand how to handle this service. It seems like every team member on your team will have either fork url or base repo url</p></li>&#xA;<li><p>build a command-line app (there are a number of options available - depends on your language of choice, for Ruby it's gem Thor, for Go it's package Cobra, etc). Usually it takes just a few hours to build a simple command-line app structure and first couple commands, but this type of automation will save you hours on a day-by-day basis and you will have a foundation to extend your command-line app as your needs increase and the concept is proven to be viable and useful. You also will have a unified interface to provision your environment across all of team members, which then becomes responsibility of a team to maintain it.</p></li>&#xA;<li>build a <code>project bootstrap</code> command to provision your environment. For example, it will:&#xA;&#xA;<ul>&#xA;<li>go over list of services from your JSON file:&#xA;&#xA;<ul>&#xA;<li>do a fork if not exists</li>&#xA;<li>clone your repo</li>&#xA;<li>add another remote to your repo, which will represent fork (apart from origin)</li>&#xA;<li>will generate <code>docker-compose.yml</code> from template in a specified directory</li>&#xA;</ul></li>&#xA;</ul></li>&#xA;</ul>&#xA;"
30460582,30449278,205557,2015-05-26T13:53:17,"<p>After working for more than a year developing and managing releases of platform built of microservices I figured repeatable process that can be automated. More on this below.</p>&#xA;&#xA;<p>Let's split release process into <strong>3 phases</strong>:</p>&#xA;&#xA;<ol>&#xA;<li>understanding what should go out in release</li>&#xA;<li>preparing changes</li>&#xA;<li>pushing them in the wild</li>&#xA;</ol>&#xA;&#xA;<p>We're using Git and <a href=""http://nvie.com/posts/a-successful-git-branching-model/"" rel=""nofollow"" title=""A successful Git branching model"">A successful Git branching model</a>, which is rather questionable, I prefer FeatureBranch workflow, but that's a different story.</p>&#xA;&#xA;<p><strong>First phase</strong>: Understanding of what should go out</p>&#xA;&#xA;<p>In our issue tracking tool, stories that should go out are marked as ""Ready for Merge"" (we use jira.com but it doesn't matter). I grab list of stories, run simple script that look like this <code>mia review --cards=MIA-1022 MIA-988 MIA-1097 MIA-688</code>. The output is a list of services affected by these stories, so that you don't need to go and manually review every story to see services affected, example output looks like this:</p>&#xA;&#xA;<pre><code>[+] [2/16] user-service: MIA-1198, MIA-2023&#xA;[+] [6/16] checkout-service:  MIA-1097 MIA-688&#xA;[+] [7/16] inventory-service: MIA-1022 MIA-988, MIA-1198, MIA-2023&#xA;</code></pre>&#xA;&#xA;<p><strong>Second phase</strong>: Preparing changes</p>&#xA;&#xA;<p>Semi-manual process for me, because in some cases the ""in-progress"" stories from <strong>develop</strong> branch need to be ignored and can't go to master. But in most cases I can merge <strong>develop</strong> straight to <strong>master</strong>, and when I can, I have another command: <code>mia merge --services=user checkout inventory</code>. This command goes over specified services and <strong>creates Pull Requests</strong> to merge <strong>develop</strong> branch to <strong>master</strong> and prints out links to pull requests.</p>&#xA;&#xA;<p><strong>Third phase</strong>: pushing changes in the wild</p>&#xA;&#xA;<p>To push something to staging environment and then to production, service has to have a <strong>version</strong>. Empirically we figured that if you do semver for services, and moreover if you do it only for services that have changes it'll be hard to understand the ""latest"". Because what if pace of development of checkout service significantly higher then inventory service, you end up with something like v3.3.6 in checkout and v1.2.0 in inventory. </p>&#xA;&#xA;<p>So to <strong>solve this</strong>: we're tagging all services with same tag version composed of the year, month, day and rc version. Example: <strong>r2015052601</strong>, and then we also have a command <code>mia diff r2015052401 r2015052601</code>, which searches for specified tag in every service and prints a diff of changes between 2 tags. Part of me thinks that tagging all services with same version violates one of the principles of microservices architecture, but for us right now it solves major pain point of tags compatibility and understanding what's latest, because you can assume that latest tag exists everywhere, and if there were no changes, then there were no changes.</p>&#xA;&#xA;<p>Thanks</p>&#xA;"
31512119,31510697,205557,2015-07-20T08:42:12,"<p>In my opinion, In the question you asked, there's a hint to your answer ""(traffic goes through load balancer <strong>from now on</strong>)"".</p>&#xA;&#xA;<p>I would say - <strong>traffic should always go thru load-balancer</strong>.</p>&#xA;&#xA;<p>In your simplest case when you have 1 instance of each service, it still has to go thru load-balancer (btw, I think it's a good idea to have at least 2 of everything).</p>&#xA;&#xA;<p>In that case, when you get 3x more traffic and want to spin up another container of the same service, once container is up and running it must register itself in service discovery tool and automatically update load-balancer config to add new 'upstream' entry.</p>&#xA;&#xA;<p>And then using this approach you will be able to scale up/down your services more easily.</p>&#xA;"
48479385,48479325,6569205,2018-01-27T18:56:01,"<p>I added <strong>Access-Control-Allow-Origin</strong> headers in lamda function (nodejs).</p>&#xA;&#xA;<p>It works for me</p>&#xA;&#xA;<pre><code>const done = (err, res) =&gt; callback(null, {&#xA;    statusCode: err ? '400' : '200',&#xA;    body: err ? err.message : JSON.stringify(res),&#xA;    headers: {&#xA;        'Content-Type': 'application/json', &#xA;        'Access-Control-Allow-Origin': '*'&#xA;    }    &#xA;});&#xA;</code></pre>&#xA;"
45655861,45625886,1202421,2017-08-12T23:42:52,"<p>When done correctly, REST improves long-term evolvability and scalability at the cost of performance and added complexity. REST is ideal for services that must be developed and maintained independently, like the Web itself. Client and server can be loosely coupled and change without breaking each other.</p>&#xA;&#xA;<p>RPC services can be simpler and perform better, at the cost of flexibility and independence. RPC services are ideal for circumstances where client and server are tightly coupled and follow the same development cycle.</p>&#xA;&#xA;<p>However, most so-called REST services don't really follow REST at all, because REST became just a buzzword for any kind of HTTP API. In fact, most REST APIs are so tightly coupled that they offer no advantage over an RPC design. </p>&#xA;&#xA;<p>Given that, my somewhat cynical answers to your question are:</p>&#xA;&#xA;<ol>&#xA;<li><p>Some people are adopting gRPC for the same reason they adopted REST a few years ago: design-by-buzzword.</p></li>&#xA;<li><p>Many people are realizing the way they implement REST amounts to RPC anyway, so why not go with an standardized RPC framework and implement it correctly, instead of insisting on poor REST implementations?</p></li>&#xA;<li><p>REST is a solution for problems that appear in projects that span several organizations and have long-term goals. Maybe people are realizing they don't really need REST and looking for better options.</p></li>&#xA;</ol>&#xA;"
29305684,29303048,1202421,2015-03-27T16:34:07,"<blockquote>&#xA;  <p>But in the microservice world, a service should have no knowledge of&#xA;  other services. AFAIK.</p>&#xA;</blockquote>&#xA;&#xA;<p>I think this is the root of your confusion. My understanding is that a service should not rely on <strong>out-of-band information</strong> to communicate with other services, for the purpose of software development. This means a service should not know anything about the internals of its peers, but it doesn't make any sense to say it should have no knowledge of other services. This does not conflict with HATEOAS, in fact, they complement each other.</p>&#xA;&#xA;<p>There's no problem with linking to other services. How else would you build a macroservice from microservices? There's a problem with relying on out-of-band information for that.</p>&#xA;"
47673579,47656406,6053621,2017-12-06T11:48:48,"<p>I would say that your calculations can fit well either in Value Objects or Domain Services.</p>&#xA;&#xA;<p>How to differentiate? Well, I understand Domain Services as services (well, obvious) with business logic (such as your calculations) that require some kind of external dependency you need to inject in order to get your logic work.</p>&#xA;&#xA;<p>On the other hand, if you can name that business logic as a business concept (i.e. <code>CustomerFee,</code> <code>CustomerCommission,</code> etc) and you don't need any injected dependency to make it work I would say it's a Value Object.</p>&#xA;&#xA;<p>For instance, imagine that you want to calculate the price of a reservation which depends on the fee you will charge to the customer (among other params):</p>&#xA;&#xA;<pre><code>ReservationPrice(CustomerFee customerFee, ItemPrice ItemPrice)&#xA;</code></pre>&#xA;&#xA;<p>Now your <code>CustomerFee</code> is also calculated based on (say any variable) something.</p>&#xA;&#xA;<p>This way you are modeling your calculations just with Value Objects which allows you to show in your code all the different business concepts they depend on. Also anyone looking at your code and files structure can get an idea about what you are calculating. </p>&#xA;"
37624088,37615250,6053621,2016-06-03T22:01:48,"<p>I see some mistakes according to DDD principles in your question. Let me try to clarify some concepts to give you hand.</p>&#xA;&#xA;<p>First, you mentioned you have an Aggregate Root which is Invoice, and then two different repositories. Having an Aggregate Root means that any change on the Entities that the Aggregate consists of should be performed via the Aggregate Root. Why? That's because you need to satisfy some business rule (invariant) that applies on the relation of those Entities. For instance, given the next business rule:</p>&#xA;&#xA;<blockquote>&#xA;  <p>Winning auction bids must always be placed before the auction ends. If a winning bid is placed after an auction ends, the domain is in an invalid state because an invariant has been broken and the model has failed to correctly apply domain rules.</p>&#xA;</blockquote>&#xA;&#xA;<p>Here there is an aggregate consisting of Auction and Bids where the Auction is the Aggregate Root.</p>&#xA;&#xA;<p>If you have a <code>BidsRepository</code>, you could easily do:</p>&#xA;&#xA;<pre><code>var newBid = new Bid(money);&#xA;BidsRepository-&gt;save(newBid);&#xA;</code></pre>&#xA;&#xA;<p>And you were saving a Bid without passing the defined business rule. However, having the repository just for the Aggregate Root you are enforcing your design because you need to do something like:</p>&#xA;&#xA;<pre><code>var newBid = new Bid(money);&#xA;auction.placeBid(newBid);&#xA;auctionRepository.save(auction);&#xA;</code></pre>&#xA;&#xA;<p>Therefore, you can check your invariant within the method <code>placeBid</code> and nobody can skip it if they want to place a new Bid. Afterwards you can save the info into as many tables as you want, that is an implementation detail.</p>&#xA;&#xA;<p>Second, you said if it's wrong injecting the repository into a Domain class. Here a quick explanation:</p>&#xA;&#xA;<blockquote>&#xA;  <p>The repository should depend on the object it returns, not the other way around. The reason for this is that your ""domain object"" (more on that later) can exist (and should be testable) without being loaded or saved (that is, having a dependency on a repository).</p>&#xA;</blockquote>&#xA;&#xA;<p>Basically your design says that in order to have an invoice, you need to provide a MySQL/Mongo/XXX instance connection which is an infrastructure detail. Your domain should not know anything about how it is persisted. Your domain knows about the behavior like in the scenario of the Auction and Bids.</p>&#xA;&#xA;<p>These concepts just help you to create code easier to maintain as well as help you to apply best practices such as SRP (Single Responsibility Principle).</p>&#xA;"
37684265,37684053,3108853,2016-06-07T16:07:50,"<p><code>localhost</code> on a docker instance will refer to the ports exposed by that instance.  If you want to access ports from another instance in the same <code>docker-compose.yml</code> file, use <a href=""https://docs.docker.com/compose/compose-file/#links"" rel=""nofollow"">links</a>, which take the form of <code>service-name:alias</code>.  (If you exclude the <code>alias</code>, it is the same as the <code>source-name</code>.)</p>&#xA;&#xA;<p>So in your example above, the config for mailcatcher <code>config/environments/development.rb</code> should be:</p>&#xA;&#xA;<pre><code>config.action_mailer.delivery_method = :smtp&#xA;config.action_mailer.smtp_settings = { address: ""mailcatcher"", port: 1025 }&#xA;</code></pre>&#xA;"
32247681,29830038,5210357,2015-08-27T11:03:43,"<p>If your logs get bigger (like in the terabytes level) you will need a big data solution. I would suggest <a href=""http://hadoop.apache.org/"" rel=""nofollow noreferrer"">Apache Hadoop</a> with <a href=""https://flume.apache.org/"" rel=""nofollow noreferrer"">Apache Flume</a> and <a href=""https://www.elastic.co/products/elasticsearch"" rel=""nofollow noreferrer"">Elasticsearch</a>. Periodically collect the newest log data and pump it into Elasticsearch, via a Flume agent feed into HDFS or <a href=""https://hbase.apache.org/"" rel=""nofollow noreferrer"">HBase</a> with log data.</p>&#xA;"
32269212,28607400,5210357,2015-08-28T10:45:10,<p>if u are using jvoid which is a project of schgoni (magento's owner) it create session id and stores it inside mysql and it has already builtin spring security module</p>&#xA;&#xA;<p>For microservice authentication i oauth2 based  security architecture would be better i think.Using oauth tokens at rest calls would solve auth problem</p>&#xA;
32269315,27865814,5210357,2015-08-28T10:50:14,<p>As to the microservice architecture there is SRP principle.Single Responsiplity principle.Each service has own unique responsiplity.DB schemea shouldbe decomposed also.Exporting services as rest inside a monolithic app not convert a monolithic app to micro service application.</p>&#xA;
32247844,25600580,5210357,2015-08-27T11:12:11,<p>As to the ISP principle (Interface segration principle) clients should depend on interface not implementations.I would suggest if it is possible sharing interfaces not implementations via this way it would be better to make system decoupled from implementation.</p>&#xA;
50891630,50889657,8043253,2018-06-16T20:48:17,<p>Ideally in user creation and authentication realtime response is given to the client side but if it involves complex process or tasks post user creation queue should be preferred.</p>&#xA;&#xA;<p>For multiple microservices synchronous interaction and to work on their API responses you can build a aggregator service which could serve as a communication medium between different services and work alongside your kafka queue consumer service.</p>&#xA;
44466352,44458709,3652853,2017-06-09T20:39:10,"<p>As far as I can tell, you have a mixed of net versions on your class path...</p>&#xA;&#xA;<ul>&#xA;<li>netty-common-4.0.27.Final</li>&#xA;<li>netty-transport-4.0.37.Final</li>&#xA;</ul>&#xA;&#xA;<p>This can be the cause of your problem.</p>&#xA;"
51116768,51090375,2801909,2018-06-30T16:48:13,<p>Please don't simply share the entire log file before analyzing it yourself. </p>&#xA;&#xA;<p>Its so apparent in the log file that you're providing the incorrect client credentials.</p>&#xA;&#xA;<p>Client user <code>fooClientIdPassword</code> is not present in the database. Here's an excerpt from the log you shared:</p>&#xA;&#xA;<pre><code>Basic Authentication Authorization header found for user 'fooClientIdPassword'&#xA;Authentication attempt using org.springframework.security.authentication.dao.DaoAuthenticationProvider&#xA;User 'fooClientIdPassword' not found&#xA;Authentication request for failed: org.springframework.security.authentication.BadCredentialsException: Bad credentials&#xA;</code></pre>&#xA;
44786713,44785105,6079221,2017-06-27T17:46:00,"<p>According to this GitHub <a href=""https://github.com/swagger-api/swagger-core/issues/1559#issuecomment-261943276"" rel=""nofollow noreferrer"">issue</a> on Swagger core project, if you add the annotation <code>@ApiImplicitParam</code> should resolve your problem.</p>&#xA;&#xA;<pre><code>@ApiImplicitParams({&#xA;    @ApiImplicitParam(&#xA;        required = true,&#xA;        dataType = ""com.example.SomeObjectDto"",&#xA;        paramType = ""body""&#xA;    )&#xA;})&#xA;</code></pre>&#xA;&#xA;<p>But normally if you just add the class on your method signature it'll work.</p>&#xA;&#xA;<pre><code>private Object retrieveByName(SomeObjectDto someObjectDto) {&#xA;    someCode();&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Also SomeObjectDto class should contain ""get"" methods for your variables like.</p>&#xA;&#xA;<pre><code>class SomeObjectDto {&#xA;    private String info;&#xA;&#xA;    getInfo(){&#xA;        return info;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Will produce the following JSon.</p>&#xA;&#xA;<blockquote>&#xA;  <p>{ info: ""string"" }</p>&#xA;</blockquote>&#xA;"
38629978,38629740,1170677,2016-07-28T07:40:04,"<p>This is a bit wide question to be asked, but here is a solution from my experience. &#xA;If you have your own user/pass db then apply spring security, include spring security in every micro service, spring security in each one will query the DB that you have for user/pass and the session, when user authenticates. This way all services will be secured.  </p>&#xA;&#xA;<p>Oauth2 is another solution </p>&#xA;&#xA;<p>Check this links too: &#xA;<a href=""https://spring.io/blog/2015/10/19/springone2gx-2015-replay-securing-microservices-with-spring-cloud-security"" rel=""nofollow"">https://spring.io/blog/2015/10/19/springone2gx-2015-replay-securing-microservices-with-spring-cloud-security</a> </p>&#xA;&#xA;<p><a href=""https://spring.io/blog/2014/11/07/webinar-replay-security-for-microservices-with-spring-and-oauth2"" rel=""nofollow"">https://spring.io/blog/2014/11/07/webinar-replay-security-for-microservices-with-spring-and-oauth2</a></p>&#xA;"
45527025,45526675,8380917,2017-08-05T22:51:15,"<p>The difference is that with sync client, step A thread will be doing a connection to step 2 endpoint and wait for a response. Making step 2 implementation async will and just return 200 directly (or whatever) will help on decreasing waiting time; but it will still be doing the connection and waiting for response. </p>&#xA;&#xA;<p>With non-blocking client instead, the step A call itself will be done by another thread. So everything is untied from step A thread. Also, system can make use of that thread for other stuff until it gets a response from step B and needs to resume work.</p>&#xA;&#xA;<p>The idea is that your origin threads will not be idle so much time waiting for responses, but instead being reused to do other work while in between.</p>&#xA;"
51390353,51386591,256245,2018-07-17T21:20:59,"<p>When I’m establishing a contract for a POST, I care about the server accepting a valid input and the way the server responds (usually 400) on bad input. There’s no need to have a contract for all the possible ways an input might be invalid, mainly because this can happen for an uncountable number of reasons.</p>&#xA;&#xA;<p>So I, from a consumer perspective, usually have only one contract for invalid inputs, unless I’m facing a very particular situation where the server might respond differently depending on the reason of the failure, and I care about the different responses.</p>&#xA;&#xA;<p>For your specific situation, write a contract for just one of the three mentioned scenarios.</p>&#xA;"
46825057,46824730,3739382,2017-10-19T07:51:18,"<p>I don't recommend you to have a simgle project with multiple mocroservices of different technologuies.</p>&#xA;&#xA;<p>If you use Java as a lenguaje you can have a master project and use it as a parent in your microservices projects. Also, you can have common libreries as dependencies of your microservices projects.</p>&#xA;&#xA;<p>If you want to do a microservices with different technology I recommend you to have a repository for each microservice.</p>&#xA;&#xA;<p>Choosing this option you can deploy and versioning each microservice when you do changues in its code and not when you have changues in another microservice.</p>&#xA;"
50745159,50725904,1517814,2018-06-07T15:34:25,<p>You need a service for your lucky-server :</p>&#xA;&#xA;<pre><code>kind: Service &#xA;apiVersion: v1 &#xA;metadata: &#xA;  name: lucky-server&#xA;spec: &#xA;  selector: &#xA;    app: lucky-server &#xA;  ports: &#xA;  - protocol: TCP &#xA;    targetPort: 8888&#xA;    port: 80 &#xA;  type: NodePort&#xA;</code></pre>&#xA;
45598961,45598647,4081910,2017-08-09T19:17:36,<p>You can create microservice to log in database which will be hosted somewhere accessible to your application. Currently I have created console application with unity container and NancyFx. You may like to explore further per your requirements. Can you please share any details on technical issues apart from generic problems.</p>&#xA;
48516468,48515460,8052214,2018-01-30T07:53:39,"<p>According to link below, it is not a good idea to use database container in Production. &#xA;But as I have experienced; if you isolate your container from your app and update your container regularly and also manage networking stuff, there seems to be no problem.</p>&#xA;&#xA;<p>Link: <a href=""https://www.quora.com/Is-it-not-advisable-to-use-database-in-Docker-container"" rel=""nofollow noreferrer"">https://www.quora.com/Is-it-not-advisable-to-use-database-in-Docker-container</a></p>&#xA;"
52044016,30496218,2556407,2018-08-27T17:09:54,"<p>You can add <code>-r</code> parameter to ab to prevent it exitting.</p>&#xA;&#xA;<p>from <a href=""https://httpd.apache.org/docs/2.4/programs/ab.html"" rel=""nofollow noreferrer"">https://httpd.apache.org/docs/2.4/programs/ab.html</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>-r</p>&#xA;&#xA;<pre><code>Don't exit on socket receive errors.&#xA;</code></pre>&#xA;</blockquote>&#xA;&#xA;<p>Connections will start to timeout and can prolong testing time by a timeout period.</p>&#xA;"
45365472,45363163,921335,2017-07-28T05:27:27,"<p>Let's suppose you have 20 services to which user can interact to, and of course we are not going to expose each and every services publicly because that will be madness (because all services will have different ports and context), so the best approach will be to use an API gateway which will act as single entry point access to our application (developed in micro service pattern) and that is where Zuul comes into picture. Zuul act as a reverse proxy to all your micro-services running behind it and is capable of following </p>&#xA;&#xA;<ul>&#xA;<li><p>Authentication</p></li>&#xA;<li><p>Dynamic Routing</p></li>&#xA;<li><p>Service Migration</p></li>&#xA;<li><p>Load Shedding</p></li>&#xA;<li><p>Security</p></li>&#xA;<li><p>Static Response handling</p></li>&#xA;<li><p>Active/Active traffic management</p></li>&#xA;</ul>&#xA;&#xA;<p>You can go through documentation <a href=""http://cloud.spring.io/spring-cloud-netflix/spring-cloud-netflix.html#_router_and_filter_zuul"" rel=""nofollow noreferrer"">here</a> </p>&#xA;"
44460055,44456131,1538039,2017-06-09T14:13:59,"<p>Service B is indicating that it's data can be cached anywhere, such as on any downstream proxy servers if you are fronting the request through a CDN (edge-caching). Subsequent requests to Service B will then be fulfilled either by the edge cache, and won't execute  code at origin to retrieve data for the next request. </p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/Hona2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Hona2.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>This should be considered separately from the concerns of Service A, which may want to cache the response data for a period of time before making another request to Service B. </p>&#xA;&#xA;<p>The things you want to minimise are the cost of inter process communication (HTTP Call, Service A -> Service B) and the serialization/deserilzation work that requires.</p>&#xA;&#xA;<p>A common pattern here is a read-through cache, where Service A will check for a cached response from an operation before deciding to make a call back to Service B  (which <em>could</em> still respond with edge-cached data)</p>&#xA;&#xA;<p><a href=""https://i.stack.imgur.com/g43xn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g43xn.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>Service A should decide on it's caching strategy, how long it will cache response in either an instance based MemoryCache or shared cache (e.g. Redis). Service B is probably returning data about some useful object as it's content, serialized in some form - 'A' should deserailize and cache this response.</p>&#xA;&#xA;<p>I would keep the caching requirements for these services seperate, don't use the max-age value to decide how long to cache the response for in Service A. This would introduce a coupling between the services - if the designer of Service B changed the max-age value then this would impact how Service A behaves. </p>&#xA;&#xA;<p>Instead, have a configuration value for Service A that defines how long it is OK to cache responses from Service B, use this to design your read through cache implementation. Store the deserialised responses from Service B in the cache for that period of time</p>&#xA;&#xA;<p>NB - I referenced images from <a href=""http://jakeydocs.readthedocs.io/en/latest/performance/caching/response.html"" rel=""nofollow noreferrer"">http://jakeydocs.readthedocs.io/en/latest/performance/caching/response.html</a> and <a href=""http://blog.ragozin.info/2011/10/grid-pattern-proactive-caching.html"" rel=""nofollow noreferrer"">http://blog.ragozin.info/2011/10/grid-pattern-proactive-caching.html</a>. </p>&#xA;"
42145137,42142284,4953079,2017-02-09T19:20:26,"<p><strong>Short answer:</strong> you cannot query the broker but you could exploit Kafka's Streams API and ""Interactive Queries"".</p>&#xA;&#xA;<p><strong>Long answer:</strong> The access pattern for reading Kafka topics, are linear scans and not random lookups. Of course, you can also reposition at any time via <code>#seek()</code>, but only by <em>offset</em> or <em>time</em>. Also topics are sharded into partitions and data is (by default) hash partitioned by key (data model is key-value pairs). So there is a notion of a key.</p>&#xA;&#xA;<p>However, you can use Kafka's <a href=""http://kafka.apache.org/documentation/streams"" rel=""nofollow noreferrer"">Streams API</a> that allows you to build an app that hold the current state -- base on a Kafka topics that is the ground truth -- as a materialized view (basically a cache). <a href=""http://docs.confluent.io/current/streams/developer-guide.html#interactive-queries"" rel=""nofollow noreferrer"">""Interactive Queries""</a> allows you to query this materialized view.</p>&#xA;&#xA;<p>For more details, see this two blog post:</p>&#xA;&#xA;<ul>&#xA;<li><a href=""https://www.confluent.io/blog/unifying-stream-processing-and-interactive-queries-in-apache-kafka/"" rel=""nofollow noreferrer"">https://www.confluent.io/blog/unifying-stream-processing-and-interactive-queries-in-apache-kafka/</a></li>&#xA;<li><a href=""https://www.confluent.io/blog/data-dichotomy-rethinking-the-way-we-treat-data-and-services/"" rel=""nofollow noreferrer"">https://www.confluent.io/blog/data-dichotomy-rethinking-the-way-we-treat-data-and-services/</a></li>&#xA;</ul>&#xA;"
51593776,51593335,1087479,2018-07-30T12:16:09,"<p>My rule of thumb:</p>&#xA;&#xA;<p>A microservice is only allowed to return Domain objects he manages. </p>&#xA;&#xA;<p>So, a PageService would only return Pages, never Books. Those Books are managed by the BookService, that itself never returns Pages.&#xA;The PageService is allowed to reference Books by linking to a Book in the BookService in an arbitrary Page.&#xA;This way you can achieve that the BookService just needs to care about books and if those Books are linked or embedded in a Page this is not its concern, because that is what the PageService is for.</p>&#xA;&#xA;<p>Also managing means that only an instance of BookService is allowed to edit or modify a Book. NOT EVEN A SIMPLE DB-SCRIPT. </p>&#xA;&#xA;<p>Of course this is just a rule of thumb, there is lots of fun in the details, but it should give you a picture.</p>&#xA;"
51685386,33399988,1087479,2018-08-04T12:04:43,"<p>My approach would be a service local DB (read: datasource per instance). In memory or in the same Pod. To synchronize the at startup always fresh DB I would use Apache Kafka. As soon as the service begins initializing, it queries Kafka for all entries it is interested in (mind the <em>compact log</em>-Feature of Kafka, that only returns the recent state of an entity) populates its DB and begins serving requests.</p>&#xA;&#xA;<p>This would increase startup time of course, but the benefit is, that the DB can be of any tech or scheme the service wants it to be (this could even change from version to version of the service). Also there is no need of an DB-Cluster, but you would need a properly configured Kafka-Service, but that could also be used for Event Sourcing among your services.</p>&#xA;"
51687833,51541318,1087479,2018-08-04T17:06:10,"<p>My preferred way would look like this:</p>&#xA;&#xA;<p>First, as already mentioned by others it is arguable if an <code>AuthorService</code> is needed, or if it is too close on the <code>Book</code> and therefore could be part of the <code>BookService</code>.</p>&#xA;&#xA;<p>So, for this example I name it <code>PersonService</code>, because this way your future <code>ConferenceService</code> could use it.</p>&#xA;&#xA;<p>By following the Microservice approach, you have to be aware of two goals you want to achieve:</p>&#xA;&#xA;<ul>&#xA;<li>Loose coupling</li>&#xA;<li>Cohesive behavior</li>&#xA;</ul>&#xA;&#xA;<p>this implies that those two services are <em>not</em> allowed to share a Database (it could be the same physically, but not logically), so that the <code>BookService</code> cannot modify the data of the <code>PersonService</code>. This has to be done by the <code>PersonService</code> exclusively. Otherwise Cohesive behavior would be put at risk.&#xA;So we have two services with a separate database each.</p>&#xA;&#xA;<p>I assume a RESTful API for these two services, what is great because we want to couple loosely. A JSON could look like this:</p>&#xA;&#xA;<pre><code>[{&#xA;    book {&#xA;        ""name"": ""Book 1"",&#xA;        ""author"": {&#xA;            ""name"": ""Author Name 1""&#xA;            ""href"": ""apis.yourdomain.com/personservice/persons/1""&#xA;            ""type"": ""application/json""&#xA;        }&#xA;&#xA;    }&#xA;},&#xA;{&#xA;    book {&#xA;        ""name"": ""Book 2"",&#xA;        ""author"": {&#xA;            ""name"": ""Author Name 2""&#xA;            ""href"": ""apis.yourdomain.com/personservice/persons/2""&#xA;            ""type"": ""application/json""&#xA;        }&#xA;    }&#xA;}]&#xA;</code></pre>&#xA;&#xA;<p>As you can see, those Authors are only represented as <code>Link</code> referencing (an entity in) the <code>PersonService</code>. This coupling is loose because a <code>Link</code> is less likely to change as a representation of a <code>Person</code> throughout the lifetime of you application.&#xA;By naming the field <code>author</code> you also get the semantics of the relation between <code>Book</code> and <code>Person</code>.&#xA;Of course frontend applications tend to not want that much requests for rendering a page, this has to be taken into account by expanding those <code>Links</code>. I prefer to have a <code>Model</code>-objekt, that holds only <code>Links</code> and a <code>Representation</code>-object that has the Links of its corresponding model expanded by calling the <code>Link</code>. In cases when you don't want to do time based caching, this has proven very valuable to me.</p>&#xA;&#xA;<p>But things could get complicated very easily. Assume a rule like ""A domain object can only be emitted by the corresponding service"" in order to achieve Cohesive behavior.  This means that only the <code>BookService</code> is allowed to deliver <code>Book</code>s. Now imagine a request like ""all books by Martin Fowler"". Which service should be queried? Following the above rule, it should be the <code>BookService</code>. But how does the <code>BookService</code> know of a thing called Martin Fowler, he just knows <code>Books</code>?&#xA;In such a case there is a thing called <em>non-authoritative cache</em> (which could also be persisted in a DB). This just means that the <code>BookService</code> is allowed to store <code>Person</code>s in a cache or in a database, but is not allowed to share them with others.&#xA;This way the above rule is still followed and the Cohesive behavior still at hand.&#xA;The non-authoritative cache does not need to reflect the full domain object <code>Person</code>, but everything necessary to fulfill the UseCases.</p>&#xA;"
51694225,51454135,1087479,2018-08-05T12:16:39,"<p>Without information it is hard to guess, but I assume that your module <code>pom.xml</code> needs to reference its parent <code>pom.xml</code> like: </p>&#xA;&#xA;<pre><code> &lt;parent&gt;&#xA;    &lt;artifactId&gt;your parent&lt;/artifactId&gt;&#xA;    &lt;groupId&gt;com.example&lt;/groupId&gt;&#xA;    &lt;version&gt;parent version&lt;/version&gt;&#xA;&lt;/parent&gt;&#xA;</code></pre>&#xA;&#xA;<p>Just look it up from the working ones.</p>&#xA;"
51694365,50531553,1087479,2018-08-05T12:35:54,"<p>Let the service download that file, extract the information and publish them via kafka.&#xA;Check beforehand if the information was already processed by querying kafka or a local DB.</p>&#xA;&#xA;<p>You also could publish an <code>DataProcessed</code>-Event that triggers the <code>EmailService</code>, that sends the corresponding E-Mail.</p>&#xA;"
51705094,50310975,1087479,2018-08-06T10:04:11,"<p>For better illustration I assume REST APIs for each Microservice.</p>&#xA;&#xA;<p>As I see it you have a <code>IncidentService</code>, a <code>FireHydrantService</code> and an <code>InspectionService</code> that all rely on the <code>LocationService</code>.</p>&#xA;&#xA;<p>By following the Microservice approach, you have to be aware of two goals you want to achieve:</p>&#xA;&#xA;<ul>&#xA;<li>Loose coupling</li>&#xA;<li>Cohesive behavior</li>&#xA;</ul>&#xA;&#xA;<p>this implies that your services are not allowed to share a Database (it could be the same physically, but not logically), so that a service cannot modify the data of a different service or access its data directly by bypassing the responsible service. This has to be done by the responsible service through its API exclusively. Otherwise Cohesive behavior would be put at risk. </p>&#xA;&#xA;<p>A RESTful API is great because we want to also couple loosely.</p>&#xA;&#xA;<p>A Resource could look like this:</p>&#xA;&#xA;<pre><code>""incident"": {&#xA;    ""time"": ""..."",&#xA;    ""location"" : {&#xA;        ""name"": ""an arbitrary address""&#xA;        ""href"": ""apis.yourdomain.com/locationservice/address/1""&#xA;        ""type"": ""application/json""&#xA;    },&#xA;    ""nearestHydrant"": {&#xA;        ""name"": ""hydrant 42""&#xA;        ""href"": ""apis.yourdomain.com/firehydrantservice/hydrant/42""&#xA;        ""type"": ""application/json""&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>As you can see, those enitites (better its representations) are only represented as Link referencing (an entity in) the responsible service. This coupling is loose because a Link is less likely to change as a representation of a Entity throughout the lifetime of you application. By naming the field you also get the semantics of the relation between <code>FireHydrant</code> and <code>Incident</code>.</p>&#xA;&#xA;<p>But things could get complicated very easily. Assume a rule like ""A domain object can only be emitted by the responsible service"" in order to maintain Cohesive behavior. This means that only the <code>FireHydrantService</code> is allowed to serve <code>FireHydrant</code>. &#xA;Now imagine a request like ""<code>Incident</code> at address 1"". Which service should be queried for the nearest <code>FireHydrant</code>? Following the above rule, it should be the <code>FireHydrantService</code>. But how does the <code>FireHydrantService</code> know, it doesn't know what  <em>nearest</em> means? </p>&#xA;&#xA;<p>In such a case there is a thing called non-authoritative cache (which could also be persisted in a DB). This just means that the <code>LocationService</code> is allowed to store <code>FireHydrant</code> in a cache or in a database, but is not allowed to share them with others. This way the above rule is still followed and the Cohesive behavior still at hand. The non-authoritative cache does not need to reflect the full domain object <code>FireHydrant</code>, but everything necessary to fulfill the UseCases.&#xA;So the <code>IncidentService</code> asks the <code>FireHydrantService</code> for the nearest <code>FireHydrant(s)</code>, what gets delegated to the <code>LocationService</code>, that queries its non-authoritative cache for the nearest location with type <code>FireHydrant</code>, the response is a reference (Link) to the <code>FireHydrantService</code> itself, that checks if the <code>FireHydrant</code> has had its <code>Inspection</code> and therefore is valid according to <em>business rules</em> (if not it would check for the next) and return a reference (Link) to the <code>IncidentService</code>.</p>&#xA;&#xA;<p>My rule of thumb is, that every service can be queried on every domain object, but can only serve the domain object it is responsible for. The ""queried on domain objects"" could live in the forementioned non-authoritative cache. This of course leads to duplication and the interesting part is to keep them in sync. A thing that instantly comes to mind is the use of Event Sourcing. A Service emits an Event ""Hydrant 42 out of order"" and the LocationService would remove Hydrant 42 from its cache.</p>&#xA;"
51705654,39015444,1087479,2018-08-06T10:34:03,"<p>I would go with the first approach, a service local DB. Each instance has its own DB instance. This enables to change the persistence layer between versions of the service.&#xA;Changing the ER model otherwise would lead to conflicts. You would also be able to change to a NoSQL solution with this approach easily.</p>&#xA;&#xA;<p>With the event driven design, I can recommend this book: <a href=""http://www.benstopford.com/2018/04/27/book-designing-event-driven-systems/"" rel=""nofollow noreferrer"">Designing Event Driven Systems</a></p>&#xA;&#xA;<p>As I see it, a service receives an request that leads to an Event. This Event is consumed by the other instances of the service, therefore the request doesn't need to be processed again, but the result has to be copied to the instances state.</p>&#xA;"
46451645,40900818,1087479,2017-09-27T15:21:52,"<p>I stick with <a href=""https://www.amazon.de/Building-Microservices-Sam-Newman/dp/1491950358"" rel=""nofollow noreferrer"">Sam Newman</a> who says in Chapter 4 ""The shared Database"" of his book something like: </p>&#xA;&#xA;<blockquote>&#xA;  <p>Remember when we talked about the core principles behind good microservices? Strong cohesion and loose coupling --with database integration, we lose both things. Database integration makes it very easy for services to share data, but does nothing about sharing behaviour. Our internal representation is exposed over the wire to our consumers, and it can be very difficult to avoid making breaking changes, wich inevitably leads to fear of any changes at all. Avoid at (nearly) all costs.</p>&#xA;</blockquote>&#xA;&#xA;<p>This is the point I make when I curse at Content-Management-Systems.</p>&#xA;&#xA;<p>In my view a microservice is autonomous, what it cannot be if it shares things or consumes shared things. The only exception I make here are Domain-Objects, those represent the shared understanding of the business model and must be used in communication between microservices solely.</p>&#xA;&#xA;<p>It depends on the microservice itself if an ER or AggregationOriented database (divided into document based or graph based) better suits the needs.&#xA;The funny thing is, by being loosley coupled and by being autonomus you are able to do just that!</p>&#xA;&#xA;<p>If an PaymentService shares the behaviour of ""how many payments for Person A""&#xA;He needs to know Person A in order to fullfill this. But Everything he knows about Person A must origin from the PersonService, maybe at runtime (the PaymentService maybe just stores an id) or event based (the PaymentService stores the data it needs up to the Domain-Object user, what gets updated triggered and supplied by the PersonService). The PaymentService itself does not share users itself.</p>&#xA;"
40340939,40205675,1371639,2016-10-31T11:07:22,"<p>I believe the main benefit (which the article wanted to convey - apart from the information disclosure) of using reference token (Opaque tokens) over by-value (JWT) is the ability to control the Access tokens when it is distributed outside the network.</p>&#xA;&#xA;<p>In other words, if we issue Access Tokens as JWTs outside the network then it is hard to revoke the access during emergency  (when a user is deactived /terminated, lost mobile phones, etc ). But reference tokens can be easily revoked since it is a pointer within the AS boundaries.</p>&#xA;&#xA;<p>A more detailed explanation on this is available <a href=""https://leastprivilege.com/2015/11/25/reference-tokens-and-introspection/"" rel=""nofollow"">here</a>.</p>&#xA;"
51431404,39485459,7138807,2018-07-19T20:42:41,"<p>I think it is safe to say that as of now it is not a solved problem. In a Microservice Architecture, each service should be loosely coupled. If when you change a producer, you have to hunt down consumers to change them too, then it indicates that you have a design flaw.&#xA;The routing based versioning presented in the link seems to have much more than a ""few drawbacks"".</p>&#xA;&#xA;<p><strong>Should you version the entire service or just  the api?</strong></p>&#xA;&#xA;<p>If you version the entire service, you will invariably open quite a can of worms. Imagine that you implemented the first version (v1) of a service A. Imagine now that after a while, a new version (v2) had to be implemented because of the business. What happens if a bug is discovered on the v2 of this service? The team should check that the same bug doesnt exist on v1 too. If it does, v1 should be corrected and redeployed too. Even if it is just copy and paste, both versions need to be retested and redeployed. And what if there was refactoring between versions and the way to fix v1 is diferent from the way to fix v2? And what if instead of 2 versions, there are 5 or 10? Someone funnier than me should make a ""That scalated quickly"" meme for this.</p>&#xA;&#xA;<p>Just by scratching the surface of how much headache this option could give, we basically can already decide by versioning just the api. But basically if you version your service on the code level:</p>&#xA;&#xA;<ol>&#xA;<li>The team just has to support one version of the service (bugs are just corrected once)</li>&#xA;<li>Less cognitive drain on the team for not having to understand tons of different and potentially very distinct versions of the service</li>&#xA;<li>Less resource consumption</li>&#xA;<li>Garantee that everything is consistent between versions (if not a garantee, at least it is much more probable)</li>&#xA;</ol>&#xA;&#xA;<p><strong>How the client communicate what version of the api they will consume?</strong></p>&#xA;&#xA;<p>For this answer, I will refer only to REST apis because its what i know enough to talk about. There are basically 4 ways of versioning the api (that i can think of):</p>&#xA;&#xA;<ol>&#xA;<li>Versioning the URI: ""<a href=""http://host/serviceA/v3/whatever"" rel=""nofollow noreferrer"">http://host/serviceA/v3/whatever</a>""</li>&#xA;<li>Version in the Accept Header</li>&#xA;<li>Version in Custom Header</li>&#xA;<li>Version as parameter</li>&#xA;</ol>&#xA;&#xA;<p>This <a href=""http://www.springboottutorial.com/spring-boot-versioning-for-rest-services"" rel=""nofollow noreferrer"">tutorial</a> will explain each one of these with their drawbacks better than me, but basically anyone of these should do just fine =)</p>&#xA;&#xA;<p><strong>How (the f*****g hell) do I sustain inumerous service versions in the same code? (are you crazy???)</strong></p>&#xA;&#xA;<p>Your service per se is just one. What you should do is apply Adapter Design pattern, so as to exist different versions of ins and outs interacting with the business layer of your service. This way you just have some versions of some objects, with it being transparent to the core of the service. It is the same conclusion of the <a href=""http://niels.nu/blog/2016/microservice-versioning.html"" rel=""nofollow noreferrer"">article</a> mentioned in the question, and it even has a neat example to showcase it.</p>&#xA;"
41714650,41636566,7415032,2017-01-18T08:35:58,"<p>Thank you for this link,</p>&#xA;&#xA;<p>My problems turn the architecture I have into a new one. For people who has the same trouble I have, there are the things I made thanks to this link : </p>&#xA;&#xA;<ul>&#xA;<li><p>Bring the Register Service and the User Service together. Why ? Because it all depends on the user information ( same database requirements ) That's the solution IlliakaillI gave.</p></li>&#xA;<li><p>Separate the management of the user's money into a ""Wallet"" that only exists in ""Order service"". Thanks to that, we don't need to retrieve user information when an order is performed, we just need to check the wallet information. ( I encoded the username in a JWT when the user performs an Authentication. Thus I can use in my wallet the username as a foreign key to recognize which wallet is being supplied, or used etc.)</p></li>&#xA;</ul>&#xA;&#xA;<p>As you can see, I don't use a message broker anymore because for now I don't need one. But I can split the mailing logic into a new microservice and then use a message broker to send mail through all my microservices if one needs to.</p>&#xA;&#xA;<p>Tell me if I'm wrong, But it sounds great to me.</p>&#xA;"
48691374,38071714,2500344,2018-02-08T17:26:42,"<p>See the article <a href=""https://labs.getninjas.com.br/sharing-data-in-a-microservices-architecture-using-graphql-97db59357602"" rel=""nofollow noreferrer"">here</a>, which says how and why approach #1 works better. Also look at the below image taken from the article I mentioned:&#xA;<a href=""https://i.stack.imgur.com/BrnFy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BrnFy.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>One of the main benefits of having everything behind a single endpoint is that data can be routed more effectively than if each request had its own service. While this is the often touted value of GraphQL, a reduction in complexity and service creep, the resultant data structure also allows data ownership to be extremely well defined, and clearly delineated.</p>&#xA;&#xA;<p>Another benefit of adopting GraphQL is the fact that you can fundamentally assert greater control over the data loading process. Because the process for data loaders goes into its own endpoint, you can either honor the request partially, fully, or with caveats, and thereby control in an extremely granular way how data is transferred.</p>&#xA;&#xA;<p>The following article explains these two benefits along with others very well: <a href=""https://nordicapis.com/7-unique-benefits-of-using-graphql-in-microservices/"" rel=""nofollow noreferrer"">https://nordicapis.com/7-unique-benefits-of-using-graphql-in-microservices/</a></p>&#xA;"
47469531,47469430,4478968,2017-11-24T08:52:18,"<p>What i have seen in my career is that there is a load balancer to distribuite the requests across the microservices. The load balancer knows the availability of the microservices invoking a status end point which returns an http status. If there is no response, or the response is negative, the load balancer excludes the microservice form the group.</p>&#xA;"
39955857,39936184,6782200,2016-10-10T10:04:59,"<p>I would like to clarify few point in this context.</p>&#xA;&#xA;<ol>&#xA;<li><p>Microservices does not mandate Polyglot technology, it just gives you the flexibility.</p></li>&#xA;<li><p>Frond end design should follow your application needs and if your functional &amp; non-functional requirements have a strong business case for using Polyglot technology for front-end you can. But microservices does not enforce anything.</p></li>&#xA;<li><p>Decision to choose ESI, BFF, SSI, Portlet etc. should be made purely based on your application and features fitment in to particular pattern.</p></li>&#xA;</ol>&#xA;"
39726613,39721025,6782200,2016-09-27T13:55:19,"<p>I think your approach towards microservices appeals to be misleading. I also understand your apprehensions towards microservices. </p>&#xA;&#xA;<p>But, decision to chose microservices strategy should not be directly dependent on the developer base. In deed it is highly dependent on the current and  future business needs of your organization. In fact if you do not anticipate any major growth or expansion of your IT services and its complexity around the systems, then you could stick to monolithic pattern. </p>&#xA;&#xA;<p>Irrespective of small/big enterprise, one key factor for microservices strategy is its growing number of services.</p>&#xA;"
39639381,39591223,6782200,2016-09-22T12:42:11,"<p>In simple terms Microservices is subset of SOA. You can refer to the following <a href=""http://martinfowler.com/articles/microservices.html"" rel=""nofollow"">9 characteristics of Microservice</a>, they still do follow SOA design principles:</p>&#xA;&#xA;<p>Componentization via Services</p>&#xA;&#xA;<p>Organized around Business Capabilities</p>&#xA;&#xA;<p>Products not Projects </p>&#xA;&#xA;<p>Smart endpoints and dumb pipes </p>&#xA;&#xA;<p>Decentralized Governance </p>&#xA;&#xA;<p>Decentralized Data Management</p>&#xA;&#xA;<p>Infrastructure Automation  </p>&#xA;&#xA;<p>Design for failure</p>&#xA;&#xA;<p>Evolutionary Design</p>&#xA;&#xA;<p>If you are referring SOA architecture implementing ESB, then using ESB does not necessarily makes you SOA compliant, unless you adhere to Service characteristics/service design principles of service during service design &amp; modelling. Hence lets decouple Services &amp; ESB. Fundamentally ESB is just an implementation of few non-functional elements of SOA.</p>&#xA;&#xA;<p>Choosing a monolith application is a fundamental phenomenon to start with Microservices. However I strongly believe Microservices can be taken to much more business scenarios.</p>&#xA;"
42393299,42393102,1226744,2017-02-22T13:48:21,"<p>You need to export the metrics to a central system.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Spring Boot provides a couple of implementations of a marker interface&#xA;  called <code>Exporter</code> which can be used to copy metric readings from the&#xA;  in-memory buffers to a place where they can be analyzed and displayed.</p>&#xA;</blockquote>&#xA;&#xA;<p>More specifically personally I like exporting metrics to statsD</p>&#xA;&#xA;<blockquote>&#xA;  <p>To export metrics to Statsd, make sure first that you have added&#xA;  <code>com.timgroup:java-statsd-client</code> as a dependency of your project&#xA;  (Spring Boot provides a dependency management for it). Then add a&#xA;  <code>spring.metrics.export.statsd.host</code> value to your <code>application.properties</code>&#xA;  file. Connections will be opened to port <code>8125</code> unless a&#xA;  <code>spring.metrics.export.statsd.port</code> override is provided. You can use&#xA;  <code>spring.metrics.export.statsd.prefix</code> if you want a custom prefix.</p>&#xA;</blockquote>&#xA;&#xA;<p>The information above is all from the Spring Boot documentation on metrics: <a href=""https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-metrics.html"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-metrics.html</a></p>&#xA;"
37604914,34020234,2865144,2016-06-03T02:16:55,"<p>The most common and simple approach is to communicate your microservices using a REST-ish approach, you will have to manually the communication and the discovery of services, which can become messy as the system grows. You can also search for some packages on npm for service discovery and remote procedure call. </p>&#xA;&#xA;<p>I strongly suggest the usage of <a href=""https://github.com/ericholiveira/studio"" rel=""nofollow"">Studio</a> which is a micro services framework. It also have a plugin to do automatic clusterization (service discovery + rpc) with really minimal configuration, this plugin also uses websocket for communication which is more light weight then rest-ish approach</p>&#xA;"
45453646,45453061,2541560,2017-08-02T06:59:14,"<p>A <a href=""https://martinfowler.com/articles/microservices.html"" rel=""noreferrer"">microservice</a> is a software architecture, which can be implemented with webservices.</p>&#xA;&#xA;<p>A <a href=""https://en.wikipedia.org/wiki/Web_service"" rel=""noreferrer"">webservice</a> is just a technology (one of many) for providing services over ""web"" or HTTP.</p>&#xA;"
51312439,51312350,3201784,2018-07-12T18:40:02,"<p>While you could, I'd be very careful on over-engineering your solution.  Using docker containers, as an example, to create microservices still has overhead for each container running.  I would recommend starting out with just a handful of microservices.  Ideal candidates are if you need to N-Scale one microservice, versus another.</p>&#xA;&#xA;<p>For example:<br>&#xA;Service-A has a high load so it needs to scale to 4 instances&#xA;Service-B has a low load so just needs to 2 instances</p>&#xA;&#xA;<p>Another great candidate is if you have a microservice that can be used across multiple projects.</p>&#xA;"
46746347,46744067,7866105,2017-10-14T15:44:33,"<p>InteliJ is able to have a lot of Spring Boot microserices/application running at the same time. It have no impact on performance, even if you run all microvervices in debug mode. I would choose first option. This is an approach, we are using in our poject:</p>&#xA;&#xA;<p>Start new project in IntelliJ. &#xA;For each microservice create new module in InteliJ.</p>&#xA;&#xA;<p>Benefits:  </p>&#xA;&#xA;<ul>&#xA;<li>If someone new want to work on that project, he can import one&#xA;project and have all microservices imported at once. Even if he would&#xA;work only in few of them, he can look how others are build.</li>&#xA;<li>in InteliJ you can create Run Configuration as ""Compaund"". So, when you want to run all your services at once, you can do it with just one click.</li>&#xA;</ul>&#xA;&#xA;<p>But, if you have 1500 employees, want go full netflix way and create 500 microservices, then better way will be to keep them separate :)</p>&#xA;"
34545332,34526251,2759417,2015-12-31T10:53:10,"<p>I had a huge experience working with containers and different container solutions, including Amazon ECS and Kubernetes, and I have found, that Kubernetes as one of the most useful solutions for managing containers in the different environments.</p>&#xA;&#xA;<p>The main benefit of Kubernetes - that it is a mature solution, originally developed by Google, but it is completely open-source! That means that anyone may look under the hood and (if necessary), modify and update the source code up to his purposes. </p>&#xA;&#xA;<p>Another huge benefit of Kubernetes - it is completely free. That mean that you may install it and run on your own infrastructure, without paying any additional costs for Kubernetes itself.</p>&#xA;&#xA;<p>You may run Kubernetes on the huge amount of <a href=""http://kubernetes.io/v1.1/docs/getting-started-guides/README.html"" rel=""noreferrer"">different providers</a>. It doesn't matter on what environment do you run the Kubernetes cluster - you should only take care of the Kubernetes cluster itself. That allows you to run, for example, the development cluster locally on Vagrant, as make the distributed production environment on the public cloud like AWS or GCE and the private cloud like OpenStack or simply using some libvirt solutions (using CoreOS for example). Again, from the point of view of Kubernetes - it doesn't matter what infrastructure solution do you use - the only one requirement for it - to be Kubernetes-enabled.</p>&#xA;&#xA;<p>Speaking about the Amazon ECS - that is a proprietary and vendor-locked solution. It also may give you the same performance as Kubernetes, but it won't give you the same flexibility.</p>&#xA;&#xA;<p>So, globally one may compare both Amazon ECS and Kubernetes, but Kubernetes is much more flexible and ready-to-customize solution.</p>&#xA;"
50402055,42918707,1189369,2018-05-18T00:07:48,"<p>I recommend you instead of Kerberos, that is not designed for distributed environments, you should use OAuth2 implementations, there is some SaaS providers, such as Okta.</p>&#xA;"
51269114,51259786,2512121,2018-07-10T15:28:57,<p>The purpose of protobuf is to be faster and smaller than JSON. If the system you send the information to can't work with them then there is no point bothering with protobuf in the first place.</p>&#xA;
40674105,40660163,1147385,2016-11-18T09:59:23,"<p>I've seen multi-national companies try to cooperate on a project (or be controlled from a central IT team) and it's a nightmare.  This response is highly subjective to what I've personally read and seen, so it's just my opinion, it's probably not everyone's opinion.  Generally broad questions aren't encouraged on Stack Overflow as they attract highly opinionated answers.</p>&#xA;&#xA;<p>I'd say DDD probably isn't the answer.  You'd need a large number of a developers to buy into the DDD idea.  If you don't have that buy-in then (unless you have a team of exceptionally self-motivated people) you'll see the developers try to build the new system on-top of the existing database.</p>&#xA;&#xA;<p>I'd also argue that <a href=""https://plainoldobjects.com/2016/03/01/thoughts-on-if-you-cant-build-a-well-structured-monolith-what-makes-you-think-microservices-is-the-answer/"" rel=""nofollow noreferrer"">microservices aren't the answer</a>.  Companies that have used microservices to their advantage are essentially using them to compartmentalise their code into small, stacks of individually running services/apps that each do a single job.  These microservices (<a href=""https://www.infoq.com/news/2015/12/microservices-spotify"" rel=""nofollow noreferrer"">from the success stories I've seen</a>) tend to be loosely coupled.  I imagine that if you have a large number of services that are highly coupled, then you've still got the spaghetti aspects of a monolith, but one that's <a href=""https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing"" rel=""nofollow noreferrer"">spread out over a network</a>.</p>&#xA;&#xA;<p>It sounds like you just need a well architected system, designed to your specific needs.  I agree that using DDD would be great, but is it a realistic goal across a multi-national project?</p>&#xA;"
51521360,51520654,1162233,2018-07-25T14:24:58,"<p>I think Saga pattern (Orchestration) enables an application to maintain data consistency across multiple services without using distributed transactions.</p>&#xA;&#xA;<p>This solution has the following drawbacks:</p>&#xA;&#xA;<p>The programming model is more complex. For example, a developer must design compensating transactions that explicitly undo changes made earlier in a saga.</p>&#xA;&#xA;<p>There are also the following issues to address:</p>&#xA;&#xA;<p>In order to be reliable, a service must atomically update its database and publish an event. It cannot use the traditional mechanism of a distributed transaction that spans the database and the message broker. Instead, it must use one of the patterns listed below.</p>&#xA;"
51425447,51425111,1162233,2018-07-19T14:37:31,"<p>What we did in <a href=""https://en.wikipedia.org/wiki/Careem"" rel=""nofollow noreferrer"">Careem</a>'s microservices system architecture, we have a dedicated service called <strong>Event</strong> which is on top of Amazon SQS. Any service can be a publisher or subscriber of this service. When any service publishes an event using our event publisher SDK API, the event is put into Amazon SQS.Then event processor SDK checks the subscriber list for this particular type of events which is stored in dynamoDB. For each subscriber, there is a separate queue (SQS) and the event processor take the event away from SQS and put it into each subscriber's queue. Then event consumer SDK peaks this event from queue and pushes to associate service. And this whole process is asynchronous meaning the publisher and subscriber services don't have to wait for any response or acknowledgements.</p>&#xA;&#xA;<p>Whether you want to consider it as an dedicated service o not, you will need event to process asynchronous tasks. </p>&#xA;"
51576241,51566509,1162233,2018-07-29T00:18:09,"<p>I believe both can be done and one is not better than the other. Either ways, you're making API call to M2 to get the <code>classification</code>. But this scenario is now telling you that the service boundary might be wrong - the vertical slice on the view to create a new player perhaps should be a single micro-service. Of course, you will add some fallback mechanism for the call to M2 so that in case of failure to M2, M1 doesn't fail completely (in this case, to create a new player).</p>&#xA;"
51152171,51150957,1162233,2018-07-03T10:22:08,"<blockquote>&#xA;  <p>knowing that users do not login on the website, there is no business&#xA;  transactions</p>&#xA;</blockquote>&#xA;&#xA;<p>You don't have to consider these for transitioning to microservices architecture from monolith. If you are sure that your system can be split/sliced into multiple services, the database is splittable and you really need to go for microservices in terms of your business need, you can consider. Once you decide and start iterating, you will get more clear pictures how to split the features and independent vertical slices on your application.</p>&#xA;&#xA;<p>For instance, if you have recommendation feature in your system, you can consider an independent recommendation service with separate storage system. If at any case, the recommendation service is down, with proper fallback mechanism, the whole system should  operate successfully without recommendation service (For example, it can show top X items to all users when recommendation service is not working).</p>&#xA;&#xA;<p>The short answer is - your system can be considered as a candidate for microservices architecture. But before jumping into, <a href=""http://blog.shippable.com/7-things-to-consider-while-moving-to-microservices"" rel=""nofollow noreferrer"">you should consider these things</a>.</p>&#xA;"
50836164,50835951,1162233,2018-06-13T11:32:04,"<p>Considering this is very broad question, I’d suggest these guidelines:</p>&#xA;&#xA;<ul>&#xA;<li><p>Don’t refactor everything all at once — it’s impossible to do it right.</p></li>&#xA;<li><p>Treat The Monolith as a black box with some APIs. They don’t necessarily have to be RESTful APIs — think of ways to interact with it.</p></li>&#xA;<li><p>When adding new features, create separate (micro)services with an API for each of them and have them interact with The Monolith’s APIs.</p></li>&#xA;<li><p>After some time you will see that the pieces of your Monolith are being accessed only through your new APIs. Even though they are still a part of the monolith code base. Move out capabilities vertically, decouple the core capability with its data and redirect all front-end applications to the new APIs.</p></li>&#xA;<li><p>Once you see bounded contexts bubble up, it might be convenient to chop them of The Monolith and have them working as separate services.</p></li>&#xA;<li><p>With microservices, you will need much more automation than before. Think in advance about Continuous Integration and Continuous Deployment (CI/CD), containers &amp; repository, central logging, and monitoring.</p></li>&#xA;</ul>&#xA;&#xA;<p>I would recommend to get some concise generalized idea before jumping into your specific problem. <a href=""https://martinfowler.com/articles/break-monolith-into-microservices.html"" rel=""nofollow noreferrer"">This</a> can be a good start.</p>&#xA;"
50836253,50835738,1162233,2018-06-13T11:36:30,"<p>There are many information missing here - e.g. what's the current architecture and technology stack of your website. Considering this is very broad question, I’d suggest these guidelines:</p>&#xA;&#xA;<ul>&#xA;<li><p>Don’t refactor everything all at once — it’s impossible to do it right.</p></li>&#xA;<li><p>Treat The Monolith as a black box with some APIs. They don’t necessarily have to be RESTful APIs — think of ways to interact with it.</p></li>&#xA;<li><p>When adding new features, create separate (micro)services with an API for each of them and have them interact with The Monolith’s APIs.</p></li>&#xA;<li><p>After some time you will see that the pieces of your Monolith are being accessed only through your new APIs. Even though they are still a part of the monolith code base. Move out capabilities vertically, decouple the core capability with its data and redirect all front-end applications to the new APIs.</p></li>&#xA;<li><p>Once you see bounded contexts bubble up, it might be convenient to chop them of The Monolith and have them working as separate services.</p></li>&#xA;<li><p>With microservices, you will need much more automation than before. Think in advance about Continuous Integration and Continuous Deployment (CI/CD), containers &amp; repository, central logging, and monitoring.</p></li>&#xA;</ul>&#xA;&#xA;<p>I would recommend to get some concise generalized idea before jumping into your specific problem. <a href=""https://martinfowler.com/articles/break-monolith-into-microservices.html"" rel=""nofollow noreferrer"">This</a> can be a good start.</p>&#xA;"
45400169,45400096,773113,2017-07-30T13:10:16,"<p>What you have is mostly correct, but you appear to be considering some things as requirements when they are not, and you are forgetting one very important characteristic that microservices are supposed to have.</p>&#xA;&#xA;<p>The main characteristics of microservices are <strong><em>statelessness</em></strong> and independence.  Whether they are ""WAR"" modules and whether they provide their services over ""HTTP"" (and certainly whether they are RESTful) are secondary concerns and you may hear arguments to the contrary.</p>&#xA;&#xA;<p>Statelessness means that no individual microservice may contain state.  (Except for caches.)  Microservices are supposed to always delegate the task of persisting data to some database module so they don't keep any state in memory.  The idea is that this way, if one microservice fails, (or if an entire machine containing many microservices fails,) you can just route incoming requests to another instance (or another machine) and everything will continue working.</p>&#xA;&#xA;<p>(Of course, if you want my opinion, it is just a cowardly acknowledgement of the fact that we don't know how to write reliable highly concurrent software, but the database guys are smart and they seem to have figured it all out, so we will just delegate the problem of maintaining our state to the software that they have written.)</p>&#xA;"
47025601,47020906,1236217,2017-10-30T23:12:24,<p>The correct answer is it depends on the project/product and needs.</p>&#xA;&#xA;<p>Microservices are arranged around business capabilities. This gives the service context which helps developers consume it. Generic services does not have this context.</p>&#xA;&#xA;<p>For example assume we have two tables one with customer information one with employee information(for simplicity sake). We are only reading from the tables.</p>&#xA;&#xA;<p>A microservice architecture will give you two services both with a read operation.</p>&#xA;&#xA;<p>Thus a developers would call a customer.listAll() if you will and a employee.listAll() for example. However the context is clear. </p>&#xA;&#xA;<p>Using a generic service might result in the following code:&#xA;service.listAll(customers). The parameter now carries the context. IN a system with a large amount of different types/tables this can become a major hasssles to call. A generic service would not have service.listCustomers() or service.listEmployees() that is specific and not generic.</p>&#xA;&#xA;<p>Remember an API that is not easy to understand is not easy to use and generic service while initially a bit easier to construct will eventually add complexity. Maintenance not development is the longest and most expensive part of software. </p>&#xA;&#xA;<p>This touches on another point. Having everything in a generic service means that if there is a bug in the customer service we have to stop the employee service too to fix it. The bug can also potentially affect both services. Microservices does not suffer from this as it allows you to take a service down without bringing down the application.</p>&#xA;&#xA;<p>One thing to keep in mind is that you could use a generic pattern to implement your microservice. In other words both the employee and customer service could use a DOA pattern in their operations. The code could be shared between them in some way maybe a library but they would be independent completely still.</p>&#xA;
36405941,35632607,4317945,2016-04-04T14:49:36,"<p><a href=""https://i.stack.imgur.com/TSFlc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TSFlc.png"" alt=""enter image description here""></a></p>&#xA;&#xA;<p>Yes, multipile PURGE requests return 200.</p>&#xA;"
47615684,38071714,9045498,2017-12-03T05:52:55,"<p>As microservice architecture does not have a proper definition, there is no specific model for this style but, most of them will come with few notable characteristics.In case of microservices architecture, each service can be broken down into individual small components, which can be individually tweaked and deployed without affecting application integrity. This means you can simply change a few services without going for application redeployment through custom <em><a href=""http://landrieturner.doodlekit.com/blog/entry/4238115/microservices-know-about-the-architecture-in-brief"" rel=""nofollow noreferrer"">microservices app development</a></em>.</p>&#xA;"
43609534,43609390,201722,2017-04-25T11:33:40,"<p>I would say yes. </p>&#xA;&#xA;<p>After all, it is typical for user credentials, authorization authentication to be centralized into a logical unit (mind you that this ""logical unit"" might be a single server or a distributed solution with a uniform interface.)</p>&#xA;&#xA;<p>This typically encompasses user profile management. Mind you that sometimes you also have a separate component or unit for that. And that can perfectly be a separate microservice.</p>&#xA;&#xA;<p>Why? Because managing a user profile is distinct from all other services, including authentication and authorization.</p>&#xA;&#xA;<p>Now, if this is a single physical deployment point, then that would be a bottleneck. But if this is a distributed service (that is, multiple instances of the same microservice, which is a topic for another discussion), then that concern is alleviated.</p>&#xA;&#xA;<p>Even in monolithic applications (when written well, that is), you typically see user authentication and management as their own isolated components separated from the rest.</p>&#xA;&#xA;<p>Hope this helps.</p>&#xA;"
43803983,42843777,3814138,2017-05-05T11:36:25,"<p>Consul DNS can provide a simple way for you to load balance services. It's especially powerful if you combine it with Consul Prepared Queries and health checks. </p>&#xA;&#xA;<p>Consul is best suited for monitoring services (via health checks) but you can use consul watch to trigger events if a service suddenly becomes unavailable.</p>&#xA;&#xA;<p>Hashicorp (the company behind Consul) offers another tool called Nomad.&#xA;Unlike Consul, Nomad is designed to run services (called jobs) and restart them if necessary. </p>&#xA;&#xA;<p>Nomad works best if you tell it where to find Consul. This enables automatic service registration for any task Nomad launches, including deregistering it if you instruct Nomad to stop running that task. Health checks are supported as well. </p>&#xA;"
34191246,34180115,3301370,2015-12-09T23:49:41,"<p>You need to register your service from the machines which run them. Consul is really designed in doing that. You need to use the agent rather than the HTTP API.</p>&#xA;&#xA;<p>By the way, if you still want to register that remote service from a Consul server directly, you will need to do extra work in order to detect on which IP it is running and submit a already correct json config.&#xA;Know that health checks don't work for remote service as those health checks are intended to be run locally.</p>&#xA;&#xA;<p>Check the answer of one Consul dev here: <a href=""https://groups.google.com/forum/#!msg/consul-tool/r4ZY97PE7BY/9QTRd3EsBgAJ"" rel=""nofollow"">https://groups.google.com/forum/#!msg/consul-tool/r4ZY97PE7BY/9QTRd3EsBgAJ</a></p>&#xA;&#xA;<blockquote>&#xA;  <p>The Catalog API is used by the agents to report the state of their&#xA;  services and checks to the Consul servers. It's very unusual to talk&#xA;  to the Catalog API directly, and Consul doesn't really support running&#xA;  checks without a Consul agent. If you run an agent on a node you can&#xA;  register services and checks for that node using the Agent API by&#xA;  doing a curl -X PUT with a body like this:</p>&#xA;  &#xA;  <p><a href=""https://www.consul.io/docs/agent/http/agent.html#agent_service_register"" rel=""nofollow"">https://www.consul.io/docs/agent/http/agent.html#agent_service_register</a></p>&#xA;  &#xA;  <p>It's also possible to define health checks in the agent's&#xA;  configuration files. The agent runs the checks and manages keeping the&#xA;  catalog up to date with the servers as the health status changes using&#xA;  an internal version of the Catalog API.</p>&#xA;  &#xA;  <p>If you called this Agent API on one of your Consul servers it's&#xA;  possible to have them run checks as well, but that's an unusual&#xA;  configuration to use if it's going to be checking the health of a&#xA;  service on another node. By running an agent on each node you get some&#xA;  basic ""node is reachable"" checking for free by Consul itself, and you&#xA;  keep the knowledge of the services and checks out at the edges where&#xA;  they are being run (the Catalog API lets the Consul servers answer&#xA;  questions across your whole fleet, but in Consul the agents are the&#xA;  source of truth and keep the catalog up to date to match what's going&#xA;  on).</p>&#xA;</blockquote>&#xA;"
26706637,26706240,27898,2014-11-03T01:43:48,"<p>They are looking not at the daemon process itself, but at managing that process and ensuring it's running properly. They cite instances of kludgey frameworks built around daemons where the daemon was not written with an eye towards that management, thus requiring an excess of resources to restart it, clean up after it, etc.</p>&#xA;&#xA;<p>They point out and recommend the use of system management facility software, including <code>smf</code> (Solaris), <code>upstart</code> (Linux), <code>launchd</code> (OSX), and even hoary old <code>init</code> and <code>ttys</code> (older Unix versions and BSD based distros). They don't mention <code>systemd</code> (also Linux), but that could be timing. They also don't mention <code>inetd</code> or <code>xinetd</code> that also make management and restart of network based daemon processes easy and simple.</p>&#xA;&#xA;<p>So they aren't really recommending not to daemonize processes; they're recommending that after you invent your nifty daemon service process, you <em>don't</em> reinvent a management framework around it. Develop your server with an awareness of how it will be managed, and that will likely make the total effort involved much less. In current terms, this is a <code>devops</code> attitude.</p>&#xA;"
45834606,38071714,8423162,2017-08-23T08:32:49,"<p>More about microservices, I think GraphQL could work perfect in serverless architecture too. I don't use GraphQL but I have <a href=""https://github.com/giapnguyen74/nextql"" rel=""nofollow noreferrer"">my own similar project</a>. I use it as  a <a href=""https://github.com/giapnguyen74/nextql-serverless"" rel=""nofollow noreferrer"">aggregator</a> which invoke and concentrates many functions into single result. I think you could apply same pattern for GraphQL.</p>&#xA;"
41829479,29303048,452090,2017-01-24T13:29:02,"<blockquote>&#xA;  <p>Mediation, Orchestration and Choreography of services is not limited&#xA;  to SOA but equally applies to Microservices architecture as well.</p>&#xA;</blockquote>&#xA;&#xA;<p>Meaning microservices can communicate with other microservices to pass on or to get some information.&#xA;For example a microservice needs to depend on stateful datastore in the stack of containers as well. So it needs to know the API/interface for the (RESTFull) database..</p>&#xA;&#xA;<blockquote>&#xA;  <p>HATEOAS primarily provides interfacing and communication design&#xA;  patterns so you are free from say fixed interface definition language&#xA;  like WSDL or Swagger.</p>&#xA;</blockquote>&#xA;&#xA;<p>While it is true that REST (HTTP) have become most famous that it sometimes taken as granted that microservice will serve as a REST API but this is not true.</p>&#xA;&#xA;<blockquote>&#xA;  <p>The beauty of microservices is that they do not restrict you to one or&#xA;  two communication patterns, its independent. There is no standard.</p>&#xA;</blockquote>&#xA;&#xA;<p>For example reactive microservices emphasize to use the message driven communication pattern and become location transparent of each other. But this does not mean not knowing about the verbs &amp; payloads of other microservices to pass on or to retrieve.</p>&#xA;&#xA;<p>Similarly we can have HATEOAS based communication patterns built into our microservices architecture in order to be fully flexible for ever changing/upgrading microservices interfaces. But still in general you need to know the location of the microservice to communicate to.&#xA;Therefore the service-discovery and the service registry patterns exist in the microservices wold; and they equally applied to HATEOAS architecture.&#xA;Where our microservices container can live and die (scale out &amp; down) depending on the load; our customers constantly need to know the active location of the microservice(s) to be consumed.</p>&#xA;"
41838562,41830058,452090,2017-01-24T20:38:23,"<p>Its OK but not a good design as you can see the dependency is pretty hard-coded.</p>&#xA;&#xA;<p>You have a choice to use messaging where you put the request message in one queue and response message in another queue.</p>&#xA;&#xA;<p>In this way your communication will be asynchronous, loosely coupled, and none of the service would need to know very specifically about each other and their location/address.&#xA;This is also the way to communicate as recommended by Reactive Manifesto.</p>&#xA;"
49640072,49637598,8381946,2018-04-03T22:36:42,"<p>As the error message (i.e. <code>AMQ119031: Unable to validate user</code>) indicates, you have not provided the proper credentials when creating your JMS connection.</p>&#xA;&#xA;<p>The username &amp; password information you've provided in the properties for your JNDI lookup apply <strong>only</strong> for the JNDI lookup (i.e. <em>not</em> for the JMS connection). JNDI and JMS are 100% independent of each other.</p>&#xA;&#xA;<p>You must configure the appropriate Spring component with your JMS username and password so it can be used when it invokes <a href=""https://docs.oracle.com/javaee/7/api/javax/jms/ConnectionFactory.html#createConnection-java.lang.String-java.lang.String-"" rel=""nofollow noreferrer"">javax.jms.ConnectionFactory.createConnection(String,String)</a> or <a href=""https://docs.oracle.com/javaee/7/api/javax/jms/ConnectionFactory.html#createContext-java.lang.String-java.lang.String-"" rel=""nofollow noreferrer"">javax.jms.ConnectionFactory.createContext(String,String)</a>. Try returning an instance of <a href=""https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/jms/connection/UserCredentialsConnectionFactoryAdapter.html"" rel=""nofollow noreferrer""><code>UserCredentialsConnectionFactoryAdapter</code></a> from your <code>getConnectionFactory()</code> method.</p>&#xA;"
44813617,44810758,72443,2017-06-28T22:38:04,"<p>If your goal is to improve security I would advice against building your own user login system. However, I think it is a good idea to separate the authentication/authorization from the profiles.</p>&#xA;"
40026427,39956391,4340987,2016-10-13T16:31:27,"<p>Ok, You just need to create new module, then set it in settings.gradle:</p>&#xA;&#xA;<pre><code>include 'service-client'&#xA;</code></pre>&#xA;&#xA;<p>And then in build.gradle of submodule where you need, add this dependency: </p>&#xA;&#xA;<pre><code>compile project(':service-client')&#xA;</code></pre>&#xA;"
49980131,49980008,174843,2018-04-23T11:34:19,"<p>Yes you can run multiple containers on a single host; docker is designed for exactly that.</p>&#xA;&#xA;<p>You cannot map two containers of different images to the same port number; you get an error response if you try. However, if your containers run the same image (e.g.2 instances of a webapp) you could run them as a service, and have them exposed on the same port. Docker will load-balance the requests. You can read more about <a href=""https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/"" rel=""nofollow noreferrer"">services here</a> or follow the <a href=""https://docs.docker.com/get-started/part3/"" rel=""nofollow noreferrer"">Get Started (Part 3, services) here</a></p>&#xA;&#xA;<p>Yes, the containers on a single host can communicate with each other, by container name. For example if you have one container running MongoDB called <em>mongo</em>, and another one running Node.js called webserver, the webserver container can connect to the database by using the name <em>mongo</em> e.g. <code>db.Connect(""mongodb://mongo:27017/testdb"")</code>.</p>&#xA;"
27140353,26491425,1224443,2014-11-26T02:27:24,"<p>To understand rules around port binding and self-contained apps, it's helpful to view things from the perspective of the platforms designed to run 12-factor apps, like <a href=""http://heroku.com/"" rel=""noreferrer"">Heroku</a> or <a href=""http://deis.io/overview/"" rel=""noreferrer"">Deis</a>.</p>&#xA;&#xA;<p>These platforms are scaling applications at the process level.  When processes are scaled up, the platform tries to place these additional workers behind the routing mesh so they can start serving traffic.  If the app is not self-contained and, for example, is tightly coupled to a front-end Apache server using <code>mod_jk</code> -- it is not possible to scale by running more isolated worker processes.</p>&#xA;&#xA;<p>Port binding exists to solve the problem of ""port brokering"" at the platform level.  If every application worker listened on port 80 there would be conflicts.  To solve this, port binding is a convention whereby the application listens on a port the platform has allocated -- and which is passed in as a <code>$PORT</code> environment variable.  This ensures a) the application worker listens on the right port and b) the platform knows where to route traffic destined for that worker.</p>&#xA;"
41655295,40564979,2797307,2017-01-14T21:40:35,"<p>I had the same problem and solved it by using the DependencyResolver.GetService() method on the HttpConfiguration object in the Startup() class. Create a new Service Fabric Stateless/Statefull WebAPI project. In the Startup() class, add the following code:</p>&#xA;&#xA;<pre><code> public static class Startup&#xA;{&#xA;    // This code configures Web API. The Startup class is specified as a type&#xA;    // parameter in the WebApp.Start method.&#xA;    public static void ConfigureApp(IAppBuilder appBuilder)&#xA;    {&#xA;        // Configure Web API for self-host.&#xA;        HttpConfiguration config = new HttpConfiguration();&#xA;        // Allow custom routes in controller attributes.&#xA;        config.MapHttpAttributeRoutes();&#xA;        config.Routes.MapHttpRoute(&#xA;            name: ""Default"",&#xA;            routeTemplate: ""{controller}/{action}/{id}"",&#xA;            defaults: new { controller = ""API"", action = ""HealthCheck"", id = RouteParameter.Optional }&#xA;        );&#xA;        //inject controllers here&#xA;        config.DependencyResolver.GetService(typeof({{YourWebAPIRootNamespace}}.Controllers.APIController));&#xA;&#xA;        appBuilder.UseWebApi(config);&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>This allows you to deploy your existing APIs into Service Fabric without having to migrate the entire code base to a new project. Don't forget to update the app.config in the new project with all applicable settings from your web.config.</p>&#xA;&#xA;<p>Full blog post here <a href=""http://thenameisirrelevant.com/hosting-an-existing-webapi-in-service-fabric"" rel=""nofollow noreferrer"">http://thenameisirrelevant.com/hosting-an-existing-webapi-in-service-fabric</a></p>&#xA;"
50984414,36701111,1620475,2018-06-22T09:04:12,<p>One option is sending a request to bill microservice using it's registred name on the eureka registry.</p>&#xA;
45324640,45311236,1620475,2017-07-26T10:43:21,"<p>You can make all your microservices registred to the same registry and then they can call each other.</p>&#xA;&#xA;<p><strong>UPDATE</strong> :  Here is how I make it works.&#xA;In the microservice consuming the data one, use RestTemplate with the current user jwt token Authorization in header  to make the api calls :</p>&#xA;&#xA;<pre><code>@Component&#xA;public class AuthenticateClientHttpRequestInterceptor implements ClientHttpRequestInterceptor {&#xA;&#xA;    @Override&#xA;    public ClientHttpResponse intercept(HttpRequest httpRequest, byte[] bytes, ClientHttpRequestExecution clientHttpRequestExecution) throws IOException {&#xA;        String token = SecurityUtils.getCurrentUserJWT();&#xA;        httpRequest.getHeaders().add(""Authorization"",""Bearer ""+token);&#xA;        return clientHttpRequestExecution.execute( httpRequest, bytes );&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>My custom restTemplate using ClientHttpRequestInterceptor for adding token in header.</p>&#xA;&#xA;<pre><code>@Configuration&#xA;public class CustomBean {&#xA;    @Autowired&#xA;    AuthenticateClientHttpRequestInterceptor interceptor;&#xA;    @Bean&#xA;    @LoadBalanced&#xA;    public RestTemplate restTemplate() {&#xA;        RestTemplate restTemplate = new RestTemplate();&#xA;        restTemplate.setInterceptors(Collections.singletonList(interceptor));&#xA;        return restTemplate;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>And in the resource controller where your are making the call for data:</p>&#xA;&#xA;<pre><code>@RestController&#xA;@RequestMapping(""/api"")&#xA;public class DataResource {    &#xA;    @Autowired&#xA;    RestTemplate restTemplate;&#xA;&#xA;            @PostMapping(""/hello"")&#xA;            @Timed&#xA;            public ResponseEntity&lt;Hello&gt; createHello(@RequestBody Hello Hello) throws URISyntaxException {&#xA;&#xA;    //The name your data micro service registrated in the Jhipster Registry&#xA;                String dataServiceName = ""data_micro_service"";&#xA;&#xA;                URI uri = UriComponentsBuilder.fromUriString(""//"" + dataServiceName + ""/api/datas"")&#xA;                    .build()&#xA;                    .toUri();&#xA;&#xA;                //call the data microservice apis&#xA;                List&lt;Data&gt; result = restTemplate.getForObject(uri, Data[].class);&#xA;&#xA;&#xA;            return ResponseEntity.created(new URI(""/api/hellos/"" + result.getId()))&#xA;                    .headers(HeaderUtil.createEntityCreationAlert(ENTITY_NAME, result.getId().toString()))&#xA;                    .body(result);&#xA;&#xA;        }&#xA;&#xA;}&#xA;</code></pre>&#xA;"
42680067,42667203,2277627,2017-03-08T19:18:53,"<p>The Google Cloud SDK on which the new Cloud Tools for Eclipse depends does not support the EAR format. For the moment, you'll need to create individual Eclipse projects for each separate service, and deploy them individually.</p>&#xA;&#xA;<p>We do need to beef up our multi-module support. Though that's not going to happen tomorrow, let me see what I can do about moving it up the stack. </p>&#xA;"
25924002,25812816,1569531,2014-09-18T23:23:37,"<p>I am assuming this is not a question for your development environment but for your deployments. The answer depends on your deployment strategy. In the past we have handled deployments where the drop wizard application is bundled as a java process that can be started and the pid being recorded and forcefully kill the process. Or bundle the java process in an upstart/init script to gracefully start and shutdown the system.</p>&#xA;&#xA;<p>On the other hand when you start a dropwizard application what it eventually does is start a jetty server. <a href=""http://eureka.ykyuen.info/2010/07/26/jetty-stop-a-jetty-server-by-command"" rel=""nofollow"">http://eureka.ykyuen.info/2010/07/26/jetty-stop-a-jetty-server-by-command</a>. This can maybe shed some light on how you can pass the stop port as arguments when you start the dropwizard application.</p>&#xA;"
37653306,37632735,789756,2016-06-06T08:59:09,<p>Obviously the cache manager is not serializable as it should not be serialized - it's not data. You have to track the field that references the <code>SpringEmbeddedCacheManagerFactoryBean</code> and make that transient.</p>&#xA;
39371422,39367722,1513980,2016-09-07T13:36:25,<p>The Problem was that I wired OAuth2RestTemplate instead of interface OAuth2RestOperations.</p>&#xA;&#xA;<p>Wireing OAuth2RestOperations works for me.</p>&#xA;
47590473,45225151,6037500,2017-12-01T09:38:47,<p>I noticed It is happening because of &#xA;<strong><em>zuul.routes.service1.stripPrefix: false</em></strong>&#xA;after removing this property application started working as desired. </p>&#xA;
39921535,39920488,1067260,2016-10-07T16:04:19,"<p>You are right. This is how Netflix uses Falcor and what the Falcor router is designed for.</p>&#xA;&#xA;<p>From the <a href=""http://netflix.github.io/falcor/documentation/router.html"" rel=""nofollow"">documentation</a>:</p>&#xA;&#xA;<blockquote>&#xA;  <p><strong>The Router is appropriate as an abstraction over a service layer</strong> or REST API. Using a Router over these types of APIs provides just enough flexibility to avoid client round-trips without introducing heavy-weight abstractions. Service-oriented architectures are common in systems that are designed for scalability. These systems typically store data in different data sources and expose them through a variety of different services. <strong>For example, Netflix uses a Router in front of its Microservice architecture.</strong></p>&#xA;  &#xA;  <p>It is rarely ideal to use a Router to directly access a single SQL Database. Applications that use a single SQL store often attempt to build one SQL Query for every server request. <strong>Routers work by splitting up requests for different sections of the JSON Graph into separate handlers and sending individual requests to services to retrieve the requested data</strong>. As a consequence, individual Router handlers rarely have sufficient context to produce a single optimized SQL query. We are currently exploring different options for supporting this type of data access pattern with Falcor in future.</p>&#xA;</blockquote>&#xA;"
38079238,35368503,1067260,2016-06-28T14:39:04,"<p>There is no real difference between Falcor and classic HTTP APIs on this point.</p>&#xA;&#xA;<ul>&#xA;<li>You may split your routes into multiple files. One route per file for example.</li>&#xA;<li>You may split your model into sub-models and host a router for each sub-model on different machines, and have the client requests each of those models separately.</li>&#xA;<li>You may do the latter and then add a ""super-router"" that routes requests to the sub-models so that the clients only talks to one model.</li>&#xA;</ul>&#xA;"
41965161,37405221,283388,2017-01-31T18:43:17,"<p>I guess by this time you will have your question answered, but anyway.&#xA;Rest communication of Microservices is synchronous, and lead to the Orchestration pattern. What you are looking for is the Choreography pattern, which usually is achieved with messaging (eg - AMQP) and event sourcing.&#xA;There are a number of examples, and you can (as always) rely on <a href=""https://martinfowler.com/articles/microservices.html#SynchronousCallsConsideredHarmful%20Fowler%20Examples"" rel=""nofollow noreferrer"">Fowler excellent description</a>.&#xA;Keep in mind that going ahed on the choreograpy is taking Eureka out of the equation. Netflix components do not provide an easy solution or implementation for this pattern.</p>&#xA;"
38992379,38989659,613628,2016-08-17T09:07:44,"<p>This is the culprit</p>&#xA;&#xA;<pre><code>&lt;dependency&gt;&#xA;    &lt;groupId&gt;javax.servlet&lt;/groupId&gt;&#xA;    &lt;artifactId&gt;servlet-api&lt;/artifactId&gt;&#xA;    &lt;version&gt;2.5&lt;/version&gt;&#xA;&lt;/dependency&gt;&#xA;</code></pre>&#xA;&#xA;<p>First, don't put <code>version</code>with Spring Boot, it is taking care to bring the right version for the things it manage. Then you're forcing an incompatible (old) servlet API, leading to that exception.</p>&#xA;"
51589821,51541318,549372,2018-07-30T08:26:05,"<p>the basic idea to require two separate services appears questionable in this case,&#xA;because <code>1</code> author may have written <code>N</code> books, just as well as <code>1</code> book can have <code>N</code> authors. and also, the idea of having to use the own database appears questionable - because there are libraries alike <a href=""https://www.npmjs.com/package/node-isbn"" rel=""nofollow noreferrer"">node-isbn</a>, which one can wrap up into a service, which one can query for author, title and the ISBN number.</p>&#xA;"
45776534,45776238,551932,2017-08-19T21:41:09,<p>How do I organize my projects?</p>&#xA;&#xA;<pre><code>|-- github.com/avelino/service1&#xA;   |-- Dockerfile&#xA;   |-- main.go&#xA;|-- github.com/avelino/service2&#xA;   |-- Dockerfile&#xA;   |-- main.go&#xA;|-- github.com/avelino/service3&#xA;   |-- Dockerfile&#xA;   |-- main.go&#xA;</code></pre>&#xA;&#xA;<h3>Packages</h3>&#xA;&#xA;<p>All Go code is organized into packages. A package in Go is simply a directory/folder with one or more .go files inside of it. Go packages provide isolation and organization of code similar to how directories/folders organize files on a computer.</p>&#xA;&#xA;<p>All Go code lives in a package and a package is the entry point to access Go code. Understanding and establishing good practices around packages is important to write effective Go code.</p>&#xA;
44972440,39485459,3501052,2017-07-07T13:47:21,"<p>There is no best practice. It's going to depend on how versioning should work in your specific case.</p>&#xA;&#xA;<p>Shawn Wildermuth discussed about api versioning in a <a href=""https://wildermuth.com/2013/06/19/Designing_APIs_for_the_Web"" rel=""nofollow noreferrer"">pluralsight video</a>. Not sure about your microservice implementation details, but if you are using rest api, you may try the versioning the actual payload</p>&#xA;&#xA;<blockquote>&#xA;  <p>You could also do this with an Accept header. Accept allows for you to&#xA;  annotate the Accept header with a version type against the API you&#xA;  want to look at. And you can also do this with content type as well.&#xA;  The application/vnd for vendor specifies the version of the returning&#xA;  data,  so that when the application then sends it, it knows what&#xA;  version of the payload it is. And this is an important technique in&#xA;  versioning, because you have to version both the API, the actual URI&#xA;  calls, as well as the shape of the data that's coming back.</p>&#xA;</blockquote>&#xA;&#xA;<p>If you find a mechanism that works well in your environment and with your customers and users, go ahead and use that.</p>&#xA;"
51299006,51280734,1153276,2018-07-12T06:51:51,"<p>It worked for me after removing cloud.aws.region.static=ap-south-1a from application.properties and pom.xml properties. </p>&#xA;&#xA;<p>But at the same time if I remove this property from my local machine project setting it fails with following error.</p>&#xA;&#xA;<pre><code>org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'amazonS3': Invocation of init method failed; nested exception is java.lang.IllegalStateException: There is no EC2 meta data available, because the application is not running in the EC2 environment. Region detection is only possible if the application is running on a EC2 instance&#xA;</code></pre>&#xA;"
45652827,45625886,200445,2017-08-12T17:02:48,"<p>The promise of REST has always been a <a href=""https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm#sec_5_1_5"" rel=""nofollow noreferrer"">uniform interface</a>. An ideal REST client would be able to talk to a wide range of RESTful resources, even those that did not exist when the client was coded.</p>&#xA;&#xA;<p>Unfortunately, this ideal has never really materialized except for REST’s original case — the World Wide Web of human-readable documents.</p>&#xA;&#xA;<p>At this point, most interfaces that call themselves “RESTful” are really a baroque kind of RPC, where request and response data is smeared over methods, query strings, headers, status codes, payloads, all in a variety of fragile formats.</p>&#xA;&#xA;<p>Most of the uniformity in today’s “RESTful” interfaces is in the heads of developers. They “know” that <code>POST /orders/</code> is probably going to add a new order. But they still have to program their clients to “know” that, for every API they talk to, often making lots of errors.</p>&#xA;&#xA;<p>Still, there is some uniformity that can actually be useful in code. For example, if you have a “RESTful” API, then you can often add a transparent, finely tunable caching layer to it nearly for free. This is possible because (semantically correct) HTTP messages already carry all the standardized information needed for caching: request method, URL, status code, <code>Cache-Control</code>, <code>Vary</code> and all that. In gRPC, you have to roll your own caching.</p>&#xA;&#xA;<p>But the real reason for the current dominance of “REST” is not this sort of minor affordances. It’s really just the success of the World Wide Web. At some point in history, it transpired that everyone <em>already had</em> a performant, flexible HTTP server (to serve their Web site) and a solid HTTP client (to view said site), so when people started adding machine-readable resources, it was just easier and cheaper to stick to the same HTTP ways. They used HTTP methods and headers and status codes because that’s what their Web servers already understood and logged. Tools like PHP allowed them to do this with zero deployment overhead over their regular Web sites.</p>&#xA;&#xA;<p>If uniformity and alignment with the World Wide Web are not important for you, then RPC is a tried and true architectural choice, and gRPC is a solid implementation that can save you some trouble, as ɐuıɥɔɐɯ explains.</p>&#xA;"
27727931,27723156,1189885,2015-01-01T02:39:16,"<p>Yes, but you should go one step further and decouple the message representation from the database representation. Define an API artifact that contains just plain DTOs for the vocabulary objects in each service API, and implement the message-driven POJOs in reference to these DTOs, using whatever backend objects are relevant. (If you're using Spring Integration, you can just register a Spring converter to map back and forth automatically.) </p>&#xA;"
39389726,39388560,3748349,2016-09-08T11:28:16,<p>The issuer could be issuing tokens to different applications and those applications could have different permissions. Not checking the audience would allow an attacker to use a token issued for application A at application B and may lead to permission elevation.</p>&#xA;&#xA;<p>To your suggestion: the claims may indeed differ per Client.</p>&#xA;
27838615,27838280,3748349,2015-01-08T11:06:38,"<p>As you state this is an internal use case where you control the resources protected by your Resource Server, the Authorization Server and the clients accessing the resources so you may choose to use <code>client_credentials</code> for accessing user data.</p>&#xA;&#xA;<p>It compares to (or actually is) a ""service account"" that may be used to manipulate user data. If that service is under your control and you trust it i.e. you trust that it does not leak it's credentials to other parties and does not abuse its powers it is OK. Since you also control the client it would not be a problem.</p>&#xA;"
39870808,39839881,3748349,2016-10-05T09:52:59,"<p>It depends on who is controlling the web application, Service A and Service B. If they're all run by the same party there's no problem in passing the token on since it stays within the same security domain.</p>&#xA;&#xA;<p>But if e.g. Service B is run by a 3rd party then things become problematic as the administrator of Service B can pickup the access token and call Service A as if it were your web application, potentially getting access to resources that it should not have access to.</p>&#xA;&#xA;<p>You'll also note that if Service A and Service B are owned by 2 different parties, other than you, your web application should also obtain two different access tokens respectively for calling Service A and Service B to prevent the same security issue.</p>&#xA;&#xA;<p>So the answer really is: it depends on who is controlling what i.e. if the token is crossing an administrative/security domain.</p>&#xA;"
33187727,33179127,3748349,2015-10-17T14:30:45,"<p>There's an RFC that is almost done that specifies and standardizes the protocol that a Resource Server can use for verification of access tokens against an Authorization Server, see: <a href=""https://tools.ietf.org/html/draft-richer-oauth-introspection"" rel=""nofollow"">https://tools.ietf.org/html/draft-richer-oauth-introspection</a>. That would be an alternative to using self-contained JWT access tokens.</p>&#xA;"
50338690,50334652,1127677,2018-05-14T20:41:07,"<p>Perhaps you need to define how your microservices are supposed to work first. If you were to, for example, expose the ATG <code>Profile</code> as a microservice, it won't, by itself, run in another environment, it simply means that you can expose the functionality for consumption by a different system via the service. Alternatively you can expose a <code>Profile</code> module on a different system and try to consume it within ATG. That too is possible.</p>&#xA;&#xA;<p>In a nutshell you can integrate various open source libraries into your ATG stack to build and expose the functionality of the <em>monolithic</em> application into microservices. To get started, read up about <code>webmvc</code>, <code>oxm</code>, <code>hateoas</code>, <code>plugin-core</code>, <code>springtonucleus</code> and perhaps <code>dozer</code>.</p>&#xA;&#xA;<p>Perhaps you need to define your architecture first before asking a much more specific question here. The real answer is just too long.</p>&#xA;"
47331230,47330878,7225085,2017-11-16T13:42:10,"<p>What you want to do is prevent API2 from accepting outside traffic, there are a number of ways you can do this for example:-</p>&#xA;&#xA;<ol>&#xA;<li>White-list a set of IP addresses that correspond to the possible IP addresses of API1</li>&#xA;<li>Put all your APIs in a VPN and only expose API1 to public traffic</li>&#xA;<li>create some internal auth strategy such as have API1 sign requests to API2 with some secret key.</li>&#xA;</ol>&#xA;"
48482824,48482639,622589,2018-01-28T03:14:05,"<p>I assume you are in the context of <a href=""http://microservices.io/patterns/data/event-sourcing.html"" rel=""nofollow noreferrer"">Event Sourcing and microservices</a>? If so I recommend that you don't publish a CreateArticleEvent to the event store, and instead directly create the article in the database and then publish the ArticleCreatedEvent to the Event store. </p>&#xA;&#xA;<p>Why you ask? Generally this pattern is created to orchestrate different microservices. In the example show in the link above, it was used to orchestrate how the Customer service should react when an Order is created. Note the <strong>past</strong> tense. The Order Service created the order, and Customer Service reacts to it.</p>&#xA;&#xA;<p>In your case it is easier (and probably better) to just insert the order into the database (by calling the ArticleService directly) and responding with the article ID. Then just publish the ArctileCreatedEvent to your event store, to trigger other microservices that may want to listen to it (like, for example, trigger a notification to the editor for review). </p>&#xA;&#xA;<p>Event Sourcing is a good pattern, but we don't need to apply it to everything.</p>&#xA;"
50310260,50289914,3572733,2018-05-12T20:06:09,"<p>Not sure whether i got your question correct. Lets say there are three cases in your payment service eg: success, failed, pending, and there are specific states defined in order service. And we don't want to share anything between these two microservices in terms of data states. Would suggest you to publish event to queue for any given payment state. And make order service to listen this event, and have conditional logic to update order status here. This way we can achieve loosely coupled services.</p>&#xA;"
50310268,50289509,3572733,2018-05-12T20:07:52,"<p>Take a look at logs for any kind of exception/error, also check mongod shell. You might not have data in mongodb for given criteria if you don't see any error/exception in logs.</p>&#xA;"
50313593,50312750,3572733,2018-05-13T07:03:19,"<p>Do not make these two service tightly coupled, you might face latency if 2nd service has dependency on any other service. What if 2nd service is not up. So try to make use of messaging queue here. Have your first service to publish interview event which can be in status(new, canceled etc. along with other details interview id, interviewers etc). Make 2nd service to listen to it. As you said interview request goes to multiple interviewers.Configure a reminder job/scheduler in 2nd service, and have configuration for that in your data store with details eg: interviewers id, interview id, time range etc so that you can send reminders to interviewers easily. This should help you. Let me know if i have not got your question correct.&#xA;Microservices use change event to get data consistency in given system. </p>&#xA;"
46011852,35409492,1042429,2017-09-02T08:17:35,"<p>Wish accessing Eureka from legacy spring ( non-boot ) is also made simple like @EnableEureka and @EnableFeignClient</p>&#xA;&#xA;<p>This is the closest I could get it working . This example is available in Eureka-examples in Git Hub</p>&#xA;&#xA;<pre><code>public class EurekaConfiguration {&#xA;&#xA;    private static ApplicationInfoManager applicationInfoManager;&#xA;    private static EurekaClient eurekaClient;&#xA;&#xA;    private static synchronized ApplicationInfoManager initializeApplicationInfoManager(&#xA;            EurekaInstanceConfig instanceConfig) {&#xA;        if (applicationInfoManager == null) {&#xA;            InstanceInfo instanceInfo = new EurekaConfigBasedInstanceInfoProvider(instanceConfig).get();&#xA;            applicationInfoManager = new ApplicationInfoManager(instanceConfig, instanceInfo);&#xA;        }&#xA;&#xA;        return applicationInfoManager;&#xA;    }&#xA;&#xA;    private static synchronized EurekaClient initializeEurekaClient(ApplicationInfoManager applicationInfoManager,&#xA;            EurekaClientConfig clientConfig) {&#xA;        if (eurekaClient == null) {&#xA;            eurekaClient = new DiscoveryClient(applicationInfoManager, clientConfig);&#xA;        }&#xA;&#xA;        return eurekaClient;&#xA;    }&#xA;&#xA;    public static EurekaClient getEurekaClient()&#xA;    {&#xA;        ApplicationInfoManager applicationInfoManager = initializeApplicationInfoManager(new MyDataCenterInstanceConfig());&#xA;        EurekaClient client = initializeEurekaClient(applicationInfoManager, new DefaultEurekaClientConfig());&#xA;        return eurekaClient;&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>My client</p>&#xA;&#xA;<pre><code>String vipAddress = ""NLPService"";&#xA;&#xA;        InstanceInfo nextServerInfo = null;&#xA;        try {&#xA;            nextServerInfo = EurekaConfiguration.getEurekaClient().getNextServerFromEureka(vipAddress, false);&#xA;        } catch (Exception e) {&#xA;            System.err.println(""Cannot get an instance of example service to talk to from eureka"");&#xA;            System.exit(-1);&#xA;        }&#xA;&#xA;        System.out.println(""Found an instance of example service to talk to from eureka: ""&#xA;                + nextServerInfo.getVIPAddress() + "":"" + nextServerInfo.getPort());&#xA;&#xA;        String serviceBaseURL = ""http://""+ nextServerInfo.getHostName()&#xA;        +"":""+nextServerInfo.getPort();&#xA;&#xA;&#xA;        String nlpServiceURL = serviceBaseURL +""/nlp"";&#xA;&#xA;        RestTemplate restTemplate = new RestTemplate();&#xA;&#xA;        NLPInputToBeTransformed input = new NLPInputToBeTransformed();&#xA;        input.setInputText("" Test Input "");&#xA;&#xA;&#xA;        NLPResponse nlpResponse = restTemplate.postForObject&#xA;                (nlpServiceURL, input, NLPResponse.class, new HashMap&lt;&gt;());&#xA;&#xA;        System.out.println( "" Service Response  "" + nlpResponse.getTags()); &#xA;</code></pre>&#xA;"
38901924,38889466,258813,2016-08-11T16:41:00,"<p>As @luboskrnac suggests - for the security you could simply extract your common logic into a separate JAR.</p>&#xA;&#xA;<p>Regarding your shared application context (I assume you are just referring to the shared URL space, rather than sharing any particular state across the apps etc), then yes - you should use something like <code>Zuul</code> - this can act as a singular interface between external and all your microservices (which, under the hood would all be running in their own unique application context namespace/port number - but <code>Zuul</code> can group those together and expose them with nice URLs on a consistent location).</p>&#xA;&#xA;<p>Luckily the whole <a href=""http://cloud.spring.io/spring-cloud-netflix/spring-cloud-netflix.html"" rel=""nofollow"">Netflix microservice</a> stack is well supported by Spring, so its reasonably straight forward to get up and running with <code>Zuul</code> and <code>Eureka</code> (the discovery service).  </p>&#xA;&#xA;<p>I have a <code>hello-world</code> setup of the stack written up here (along with the code): <a href=""http://automateddeveloper.blogspot.co.uk/2015/09/spring-boot-netflix-oss-adventure-into.html"" rel=""nofollow"">http://automateddeveloper.blogspot.co.uk/2015/09/spring-boot-netflix-oss-adventure-into.html</a></p>&#xA;"
39458754,39450504,1112317,2016-09-12T21:03:43,"<p>I can't speak directly to Serverless framework support, but this is certainly possible in API Gateway.</p>&#xA;&#xA;<p>You can maintain multiple Swagger files for each ""sub API"", and use import?mode=merge to import both definitions into the same API.</p>&#xA;&#xA;<p>See &#xA;<a href=""http://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-import-api.html"" rel=""nofollow"">http://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-import-api.html</a></p>&#xA;&#xA;<p>Thanks,&#xA;Ryan</p>&#xA;"
51992929,51991404,7147006,2018-08-23T19:23:04,"<p>The module <code>config-server</code> has the maven plugin <code>git-commit-id-plugin</code> defined in it's <code>pom.xml</code>. I don't know exactly what it does but I assume by the name it tries to perform some git commands.</p>&#xA;&#xA;<p>Edit: Your project folder does not seem to contain a <code>.git</code> folder, which means that it's not a git repository, which causes the maven plugin to fail.</p>&#xA;&#xA;<p>Clone the linked repo with <code>git clone https://github.com/sivaprasadreddy/spring-boot-microservices-series.git</code></p>&#xA;&#xA;<p>Or initialize a new repo in the project folder using <code>git init</code>.</p>&#xA;"
51304582,51304242,1192702,2018-07-12T11:32:53,"<p>As said in <a href=""https://docs.spring.io/spring/docs/3.2.x/javadoc-api/org/springframework/web/bind/annotation/RequestParam.html"" rel=""nofollow noreferrer"">javadocs</a>, <code>@RequestParam Map&lt;String, String&gt; limitFilters</code> tells spring to map any request params to this <code>Map</code>:</p>&#xA;&#xA;<blockquote>&#xA;  <p>If the method parameter is <code>Map&lt;String, String&gt;</code> or&#xA;  <code>MultiValueMap&lt;String, String&gt;</code> and a parameter name is not specified,&#xA;  then the map parameter is populated with all request parameter names&#xA;  and values.</p>&#xA;</blockquote>&#xA;&#xA;<p>So imagine you would have <code>id</code> param in your filter. It will mess up your controller. So the easiest way would be to change paths</p>&#xA;"
49038372,47456945,2182398,2018-02-28T20:55:07,<p>Zuul will redirect your request to A service HTTP XXXX port /api/v1/input. This microservice as a producer will put message to kafka channel. After kafka consumer will get message and store or analyze. Another microservice can read from database and return response after frontend request or do push using Server Sent Events or Vertx message bus....</p>&#xA;
49201041,49176544,2182398,2018-03-09T19:41:46,"<p>Fortunately problem solved by configuration, no any bug in Spring boot and Spring cloud. Many thanks to Josh Long ! &#xA;Here you can find full project with config.&#xA;<a href=""https://github.com/armdev/reactive-spring-cloud/blob/master/google/src/main/resources/application.properties"" rel=""nofollow noreferrer"">https://github.com/armdev/reactive-spring-cloud/blob/master/google/src/main/resources/application.properties</a></p>&#xA;&#xA;<pre><code>server.port=8082&#xA;spring.application.name=turbine&#xA;management.endpoint.health.enabled=true&#xA;management.endpoints.jmx.exposure.include=*&#xA;management.endpoints.web.exposure.include=*&#xA;management.endpoints.web.base-path=/actuator&#xA;management.endpoints.web.cors.allowed-origins=true&#xA;management.endpoint.health.show-details=always&#xA;eureka.client.serviceUrl.defaultZone=${EUREKA_URI:http://localhost:8761/eureka}&#xA;eureka.instance.lease-expiration-duration-in-seconds=5&#xA;eureka.instance.lease-renewal-interval-in-seconds=5&#xA;turbine.aggregator.cluster-config=default&#xA;turbine.app-config=google&#xA;turbine.cluster-name-expression= new String(""default"")&#xA;turbine.combine-host-port=true&#xA;turbine.instanceUrlSuffix.default: actuator/hystrix.stream&#xA;</code></pre>&#xA;"
44240716,44082917,2182398,2017-05-29T10:53:00,"<p>in the config file configure threads part</p>&#xA;&#xA;<pre><code> maxThreads: 20480&#xA;  minThreads: 50&#xA;</code></pre>&#xA;&#xA;<p><a href=""https://github.com/armdev/tosptube/blob/master/tosp-auth/src/main/resources/config.yml"" rel=""nofollow noreferrer"">https://github.com/armdev/tosptube/blob/master/tosp-auth/src/main/resources/config.yml</a></p>&#xA;"
35473353,31605799,225022,2016-02-18T05:14:11,"<blockquote>&#xA;  <p>what I should use for the auth service. Do I want an OAuth2 + OpenID connect service? Perhaps something much simpler? Should I be using something off the shelf?</p>&#xA;</blockquote>&#xA;&#xA;<ul>&#xA;<li><p>+1 for OAuth2, it is a great authentication and authorization standard for the architecture you want to build. Mainly because it centralizes and isolates the security into a separate service that can be shared between many clients and services</p></li>&#xA;<li><p>The simplest form is to use a SaaS based <a href=""https://auth0.com/"" rel=""nofollow"">OAuth2</a> service, something like Auth0 has virtually 0 starting costs to use, but it requires internet access/exposure, and you mentioned your apps are internal, so it might not be a great fit.</p></li>&#xA;<li><p>Next simplest form is an OAuth2 server that you install and configure without having to develop one, which I'm pretty sure plenty are available (paid &amp; free)</p>&#xA;&#xA;<ul>&#xA;<li>For example, there is <a href=""https://identityserver.github.io/"" rel=""nofollow"">https://identityserver.github.io/</a> that I would use myself (because my main stack is Microsoft .net)</li>&#xA;</ul></li>&#xA;</ul>&#xA;"
37921274,37915326,225022,2016-06-20T11:23:15,"<blockquote>&#xA;  <p>each microservice need its own database</p>&#xA;</blockquote>&#xA;&#xA;<p>A separate DB per microservice is not a prerequisite (nor a requirement, really). </p>&#xA;&#xA;<p>You can have as many microservices as you want working on top of the same database, but use different schemas for example. </p>&#xA;&#xA;<p>The bounded context of a microservice should be the boundary.</p>&#xA;&#xA;<blockquote>&#xA;  <p>Lets say we have very high load on this service, so we choose to scale out 20x.</p>&#xA;</blockquote>&#xA;&#xA;<p>Scaling to (X) instances of the same microservice does not mean necessarily having a separate database per each instance of that same service.</p>&#xA;&#xA;<p>Most databases are designed with concurrent connections, users, transactions in mind. a single database instance (with some optimistic concurrency) can handle hundreds (if not thousands) of concurrent connections gracefully.</p>&#xA;&#xA;<p>If you explicitly chose to have a separate DB per instance of the same service, then you will have to sync those databases up. and, most likely, data consistency will suffer for it.</p>&#xA;&#xA;<p>Here are some suggestions:</p>&#xA;&#xA;<ul>&#xA;<li><p>use a single database per microservice (not per instance) no matter how many instances are using it. And only consider a DB per instance when you're sure a single DB cannot handle the load.</p></li>&#xA;<li><p>Use a shared cache layer on top of the DB (maybe redis cache)</p></li>&#xA;<li><p>Use a database cluster to deal with high load/availability of databases.</p></li>&#xA;</ul>&#xA;"
37857015,37830008,225022,2016-06-16T10:48:50,"<p>Another useful pattern beside the Aggregator that Tom mentioned above is <a href=""https://msdn.microsoft.com/en-us/library/jj591569.aspx"">a saga pattern</a> (a mini workflow). &#xA;I've used it before with messaging library called <a href=""http://docs.particular.net/nservicebus/sagas/"">NServiceBus</a> to handle coordinating multiple messages that are correlated to each other.</p>&#xA;&#xA;<p>the pattern is very useful and fits nicely for long-running processes. even if your correlated messages are different messages, like OrderStarted, OrderLineProcessed, OrderCompleted.</p>&#xA;"
39337900,39323769,225022,2016-09-05T21:29:57,"<p>You have some other options: instead of reinventing the wheel, you can use an OAuth 2.0 identity provider/broker out of the box.</p>&#xA;&#xA;<p>Keycloak is a free open source identity provider and broker that provides all these functionalities.</p>&#xA;&#xA;<p>Auth0 is a managed commercial service that provides all these functionalities too.</p>&#xA;&#xA;<p>And I'm sure there are many other services exist for OAuth 2.0 protocol.</p>&#xA;&#xA;<p>As for your 2 options:</p>&#xA;&#xA;<p>You can extend the JWT with your custom authorisation claims (roles or permissions) and since the JWT will be signed, then it is safe to assume the data is correct by your service. Give that the JWT is a short lived token that need to renew every few minutes.</p>&#xA;"
50499259,50455134,225022,2018-05-24T00:30:57,"<p>The answer to your question is very subjective and will be dependant on how much architecture &amp; design effort have been put upfront into the overall solution/system and how much governance controls are in place for architectural changes to the system around integrations, deployments, etc.</p>&#xA;&#xA;<p>But from experience, I can (almost) absolutely guarantee that architectures evolve over time and different solutions will emerge out of unforeseen and/or unplanned functional and technical requirements. No architecture will stay intact in the face of agile, evolving requirements.</p>&#xA;&#xA;<p>The other factor to take into consideration as well, is that tools, patterns, and techniques change very quickly in our industry. So, the longer it takes for a project to complete, the more exposure it has to a change in these areas. That's another fact (in my opinion)</p>&#xA;&#xA;<p>What does not change, however, is the motivation behind those architectural decisions. The Why?&#xA;Architecture governance needs to capture those motivations and ensure that they are fulfilled with whatever evolving tool, pattern, or technique used in the long term.</p>&#xA;&#xA;<p>For my own projects, I ensure that a big factor of any architecture decisions we make is driven by a balanced view of business drivers vs developer experience (DX) to ensure clarity, productivity, mastery, stability, quality, etc.</p>&#xA;"
42407670,42404800,225022,2017-02-23T05:21:49,"<p>let me try to answer your 2 questions directly without discussing either approaches.</p>&#xA;&#xA;<h3>How will this be implemented?</h3>&#xA;&#xA;<p>Modular Web Apps are possible if you use an MV* pattern. AngularJS and ReactJS both support building modular Apps and load modules on demand via webpack or requireJs.</p>&#xA;&#xA;<p>Since your UI is composed of fragments coming from different microservices, you might need a reverse proxy and some CDN or caching to speed up page load by curating client requests to a single host (domain).</p>&#xA;&#xA;<h3>what if I want to serve Android and iOS apps?</h3>&#xA;&#xA;<p>Native Apps will require an API layer and you can curate your microservices into a single API gateway, maybe even develop a mediator API between mobile apps and actual microservices to expose only a simple subset of your actual backend.</p>&#xA;"
36795717,36775802,225022,2016-04-22T13:54:09,"<p>Your problem here is not ""sharing huge data"", but rather the boundaries you choose to separate your micro services based on.</p>&#xA;&#xA;<p>I can tell from your requirements that the 3 micro services you chose to separate (Reviews, Validations, Import/Export) are actually operating on the same context and business domain .. which is Reviews.</p>&#xA;&#xA;<p>I would encourage you to reconsider your design decision and consider Reviews, as a single micro service, that handles all reviews operations and logic as a black box.</p>&#xA;"
35401130,35381163,225022,2016-02-15T03:39:03,"<p>This sounds familiar to a problem I was solving recently</p>&#xA;&#xA;<p>Assuming your services are HTTP based, then I would recommend you check out <a href=""http://oauth.net/2/"" rel=""nofollow noreferrer"">oAuth 2.0</a></p>&#xA;&#xA;<h2>A short copy from <a href=""http://tools.ietf.org/html/rfc6749"" rel=""nofollow noreferrer"">RFC 6749</a></h2>&#xA;&#xA;<blockquote>&#xA;  <p>OAuth addresses these issues by introducing an authorization layer&#xA;     and separating the role of the client from that of the resource&#xA;     owner.  In OAuth, the client requests access to resources controlled&#xA;     by the resource owner and hosted by the resource server, and is&#xA;     issued a different set of credentials than those of the resource&#xA;     owner.</p>&#xA;  &#xA;  <p>Instead of using the resource owner's credentials to access protected&#xA;     resources, the client obtains an access token -- a string denoting a&#xA;     specific scope, lifetime, and other access attributes.  Access tokens&#xA;     are issued to third-party clients by an authorization server with the&#xA;     approval of the resource owner.  The client uses the access token to&#xA;     access the protected resources hosted by the resource server.</p>&#xA;  &#xA;  <p>For example, an end-user (resource owner) can grant a printing&#xA;     service (client) access to her protected photos stored at a photo-&#xA;     sharing service (resource server), without sharing her username and&#xA;     password with the printing service.  Instead, she authenticates&#xA;     directly with a server trusted by the photo-sharing service&#xA;     (authorization server), which issues the printing service delegation-&#xA;     specific credentials (access token).</p>&#xA;</blockquote>&#xA;&#xA;<p>It simply models the authentication and authorization into a workflow between</p>&#xA;&#xA;<h1>A User</h1>&#xA;&#xA;<ul>&#xA;<li>Owns some data, hence it is also called <em>Resource Owner</em></li>&#xA;<li>Has credential(s)</li>&#xA;</ul>&#xA;&#xA;<h1>Authorization Server</h1>&#xA;&#xA;<ul>&#xA;<li>Owns and Controls the User Identity, Credentials, and Claims</li>&#xA;<li>Controls granting &amp; denying access to User's resources (<em>not really required in this scenario</em>)</li>&#xA;<li>Exchanges a user's credentials for an access_token that a Client can then use to access information from a Resource Provider</li>&#xA;<li><em>Optionally</em> grants a refresh_token that can be used to renew an expired access_token</li>&#xA;</ul>&#xA;&#xA;<h1>Resource Provider(s)</h1>&#xA;&#xA;<ul>&#xA;<li>Service that has information</li>&#xA;<li>Trusts the Authorization Server</li>&#xA;<li>Verify access_token is valid (has not expired, signed correctly, etc.)</li>&#xA;<li>Verify required claims are present (user, roles, etc)</li>&#xA;<li>And Release information to a requesting Client</li>&#xA;</ul>&#xA;&#xA;<h1>Client(s)</h1>&#xA;&#xA;<ul>&#xA;<li>An Application (internal or 3rd party)</li>&#xA;<li>Authenticates the user via the known authorization server</li>&#xA;<li>Obtains an access_token</li>&#xA;<li>Uses the access_token to call resource provider(s) to obtain information</li>&#xA;</ul>&#xA;&#xA;<h1>Claims Identity</h1>&#xA;&#xA;<p>A Claims Identity (<a href=""https://stackoverflow.com/questions/6786887/explain-claims-based-authentication-to-a-5-year-old"">explained better in more details here</a>) is not just a username &amp; password, it can carry many claims such as an email, a date of birth, etc. for an authenticated user, and you can use those claims to communicate any common user properties to your various services.</p>&#xA;&#xA;<h1>Shared Attributes</h1>&#xA;&#xA;<p>Now, your last questions was about linking a user (or an identity) to an entity  in each service that represents some unique information in that service's context... this can be achieved by linking an existing authenticated identity and access_token to an internal representation of the user in each service.</p>&#xA;&#xA;<p><strong>Something like:</strong></p>&#xA;&#xA;<ul>&#xA;<li>A Seller Is a User</li>&#xA;<li>A Buyer Is a User</li>&#xA;<li>A User has (Claims, access_token)</li>&#xA;<li>A Claim is a key value pair</li>&#xA;<li>A Claim can be (name, email, role, ... etc)</li>&#xA;</ul>&#xA;"
39129498,39126827,2729470,2016-08-24T17:23:42,"<p>I'm no expert but that flow looks okay to me if you are required to use different applications for this.</p>&#xA;&#xA;<p>Regarding your second question, yes this can be middleware</p>&#xA;"
39858845,32534401,1092350,2016-10-04T17:58:06,"<p>With the evolution of standards such as RAML and Swagger to define the REST APIs, the difference between SOAP and REST web services is narrowing. If the focus is on exposing APIs as the earlier answerer mentioned, REST is a better choice as there are plenty of lightweight API management products that support this.</p>&#xA;"
36199883,36197572,1496062,2016-03-24T12:10:41,"<p>Microservices are not meant to be tightly coupled , RMI requires your code on both end  , which was fun when you didnt control the other side eg clients who do not want to upgrade and it was a B!@*! to get through firewalls.</p>&#xA;&#xA;<p>Soap solved most of those things you mention unfortunately Java never had a good Soap stack. That said Rest has other advantages especially when accessing the service from a web page and javascript.</p>&#xA;"
47490868,26706240,590335,2017-11-25T21:53:49,"<p>Basically the intention is to remove the service management logic from the container and have it as part of the infrastructure</p>&#xA;&#xA;<p>consider the scenario of a service that crashes - instead of restarting the service within the image, which may not help in cases where the server is loaded or malfunctioning, the infrastructure may decide to restart it on a different server</p>&#xA;"
52125266,52125150,5255935,2018-09-01T05:52:07,"<p>Consider doing:</p>&#xA;&#xA;<ol>&#xA;<li><p>On the payment page, you should re-check if the product is still available. This can be a simple HTTP GET.</p></li>&#xA;<li><p>If the GET call is slow for you, consider caching recent product added by user to some in-memory databases (eg. REDIS). Now if first users successfully processes the payment, decrease counter for that product-id in redis. And before proceeding payment for second user, check the counter of that product-id in redis.</p></li>&#xA;</ol>&#xA;&#xA;<p>(BONUS: Redis offers atomic operations, so you can successfully handle the race condition in ordering the product as well.)</p>&#xA;"
41903749,41893566,221951,2017-01-27T22:02:25,"<p>I don't think that there is anything to worry about using AOP. This is how Spring does things usually under the hood.</p>&#xA;&#xA;<p>I'd just be careful with logging method invocations, especially parameters which is the common use (abuse) of AOP. </p>&#xA;&#xA;<p>Ususally the less code you write the better. If you can use standard, proven way of doing things - use it.</p>&#xA;"
49609010,49606124,9224959,2018-04-02T09:52:20,"<p>I would recommend to have one Neo4j instance per microservice. Then, every microservice owns their own database.</p>&#xA;"
